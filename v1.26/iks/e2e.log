I0327 20:49:44.617326      20 e2e.go:126] Starting e2e run "d74a9d93-502c-4e4a-9eef-4f4e0d8780d7" on Ginkgo node 1
Mar 27 20:49:44.633: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1679950184 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 27 20:49:44.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:49:44.885: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0327 20:49:44.891983      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 27 20:49:44.937: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 27 20:49:45.065: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 27 20:49:45.065: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
Mar 27 20:49:45.065: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Mar 27 20:49:45.097: INFO: e2e test version: v1.26.3
Mar 27 20:49:45.103: INFO: kube-apiserver version: v1.26.3+IKS
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Mar 27 20:49:45.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:49:45.114: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.234 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 27 20:49:44.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:49:44.885: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0327 20:49:44.891983      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 27 20:49:44.937: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 27 20:49:45.065: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 27 20:49:45.065: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
    Mar 27 20:49:45.065: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
    Mar 27 20:49:45.097: INFO: e2e test version: v1.26.3
    Mar 27 20:49:45.103: INFO: kube-apiserver version: v1.26.3+IKS
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Mar 27 20:49:45.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:49:45.114: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:49:45.147
Mar 27 20:49:45.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 20:49:45.148
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:45.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:45.209
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/27/23 20:49:45.221
Mar 27 20:49:45.253: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 20:49:50.269: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 20:49:50.269
STEP: getting scale subresource 03/27/23 20:49:50.269
STEP: updating a scale subresource 03/27/23 20:49:50.284
STEP: verifying the replicaset Spec.Replicas was modified 03/27/23 20:49:50.303
STEP: Patch a scale subresource 03/27/23 20:49:50.318
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 20:49:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-292" for this suite. 03/27/23 20:49:50.439
------------------------------
• [SLOW TEST] [5.315 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:49:45.147
    Mar 27 20:49:45.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 20:49:45.148
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:45.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:45.209
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/27/23 20:49:45.221
    Mar 27 20:49:45.253: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 20:49:50.269: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 20:49:50.269
    STEP: getting scale subresource 03/27/23 20:49:50.269
    STEP: updating a scale subresource 03/27/23 20:49:50.284
    STEP: verifying the replicaset Spec.Replicas was modified 03/27/23 20:49:50.303
    STEP: Patch a scale subresource 03/27/23 20:49:50.318
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:49:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-292" for this suite. 03/27/23 20:49:50.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:49:50.468
Mar 27 20:49:50.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 20:49:50.471
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:50.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:50.52
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 20:49:50.53
Mar 27 20:49:50.551: INFO: Waiting up to 5m0s for pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350" in namespace "emptydir-5296" to be "Succeeded or Failed"
Mar 27 20:49:50.570: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Pending", Reason="", readiness=false. Elapsed: 18.434717ms
Mar 27 20:49:52.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03438361s
Mar 27 20:49:54.585: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Running", Reason="", readiness=false. Elapsed: 4.03326744s
Mar 27 20:49:56.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03447967s
STEP: Saw pod success 03/27/23 20:49:56.586
Mar 27 20:49:56.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350" satisfied condition "Succeeded or Failed"
Mar 27 20:49:56.601: INFO: Trying to get logs from node 10.176.99.177 pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 container test-container: <nil>
STEP: delete the pod 03/27/23 20:49:56.662
Mar 27 20:49:56.713: INFO: Waiting for pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 to disappear
Mar 27 20:49:56.747: INFO: Pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 20:49:56.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5296" for this suite. 03/27/23 20:49:56.768
------------------------------
• [SLOW TEST] [6.359 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:49:50.468
    Mar 27 20:49:50.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 20:49:50.471
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:50.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:50.52
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 20:49:50.53
    Mar 27 20:49:50.551: INFO: Waiting up to 5m0s for pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350" in namespace "emptydir-5296" to be "Succeeded or Failed"
    Mar 27 20:49:50.570: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Pending", Reason="", readiness=false. Elapsed: 18.434717ms
    Mar 27 20:49:52.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03438361s
    Mar 27 20:49:54.585: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Running", Reason="", readiness=false. Elapsed: 4.03326744s
    Mar 27 20:49:56.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03447967s
    STEP: Saw pod success 03/27/23 20:49:56.586
    Mar 27 20:49:56.586: INFO: Pod "pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350" satisfied condition "Succeeded or Failed"
    Mar 27 20:49:56.601: INFO: Trying to get logs from node 10.176.99.177 pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 container test-container: <nil>
    STEP: delete the pod 03/27/23 20:49:56.662
    Mar 27 20:49:56.713: INFO: Waiting for pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 to disappear
    Mar 27 20:49:56.747: INFO: Pod pod-fd2e2a79-6b5b-4f10-9e6d-a08b92f2b350 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:49:56.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5296" for this suite. 03/27/23 20:49:56.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:49:56.834
Mar 27 20:49:56.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename proxy 03/27/23 20:49:56.835
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:56.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:56.881
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 27 20:49:56.892: INFO: Creating pod...
Mar 27 20:49:56.914: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6609" to be "running"
Mar 27 20:49:56.928: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.23314ms
Mar 27 20:49:58.944: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.030234435s
Mar 27 20:49:58.944: INFO: Pod "agnhost" satisfied condition "running"
Mar 27 20:49:58.944: INFO: Creating service...
Mar 27 20:49:58.965: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/DELETE
Mar 27 20:49:59.000: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 20:49:59.000: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/GET
Mar 27 20:49:59.019: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 27 20:49:59.019: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/HEAD
Mar 27 20:49:59.053: INFO: http.Client request:HEAD | StatusCode:200
Mar 27 20:49:59.053: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 27 20:49:59.071: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 20:49:59.071: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/PATCH
Mar 27 20:49:59.090: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 20:49:59.090: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/POST
Mar 27 20:49:59.108: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 20:49:59.108: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/PUT
Mar 27 20:49:59.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 20:49:59.143: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/DELETE
Mar 27 20:49:59.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 20:49:59.164: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/GET
Mar 27 20:49:59.185: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 27 20:49:59.185: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/HEAD
Mar 27 20:49:59.218: INFO: http.Client request:HEAD | StatusCode:200
Mar 27 20:49:59.218: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/OPTIONS
Mar 27 20:49:59.245: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 20:49:59.245: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/PATCH
Mar 27 20:49:59.266: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 20:49:59.266: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/POST
Mar 27 20:49:59.287: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 20:49:59.287: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/PUT
Mar 27 20:49:59.308: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 27 20:49:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6609" for this suite. 03/27/23 20:49:59.333
------------------------------
• [2.527 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:49:56.834
    Mar 27 20:49:56.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename proxy 03/27/23 20:49:56.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:56.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:56.881
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 27 20:49:56.892: INFO: Creating pod...
    Mar 27 20:49:56.914: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6609" to be "running"
    Mar 27 20:49:56.928: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.23314ms
    Mar 27 20:49:58.944: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.030234435s
    Mar 27 20:49:58.944: INFO: Pod "agnhost" satisfied condition "running"
    Mar 27 20:49:58.944: INFO: Creating service...
    Mar 27 20:49:58.965: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/DELETE
    Mar 27 20:49:59.000: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 20:49:59.000: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/GET
    Mar 27 20:49:59.019: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 27 20:49:59.019: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/HEAD
    Mar 27 20:49:59.053: INFO: http.Client request:HEAD | StatusCode:200
    Mar 27 20:49:59.053: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 27 20:49:59.071: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 20:49:59.071: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/PATCH
    Mar 27 20:49:59.090: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 20:49:59.090: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/POST
    Mar 27 20:49:59.108: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 20:49:59.108: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/pods/agnhost/proxy/some/path/with/PUT
    Mar 27 20:49:59.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 20:49:59.143: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/DELETE
    Mar 27 20:49:59.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 20:49:59.164: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/GET
    Mar 27 20:49:59.185: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 27 20:49:59.185: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/HEAD
    Mar 27 20:49:59.218: INFO: http.Client request:HEAD | StatusCode:200
    Mar 27 20:49:59.218: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/OPTIONS
    Mar 27 20:49:59.245: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 20:49:59.245: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/PATCH
    Mar 27 20:49:59.266: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 20:49:59.266: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/POST
    Mar 27 20:49:59.287: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 20:49:59.287: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6609/services/test-service/proxy/some/path/with/PUT
    Mar 27 20:49:59.308: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:49:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6609" for this suite. 03/27/23 20:49:59.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:49:59.373
Mar 27 20:49:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 20:49:59.374
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:59.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:59.424
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-66c8620a-65bf-41d8-8ce4-ec4b8f318835 03/27/23 20:49:59.438
STEP: Creating a pod to test consume configMaps 03/27/23 20:49:59.457
Mar 27 20:49:59.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953" in namespace "configmap-8756" to be "Succeeded or Failed"
Mar 27 20:49:59.513: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Pending", Reason="", readiness=false. Elapsed: 33.698412ms
Mar 27 20:50:01.529: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050106268s
Mar 27 20:50:03.531: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051748382s
STEP: Saw pod success 03/27/23 20:50:03.532
Mar 27 20:50:03.532: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953" satisfied condition "Succeeded or Failed"
Mar 27 20:50:03.547: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 20:50:03.577
Mar 27 20:50:03.623: INFO: Waiting for pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 to disappear
Mar 27 20:50:03.637: INFO: Pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 20:50:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8756" for this suite. 03/27/23 20:50:03.658
------------------------------
• [4.328 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:49:59.373
    Mar 27 20:49:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 20:49:59.374
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:49:59.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:49:59.424
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-66c8620a-65bf-41d8-8ce4-ec4b8f318835 03/27/23 20:49:59.438
    STEP: Creating a pod to test consume configMaps 03/27/23 20:49:59.457
    Mar 27 20:49:59.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953" in namespace "configmap-8756" to be "Succeeded or Failed"
    Mar 27 20:49:59.513: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Pending", Reason="", readiness=false. Elapsed: 33.698412ms
    Mar 27 20:50:01.529: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050106268s
    Mar 27 20:50:03.531: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051748382s
    STEP: Saw pod success 03/27/23 20:50:03.532
    Mar 27 20:50:03.532: INFO: Pod "pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953" satisfied condition "Succeeded or Failed"
    Mar 27 20:50:03.547: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 20:50:03.577
    Mar 27 20:50:03.623: INFO: Waiting for pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 to disappear
    Mar 27 20:50:03.637: INFO: Pod pod-configmaps-4116784f-b973-4b9c-bff1-cb276036e953 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:50:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8756" for this suite. 03/27/23 20:50:03.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:50:03.702
Mar 27 20:50:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename ephemeral-containers-test 03/27/23 20:50:03.703
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:03.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:03.752
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/27/23 20:50:03.763
Mar 27 20:50:03.786: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9368" to be "running and ready"
Mar 27 20:50:03.800: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.956543ms
Mar 27 20:50:03.800: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:50:05.815: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029077077s
Mar 27 20:50:05.815: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:50:07.817: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030913411s
Mar 27 20:50:07.817: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 27 20:50:07.817: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/27/23 20:50:07.832
Mar 27 20:50:07.856: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9368" to be "container debugger running"
Mar 27 20:50:07.870: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.60109ms
Mar 27 20:50:09.886: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029800511s
Mar 27 20:50:09.886: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/27/23 20:50:09.886
Mar 27 20:50:09.886: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9368 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 20:50:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:50:09.887: INFO: ExecWithOptions: Clientset creation
Mar 27 20:50:09.887: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-9368/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 27 20:50:10.042: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:50:10.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-9368" for this suite. 03/27/23 20:50:10.095
------------------------------
• [SLOW TEST] [6.419 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:50:03.702
    Mar 27 20:50:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/27/23 20:50:03.703
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:03.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:03.752
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/27/23 20:50:03.763
    Mar 27 20:50:03.786: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9368" to be "running and ready"
    Mar 27 20:50:03.800: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.956543ms
    Mar 27 20:50:03.800: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:50:05.815: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029077077s
    Mar 27 20:50:05.815: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:50:07.817: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.030913411s
    Mar 27 20:50:07.817: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 27 20:50:07.817: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/27/23 20:50:07.832
    Mar 27 20:50:07.856: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9368" to be "container debugger running"
    Mar 27 20:50:07.870: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.60109ms
    Mar 27 20:50:09.886: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029800511s
    Mar 27 20:50:09.886: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/27/23 20:50:09.886
    Mar 27 20:50:09.886: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9368 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 20:50:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:50:09.887: INFO: ExecWithOptions: Clientset creation
    Mar 27 20:50:09.887: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-9368/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 27 20:50:10.042: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:50:10.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-9368" for this suite. 03/27/23 20:50:10.095
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:50:10.121
Mar 27 20:50:10.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 20:50:10.123
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.175
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-eccaf91b-ac39-47a3-ba59-3685a0af8540 03/27/23 20:50:10.186
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 20:50:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-892" for this suite. 03/27/23 20:50:10.214
------------------------------
• [0.118 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:50:10.121
    Mar 27 20:50:10.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 20:50:10.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.175
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-eccaf91b-ac39-47a3-ba59-3685a0af8540 03/27/23 20:50:10.186
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:50:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-892" for this suite. 03/27/23 20:50:10.214
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:50:10.241
Mar 27 20:50:10.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 20:50:10.243
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.302
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 03/27/23 20:50:10.313
Mar 27 20:50:10.328: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/27/23 20:50:10.328
Mar 27 20:50:10.346: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/27/23 20:50:10.346
Mar 27 20:50:10.378: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:50:10.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6557" for this suite. 03/27/23 20:50:10.399
------------------------------
• [0.183 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:50:10.241
    Mar 27 20:50:10.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 20:50:10.243
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.302
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 03/27/23 20:50:10.313
    Mar 27 20:50:10.328: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/27/23 20:50:10.328
    Mar 27 20:50:10.346: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/27/23 20:50:10.346
    Mar 27 20:50:10.378: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:50:10.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6557" for this suite. 03/27/23 20:50:10.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:50:10.428
Mar 27 20:50:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-runtime 03/27/23 20:50:10.43
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.502
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/27/23 20:50:10.566
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/27/23 20:50:27.907
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/27/23 20:50:27.923
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/27/23 20:50:27.955
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/27/23 20:50:27.956
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/27/23 20:50:28.046
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/27/23 20:50:31.108
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/27/23 20:50:33.154
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/27/23 20:50:33.182
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/27/23 20:50:33.182
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/27/23 20:50:33.251
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/27/23 20:50:34.282
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/27/23 20:50:37.378
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/27/23 20:50:37.407
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/27/23 20:50:37.407
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 27 20:50:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9870" for this suite. 03/27/23 20:50:37.534
------------------------------
• [SLOW TEST] [27.131 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:50:10.428
    Mar 27 20:50:10.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-runtime 03/27/23 20:50:10.43
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:10.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:10.502
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/27/23 20:50:10.566
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/27/23 20:50:27.907
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/27/23 20:50:27.923
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/27/23 20:50:27.955
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/27/23 20:50:27.956
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/27/23 20:50:28.046
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/27/23 20:50:31.108
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/27/23 20:50:33.154
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/27/23 20:50:33.182
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/27/23 20:50:33.182
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/27/23 20:50:33.251
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/27/23 20:50:34.282
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/27/23 20:50:37.378
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/27/23 20:50:37.407
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/27/23 20:50:37.407
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:50:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9870" for this suite. 03/27/23 20:50:37.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:50:37.564
Mar 27 20:50:37.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption 03/27/23 20:50:37.566
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:37.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:37.614
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 27 20:50:37.662: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 20:51:37.780: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 03/27/23 20:51:37.794
Mar 27 20:51:37.901: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 27 20:51:37.928: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 27 20:51:37.997: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 27 20:51:38.015: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 27 20:51:38.057: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 27 20:51:38.081: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/27/23 20:51:38.081
Mar 27 20:51:38.081: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:38.119: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 38.453191ms
Mar 27 20:51:40.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056268551s
Mar 27 20:51:42.139: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.058173909s
Mar 27 20:51:42.139: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 27 20:51:42.139: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:42.155: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.25905ms
Mar 27 20:51:42.155: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 20:51:42.155: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:42.169: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.522428ms
Mar 27 20:51:42.169: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 20:51:42.169: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:42.184: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.739656ms
Mar 27 20:51:42.184: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 20:51:42.184: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:42.198: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.065455ms
Mar 27 20:51:42.198: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 20:51:42.198: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
Mar 27 20:51:42.213: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.856269ms
Mar 27 20:51:42.213: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/27/23 20:51:42.213
Mar 27 20:51:42.245: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 27 20:51:42.270: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.888847ms
Mar 27 20:51:44.286: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040277145s
Mar 27 20:51:46.290: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.044229445s
Mar 27 20:51:46.290: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:51:46.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1385" for this suite. 03/27/23 20:51:46.626
------------------------------
• [SLOW TEST] [69.086 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:50:37.564
    Mar 27 20:50:37.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 20:50:37.566
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:50:37.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:50:37.614
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 27 20:50:37.662: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 20:51:37.780: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 03/27/23 20:51:37.794
    Mar 27 20:51:37.901: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 27 20:51:37.928: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 27 20:51:37.997: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 27 20:51:38.015: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 27 20:51:38.057: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 27 20:51:38.081: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/27/23 20:51:38.081
    Mar 27 20:51:38.081: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:38.119: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 38.453191ms
    Mar 27 20:51:40.137: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056268551s
    Mar 27 20:51:42.139: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.058173909s
    Mar 27 20:51:42.139: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 27 20:51:42.139: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:42.155: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.25905ms
    Mar 27 20:51:42.155: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 20:51:42.155: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:42.169: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.522428ms
    Mar 27 20:51:42.169: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 20:51:42.169: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:42.184: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.739656ms
    Mar 27 20:51:42.184: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 20:51:42.184: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:42.198: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.065455ms
    Mar 27 20:51:42.198: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 20:51:42.198: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1385" to be "running"
    Mar 27 20:51:42.213: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.856269ms
    Mar 27 20:51:42.213: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/27/23 20:51:42.213
    Mar 27 20:51:42.245: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 27 20:51:42.270: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.888847ms
    Mar 27 20:51:44.286: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040277145s
    Mar 27 20:51:46.290: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.044229445s
    Mar 27 20:51:46.290: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:51:46.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1385" for this suite. 03/27/23 20:51:46.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:51:46.657
Mar 27 20:51:46.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context 03/27/23 20:51:46.658
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:51:46.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:51:46.747
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 20:51:46.758
Mar 27 20:51:46.783: INFO: Waiting up to 5m0s for pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005" in namespace "security-context-9942" to be "Succeeded or Failed"
Mar 27 20:51:46.822: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Pending", Reason="", readiness=false. Elapsed: 38.99949ms
Mar 27 20:51:48.846: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06308665s
Mar 27 20:51:50.839: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055757032s
STEP: Saw pod success 03/27/23 20:51:50.839
Mar 27 20:51:50.839: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005" satisfied condition "Succeeded or Failed"
Mar 27 20:51:50.869: INFO: Trying to get logs from node 10.176.99.177 pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 container test-container: <nil>
STEP: delete the pod 03/27/23 20:51:50.926
Mar 27 20:51:50.979: INFO: Waiting for pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 to disappear
Mar 27 20:51:50.995: INFO: Pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 20:51:50.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9942" for this suite. 03/27/23 20:51:51.018
------------------------------
• [4.387 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:51:46.657
    Mar 27 20:51:46.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context 03/27/23 20:51:46.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:51:46.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:51:46.747
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 20:51:46.758
    Mar 27 20:51:46.783: INFO: Waiting up to 5m0s for pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005" in namespace "security-context-9942" to be "Succeeded or Failed"
    Mar 27 20:51:46.822: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Pending", Reason="", readiness=false. Elapsed: 38.99949ms
    Mar 27 20:51:48.846: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06308665s
    Mar 27 20:51:50.839: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055757032s
    STEP: Saw pod success 03/27/23 20:51:50.839
    Mar 27 20:51:50.839: INFO: Pod "security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005" satisfied condition "Succeeded or Failed"
    Mar 27 20:51:50.869: INFO: Trying to get logs from node 10.176.99.177 pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 container test-container: <nil>
    STEP: delete the pod 03/27/23 20:51:50.926
    Mar 27 20:51:50.979: INFO: Waiting for pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 to disappear
    Mar 27 20:51:50.995: INFO: Pod security-context-a3796fa4-0eac-4e6a-95c8-7e33553cc005 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:51:50.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9942" for this suite. 03/27/23 20:51:51.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:51:51.047
Mar 27 20:51:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename hostport 03/27/23 20:51:51.048
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:51:51.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:51:51.098
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/27/23 20:51:51.125
Mar 27 20:51:51.148: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1094" to be "running and ready"
Mar 27 20:51:51.163: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.917049ms
Mar 27 20:51:51.163: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:51:53.182: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034717697s
Mar 27 20:51:53.182: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:51:55.183: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.035672205s
Mar 27 20:51:55.183: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 20:51:55.183: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.176.99.177 on the node which pod1 resides and expect scheduled 03/27/23 20:51:55.183
Mar 27 20:51:55.212: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1094" to be "running and ready"
Mar 27 20:51:55.227: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.550714ms
Mar 27 20:51:55.227: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:51:57.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.03091327s
Mar 27 20:51:57.243: INFO: The phase of Pod pod2 is Running (Ready = false)
Mar 27 20:51:59.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.031337979s
Mar 27 20:51:59.243: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 20:51:59.243: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.176.99.177 but use UDP protocol on the node which pod2 resides 03/27/23 20:51:59.243
Mar 27 20:51:59.260: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1094" to be "running and ready"
Mar 27 20:51:59.275: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.337539ms
Mar 27 20:51:59.275: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:52:01.290: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029823407s
Mar 27 20:52:01.290: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 27 20:52:01.290: INFO: Pod "pod3" satisfied condition "running and ready"
Mar 27 20:52:01.307: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1094" to be "running and ready"
Mar 27 20:52:01.321: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.619941ms
Mar 27 20:52:01.321: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:52:03.338: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.0306703s
Mar 27 20:52:03.338: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 27 20:52:03.338: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/27/23 20:52:03.353
Mar 27 20:52:03.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.176.99.177 http://127.0.0.1:54323/hostname] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 20:52:03.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:52:03.354: INFO: ExecWithOptions: Clientset creation
Mar 27 20:52:03.354: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.176.99.177+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.176.99.177, port: 54323 03/27/23 20:52:03.548
Mar 27 20:52:03.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.176.99.177:54323/hostname] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 20:52:03.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:52:03.549: INFO: ExecWithOptions: Clientset creation
Mar 27 20:52:03.549: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.176.99.177%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.176.99.177, port: 54323 UDP 03/27/23 20:52:03.682
Mar 27 20:52:03.683: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.176.99.177 54323] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 20:52:03.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 20:52:03.684: INFO: ExecWithOptions: Clientset creation
Mar 27 20:52:03.684: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.176.99.177+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Mar 27 20:52:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-1094" for this suite. 03/27/23 20:52:08.898
------------------------------
• [SLOW TEST] [17.877 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:51:51.047
    Mar 27 20:51:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename hostport 03/27/23 20:51:51.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:51:51.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:51:51.098
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/27/23 20:51:51.125
    Mar 27 20:51:51.148: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1094" to be "running and ready"
    Mar 27 20:51:51.163: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.917049ms
    Mar 27 20:51:51.163: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:51:53.182: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034717697s
    Mar 27 20:51:53.182: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:51:55.183: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.035672205s
    Mar 27 20:51:55.183: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 20:51:55.183: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.176.99.177 on the node which pod1 resides and expect scheduled 03/27/23 20:51:55.183
    Mar 27 20:51:55.212: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1094" to be "running and ready"
    Mar 27 20:51:55.227: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.550714ms
    Mar 27 20:51:55.227: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:51:57.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.03091327s
    Mar 27 20:51:57.243: INFO: The phase of Pod pod2 is Running (Ready = false)
    Mar 27 20:51:59.243: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.031337979s
    Mar 27 20:51:59.243: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 20:51:59.243: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.176.99.177 but use UDP protocol on the node which pod2 resides 03/27/23 20:51:59.243
    Mar 27 20:51:59.260: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1094" to be "running and ready"
    Mar 27 20:51:59.275: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.337539ms
    Mar 27 20:51:59.275: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:52:01.290: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.029823407s
    Mar 27 20:52:01.290: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 27 20:52:01.290: INFO: Pod "pod3" satisfied condition "running and ready"
    Mar 27 20:52:01.307: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1094" to be "running and ready"
    Mar 27 20:52:01.321: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.619941ms
    Mar 27 20:52:01.321: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:52:03.338: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.0306703s
    Mar 27 20:52:03.338: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 27 20:52:03.338: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/27/23 20:52:03.353
    Mar 27 20:52:03.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.176.99.177 http://127.0.0.1:54323/hostname] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 20:52:03.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:52:03.354: INFO: ExecWithOptions: Clientset creation
    Mar 27 20:52:03.354: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.176.99.177+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.176.99.177, port: 54323 03/27/23 20:52:03.548
    Mar 27 20:52:03.548: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.176.99.177:54323/hostname] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 20:52:03.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:52:03.549: INFO: ExecWithOptions: Clientset creation
    Mar 27 20:52:03.549: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.176.99.177%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.176.99.177, port: 54323 UDP 03/27/23 20:52:03.682
    Mar 27 20:52:03.683: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.176.99.177 54323] Namespace:hostport-1094 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 20:52:03.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 20:52:03.684: INFO: ExecWithOptions: Clientset creation
    Mar 27 20:52:03.684: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1094/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.176.99.177+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:52:08.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-1094" for this suite. 03/27/23 20:52:08.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:52:08.926
Mar 27 20:52:08.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 20:52:08.928
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:08.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:08.985
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 20:52:09.055
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 20:52:09.999
STEP: Deploying the webhook pod 03/27/23 20:52:10.017
STEP: Wait for the deployment to be ready 03/27/23 20:52:10.06
Mar 27 20:52:10.096: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 27 20:52:12.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/27/23 20:52:14.17
STEP: Verifying the service has paired with the endpoint 03/27/23 20:52:14.192
Mar 27 20:52:15.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 03/27/23 20:52:15.203
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/27/23 20:52:15.275
STEP: Creating a configMap that should not be mutated 03/27/23 20:52:15.295
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/27/23 20:52:15.348
STEP: Creating a configMap that should be mutated 03/27/23 20:52:15.369
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:52:15.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9139" for this suite. 03/27/23 20:52:15.605
STEP: Destroying namespace "webhook-9139-markers" for this suite. 03/27/23 20:52:15.63
------------------------------
• [SLOW TEST] [6.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:52:08.926
    Mar 27 20:52:08.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 20:52:08.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:08.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:08.985
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 20:52:09.055
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 20:52:09.999
    STEP: Deploying the webhook pod 03/27/23 20:52:10.017
    STEP: Wait for the deployment to be ready 03/27/23 20:52:10.06
    Mar 27 20:52:10.096: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 27 20:52:12.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 20, 52, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/27/23 20:52:14.17
    STEP: Verifying the service has paired with the endpoint 03/27/23 20:52:14.192
    Mar 27 20:52:15.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 03/27/23 20:52:15.203
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/27/23 20:52:15.275
    STEP: Creating a configMap that should not be mutated 03/27/23 20:52:15.295
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/27/23 20:52:15.348
    STEP: Creating a configMap that should be mutated 03/27/23 20:52:15.369
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:52:15.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9139" for this suite. 03/27/23 20:52:15.605
    STEP: Destroying namespace "webhook-9139-markers" for this suite. 03/27/23 20:52:15.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:52:15.666
Mar 27 20:52:15.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 20:52:15.674
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:15.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:15.75
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 03/27/23 20:52:15.761
Mar 27 20:52:15.784: INFO: Waiting up to 5m0s for pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8" in namespace "emptydir-492" to be "Succeeded or Failed"
Mar 27 20:52:15.799: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.502884ms
Mar 27 20:52:17.814: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029890206s
Mar 27 20:52:19.815: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031031507s
STEP: Saw pod success 03/27/23 20:52:19.815
Mar 27 20:52:19.816: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8" satisfied condition "Succeeded or Failed"
Mar 27 20:52:19.831: INFO: Trying to get logs from node 10.176.99.175 pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 container test-container: <nil>
STEP: delete the pod 03/27/23 20:52:19.892
Mar 27 20:52:19.930: INFO: Waiting for pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 to disappear
Mar 27 20:52:19.945: INFO: Pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 20:52:19.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-492" for this suite. 03/27/23 20:52:19.968
------------------------------
• [4.330 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:52:15.666
    Mar 27 20:52:15.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 20:52:15.674
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:15.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:15.75
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 03/27/23 20:52:15.761
    Mar 27 20:52:15.784: INFO: Waiting up to 5m0s for pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8" in namespace "emptydir-492" to be "Succeeded or Failed"
    Mar 27 20:52:15.799: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.502884ms
    Mar 27 20:52:17.814: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029890206s
    Mar 27 20:52:19.815: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031031507s
    STEP: Saw pod success 03/27/23 20:52:19.815
    Mar 27 20:52:19.816: INFO: Pod "pod-17591715-bccc-43bb-9825-3b55a84a0ad8" satisfied condition "Succeeded or Failed"
    Mar 27 20:52:19.831: INFO: Trying to get logs from node 10.176.99.175 pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 container test-container: <nil>
    STEP: delete the pod 03/27/23 20:52:19.892
    Mar 27 20:52:19.930: INFO: Waiting for pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 to disappear
    Mar 27 20:52:19.945: INFO: Pod pod-17591715-bccc-43bb-9825-3b55a84a0ad8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:52:19.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-492" for this suite. 03/27/23 20:52:19.968
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:52:19.998
Mar 27 20:52:19.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 20:52:20
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:20.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:20.051
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec in namespace container-probe-4218 03/27/23 20:52:20.062
Mar 27 20:52:20.084: INFO: Waiting up to 5m0s for pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec" in namespace "container-probe-4218" to be "not pending"
Mar 27 20:52:20.102: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.08983ms
Mar 27 20:52:22.117: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032319628s
Mar 27 20:52:24.127: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Running", Reason="", readiness=true. Elapsed: 4.042284607s
Mar 27 20:52:24.127: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec" satisfied condition "not pending"
Mar 27 20:52:24.127: INFO: Started pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec in namespace container-probe-4218
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 20:52:24.127
Mar 27 20:52:24.143: INFO: Initial restart count of pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec is 0
STEP: deleting the pod 03/27/23 20:56:24.151
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:24.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4218" for this suite. 03/27/23 20:56:24.225
------------------------------
• [SLOW TEST] [244.252 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:52:19.998
    Mar 27 20:52:19.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 20:52:20
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:52:20.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:52:20.051
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec in namespace container-probe-4218 03/27/23 20:52:20.062
    Mar 27 20:52:20.084: INFO: Waiting up to 5m0s for pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec" in namespace "container-probe-4218" to be "not pending"
    Mar 27 20:52:20.102: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 17.08983ms
    Mar 27 20:52:22.117: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032319628s
    Mar 27 20:52:24.127: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec": Phase="Running", Reason="", readiness=true. Elapsed: 4.042284607s
    Mar 27 20:52:24.127: INFO: Pod "liveness-94759745-4259-4a30-a603-77c9f6ee58ec" satisfied condition "not pending"
    Mar 27 20:52:24.127: INFO: Started pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec in namespace container-probe-4218
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 20:52:24.127
    Mar 27 20:52:24.143: INFO: Initial restart count of pod liveness-94759745-4259-4a30-a603-77c9f6ee58ec is 0
    STEP: deleting the pod 03/27/23 20:56:24.151
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:24.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4218" for this suite. 03/27/23 20:56:24.225
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:24.25
Mar 27 20:56:24.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 20:56:24.252
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:24.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:24.302
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 03/27/23 20:56:24.313
Mar 27 20:56:24.336: INFO: Waiting up to 5m0s for pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4" in namespace "pods-5370" to be "running and ready"
Mar 27 20:56:24.351: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.515533ms
Mar 27 20:56:24.352: INFO: The phase of Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:56:26.368: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031682531s
Mar 27 20:56:26.368: INFO: The phase of Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 is Running (Ready = true)
Mar 27 20:56:26.368: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4" satisfied condition "running and ready"
Mar 27 20:56:26.396: INFO: Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 has hostIP: 10.176.99.177
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5370" for this suite. 03/27/23 20:56:26.425
------------------------------
• [2.200 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:24.25
    Mar 27 20:56:24.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 20:56:24.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:24.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:24.302
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 03/27/23 20:56:24.313
    Mar 27 20:56:24.336: INFO: Waiting up to 5m0s for pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4" in namespace "pods-5370" to be "running and ready"
    Mar 27 20:56:24.351: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.515533ms
    Mar 27 20:56:24.352: INFO: The phase of Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:56:26.368: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031682531s
    Mar 27 20:56:26.368: INFO: The phase of Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 is Running (Ready = true)
    Mar 27 20:56:26.368: INFO: Pod "pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4" satisfied condition "running and ready"
    Mar 27 20:56:26.396: INFO: Pod pod-hostip-374b2ffa-ea2b-4f19-8165-dec18ddec4b4 has hostIP: 10.176.99.177
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5370" for this suite. 03/27/23 20:56:26.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:26.453
Mar 27 20:56:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename init-container 03/27/23 20:56:26.454
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:26.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:26.501
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 03/27/23 20:56:26.512
Mar 27 20:56:26.512: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:31.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1358" for this suite. 03/27/23 20:56:32.006
------------------------------
• [SLOW TEST] [5.581 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:26.453
    Mar 27 20:56:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename init-container 03/27/23 20:56:26.454
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:26.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:26.501
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 03/27/23 20:56:26.512
    Mar 27 20:56:26.512: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:31.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1358" for this suite. 03/27/23 20:56:32.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:32.034
Mar 27 20:56:32.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 20:56:32.036
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:32.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:32.086
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-a43c0bea-2340-4c04-8e93-65240aa4bd9b 03/27/23 20:56:32.096
STEP: Creating a pod to test consume secrets 03/27/23 20:56:32.112
Mar 27 20:56:32.134: INFO: Waiting up to 5m0s for pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae" in namespace "secrets-4018" to be "Succeeded or Failed"
Mar 27 20:56:32.148: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Pending", Reason="", readiness=false. Elapsed: 13.999143ms
Mar 27 20:56:34.189: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055122386s
Mar 27 20:56:36.164: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029628451s
STEP: Saw pod success 03/27/23 20:56:36.164
Mar 27 20:56:36.164: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae" satisfied condition "Succeeded or Failed"
Mar 27 20:56:36.184: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 20:56:36.257
Mar 27 20:56:36.309: INFO: Waiting for pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae to disappear
Mar 27 20:56:36.322: INFO: Pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:36.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4018" for this suite. 03/27/23 20:56:36.357
------------------------------
• [4.350 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:32.034
    Mar 27 20:56:32.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 20:56:32.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:32.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:32.086
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-a43c0bea-2340-4c04-8e93-65240aa4bd9b 03/27/23 20:56:32.096
    STEP: Creating a pod to test consume secrets 03/27/23 20:56:32.112
    Mar 27 20:56:32.134: INFO: Waiting up to 5m0s for pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae" in namespace "secrets-4018" to be "Succeeded or Failed"
    Mar 27 20:56:32.148: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Pending", Reason="", readiness=false. Elapsed: 13.999143ms
    Mar 27 20:56:34.189: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055122386s
    Mar 27 20:56:36.164: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029628451s
    STEP: Saw pod success 03/27/23 20:56:36.164
    Mar 27 20:56:36.164: INFO: Pod "pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae" satisfied condition "Succeeded or Failed"
    Mar 27 20:56:36.184: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 20:56:36.257
    Mar 27 20:56:36.309: INFO: Waiting for pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae to disappear
    Mar 27 20:56:36.322: INFO: Pod pod-secrets-b53c2795-492a-4f35-ae44-c8057f4b43ae no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:36.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4018" for this suite. 03/27/23 20:56:36.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:36.391
Mar 27 20:56:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 20:56:36.392
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:36.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:36.441
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 03/27/23 20:56:36.451
STEP: submitting the pod to kubernetes 03/27/23 20:56:36.452
Mar 27 20:56:36.474: INFO: Waiting up to 5m0s for pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" in namespace "pods-6974" to be "running and ready"
Mar 27 20:56:36.489: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.031495ms
Mar 27 20:56:36.489: INFO: The phase of Pod pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:56:38.505: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Running", Reason="", readiness=true. Elapsed: 2.031023887s
Mar 27 20:56:38.505: INFO: The phase of Pod pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b is Running (Ready = true)
Mar 27 20:56:38.506: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/27/23 20:56:38.523
STEP: updating the pod 03/27/23 20:56:38.537
Mar 27 20:56:39.074: INFO: Successfully updated pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b"
Mar 27 20:56:39.074: INFO: Waiting up to 5m0s for pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" in namespace "pods-6974" to be "running"
Mar 27 20:56:39.089: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Running", Reason="", readiness=true. Elapsed: 14.798232ms
Mar 27 20:56:39.089: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/27/23 20:56:39.089
Mar 27 20:56:39.106: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:39.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6974" for this suite. 03/27/23 20:56:39.136
------------------------------
• [2.795 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:36.391
    Mar 27 20:56:36.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 20:56:36.392
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:36.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:36.441
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 03/27/23 20:56:36.451
    STEP: submitting the pod to kubernetes 03/27/23 20:56:36.452
    Mar 27 20:56:36.474: INFO: Waiting up to 5m0s for pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" in namespace "pods-6974" to be "running and ready"
    Mar 27 20:56:36.489: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.031495ms
    Mar 27 20:56:36.489: INFO: The phase of Pod pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:56:38.505: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Running", Reason="", readiness=true. Elapsed: 2.031023887s
    Mar 27 20:56:38.505: INFO: The phase of Pod pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b is Running (Ready = true)
    Mar 27 20:56:38.506: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/27/23 20:56:38.523
    STEP: updating the pod 03/27/23 20:56:38.537
    Mar 27 20:56:39.074: INFO: Successfully updated pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b"
    Mar 27 20:56:39.074: INFO: Waiting up to 5m0s for pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" in namespace "pods-6974" to be "running"
    Mar 27 20:56:39.089: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b": Phase="Running", Reason="", readiness=true. Elapsed: 14.798232ms
    Mar 27 20:56:39.089: INFO: Pod "pod-update-0dd48e28-ee56-46e1-a4ab-ed380da5881b" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/27/23 20:56:39.089
    Mar 27 20:56:39.106: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:39.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6974" for this suite. 03/27/23 20:56:39.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:39.199
Mar 27 20:56:39.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 20:56:39.201
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:39.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:39.251
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Mar 27 20:56:39.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 create -f -'
Mar 27 20:56:39.681: INFO: stderr: ""
Mar 27 20:56:39.681: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 27 20:56:39.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 create -f -'
Mar 27 20:56:39.976: INFO: stderr: ""
Mar 27 20:56:39.976: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 20:56:39.976
Mar 27 20:56:40.997: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 20:56:40.997: INFO: Found 0 / 1
Mar 27 20:56:41.991: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 20:56:41.991: INFO: Found 1 / 1
Mar 27 20:56:41.991: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 27 20:56:42.008: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 20:56:42.008: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 20:56:42.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe pod agnhost-primary-r4hpk'
Mar 27 20:56:42.140: INFO: stderr: ""
Mar 27 20:56:42.140: INFO: stdout: "Name:             agnhost-primary-r4hpk\nNamespace:        kubectl-380\nPriority:         0\nService Account:  default\nNode:             10.176.99.177/10.176.99.177\nStart Time:       Mon, 27 Mar 2023 20:56:39 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 745ee594abe505375d9e47b723c8de06396f970672bb8200f66181b25f3dca4a\n                  cni.projectcalico.org/podIP: 172.30.85.181/32\n                  cni.projectcalico.org/podIPs: 172.30.85.181/32\nStatus:           Running\nIP:               172.30.85.181\nIPs:\n  IP:           172.30.85.181\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f1dfc7aaae1608dcb74fe6de9b825412f1a44ad1977900e8492a586c05804bab\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Mar 2023 20:56:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4hmsp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4hmsp:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-380/agnhost-primary-r4hpk to 10.176.99.177\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar 27 20:56:42.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe rc agnhost-primary'
Mar 27 20:56:42.267: INFO: stderr: ""
Mar 27 20:56:42.268: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-380\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-r4hpk\n"
Mar 27 20:56:42.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe service agnhost-primary'
Mar 27 20:56:42.380: INFO: stderr: ""
Mar 27 20:56:42.380: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-380\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.104.111\nIPs:               172.21.104.111\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.85.181:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 27 20:56:42.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe node 10.176.99.175'
Mar 27 20:56:42.699: INFO: stderr: ""
Mar 27 20:56:42.699: INFO: stdout: "Name:               10.176.99.175\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal10\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.46.48.135\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.176.99.175\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_20_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cggu674d0f9r07ur77kg-kubee2epvg3-default-00000354\n                    ibm-cloud.kubernetes.io/worker-pool-id=cggu674d0f9r07ur77kg-444369f\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.3_1529\n                    ibm-cloud.kubernetes.io/zone=dal10\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.176.99.175\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722932\n                    publicVLAN=2722926\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal10\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.176.99.175/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.56.64\nCreationTimestamp:  Mon, 27 Mar 2023 19:00:18 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.176.99.175\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Mar 2023 20:56:41 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Mar 2023 19:00:42 +0000   Mon, 27 Mar 2023 19:00:42 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:26 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.176.99.175\n  ExternalIP:  169.46.48.135\n  Hostname:    10.176.99.175\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      102609848Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16177420Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      93913280025\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13409548Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                  596ce0adbe024df8aff08d4a0cf54b46\n  System UUID:                 96abd8da-0189-aa75-5808-289dafebff6e\n  Boot ID:                     b686ebed-d8cc-4e51-811e-ad8e65ffa09c\n  Kernel Version:              5.4.0-144-generic\n  OS Image:                    Ubuntu 20.04.6 LTS\n  Operating System:            linux\n  Architecture:                amd64\n  Container Runtime Version:   containerd://1.7.0\n  Kubelet Version:             v1.26.3+IKS\n  Kube-Proxy Version:          v1.26.3+IKS\nProviderID:                    ibm://fee034388aa6435883a1f720010ab3a2///cggu674d0f9r07ur77kg/kube-cggu674d0f9r07ur77kg-kubee2epvg3-default-00000354\nNon-terminated Pods:           (12 in total)\n  Namespace                    Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                    ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                   ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         108m\n  kube-system                  calico-node-cmh2z                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         116m\n  kube-system                  calico-typha-7f67cb7cc9-lsk8p                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         125m\n  kube-system                  coredns-5845f98d4-rvn24                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     103m\n  kube-system                  ibm-keepalived-watcher-d9666                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         116m\n  kube-system                  ibm-master-proxy-static-10.176.99.175                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      116m\n  kube-system                  ibmcloud-block-storage-driver-vk6lt                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     116m\n  kube-system                  konnectivity-agent-xqxrf                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     103m\n  kube-system                  public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl        20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         104m\n  sonobuoy                     sonobuoy-e2e-job-5e955573fdc94d86                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m7s\n  sonobuoy                     sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m7s\n  test-k8s-e2e-pvg-privileged  test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests       Limits\n  --------               --------       ------\n  cpu                    715m (18%)     800m (20%)\n  memory                 527890Ki (3%)  1728800Ki (12%)\n  ephemeral-storage      0 (0%)         0 (0%)\n  hugepages-1Gi          0 (0%)         0 (0%)\n  hugepages-2Mi          0 (0%)         0 (0%)\n  scheduling.k8s.io/foo  0              0\nEvents:                  <none>\n"
Mar 27 20:56:42.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe namespace kubectl-380'
Mar 27 20:56:42.841: INFO: stderr: ""
Mar 27 20:56:42.841: INFO: stdout: "Name:         kubectl-380\nLabels:       e2e-framework=kubectl\n              e2e-run=d74a9d93-502c-4e4a-9eef-4f4e0d8780d7\n              kubernetes.io/metadata.name=kubectl-380\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:42.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-380" for this suite. 03/27/23 20:56:42.891
------------------------------
• [3.718 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:39.199
    Mar 27 20:56:39.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 20:56:39.201
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:39.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:39.251
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Mar 27 20:56:39.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 create -f -'
    Mar 27 20:56:39.681: INFO: stderr: ""
    Mar 27 20:56:39.681: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 27 20:56:39.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 create -f -'
    Mar 27 20:56:39.976: INFO: stderr: ""
    Mar 27 20:56:39.976: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 20:56:39.976
    Mar 27 20:56:40.997: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 20:56:40.997: INFO: Found 0 / 1
    Mar 27 20:56:41.991: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 20:56:41.991: INFO: Found 1 / 1
    Mar 27 20:56:41.991: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 27 20:56:42.008: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 20:56:42.008: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 20:56:42.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe pod agnhost-primary-r4hpk'
    Mar 27 20:56:42.140: INFO: stderr: ""
    Mar 27 20:56:42.140: INFO: stdout: "Name:             agnhost-primary-r4hpk\nNamespace:        kubectl-380\nPriority:         0\nService Account:  default\nNode:             10.176.99.177/10.176.99.177\nStart Time:       Mon, 27 Mar 2023 20:56:39 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 745ee594abe505375d9e47b723c8de06396f970672bb8200f66181b25f3dca4a\n                  cni.projectcalico.org/podIP: 172.30.85.181/32\n                  cni.projectcalico.org/podIPs: 172.30.85.181/32\nStatus:           Running\nIP:               172.30.85.181\nIPs:\n  IP:           172.30.85.181\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://f1dfc7aaae1608dcb74fe6de9b825412f1a44ad1977900e8492a586c05804bab\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Mar 2023 20:56:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4hmsp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4hmsp:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-380/agnhost-primary-r4hpk to 10.176.99.177\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Mar 27 20:56:42.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe rc agnhost-primary'
    Mar 27 20:56:42.267: INFO: stderr: ""
    Mar 27 20:56:42.268: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-380\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-r4hpk\n"
    Mar 27 20:56:42.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe service agnhost-primary'
    Mar 27 20:56:42.380: INFO: stderr: ""
    Mar 27 20:56:42.380: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-380\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.104.111\nIPs:               172.21.104.111\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.85.181:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 27 20:56:42.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe node 10.176.99.175'
    Mar 27 20:56:42.699: INFO: stderr: ""
    Mar 27 20:56:42.699: INFO: stdout: "Name:               10.176.99.175\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal10\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.46.48.135\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.176.99.175\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_20_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cggu674d0f9r07ur77kg-kubee2epvg3-default-00000354\n                    ibm-cloud.kubernetes.io/worker-pool-id=cggu674d0f9r07ur77kg-444369f\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.3_1529\n                    ibm-cloud.kubernetes.io/zone=dal10\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.176.99.175\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722932\n                    publicVLAN=2722926\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal10\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.176.99.175/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.56.64\nCreationTimestamp:  Mon, 27 Mar 2023 19:00:18 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.176.99.175\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Mar 2023 20:56:41 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Mar 2023 19:00:42 +0000   Mon, 27 Mar 2023 19:00:42 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Mar 2023 20:52:21 +0000   Mon, 27 Mar 2023 19:00:26 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.176.99.175\n  ExternalIP:  169.46.48.135\n  Hostname:    10.176.99.175\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      102609848Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16177420Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    3910m\n  ephemeral-storage:      93913280025\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 13409548Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                  596ce0adbe024df8aff08d4a0cf54b46\n  System UUID:                 96abd8da-0189-aa75-5808-289dafebff6e\n  Boot ID:                     b686ebed-d8cc-4e51-811e-ad8e65ffa09c\n  Kernel Version:              5.4.0-144-generic\n  OS Image:                    Ubuntu 20.04.6 LTS\n  Operating System:            linux\n  Architecture:                amd64\n  Container Runtime Version:   containerd://1.7.0\n  Kubelet Version:             v1.26.3+IKS\n  Kube-Proxy Version:          v1.26.3+IKS\nProviderID:                    ibm://fee034388aa6435883a1f720010ab3a2///cggu674d0f9r07ur77kg/kube-cggu674d0f9r07ur77kg-kubee2epvg3-default-00000354\nNon-terminated Pods:           (12 in total)\n  Namespace                    Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                    ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                   ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         108m\n  kube-system                  calico-node-cmh2z                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         116m\n  kube-system                  calico-typha-7f67cb7cc9-lsk8p                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         125m\n  kube-system                  coredns-5845f98d4-rvn24                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     103m\n  kube-system                  ibm-keepalived-watcher-d9666                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         116m\n  kube-system                  ibm-master-proxy-static-10.176.99.175                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      116m\n  kube-system                  ibmcloud-block-storage-driver-vk6lt                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     116m\n  kube-system                  konnectivity-agent-xqxrf                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     103m\n  kube-system                  public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl        20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         104m\n  sonobuoy                     sonobuoy-e2e-job-5e955573fdc94d86                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m7s\n  sonobuoy                     sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m7s\n  test-k8s-e2e-pvg-privileged  test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests       Limits\n  --------               --------       ------\n  cpu                    715m (18%)     800m (20%)\n  memory                 527890Ki (3%)  1728800Ki (12%)\n  ephemeral-storage      0 (0%)         0 (0%)\n  hugepages-1Gi          0 (0%)         0 (0%)\n  hugepages-2Mi          0 (0%)         0 (0%)\n  scheduling.k8s.io/foo  0              0\nEvents:                  <none>\n"
    Mar 27 20:56:42.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-380 describe namespace kubectl-380'
    Mar 27 20:56:42.841: INFO: stderr: ""
    Mar 27 20:56:42.841: INFO: stdout: "Name:         kubectl-380\nLabels:       e2e-framework=kubectl\n              e2e-run=d74a9d93-502c-4e4a-9eef-4f4e0d8780d7\n              kubernetes.io/metadata.name=kubectl-380\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:42.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-380" for this suite. 03/27/23 20:56:42.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:42.921
Mar 27 20:56:42.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 20:56:42.922
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:42.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:42.976
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 20:56:43.029
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 20:56:44.221
STEP: Deploying the webhook pod 03/27/23 20:56:44.31
STEP: Wait for the deployment to be ready 03/27/23 20:56:44.372
Mar 27 20:56:44.407: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 20:56:46.495
STEP: Verifying the service has paired with the endpoint 03/27/23 20:56:46.516
Mar 27 20:56:47.516: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 03/27/23 20:56:47.525
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/27/23 20:56:47.53
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 20:56:47.53
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/27/23 20:56:47.531
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/27/23 20:56:47.535
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 20:56:47.536
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 20:56:47.54
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:47.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7491" for this suite. 03/27/23 20:56:47.685
STEP: Destroying namespace "webhook-7491-markers" for this suite. 03/27/23 20:56:47.74
------------------------------
• [4.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:42.921
    Mar 27 20:56:42.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 20:56:42.922
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:42.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:42.976
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 20:56:43.029
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 20:56:44.221
    STEP: Deploying the webhook pod 03/27/23 20:56:44.31
    STEP: Wait for the deployment to be ready 03/27/23 20:56:44.372
    Mar 27 20:56:44.407: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 20:56:46.495
    STEP: Verifying the service has paired with the endpoint 03/27/23 20:56:46.516
    Mar 27 20:56:47.516: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 03/27/23 20:56:47.525
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/27/23 20:56:47.53
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 20:56:47.53
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/27/23 20:56:47.531
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/27/23 20:56:47.535
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 20:56:47.536
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/27/23 20:56:47.54
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:47.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7491" for this suite. 03/27/23 20:56:47.685
    STEP: Destroying namespace "webhook-7491-markers" for this suite. 03/27/23 20:56:47.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:47.791
Mar 27 20:56:47.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename proxy 03/27/23 20:56:47.796
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:47.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:47.848
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/27/23 20:56:47.885
STEP: creating replication controller proxy-service-xgpb7 in namespace proxy-6956 03/27/23 20:56:47.885
I0327 20:56:47.900846      20 runners.go:193] Created replication controller with name: proxy-service-xgpb7, namespace: proxy-6956, replica count: 1
I0327 20:56:48.952017      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 20:56:49.952177      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 20:56:50.952418      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0327 20:56:51.952613      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 20:56:51.961: INFO: setup took 4.101868274s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/27/23 20:56:51.961
Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 53.451036ms)
Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 53.02542ms)
Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 53.63169ms)
Mar 27 20:56:52.016: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 55.011683ms)
Mar 27 20:56:52.016: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 55.267697ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 56.852419ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 56.651579ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 57.140728ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 56.35108ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 56.660447ms)
Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 56.751982ms)
Mar 27 20:56:52.022: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 60.93919ms)
Mar 27 20:56:52.023: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 61.357592ms)
Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 65.477273ms)
Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 65.94551ms)
Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 65.250564ms)
Mar 27 20:56:52.050: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 22.374707ms)
Mar 27 20:56:52.056: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.661728ms)
Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.080694ms)
Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 28.334036ms)
Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 28.62907ms)
Mar 27 20:56:52.058: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 30.274514ms)
Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 30.586435ms)
Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.108061ms)
Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 31.098273ms)
Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 32.050284ms)
Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 31.502727ms)
Mar 27 20:56:52.061: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 33.228859ms)
Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.728348ms)
Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 34.18246ms)
Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 34.323255ms)
Mar 27 20:56:52.063: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 34.278311ms)
Mar 27 20:56:52.082: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 19.56733ms)
Mar 27 20:56:52.088: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.273392ms)
Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 26.12515ms)
Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.861408ms)
Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.224961ms)
Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.339209ms)
Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.560691ms)
Mar 27 20:56:52.090: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.781222ms)
Mar 27 20:56:52.090: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.752188ms)
Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 29.674107ms)
Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 29.722396ms)
Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.06023ms)
Mar 27 20:56:52.098: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 35.283416ms)
Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 35.937513ms)
Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 36.211887ms)
Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 36.234534ms)
Mar 27 20:56:52.119: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 18.931155ms)
Mar 27 20:56:52.125: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.011179ms)
Mar 27 20:56:52.125: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.536371ms)
Mar 27 20:56:52.126: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.687823ms)
Mar 27 20:56:52.127: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.154074ms)
Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.949074ms)
Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 28.273359ms)
Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.990619ms)
Mar 27 20:56:52.129: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.544172ms)
Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.547571ms)
Mar 27 20:56:52.129: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 29.030267ms)
Mar 27 20:56:52.131: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 30.610357ms)
Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 31.229372ms)
Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 32.122778ms)
Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 31.67558ms)
Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.843202ms)
Mar 27 20:56:52.151: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 18.247007ms)
Mar 27 20:56:52.152: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 19.33669ms)
Mar 27 20:56:52.152: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 19.412612ms)
Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.108096ms)
Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 25.335264ms)
Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 23.438516ms)
Mar 27 20:56:52.159: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.993471ms)
Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.008446ms)
Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 26.528348ms)
Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.412556ms)
Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.088852ms)
Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 29.200263ms)
Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 29.771878ms)
Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 29.990568ms)
Mar 27 20:56:52.164: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.445419ms)
Mar 27 20:56:52.164: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.601071ms)
Mar 27 20:56:52.181: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 17.334884ms)
Mar 27 20:56:52.188: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.15458ms)
Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.680281ms)
Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.536734ms)
Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.543325ms)
Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.511046ms)
Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.217526ms)
Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.497141ms)
Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.410835ms)
Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.61115ms)
Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 27.363143ms)
Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 42.180512ms)
Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 42.231558ms)
Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 42.486786ms)
Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 42.713482ms)
Mar 27 20:56:52.208: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 43.27011ms)
Mar 27 20:56:52.228: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 20.508945ms)
Mar 27 20:56:52.235: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.242168ms)
Mar 27 20:56:52.235: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 27.067561ms)
Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 27.861996ms)
Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 27.260419ms)
Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 27.221088ms)
Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.326805ms)
Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 28.791041ms)
Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 28.75159ms)
Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.498235ms)
Mar 27 20:56:52.240: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.827265ms)
Mar 27 20:56:52.241: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 33.090095ms)
Mar 27 20:56:52.242: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.374448ms)
Mar 27 20:56:52.242: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 33.584729ms)
Mar 27 20:56:52.243: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 34.475443ms)
Mar 27 20:56:52.243: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 34.283352ms)
Mar 27 20:56:52.260: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 16.969416ms)
Mar 27 20:56:52.266: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 22.579786ms)
Mar 27 20:56:52.266: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.549399ms)
Mar 27 20:56:52.267: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 23.166286ms)
Mar 27 20:56:52.268: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.13359ms)
Mar 27 20:56:52.268: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.396809ms)
Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 25.775968ms)
Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.946815ms)
Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 25.834052ms)
Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.302976ms)
Mar 27 20:56:52.270: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.433052ms)
Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 28.579271ms)
Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 28.83598ms)
Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 28.396937ms)
Mar 27 20:56:52.273: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 29.054475ms)
Mar 27 20:56:52.273: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.992251ms)
Mar 27 20:56:52.291: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 18.087728ms)
Mar 27 20:56:52.297: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 23.265586ms)
Mar 27 20:56:52.297: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 23.893501ms)
Mar 27 20:56:52.300: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.411489ms)
Mar 27 20:56:52.300: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.705645ms)
Mar 27 20:56:52.301: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.194338ms)
Mar 27 20:56:52.301: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 27.909878ms)
Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 28.382969ms)
Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 28.444473ms)
Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.452422ms)
Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.650542ms)
Mar 27 20:56:52.304: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 30.642061ms)
Mar 27 20:56:52.304: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 30.729293ms)
Mar 27 20:56:52.308: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 34.494328ms)
Mar 27 20:56:52.309: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 35.821899ms)
Mar 27 20:56:52.309: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 35.775324ms)
Mar 27 20:56:52.329: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 18.856616ms)
Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.288654ms)
Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 23.887661ms)
Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.110303ms)
Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 24.810702ms)
Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.889723ms)
Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.778579ms)
Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.720049ms)
Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.796653ms)
Mar 27 20:56:52.336: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 25.476536ms)
Mar 27 20:56:52.336: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 25.763913ms)
Mar 27 20:56:52.340: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.220657ms)
Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 30.518905ms)
Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 30.855819ms)
Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.73311ms)
Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 31.702337ms)
Mar 27 20:56:52.364: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 22.644633ms)
Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.88201ms)
Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.931485ms)
Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 26.241904ms)
Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.393393ms)
Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 27.31865ms)
Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.771748ms)
Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.095601ms)
Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 27.490785ms)
Mar 27 20:56:52.370: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.98943ms)
Mar 27 20:56:52.373: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.893658ms)
Mar 27 20:56:52.374: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.445636ms)
Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.665487ms)
Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 33.534646ms)
Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 33.477253ms)
Mar 27 20:56:52.376: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 33.628719ms)
Mar 27 20:56:52.395: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 19.162977ms)
Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.299545ms)
Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.948598ms)
Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.112437ms)
Mar 27 20:56:52.403: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.295144ms)
Mar 27 20:56:52.403: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.268446ms)
Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.204482ms)
Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.376208ms)
Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 27.257072ms)
Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.354653ms)
Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.421247ms)
Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 31.804129ms)
Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.94087ms)
Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 32.175707ms)
Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 32.231005ms)
Mar 27 20:56:52.409: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.136935ms)
Mar 27 20:56:52.426: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 17.446555ms)
Mar 27 20:56:52.432: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 22.215257ms)
Mar 27 20:56:52.432: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.699951ms)
Mar 27 20:56:52.433: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 23.68394ms)
Mar 27 20:56:52.433: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 23.797431ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.46086ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 24.435598ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.786903ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.712802ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.575863ms)
Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.68805ms)
Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 28.638979ms)
Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 28.528081ms)
Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 28.281872ms)
Mar 27 20:56:52.438: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 29.178391ms)
Mar 27 20:56:52.438: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.615996ms)
Mar 27 20:56:52.456: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 18.206921ms)
Mar 27 20:56:52.463: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.921506ms)
Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.408503ms)
Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.551093ms)
Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.643024ms)
Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.537435ms)
Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 30.738392ms)
Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 30.535048ms)
Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.414486ms)
Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 30.514108ms)
Mar 27 20:56:52.470: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 30.875129ms)
Mar 27 20:56:52.471: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.504643ms)
Mar 27 20:56:52.471: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 32.556514ms)
Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 32.833247ms)
Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 33.427402ms)
Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.677887ms)
Mar 27 20:56:52.496: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.358573ms)
Mar 27 20:56:52.497: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.30665ms)
Mar 27 20:56:52.498: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.443519ms)
Mar 27 20:56:52.498: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.761707ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 26.030311ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.532965ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.935513ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.782344ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.537484ms)
Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.850251ms)
Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 29.925133ms)
Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 29.53691ms)
Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 29.561998ms)
Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 30.116569ms)
Mar 27 20:56:52.504: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.867574ms)
Mar 27 20:56:52.504: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 30.759097ms)
Mar 27 20:56:52.522: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 16.622486ms)
Mar 27 20:56:52.528: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 22.439059ms)
Mar 27 20:56:52.529: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 23.438055ms)
Mar 27 20:56:52.529: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.182933ms)
Mar 27 20:56:52.530: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 24.267704ms)
Mar 27 20:56:52.531: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.307327ms)
Mar 27 20:56:52.531: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.321687ms)
Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 25.836074ms)
Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.037926ms)
Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.78638ms)
Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.12959ms)
Mar 27 20:56:52.537: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.695489ms)
Mar 27 20:56:52.542: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 36.362203ms)
Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 37.252644ms)
Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 37.084233ms)
Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 37.36208ms)
Mar 27 20:56:52.563: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 20.006333ms)
Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 22.996978ms)
Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.997011ms)
Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 24.774557ms)
Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.037663ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.56824ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.087128ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.931612ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.817626ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 23.792117ms)
Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.052608ms)
Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 30.90857ms)
Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 30.081187ms)
Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 29.095107ms)
Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 29.781904ms)
Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 30.28585ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 34.424989ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 33.761214ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 32.489693ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 33.328795ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 34.466982ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 31.480698ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 34.056588ms)
Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 33.070657ms)
Mar 27 20:56:52.611: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 34.397997ms)
Mar 27 20:56:52.615: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 38.945563ms)
Mar 27 20:56:52.615: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 39.536831ms)
Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 39.13481ms)
Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 40.017155ms)
Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 37.756731ms)
Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 40.077235ms)
Mar 27 20:56:52.618: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 39.973954ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 42.63361ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 42.378938ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 42.629059ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 42.856821ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 43.346602ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 43.450706ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 43.279236ms)
Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 43.965651ms)
Mar 27 20:56:52.663: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 44.330955ms)
Mar 27 20:56:52.669: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 49.695519ms)
Mar 27 20:56:52.669: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 49.409415ms)
Mar 27 20:56:52.672: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 52.99862ms)
Mar 27 20:56:52.672: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 53.174093ms)
Mar 27 20:56:52.673: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 54.237266ms)
Mar 27 20:56:52.679: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 60.202872ms)
Mar 27 20:56:52.679: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 60.367947ms)
Mar 27 20:56:52.694: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 15.361346ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 21.292241ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 21.442403ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 21.875478ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 22.061251ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 21.892222ms)
Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 22.106341ms)
Mar 27 20:56:52.705: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.46421ms)
Mar 27 20:56:52.706: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.259835ms)
Mar 27 20:56:52.706: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.242802ms)
Mar 27 20:56:52.725: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 45.626844ms)
Mar 27 20:56:52.731: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 51.754077ms)
Mar 27 20:56:52.731: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 51.987404ms)
Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 67.495269ms)
Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 67.719469ms)
Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 68.221573ms)
STEP: deleting ReplicationController proxy-service-xgpb7 in namespace proxy-6956, will wait for the garbage collector to delete the pods 03/27/23 20:56:52.748
Mar 27 20:56:52.851: INFO: Deleting ReplicationController proxy-service-xgpb7 took: 23.436471ms
Mar 27 20:56:52.952: INFO: Terminating ReplicationController proxy-service-xgpb7 pods took: 100.729292ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6956" for this suite. 03/27/23 20:56:54.172
------------------------------
• [SLOW TEST] [6.405 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:47.791
    Mar 27 20:56:47.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename proxy 03/27/23 20:56:47.796
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:47.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:47.848
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/27/23 20:56:47.885
    STEP: creating replication controller proxy-service-xgpb7 in namespace proxy-6956 03/27/23 20:56:47.885
    I0327 20:56:47.900846      20 runners.go:193] Created replication controller with name: proxy-service-xgpb7, namespace: proxy-6956, replica count: 1
    I0327 20:56:48.952017      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 20:56:49.952177      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 20:56:50.952418      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0327 20:56:51.952613      20 runners.go:193] proxy-service-xgpb7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 20:56:51.961: INFO: setup took 4.101868274s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/27/23 20:56:51.961
    Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 53.451036ms)
    Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 53.02542ms)
    Mar 27 20:56:52.015: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 53.63169ms)
    Mar 27 20:56:52.016: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 55.011683ms)
    Mar 27 20:56:52.016: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 55.267697ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 56.852419ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 56.651579ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 57.140728ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 56.35108ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 56.660447ms)
    Mar 27 20:56:52.018: INFO: (0) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 56.751982ms)
    Mar 27 20:56:52.022: INFO: (0) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 60.93919ms)
    Mar 27 20:56:52.023: INFO: (0) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 61.357592ms)
    Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 65.477273ms)
    Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 65.94551ms)
    Mar 27 20:56:52.027: INFO: (0) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 65.250564ms)
    Mar 27 20:56:52.050: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 22.374707ms)
    Mar 27 20:56:52.056: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.661728ms)
    Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.080694ms)
    Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 28.334036ms)
    Mar 27 20:56:52.057: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 28.62907ms)
    Mar 27 20:56:52.058: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 30.274514ms)
    Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 30.586435ms)
    Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.108061ms)
    Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 31.098273ms)
    Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 32.050284ms)
    Mar 27 20:56:52.059: INFO: (1) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 31.502727ms)
    Mar 27 20:56:52.061: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 33.228859ms)
    Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.728348ms)
    Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 34.18246ms)
    Mar 27 20:56:52.062: INFO: (1) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 34.323255ms)
    Mar 27 20:56:52.063: INFO: (1) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 34.278311ms)
    Mar 27 20:56:52.082: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 19.56733ms)
    Mar 27 20:56:52.088: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.273392ms)
    Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 26.12515ms)
    Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.861408ms)
    Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.224961ms)
    Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.339209ms)
    Mar 27 20:56:52.089: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.560691ms)
    Mar 27 20:56:52.090: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.781222ms)
    Mar 27 20:56:52.090: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.752188ms)
    Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 29.674107ms)
    Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 29.722396ms)
    Mar 27 20:56:52.093: INFO: (2) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.06023ms)
    Mar 27 20:56:52.098: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 35.283416ms)
    Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 35.937513ms)
    Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 36.211887ms)
    Mar 27 20:56:52.099: INFO: (2) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 36.234534ms)
    Mar 27 20:56:52.119: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 18.931155ms)
    Mar 27 20:56:52.125: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.011179ms)
    Mar 27 20:56:52.125: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.536371ms)
    Mar 27 20:56:52.126: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.687823ms)
    Mar 27 20:56:52.127: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.154074ms)
    Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.949074ms)
    Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 28.273359ms)
    Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.990619ms)
    Mar 27 20:56:52.129: INFO: (3) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.544172ms)
    Mar 27 20:56:52.128: INFO: (3) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.547571ms)
    Mar 27 20:56:52.129: INFO: (3) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 29.030267ms)
    Mar 27 20:56:52.131: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 30.610357ms)
    Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 31.229372ms)
    Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 32.122778ms)
    Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 31.67558ms)
    Mar 27 20:56:52.132: INFO: (3) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.843202ms)
    Mar 27 20:56:52.151: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 18.247007ms)
    Mar 27 20:56:52.152: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 19.33669ms)
    Mar 27 20:56:52.152: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 19.412612ms)
    Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.108096ms)
    Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 25.335264ms)
    Mar 27 20:56:52.157: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 23.438516ms)
    Mar 27 20:56:52.159: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.993471ms)
    Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.008446ms)
    Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 26.528348ms)
    Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.412556ms)
    Mar 27 20:56:52.160: INFO: (4) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.088852ms)
    Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 29.200263ms)
    Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 29.771878ms)
    Mar 27 20:56:52.163: INFO: (4) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 29.990568ms)
    Mar 27 20:56:52.164: INFO: (4) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.445419ms)
    Mar 27 20:56:52.164: INFO: (4) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.601071ms)
    Mar 27 20:56:52.181: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 17.334884ms)
    Mar 27 20:56:52.188: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.15458ms)
    Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.680281ms)
    Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.536734ms)
    Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.543325ms)
    Mar 27 20:56:52.191: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.511046ms)
    Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.217526ms)
    Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.497141ms)
    Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.410835ms)
    Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.61115ms)
    Mar 27 20:56:52.192: INFO: (5) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 27.363143ms)
    Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 42.180512ms)
    Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 42.231558ms)
    Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 42.486786ms)
    Mar 27 20:56:52.207: INFO: (5) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 42.713482ms)
    Mar 27 20:56:52.208: INFO: (5) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 43.27011ms)
    Mar 27 20:56:52.228: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 20.508945ms)
    Mar 27 20:56:52.235: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.242168ms)
    Mar 27 20:56:52.235: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 27.067561ms)
    Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 27.861996ms)
    Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 27.260419ms)
    Mar 27 20:56:52.236: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 27.221088ms)
    Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.326805ms)
    Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 28.791041ms)
    Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 28.75159ms)
    Mar 27 20:56:52.237: INFO: (6) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.498235ms)
    Mar 27 20:56:52.240: INFO: (6) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.827265ms)
    Mar 27 20:56:52.241: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 33.090095ms)
    Mar 27 20:56:52.242: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.374448ms)
    Mar 27 20:56:52.242: INFO: (6) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 33.584729ms)
    Mar 27 20:56:52.243: INFO: (6) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 34.475443ms)
    Mar 27 20:56:52.243: INFO: (6) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 34.283352ms)
    Mar 27 20:56:52.260: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 16.969416ms)
    Mar 27 20:56:52.266: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 22.579786ms)
    Mar 27 20:56:52.266: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.549399ms)
    Mar 27 20:56:52.267: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 23.166286ms)
    Mar 27 20:56:52.268: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.13359ms)
    Mar 27 20:56:52.268: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.396809ms)
    Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 25.775968ms)
    Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.946815ms)
    Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 25.834052ms)
    Mar 27 20:56:52.269: INFO: (7) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.302976ms)
    Mar 27 20:56:52.270: INFO: (7) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.433052ms)
    Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 28.579271ms)
    Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 28.83598ms)
    Mar 27 20:56:52.272: INFO: (7) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 28.396937ms)
    Mar 27 20:56:52.273: INFO: (7) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 29.054475ms)
    Mar 27 20:56:52.273: INFO: (7) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.992251ms)
    Mar 27 20:56:52.291: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 18.087728ms)
    Mar 27 20:56:52.297: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 23.265586ms)
    Mar 27 20:56:52.297: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 23.893501ms)
    Mar 27 20:56:52.300: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.411489ms)
    Mar 27 20:56:52.300: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.705645ms)
    Mar 27 20:56:52.301: INFO: (8) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.194338ms)
    Mar 27 20:56:52.301: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 27.909878ms)
    Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 28.382969ms)
    Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 28.444473ms)
    Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 28.452422ms)
    Mar 27 20:56:52.302: INFO: (8) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 28.650542ms)
    Mar 27 20:56:52.304: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 30.642061ms)
    Mar 27 20:56:52.304: INFO: (8) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 30.729293ms)
    Mar 27 20:56:52.308: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 34.494328ms)
    Mar 27 20:56:52.309: INFO: (8) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 35.821899ms)
    Mar 27 20:56:52.309: INFO: (8) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 35.775324ms)
    Mar 27 20:56:52.329: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 18.856616ms)
    Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.288654ms)
    Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 23.887661ms)
    Mar 27 20:56:52.334: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.110303ms)
    Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 24.810702ms)
    Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.889723ms)
    Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.778579ms)
    Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.720049ms)
    Mar 27 20:56:52.335: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.796653ms)
    Mar 27 20:56:52.336: INFO: (9) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 25.476536ms)
    Mar 27 20:56:52.336: INFO: (9) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 25.763913ms)
    Mar 27 20:56:52.340: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.220657ms)
    Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 30.518905ms)
    Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 30.855819ms)
    Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.73311ms)
    Mar 27 20:56:52.341: INFO: (9) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 31.702337ms)
    Mar 27 20:56:52.364: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 22.644633ms)
    Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.88201ms)
    Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.931485ms)
    Mar 27 20:56:52.368: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 26.241904ms)
    Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.393393ms)
    Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 27.31865ms)
    Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.771748ms)
    Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.095601ms)
    Mar 27 20:56:52.369: INFO: (10) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 27.490785ms)
    Mar 27 20:56:52.370: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.98943ms)
    Mar 27 20:56:52.373: INFO: (10) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.893658ms)
    Mar 27 20:56:52.374: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 31.445636ms)
    Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.665487ms)
    Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 33.534646ms)
    Mar 27 20:56:52.375: INFO: (10) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 33.477253ms)
    Mar 27 20:56:52.376: INFO: (10) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 33.628719ms)
    Mar 27 20:56:52.395: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 19.162977ms)
    Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.299545ms)
    Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 25.948598ms)
    Mar 27 20:56:52.402: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 26.112437ms)
    Mar 27 20:56:52.403: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 26.295144ms)
    Mar 27 20:56:52.403: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.268446ms)
    Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 27.204482ms)
    Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 27.376208ms)
    Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 27.257072ms)
    Mar 27 20:56:52.404: INFO: (11) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 27.354653ms)
    Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.421247ms)
    Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 31.804129ms)
    Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 31.94087ms)
    Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 32.175707ms)
    Mar 27 20:56:52.408: INFO: (11) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 32.231005ms)
    Mar 27 20:56:52.409: INFO: (11) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.136935ms)
    Mar 27 20:56:52.426: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 17.446555ms)
    Mar 27 20:56:52.432: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 22.215257ms)
    Mar 27 20:56:52.432: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 23.699951ms)
    Mar 27 20:56:52.433: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 23.68394ms)
    Mar 27 20:56:52.433: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 23.797431ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.46086ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 24.435598ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.786903ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.712802ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.575863ms)
    Mar 27 20:56:52.434: INFO: (12) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.68805ms)
    Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 28.638979ms)
    Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 28.528081ms)
    Mar 27 20:56:52.437: INFO: (12) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 28.281872ms)
    Mar 27 20:56:52.438: INFO: (12) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 29.178391ms)
    Mar 27 20:56:52.438: INFO: (12) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 28.615996ms)
    Mar 27 20:56:52.456: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 18.206921ms)
    Mar 27 20:56:52.463: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 24.921506ms)
    Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.408503ms)
    Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 24.551093ms)
    Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.643024ms)
    Mar 27 20:56:52.464: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 24.537435ms)
    Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 30.738392ms)
    Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 30.535048ms)
    Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 30.414486ms)
    Mar 27 20:56:52.469: INFO: (13) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 30.514108ms)
    Mar 27 20:56:52.470: INFO: (13) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 30.875129ms)
    Mar 27 20:56:52.471: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 32.504643ms)
    Mar 27 20:56:52.471: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 32.556514ms)
    Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 32.833247ms)
    Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 33.427402ms)
    Mar 27 20:56:52.472: INFO: (13) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 33.677887ms)
    Mar 27 20:56:52.496: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.358573ms)
    Mar 27 20:56:52.497: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.30665ms)
    Mar 27 20:56:52.498: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.443519ms)
    Mar 27 20:56:52.498: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 24.761707ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 26.030311ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.532965ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.935513ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.782344ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.537484ms)
    Mar 27 20:56:52.499: INFO: (14) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 26.850251ms)
    Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 29.925133ms)
    Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 29.53691ms)
    Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 29.561998ms)
    Mar 27 20:56:52.503: INFO: (14) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 30.116569ms)
    Mar 27 20:56:52.504: INFO: (14) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 30.867574ms)
    Mar 27 20:56:52.504: INFO: (14) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 30.759097ms)
    Mar 27 20:56:52.522: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 16.622486ms)
    Mar 27 20:56:52.528: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 22.439059ms)
    Mar 27 20:56:52.529: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 23.438055ms)
    Mar 27 20:56:52.529: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.182933ms)
    Mar 27 20:56:52.530: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 24.267704ms)
    Mar 27 20:56:52.531: INFO: (15) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.307327ms)
    Mar 27 20:56:52.531: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.321687ms)
    Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 25.836074ms)
    Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.037926ms)
    Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 26.78638ms)
    Mar 27 20:56:52.532: INFO: (15) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 27.12959ms)
    Mar 27 20:56:52.537: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 31.695489ms)
    Mar 27 20:56:52.542: INFO: (15) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 36.362203ms)
    Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 37.252644ms)
    Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 37.084233ms)
    Mar 27 20:56:52.543: INFO: (15) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 37.36208ms)
    Mar 27 20:56:52.563: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 20.006333ms)
    Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 22.996978ms)
    Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 23.997011ms)
    Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 24.774557ms)
    Mar 27 20:56:52.569: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 25.037663ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 25.56824ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 24.087128ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 24.931612ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 25.817626ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 23.792117ms)
    Mar 27 20:56:52.570: INFO: (16) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 26.052608ms)
    Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 30.90857ms)
    Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 30.081187ms)
    Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 29.095107ms)
    Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 29.781904ms)
    Mar 27 20:56:52.575: INFO: (16) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 30.28585ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 34.424989ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 33.761214ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 32.489693ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 33.328795ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 34.466982ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 31.480698ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 34.056588ms)
    Mar 27 20:56:52.610: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 33.070657ms)
    Mar 27 20:56:52.611: INFO: (17) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 34.397997ms)
    Mar 27 20:56:52.615: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 38.945563ms)
    Mar 27 20:56:52.615: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 39.536831ms)
    Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 39.13481ms)
    Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 40.017155ms)
    Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 37.756731ms)
    Mar 27 20:56:52.616: INFO: (17) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 40.077235ms)
    Mar 27 20:56:52.618: INFO: (17) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 39.973954ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 42.63361ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 42.378938ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 42.629059ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 42.856821ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 43.346602ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 43.450706ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 43.279236ms)
    Mar 27 20:56:52.662: INFO: (18) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 43.965651ms)
    Mar 27 20:56:52.663: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 44.330955ms)
    Mar 27 20:56:52.669: INFO: (18) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 49.695519ms)
    Mar 27 20:56:52.669: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 49.409415ms)
    Mar 27 20:56:52.672: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 52.99862ms)
    Mar 27 20:56:52.672: INFO: (18) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 53.174093ms)
    Mar 27 20:56:52.673: INFO: (18) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 54.237266ms)
    Mar 27 20:56:52.679: INFO: (18) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 60.202872ms)
    Mar 27 20:56:52.679: INFO: (18) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 60.367947ms)
    Mar 27 20:56:52.694: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:460/proxy/: tls baz (200; 15.361346ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">... (200; 21.292241ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw/proxy/rewriteme">test</a> (200; 21.442403ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:1080/proxy/rewriteme">test<... (200; 21.875478ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 22.061251ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/: <a href="/api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:443/proxy/tlsrewritem... (200; 21.892222ms)
    Mar 27 20:56:52.701: INFO: (19) /api/v1/namespaces/proxy-6956/pods/proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 22.106341ms)
    Mar 27 20:56:52.705: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:160/proxy/: foo (200; 25.46421ms)
    Mar 27 20:56:52.706: INFO: (19) /api/v1/namespaces/proxy-6956/pods/http:proxy-service-xgpb7-54kqw:162/proxy/: bar (200; 26.259835ms)
    Mar 27 20:56:52.706: INFO: (19) /api/v1/namespaces/proxy-6956/pods/https:proxy-service-xgpb7-54kqw:462/proxy/: tls qux (200; 26.242802ms)
    Mar 27 20:56:52.725: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname1/proxy/: tls baz (200; 45.626844ms)
    Mar 27 20:56:52.731: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname2/proxy/: bar (200; 51.754077ms)
    Mar 27 20:56:52.731: INFO: (19) /api/v1/namespaces/proxy-6956/services/http:proxy-service-xgpb7:portname1/proxy/: foo (200; 51.987404ms)
    Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname1/proxy/: foo (200; 67.495269ms)
    Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/https:proxy-service-xgpb7:tlsportname2/proxy/: tls qux (200; 67.719469ms)
    Mar 27 20:56:52.747: INFO: (19) /api/v1/namespaces/proxy-6956/services/proxy-service-xgpb7:portname2/proxy/: bar (200; 68.221573ms)
    STEP: deleting ReplicationController proxy-service-xgpb7 in namespace proxy-6956, will wait for the garbage collector to delete the pods 03/27/23 20:56:52.748
    Mar 27 20:56:52.851: INFO: Deleting ReplicationController proxy-service-xgpb7 took: 23.436471ms
    Mar 27 20:56:52.952: INFO: Terminating ReplicationController proxy-service-xgpb7 pods took: 100.729292ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6956" for this suite. 03/27/23 20:56:54.172
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:54.198
Mar 27 20:56:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-pred 03/27/23 20:56:54.2
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:54.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:54.265
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 27 20:56:54.277: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 20:56:54.308: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 20:56:54.333: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.175 before test
Mar 27 20:56:54.364: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.364: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 20:56:54.364: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.365: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 20:56:54.365: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.365: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 20:56:54.365: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.365: INFO: 	Container coredns ready: true, restart count 0
Mar 27 20:56:54.365: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.365: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 20:56:54.366: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.366: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 20:56:54.366: INFO: 	Container pause ready: true, restart count 0
Mar 27 20:56:54.366: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.366: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 20:56:54.366: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.366: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 20:56:54.366: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.367: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 20:56:54.367: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.367: INFO: 	Container e2e ready: true, restart count 0
Mar 27 20:56:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 20:56:54.367: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 20:56:54.368: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 20:56:54.368: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.368: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar 27 20:56:54.368: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.177 before test
Mar 27 20:56:54.394: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-4dv5m from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 20:56:54.394: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 20:56:54.394: INFO: calico-typha-7f67cb7cc9-r6hcj from kube-system started at 2023-03-27 19:00:01 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 20:56:54.394: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 20:56:54.394: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 20:56:54.394: INFO: 	Container pause ready: true, restart count 0
Mar 27 20:56:54.394: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 20:56:54.394: INFO: ingress-cluster-healthcheck-b895f86ff-ttch8 from kube-system started at 2023-03-27 19:08:06 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Mar 27 20:56:54.394: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 20:56:54.394: INFO: metrics-server-6c65f45547-2qssm from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 20:56:54.394: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 20:56:54.394: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 20:56:54.394: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-tqnwf from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 20:56:54.394: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 20:56:54.394: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 20:56:54.394: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 20:56:54.394: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.178 before test
Mar 27 20:56:54.425: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.425: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 20:56:54.425: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.425: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 20:56:54.425: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.425: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 20:56:54.425: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container coredns ready: true, restart count 0
Mar 27 20:56:54.426: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container coredns ready: true, restart count 0
Mar 27 20:56:54.426: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 20:56:54.426: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 20:56:54.426: INFO: 	Container pause ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 20:56:54.426: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 27 20:56:54.426: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 20:56:54.426: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 27 20:56:54.426: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 20:56:54.426: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 20:56:54.426: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 20:56:54.426: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 20:56:54.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 20:56:54.427: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/27/23 20:56:54.427
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1750622ee1bfb670], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 03/27/23 20:56:54.521
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:56:55.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6994" for this suite. 03/27/23 20:56:55.536
------------------------------
• [1.361 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:54.198
    Mar 27 20:56:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-pred 03/27/23 20:56:54.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:54.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:54.265
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 27 20:56:54.277: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 20:56:54.308: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 20:56:54.333: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.175 before test
    Mar 27 20:56:54.364: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.364: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 20:56:54.364: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.365: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 20:56:54.365: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.365: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 20:56:54.365: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.365: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 20:56:54.365: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.365: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 20:56:54.366: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.366: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 20:56:54.366: INFO: 	Container pause ready: true, restart count 0
    Mar 27 20:56:54.366: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.366: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 20:56:54.366: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.366: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 20:56:54.366: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.367: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 20:56:54.367: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.367: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 20:56:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 20:56:54.367: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 20:56:54.368: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 20:56:54.368: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.368: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Mar 27 20:56:54.368: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.177 before test
    Mar 27 20:56:54.394: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-4dv5m from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: calico-typha-7f67cb7cc9-r6hcj from kube-system started at 2023-03-27 19:00:01 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: 	Container pause ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: ingress-cluster-healthcheck-b895f86ff-ttch8 from kube-system started at 2023-03-27 19:08:06 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: metrics-server-6c65f45547-2qssm from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-tqnwf from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 20:56:54.394: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.178 before test
    Mar 27 20:56:54.425: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.425: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 20:56:54.425: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.425: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 20:56:54.425: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.425: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 20:56:54.425: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: 	Container pause ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 20:56:54.426: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 20:56:54.426: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 20:56:54.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 20:56:54.427: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/27/23 20:56:54.427
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1750622ee1bfb670], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 03/27/23 20:56:54.521
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:56:55.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6994" for this suite. 03/27/23 20:56:55.536
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:56:55.563
Mar 27 20:56:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 20:56:55.565
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:55.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:55.64
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/27/23 20:56:55.666
Mar 27 20:56:55.686: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5107" to be "running and ready"
Mar 27 20:56:55.698: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.345038ms
Mar 27 20:56:55.698: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:56:57.710: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02472743s
Mar 27 20:56:57.710: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 20:56:57.710: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 03/27/23 20:56:57.723
Mar 27 20:56:57.740: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5107" to be "running and ready"
Mar 27 20:56:57.752: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.640653ms
Mar 27 20:56:57.752: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 20:56:59.764: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024295695s
Mar 27 20:56:59.764: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 27 20:56:59.764: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/27/23 20:56:59.776
Mar 27 20:56:59.796: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 20:56:59.808: INFO: Pod pod-with-prestop-http-hook still exists
Mar 27 20:57:01.810: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 20:57:01.822: INFO: Pod pod-with-prestop-http-hook still exists
Mar 27 20:57:03.809: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 27 20:57:03.822: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/27/23 20:57:03.822
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 27 20:57:03.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5107" for this suite. 03/27/23 20:57:03.916
------------------------------
• [SLOW TEST] [8.376 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:56:55.563
    Mar 27 20:56:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 20:56:55.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:56:55.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:56:55.64
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 20:56:55.666
    Mar 27 20:56:55.686: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5107" to be "running and ready"
    Mar 27 20:56:55.698: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.345038ms
    Mar 27 20:56:55.698: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:56:57.710: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.02472743s
    Mar 27 20:56:57.710: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 20:56:57.710: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 03/27/23 20:56:57.723
    Mar 27 20:56:57.740: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5107" to be "running and ready"
    Mar 27 20:56:57.752: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.640653ms
    Mar 27 20:56:57.752: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 20:56:59.764: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024295695s
    Mar 27 20:56:59.764: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 27 20:56:59.764: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/27/23 20:56:59.776
    Mar 27 20:56:59.796: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 20:56:59.808: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 27 20:57:01.810: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 20:57:01.822: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 27 20:57:03.809: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 27 20:57:03.822: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/27/23 20:57:03.822
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:57:03.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5107" for this suite. 03/27/23 20:57:03.916
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:57:03.94
Mar 27 20:57:03.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 20:57:03.942
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:03.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:04
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 20:57:04.082
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 20:57:04.097
Mar 27 20:57:04.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 20:57:04.123: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:05.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 20:57:05.159: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:06.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 20:57:06.156: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:07.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 20:57:07.185: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:08.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 20:57:08.163: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:09.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 20:57:09.158: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:10.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 20:57:10.157: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 20:57:11.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 20:57:11.155: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/27/23 20:57:11.167
STEP: DeleteCollection of the DaemonSets 03/27/23 20:57:11.184
STEP: Verify that ReplicaSets have been deleted 03/27/23 20:57:11.209
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Mar 27 20:57:11.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19525"},"items":null}

Mar 27 20:57:11.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19528"},"items":[{"metadata":{"name":"daemon-set-mqpnp","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"33c0058c-d468-43a4-9726-353bef24439b","resourceVersion":"19525","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c7fe448fb2ca3b60e69ae6b4892e3d229b2d11c03fd31db6d359a4b1b15ef8c0","cni.projectcalico.org/podIP":"172.30.56.121/32","cni.projectcalico.org/podIPs":"172.30.56.121/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cjm6v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cjm6v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.175","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.175"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.175","podIP":"172.30.56.121","podIPs":[{"ip":"172.30.56.121"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://411f80303f12ad5774d73dcdfe58d2c24d0a9bd1c0124dae1380a787c84ea43b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qwwrw","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"1001a0e3-bf7d-45d4-be38-d3d332bb3721","resourceVersion":"19526","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"07e9f004aac2ffcc2e3fee553aba44c7de0604797b0d69d6928ee652a255195b","cni.projectcalico.org/podIP":"172.30.4.102/32","cni.projectcalico.org/podIPs":"172.30.4.102/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qblnt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qblnt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.178","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.178"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.178","podIP":"172.30.4.102","podIPs":[{"ip":"172.30.4.102"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e73e04abeac0298bd3f52cb484192c4774f2bebdd5ec7ce742c796e502722200","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t46s8","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"867e7e92-c909-41f7-8d86-67f121244776","resourceVersion":"19523","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c667504aa04f81388b6b7bf75306ade316e4627ae4505d3f9e953e24e1d6421d","cni.projectcalico.org/podIP":"172.30.85.187/32","cni.projectcalico.org/podIPs":"172.30.85.187/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2gh7w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2gh7w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.177","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.177"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.177","podIP":"172.30.85.187","podIPs":[{"ip":"172.30.85.187"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a7806d2579123df715a514fd5315b632f58f94b6c927c6e937919ec86d4da78e","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 20:57:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8974" for this suite. 03/27/23 20:57:11.361
------------------------------
• [SLOW TEST] [7.444 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:57:03.94
    Mar 27 20:57:03.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 20:57:03.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:03.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:04
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 20:57:04.082
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 20:57:04.097
    Mar 27 20:57:04.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 20:57:04.123: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:05.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 20:57:05.159: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:06.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 20:57:06.156: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:07.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 20:57:07.185: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:08.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 20:57:08.163: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:09.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 20:57:09.158: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:10.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 20:57:10.157: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 20:57:11.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 20:57:11.155: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/27/23 20:57:11.167
    STEP: DeleteCollection of the DaemonSets 03/27/23 20:57:11.184
    STEP: Verify that ReplicaSets have been deleted 03/27/23 20:57:11.209
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Mar 27 20:57:11.250: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19525"},"items":null}

    Mar 27 20:57:11.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19528"},"items":[{"metadata":{"name":"daemon-set-mqpnp","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"33c0058c-d468-43a4-9726-353bef24439b","resourceVersion":"19525","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c7fe448fb2ca3b60e69ae6b4892e3d229b2d11c03fd31db6d359a4b1b15ef8c0","cni.projectcalico.org/podIP":"172.30.56.121/32","cni.projectcalico.org/podIPs":"172.30.56.121/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cjm6v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cjm6v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.175","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.175"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.175","podIP":"172.30.56.121","podIPs":[{"ip":"172.30.56.121"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://411f80303f12ad5774d73dcdfe58d2c24d0a9bd1c0124dae1380a787c84ea43b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qwwrw","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"1001a0e3-bf7d-45d4-be38-d3d332bb3721","resourceVersion":"19526","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"07e9f004aac2ffcc2e3fee553aba44c7de0604797b0d69d6928ee652a255195b","cni.projectcalico.org/podIP":"172.30.4.102/32","cni.projectcalico.org/podIPs":"172.30.4.102/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qblnt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qblnt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.178","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.178"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.178","podIP":"172.30.4.102","podIPs":[{"ip":"172.30.4.102"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e73e04abeac0298bd3f52cb484192c4774f2bebdd5ec7ce742c796e502722200","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t46s8","generateName":"daemon-set-","namespace":"daemonsets-8974","uid":"867e7e92-c909-41f7-8d86-67f121244776","resourceVersion":"19523","creationTimestamp":"2023-03-27T20:57:04Z","deletionTimestamp":"2023-03-27T20:57:41Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c667504aa04f81388b6b7bf75306ade316e4627ae4505d3f9e953e24e1d6421d","cni.projectcalico.org/podIP":"172.30.85.187/32","cni.projectcalico.org/podIPs":"172.30.85.187/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b17da6f2-686e-48f2-bc98-c17efbf0d64e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17da6f2-686e-48f2-bc98-c17efbf0d64e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-27T20:57:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2gh7w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2gh7w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.176.99.177","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.176.99.177"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:06Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:06Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-27T20:57:04Z"}],"hostIP":"10.176.99.177","podIP":"172.30.85.187","podIPs":[{"ip":"172.30.85.187"}],"startTime":"2023-03-27T20:57:04Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-27T20:57:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a7806d2579123df715a514fd5315b632f58f94b6c927c6e937919ec86d4da78e","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:57:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8974" for this suite. 03/27/23 20:57:11.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:57:11.389
Mar 27 20:57:11.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 20:57:11.39
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:11.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:11.452
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 03/27/23 20:57:11.463
Mar 27 20:57:11.491: INFO: Waiting up to 5m0s for pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc" in namespace "var-expansion-2920" to be "Succeeded or Failed"
Mar 27 20:57:11.502: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.10297ms
Mar 27 20:57:13.514: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023021753s
Mar 27 20:57:15.516: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024516749s
STEP: Saw pod success 03/27/23 20:57:15.516
Mar 27 20:57:15.516: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc" satisfied condition "Succeeded or Failed"
Mar 27 20:57:15.526: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc container dapi-container: <nil>
STEP: delete the pod 03/27/23 20:57:15.556
Mar 27 20:57:15.592: INFO: Waiting for pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc to disappear
Mar 27 20:57:15.602: INFO: Pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 20:57:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2920" for this suite. 03/27/23 20:57:15.62
------------------------------
• [4.262 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:57:11.389
    Mar 27 20:57:11.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 20:57:11.39
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:11.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:11.452
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 03/27/23 20:57:11.463
    Mar 27 20:57:11.491: INFO: Waiting up to 5m0s for pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc" in namespace "var-expansion-2920" to be "Succeeded or Failed"
    Mar 27 20:57:11.502: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.10297ms
    Mar 27 20:57:13.514: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023021753s
    Mar 27 20:57:15.516: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024516749s
    STEP: Saw pod success 03/27/23 20:57:15.516
    Mar 27 20:57:15.516: INFO: Pod "var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc" satisfied condition "Succeeded or Failed"
    Mar 27 20:57:15.526: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc container dapi-container: <nil>
    STEP: delete the pod 03/27/23 20:57:15.556
    Mar 27 20:57:15.592: INFO: Waiting for pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc to disappear
    Mar 27 20:57:15.602: INFO: Pod var-expansion-608206fd-8f4c-4eb9-a213-526a9f1b18cc no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:57:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2920" for this suite. 03/27/23 20:57:15.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:57:15.654
Mar 27 20:57:15.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 20:57:15.655
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:15.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:15.718
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 03/27/23 20:57:15.729
Mar 27 20:57:15.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e" in namespace "downward-api-109" to be "Succeeded or Failed"
Mar 27 20:57:15.765: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.869892ms
Mar 27 20:57:17.777: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023927187s
Mar 27 20:57:19.780: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026249712s
STEP: Saw pod success 03/27/23 20:57:19.78
Mar 27 20:57:19.780: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e" satisfied condition "Succeeded or Failed"
Mar 27 20:57:19.792: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e container client-container: <nil>
STEP: delete the pod 03/27/23 20:57:19.82
Mar 27 20:57:19.860: INFO: Waiting for pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e to disappear
Mar 27 20:57:19.872: INFO: Pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 20:57:19.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-109" for this suite. 03/27/23 20:57:19.892
------------------------------
• [4.264 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:57:15.654
    Mar 27 20:57:15.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 20:57:15.655
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:15.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:15.718
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 03/27/23 20:57:15.729
    Mar 27 20:57:15.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e" in namespace "downward-api-109" to be "Succeeded or Failed"
    Mar 27 20:57:15.765: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.869892ms
    Mar 27 20:57:17.777: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023927187s
    Mar 27 20:57:19.780: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026249712s
    STEP: Saw pod success 03/27/23 20:57:19.78
    Mar 27 20:57:19.780: INFO: Pod "downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e" satisfied condition "Succeeded or Failed"
    Mar 27 20:57:19.792: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e container client-container: <nil>
    STEP: delete the pod 03/27/23 20:57:19.82
    Mar 27 20:57:19.860: INFO: Waiting for pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e to disappear
    Mar 27 20:57:19.872: INFO: Pod downwardapi-volume-4f4ebbb8-91e5-4c29-acc5-7b965bb56e3e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:57:19.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-109" for this suite. 03/27/23 20:57:19.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:57:19.919
Mar 27 20:57:19.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 20:57:19.92
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:19.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:19.981
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 03/27/23 20:57:19.991
STEP: Ensuring active pods == parallelism 03/27/23 20:57:20.007
STEP: delete a job 03/27/23 20:57:24.024
STEP: deleting Job.batch foo in namespace job-6064, will wait for the garbage collector to delete the pods 03/27/23 20:57:24.024
Mar 27 20:57:24.105: INFO: Deleting Job.batch foo took: 19.313571ms
Mar 27 20:57:24.205: INFO: Terminating Job.batch foo pods took: 100.686636ms
STEP: Ensuring job was deleted 03/27/23 20:57:56.406
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 20:57:56.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6064" for this suite. 03/27/23 20:57:56.438
------------------------------
• [SLOW TEST] [36.540 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:57:19.919
    Mar 27 20:57:19.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 20:57:19.92
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:19.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:19.981
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 03/27/23 20:57:19.991
    STEP: Ensuring active pods == parallelism 03/27/23 20:57:20.007
    STEP: delete a job 03/27/23 20:57:24.024
    STEP: deleting Job.batch foo in namespace job-6064, will wait for the garbage collector to delete the pods 03/27/23 20:57:24.024
    Mar 27 20:57:24.105: INFO: Deleting Job.batch foo took: 19.313571ms
    Mar 27 20:57:24.205: INFO: Terminating Job.batch foo pods took: 100.686636ms
    STEP: Ensuring job was deleted 03/27/23 20:57:56.406
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:57:56.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6064" for this suite. 03/27/23 20:57:56.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:57:56.461
Mar 27 20:57:56.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 20:57:56.462
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:56.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:56.729
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 03/27/23 20:57:56.74
Mar 27 20:57:56.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082" in namespace "downward-api-2411" to be "Succeeded or Failed"
Mar 27 20:57:56.775: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535049ms
Mar 27 20:57:58.788: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024869232s
Mar 27 20:58:00.787: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024125011s
STEP: Saw pod success 03/27/23 20:58:00.788
Mar 27 20:58:00.788: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082" satisfied condition "Succeeded or Failed"
Mar 27 20:58:00.799: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 container client-container: <nil>
STEP: delete the pod 03/27/23 20:58:00.826
Mar 27 20:58:00.861: INFO: Waiting for pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 to disappear
Mar 27 20:58:00.872: INFO: Pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 20:58:00.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2411" for this suite. 03/27/23 20:58:00.891
------------------------------
• [4.452 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:57:56.461
    Mar 27 20:57:56.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 20:57:56.462
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:57:56.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:57:56.729
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 03/27/23 20:57:56.74
    Mar 27 20:57:56.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082" in namespace "downward-api-2411" to be "Succeeded or Failed"
    Mar 27 20:57:56.775: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535049ms
    Mar 27 20:57:58.788: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024869232s
    Mar 27 20:58:00.787: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024125011s
    STEP: Saw pod success 03/27/23 20:58:00.788
    Mar 27 20:58:00.788: INFO: Pod "downwardapi-volume-cb795827-1383-40bd-8871-765533560082" satisfied condition "Succeeded or Failed"
    Mar 27 20:58:00.799: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 container client-container: <nil>
    STEP: delete the pod 03/27/23 20:58:00.826
    Mar 27 20:58:00.861: INFO: Waiting for pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 to disappear
    Mar 27 20:58:00.872: INFO: Pod downwardapi-volume-cb795827-1383-40bd-8871-765533560082 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 20:58:00.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2411" for this suite. 03/27/23 20:58:00.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 20:58:00.918
Mar 27 20:58:00.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename cronjob 03/27/23 20:58:00.92
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:58:00.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:58:00.981
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/27/23 20:58:00.993
STEP: Ensuring no jobs are scheduled 03/27/23 20:58:01.017
STEP: Ensuring no job exists by listing jobs explicitly 03/27/23 21:03:01.045
STEP: Removing cronjob 03/27/23 21:03:01.055
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9054" for this suite. 03/27/23 21:03:01.095
------------------------------
• [SLOW TEST] [300.200 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 20:58:00.918
    Mar 27 20:58:00.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename cronjob 03/27/23 20:58:00.92
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 20:58:00.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 20:58:00.981
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/27/23 20:58:00.993
    STEP: Ensuring no jobs are scheduled 03/27/23 20:58:01.017
    STEP: Ensuring no job exists by listing jobs explicitly 03/27/23 21:03:01.045
    STEP: Removing cronjob 03/27/23 21:03:01.055
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9054" for this suite. 03/27/23 21:03:01.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:01.122
Mar 27 21:03:01.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:03:01.123
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:01.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:01.201
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-406966df-134d-406b-bc72-82f08c7dbe6a 03/27/23 21:03:01.214
STEP: Creating a pod to test consume secrets 03/27/23 21:03:01.228
Mar 27 21:03:01.254: INFO: Waiting up to 5m0s for pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0" in namespace "secrets-9879" to be "Succeeded or Failed"
Mar 27 21:03:01.292: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.908393ms
Mar 27 21:03:03.306: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05147185s
Mar 27 21:03:05.307: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052615703s
STEP: Saw pod success 03/27/23 21:03:05.307
Mar 27 21:03:05.308: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0" satisfied condition "Succeeded or Failed"
Mar 27 21:03:05.320: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:03:05.529
Mar 27 21:03:05.567: INFO: Waiting for pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 to disappear
Mar 27 21:03:05.580: INFO: Pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:05.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9879" for this suite. 03/27/23 21:03:05.6
------------------------------
• [4.503 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:01.122
    Mar 27 21:03:01.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:03:01.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:01.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:01.201
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-406966df-134d-406b-bc72-82f08c7dbe6a 03/27/23 21:03:01.214
    STEP: Creating a pod to test consume secrets 03/27/23 21:03:01.228
    Mar 27 21:03:01.254: INFO: Waiting up to 5m0s for pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0" in namespace "secrets-9879" to be "Succeeded or Failed"
    Mar 27 21:03:01.292: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.908393ms
    Mar 27 21:03:03.306: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05147185s
    Mar 27 21:03:05.307: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052615703s
    STEP: Saw pod success 03/27/23 21:03:05.307
    Mar 27 21:03:05.308: INFO: Pod "pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0" satisfied condition "Succeeded or Failed"
    Mar 27 21:03:05.320: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:03:05.529
    Mar 27 21:03:05.567: INFO: Waiting for pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 to disappear
    Mar 27 21:03:05.580: INFO: Pod pod-secrets-e2ececd0-035d-4cb0-8609-c8bf8d76b9b0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:05.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9879" for this suite. 03/27/23 21:03:05.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:05.626
Mar 27 21:03:05.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:03:05.628
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:05.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:05.689
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/27/23 21:03:05.7
Mar 27 21:03:05.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:03:08.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:16.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2422" for this suite. 03/27/23 21:03:16.599
------------------------------
• [SLOW TEST] [10.999 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:05.626
    Mar 27 21:03:05.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:03:05.628
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:05.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:05.689
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/27/23 21:03:05.7
    Mar 27 21:03:05.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:03:08.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:16.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2422" for this suite. 03/27/23 21:03:16.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:16.628
Mar 27 21:03:16.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:03:16.629
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:16.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:16.695
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 03/27/23 21:03:16.711
Mar 27 21:03:16.744: INFO: Waiting up to 5m0s for pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec" in namespace "downward-api-2997" to be "Succeeded or Failed"
Mar 27 21:03:16.764: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.709776ms
Mar 27 21:03:18.782: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037244756s
Mar 27 21:03:20.783: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038894441s
STEP: Saw pod success 03/27/23 21:03:20.783
Mar 27 21:03:20.784: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec" satisfied condition "Succeeded or Failed"
Mar 27 21:03:20.800: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:03:20.892
Mar 27 21:03:21.006: INFO: Waiting for pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec to disappear
Mar 27 21:03:21.023: INFO: Pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2997" for this suite. 03/27/23 21:03:21.05
------------------------------
• [4.454 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:16.628
    Mar 27 21:03:16.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:03:16.629
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:16.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:16.695
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 03/27/23 21:03:16.711
    Mar 27 21:03:16.744: INFO: Waiting up to 5m0s for pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec" in namespace "downward-api-2997" to be "Succeeded or Failed"
    Mar 27 21:03:16.764: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.709776ms
    Mar 27 21:03:18.782: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037244756s
    Mar 27 21:03:20.783: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038894441s
    STEP: Saw pod success 03/27/23 21:03:20.783
    Mar 27 21:03:20.784: INFO: Pod "downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec" satisfied condition "Succeeded or Failed"
    Mar 27 21:03:20.800: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:03:20.892
    Mar 27 21:03:21.006: INFO: Waiting for pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec to disappear
    Mar 27 21:03:21.023: INFO: Pod downward-api-9f0f10ac-308b-40ed-9372-c80216c36fec no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2997" for this suite. 03/27/23 21:03:21.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:21.084
Mar 27 21:03:21.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:03:21.085
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:21.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:21.158
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 21:03:21.173
Mar 27 21:03:21.207: INFO: Waiting up to 5m0s for pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c" in namespace "emptydir-2999" to be "Succeeded or Failed"
Mar 27 21:03:21.228: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.162393ms
Mar 27 21:03:23.247: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039756459s
Mar 27 21:03:25.245: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037937666s
STEP: Saw pod success 03/27/23 21:03:25.245
Mar 27 21:03:25.246: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c" satisfied condition "Succeeded or Failed"
Mar 27 21:03:25.263: INFO: Trying to get logs from node 10.176.99.177 pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c container test-container: <nil>
STEP: delete the pod 03/27/23 21:03:25.295
Mar 27 21:03:25.344: INFO: Waiting for pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c to disappear
Mar 27 21:03:25.361: INFO: Pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:25.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2999" for this suite. 03/27/23 21:03:25.384
------------------------------
• [4.326 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:21.084
    Mar 27 21:03:21.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:03:21.085
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:21.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:21.158
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 21:03:21.173
    Mar 27 21:03:21.207: INFO: Waiting up to 5m0s for pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c" in namespace "emptydir-2999" to be "Succeeded or Failed"
    Mar 27 21:03:21.228: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.162393ms
    Mar 27 21:03:23.247: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039756459s
    Mar 27 21:03:25.245: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037937666s
    STEP: Saw pod success 03/27/23 21:03:25.245
    Mar 27 21:03:25.246: INFO: Pod "pod-0ceaae98-1cc2-4258-815c-e2f96115003c" satisfied condition "Succeeded or Failed"
    Mar 27 21:03:25.263: INFO: Trying to get logs from node 10.176.99.177 pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c container test-container: <nil>
    STEP: delete the pod 03/27/23 21:03:25.295
    Mar 27 21:03:25.344: INFO: Waiting for pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c to disappear
    Mar 27 21:03:25.361: INFO: Pod pod-0ceaae98-1cc2-4258-815c-e2f96115003c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:25.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2999" for this suite. 03/27/23 21:03:25.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:25.419
Mar 27 21:03:25.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:03:25.42
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:25.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:25.484
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-bb71186d-2347-4c28-b5b0-054818c55713 03/27/23 21:03:25.499
STEP: Creating a pod to test consume configMaps 03/27/23 21:03:25.518
Mar 27 21:03:25.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce" in namespace "configmap-3417" to be "Succeeded or Failed"
Mar 27 21:03:25.571: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.232307ms
Mar 27 21:03:27.589: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035547558s
Mar 27 21:03:29.590: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035730679s
STEP: Saw pod success 03/27/23 21:03:29.59
Mar 27 21:03:29.590: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce" satisfied condition "Succeeded or Failed"
Mar 27 21:03:29.606: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:03:29.643
Mar 27 21:03:29.695: INFO: Waiting for pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce to disappear
Mar 27 21:03:29.712: INFO: Pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:29.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3417" for this suite. 03/27/23 21:03:29.734
------------------------------
• [4.340 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:25.419
    Mar 27 21:03:25.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:03:25.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:25.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:25.484
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-bb71186d-2347-4c28-b5b0-054818c55713 03/27/23 21:03:25.499
    STEP: Creating a pod to test consume configMaps 03/27/23 21:03:25.518
    Mar 27 21:03:25.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce" in namespace "configmap-3417" to be "Succeeded or Failed"
    Mar 27 21:03:25.571: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.232307ms
    Mar 27 21:03:27.589: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035547558s
    Mar 27 21:03:29.590: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035730679s
    STEP: Saw pod success 03/27/23 21:03:29.59
    Mar 27 21:03:29.590: INFO: Pod "pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce" satisfied condition "Succeeded or Failed"
    Mar 27 21:03:29.606: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:03:29.643
    Mar 27 21:03:29.695: INFO: Waiting for pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce to disappear
    Mar 27 21:03:29.712: INFO: Pod pod-configmaps-2af6805b-8afe-46ee-97f8-9319c79f8bce no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:29.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3417" for this suite. 03/27/23 21:03:29.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:29.762
Mar 27 21:03:29.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:03:29.763
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:29.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:29.827
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3596" for this suite. 03/27/23 21:03:30.033
------------------------------
• [0.297 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:29.762
    Mar 27 21:03:29.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:03:29.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:29.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:29.827
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3596" for this suite. 03/27/23 21:03:30.033
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:30.059
Mar 27 21:03:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 21:03:30.061
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:30.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:30.132
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992 03/27/23 21:03:30.148
Mar 27 21:03:30.184: INFO: Pod name my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Found 0 pods out of 1
Mar 27 21:03:35.205: INFO: Pod name my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Found 1 pods out of 1
Mar 27 21:03:35.205: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992" are running
Mar 27 21:03:35.205: INFO: Waiting up to 5m0s for pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" in namespace "replication-controller-2041" to be "running"
Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc": Phase="Running", Reason="", readiness=true. Elapsed: 22.68339ms
Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" satisfied condition "running"
Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:30 +0000 UTC Reason: Message:}])
Mar 27 21:03:35.227: INFO: Trying to dial the pod
Mar 27 21:03:40.329: INFO: Controller my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Got expected result from replica 1 [my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc]: "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:40.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2041" for this suite. 03/27/23 21:03:40.35
------------------------------
• [SLOW TEST] [10.318 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:30.059
    Mar 27 21:03:30.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 21:03:30.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:30.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:30.132
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992 03/27/23 21:03:30.148
    Mar 27 21:03:30.184: INFO: Pod name my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Found 0 pods out of 1
    Mar 27 21:03:35.205: INFO: Pod name my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Found 1 pods out of 1
    Mar 27 21:03:35.205: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992" are running
    Mar 27 21:03:35.205: INFO: Waiting up to 5m0s for pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" in namespace "replication-controller-2041" to be "running"
    Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc": Phase="Running", Reason="", readiness=true. Elapsed: 22.68339ms
    Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" satisfied condition "running"
    Mar 27 21:03:35.227: INFO: Pod "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:03:30 +0000 UTC Reason: Message:}])
    Mar 27 21:03:35.227: INFO: Trying to dial the pod
    Mar 27 21:03:40.329: INFO: Controller my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992: Got expected result from replica 1 [my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc]: "my-hostname-basic-32bb5c61-b562-4732-b544-b7e28d38c992-slbxc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:40.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2041" for this suite. 03/27/23 21:03:40.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:40.379
Mar 27 21:03:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context-test 03/27/23 21:03:40.38
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:40.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:40.466
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Mar 27 21:03:40.512: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857" in namespace "security-context-test-8217" to be "Succeeded or Failed"
Mar 27 21:03:40.530: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744683ms
Mar 27 21:03:42.548: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035918294s
Mar 27 21:03:44.549: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036983964s
Mar 27 21:03:44.549: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:44.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8217" for this suite. 03/27/23 21:03:44.573
------------------------------
• [4.220 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:40.379
    Mar 27 21:03:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context-test 03/27/23 21:03:40.38
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:40.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:40.466
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Mar 27 21:03:40.512: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857" in namespace "security-context-test-8217" to be "Succeeded or Failed"
    Mar 27 21:03:40.530: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744683ms
    Mar 27 21:03:42.548: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035918294s
    Mar 27 21:03:44.549: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036983964s
    Mar 27 21:03:44.549: INFO: Pod "busybox-readonly-false-e9d7e567-a535-442c-ad61-2a3976d03857" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:44.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8217" for this suite. 03/27/23 21:03:44.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:44.603
Mar 27 21:03:44.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:03:44.604
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:44.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:44.678
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-507b96d9-04d5-436e-bf74-83959d9d4c93 03/27/23 21:03:44.693
STEP: Creating a pod to test consume secrets 03/27/23 21:03:44.712
Mar 27 21:03:44.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7" in namespace "projected-8749" to be "Succeeded or Failed"
Mar 27 21:03:44.767: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.112464ms
Mar 27 21:03:46.786: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041136776s
Mar 27 21:03:48.785: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040268745s
STEP: Saw pod success 03/27/23 21:03:48.785
Mar 27 21:03:48.786: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7" satisfied condition "Succeeded or Failed"
Mar 27 21:03:48.803: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:03:48.837
Mar 27 21:03:48.888: INFO: Waiting for pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 to disappear
Mar 27 21:03:48.905: INFO: Pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:03:48.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8749" for this suite. 03/27/23 21:03:48.928
------------------------------
• [4.351 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:44.603
    Mar 27 21:03:44.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:03:44.604
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:44.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:44.678
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-507b96d9-04d5-436e-bf74-83959d9d4c93 03/27/23 21:03:44.693
    STEP: Creating a pod to test consume secrets 03/27/23 21:03:44.712
    Mar 27 21:03:44.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7" in namespace "projected-8749" to be "Succeeded or Failed"
    Mar 27 21:03:44.767: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.112464ms
    Mar 27 21:03:46.786: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041136776s
    Mar 27 21:03:48.785: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040268745s
    STEP: Saw pod success 03/27/23 21:03:48.785
    Mar 27 21:03:48.786: INFO: Pod "pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7" satisfied condition "Succeeded or Failed"
    Mar 27 21:03:48.803: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:03:48.837
    Mar 27 21:03:48.888: INFO: Waiting for pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 to disappear
    Mar 27 21:03:48.905: INFO: Pod pod-projected-secrets-9eac94eb-1526-4193-9f49-bfbcdf1a6be7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:03:48.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8749" for this suite. 03/27/23 21:03:48.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:03:48.957
Mar 27 21:03:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 21:03:48.959
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:49.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:49.029
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3184 03/27/23 21:03:49.047
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3184 03/27/23 21:03:49.065
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3184 03/27/23 21:03:49.085
Mar 27 21:03:49.102: INFO: Found 0 stateful pods, waiting for 1
Mar 27 21:03:59.122: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/27/23 21:03:59.122
Mar 27 21:03:59.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:03:59.463: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:03:59.463: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:03:59.463: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:03:59.481: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 27 21:04:09.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:04:09.502: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:04:09.568: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 27 21:04:09.568: INFO: ss-0  10.176.99.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
Mar 27 21:04:09.568: INFO: ss-1                 Pending         []
Mar 27 21:04:09.568: INFO: 
Mar 27 21:04:09.568: INFO: StatefulSet ss has not reached scale 3, at 2
Mar 27 21:04:10.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.977037476s
Mar 27 21:04:11.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95857571s
Mar 27 21:04:12.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.941338769s
Mar 27 21:04:13.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.920839751s
Mar 27 21:04:14.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897856865s
Mar 27 21:04:15.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.877618168s
Mar 27 21:04:16.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.858108826s
Mar 27 21:04:17.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.840295359s
Mar 27 21:04:18.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 820.631431ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3184 03/27/23 21:04:19.745
Mar 27 21:04:19.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:04:20.072: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:04:20.072: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:04:20.072: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:04:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:04:20.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 27 21:04:20.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:04:20.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:04:20.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:04:20.638: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 27 21:04:20.638: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:04:20.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:04:20.661: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:04:20.661: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:04:20.661: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/27/23 21:04:20.661
Mar 27 21:04:20.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:04:20.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:04:20.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:04:20.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:04:20.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:04:21.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:04:21.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:04:21.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:04:21.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:04:21.498: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:04:21.498: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:04:21.498: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:04:21.498: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:04:21.511: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 27 21:04:31.546: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:04:31.547: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:04:31.547: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:04:31.609: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 27 21:04:31.609: INFO: ss-0  10.176.99.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
Mar 27 21:04:31.609: INFO: ss-1  10.176.99.175  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
Mar 27 21:04:31.609: INFO: ss-2  10.176.99.178  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
Mar 27 21:04:31.609: INFO: 
Mar 27 21:04:31.609: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 27 21:04:32.635: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 27 21:04:32.635: INFO: ss-0  10.176.99.177  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
Mar 27 21:04:32.635: INFO: ss-2  10.176.99.178  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
Mar 27 21:04:32.635: INFO: 
Mar 27 21:04:32.635: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 27 21:04:33.653: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.956278559s
Mar 27 21:04:34.671: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.938127314s
Mar 27 21:04:35.690: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.920049323s
Mar 27 21:04:36.708: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.901401609s
Mar 27 21:04:37.728: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.882649145s
Mar 27 21:04:38.745: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.862708297s
Mar 27 21:04:39.763: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.845081791s
Mar 27 21:04:40.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 827.457643ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3184 03/27/23 21:04:41.782
Mar 27 21:04:41.805: INFO: Scaling statefulset ss to 0
Mar 27 21:04:41.857: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 21:04:41.871: INFO: Deleting all statefulset in ns statefulset-3184
Mar 27 21:04:41.887: INFO: Scaling statefulset ss to 0
Mar 27 21:04:41.934: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:04:41.949: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:04:42.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3184" for this suite. 03/27/23 21:04:42.028
------------------------------
• [SLOW TEST] [53.098 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:03:48.957
    Mar 27 21:03:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 21:03:48.959
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:03:49.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:03:49.029
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3184 03/27/23 21:03:49.047
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3184 03/27/23 21:03:49.065
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3184 03/27/23 21:03:49.085
    Mar 27 21:03:49.102: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 21:03:59.122: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/27/23 21:03:59.122
    Mar 27 21:03:59.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:03:59.463: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:03:59.463: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:03:59.463: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:03:59.481: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 27 21:04:09.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:04:09.502: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:04:09.568: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar 27 21:04:09.568: INFO: ss-0  10.176.99.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
    Mar 27 21:04:09.568: INFO: ss-1                 Pending         []
    Mar 27 21:04:09.568: INFO: 
    Mar 27 21:04:09.568: INFO: StatefulSet ss has not reached scale 3, at 2
    Mar 27 21:04:10.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.977037476s
    Mar 27 21:04:11.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95857571s
    Mar 27 21:04:12.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.941338769s
    Mar 27 21:04:13.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.920839751s
    Mar 27 21:04:14.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897856865s
    Mar 27 21:04:15.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.877618168s
    Mar 27 21:04:16.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.858108826s
    Mar 27 21:04:17.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.840295359s
    Mar 27 21:04:18.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 820.631431ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3184 03/27/23 21:04:19.745
    Mar 27 21:04:19.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:04:20.072: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:04:20.072: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:04:20.072: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:04:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:04:20.343: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 27 21:04:20.343: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:04:20.343: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:04:20.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:04:20.638: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 27 21:04:20.638: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:04:20.638: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:04:20.661: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:04:20.661: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:04:20.661: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/27/23 21:04:20.661
    Mar 27 21:04:20.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:04:20.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:04:20.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:04:20.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:04:20.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:04:21.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:04:21.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:04:21.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:04:21.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-3184 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:04:21.498: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:04:21.498: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:04:21.498: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:04:21.498: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:04:21.511: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 27 21:04:31.546: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:04:31.547: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:04:31.547: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:04:31.609: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar 27 21:04:31.609: INFO: ss-0  10.176.99.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
    Mar 27 21:04:31.609: INFO: ss-1  10.176.99.175  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
    Mar 27 21:04:31.609: INFO: ss-2  10.176.99.178  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
    Mar 27 21:04:31.609: INFO: 
    Mar 27 21:04:31.609: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 27 21:04:32.635: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Mar 27 21:04:32.635: INFO: ss-0  10.176.99.177  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:03:49 +0000 UTC  }]
    Mar 27 21:04:32.635: INFO: ss-2  10.176.99.178  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 21:04:09 +0000 UTC  }]
    Mar 27 21:04:32.635: INFO: 
    Mar 27 21:04:32.635: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar 27 21:04:33.653: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.956278559s
    Mar 27 21:04:34.671: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.938127314s
    Mar 27 21:04:35.690: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.920049323s
    Mar 27 21:04:36.708: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.901401609s
    Mar 27 21:04:37.728: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.882649145s
    Mar 27 21:04:38.745: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.862708297s
    Mar 27 21:04:39.763: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.845081791s
    Mar 27 21:04:40.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 827.457643ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3184 03/27/23 21:04:41.782
    Mar 27 21:04:41.805: INFO: Scaling statefulset ss to 0
    Mar 27 21:04:41.857: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 21:04:41.871: INFO: Deleting all statefulset in ns statefulset-3184
    Mar 27 21:04:41.887: INFO: Scaling statefulset ss to 0
    Mar 27 21:04:41.934: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:04:41.949: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:04:42.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3184" for this suite. 03/27/23 21:04:42.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:04:42.059
Mar 27 21:04:42.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption 03/27/23 21:04:42.061
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:42.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:42.125
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 03/27/23 21:04:42.157
STEP: Updating PodDisruptionBudget status 03/27/23 21:04:44.185
STEP: Waiting for all pods to be running 03/27/23 21:04:44.218
Mar 27 21:04:44.236: INFO: running pods: 0 < 1
STEP: locating a running pod 03/27/23 21:04:46.255
STEP: Waiting for the pdb to be processed 03/27/23 21:04:46.296
STEP: Patching PodDisruptionBudget status 03/27/23 21:04:46.338
STEP: Waiting for the pdb to be processed 03/27/23 21:04:46.367
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:04:46.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-592" for this suite. 03/27/23 21:04:46.405
------------------------------
• [4.372 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:04:42.059
    Mar 27 21:04:42.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption 03/27/23 21:04:42.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:42.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:42.125
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 03/27/23 21:04:42.157
    STEP: Updating PodDisruptionBudget status 03/27/23 21:04:44.185
    STEP: Waiting for all pods to be running 03/27/23 21:04:44.218
    Mar 27 21:04:44.236: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/27/23 21:04:46.255
    STEP: Waiting for the pdb to be processed 03/27/23 21:04:46.296
    STEP: Patching PodDisruptionBudget status 03/27/23 21:04:46.338
    STEP: Waiting for the pdb to be processed 03/27/23 21:04:46.367
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:04:46.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-592" for this suite. 03/27/23 21:04:46.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:04:46.434
Mar 27 21:04:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:04:46.436
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:46.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:46.495
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:04:50.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-202" for this suite. 03/27/23 21:04:50.614
------------------------------
• [4.204 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:04:46.434
    Mar 27 21:04:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:04:46.436
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:46.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:46.495
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:04:50.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-202" for this suite. 03/27/23 21:04:50.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:04:50.646
Mar 27 21:04:50.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 21:04:50.648
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:50.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:50.718
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/27/23 21:04:50.758
Mar 27 21:04:50.792: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2504" to be "running and ready"
Mar 27 21:04:50.809: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.940393ms
Mar 27 21:04:50.809: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:04:52.848: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056489664s
Mar 27 21:04:52.848: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:04:54.839: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.047108526s
Mar 27 21:04:54.839: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 21:04:54.839: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 03/27/23 21:04:54.857
Mar 27 21:04:54.879: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2504" to be "running and ready"
Mar 27 21:04:54.896: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.255489ms
Mar 27 21:04:54.896: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:04:56.915: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035433262s
Mar 27 21:04:56.915: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 27 21:04:56.915: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/27/23 21:04:56.931
STEP: delete the pod with lifecycle hook 03/27/23 21:04:57.036
Mar 27 21:04:57.066: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 27 21:04:57.083: INFO: Pod pod-with-poststart-http-hook still exists
Mar 27 21:04:59.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 27 21:04:59.104: INFO: Pod pod-with-poststart-http-hook still exists
Mar 27 21:05:01.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 27 21:05:01.100: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2504" for this suite. 03/27/23 21:05:01.124
------------------------------
• [SLOW TEST] [10.502 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:04:50.646
    Mar 27 21:04:50.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 21:04:50.648
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:04:50.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:04:50.718
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 21:04:50.758
    Mar 27 21:04:50.792: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2504" to be "running and ready"
    Mar 27 21:04:50.809: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.940393ms
    Mar 27 21:04:50.809: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:04:52.848: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056489664s
    Mar 27 21:04:52.848: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:04:54.839: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.047108526s
    Mar 27 21:04:54.839: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 21:04:54.839: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 03/27/23 21:04:54.857
    Mar 27 21:04:54.879: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2504" to be "running and ready"
    Mar 27 21:04:54.896: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.255489ms
    Mar 27 21:04:54.896: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:04:56.915: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.035433262s
    Mar 27 21:04:56.915: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 27 21:04:56.915: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/27/23 21:04:56.931
    STEP: delete the pod with lifecycle hook 03/27/23 21:04:57.036
    Mar 27 21:04:57.066: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 27 21:04:57.083: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 27 21:04:59.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 27 21:04:59.104: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 27 21:05:01.084: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 27 21:05:01.100: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:01.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2504" for this suite. 03/27/23 21:05:01.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:01.149
Mar 27 21:05:01.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-runtime 03/27/23 21:05:01.15
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:01.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:01.23
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 03/27/23 21:05:01.246
STEP: wait for the container to reach Succeeded 03/27/23 21:05:01.281
STEP: get the container status 03/27/23 21:05:06.392
STEP: the container should be terminated 03/27/23 21:05:06.411
STEP: the termination message should be set 03/27/23 21:05:06.411
Mar 27 21:05:06.411: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/27/23 21:05:06.411
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:06.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6888" for this suite. 03/27/23 21:05:06.492
------------------------------
• [SLOW TEST] [5.371 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:01.149
    Mar 27 21:05:01.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-runtime 03/27/23 21:05:01.15
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:01.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:01.23
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 03/27/23 21:05:01.246
    STEP: wait for the container to reach Succeeded 03/27/23 21:05:01.281
    STEP: get the container status 03/27/23 21:05:06.392
    STEP: the container should be terminated 03/27/23 21:05:06.411
    STEP: the termination message should be set 03/27/23 21:05:06.411
    Mar 27 21:05:06.411: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/27/23 21:05:06.411
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:06.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6888" for this suite. 03/27/23 21:05:06.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:06.52
Mar 27 21:05:06.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename podtemplate 03/27/23 21:05:06.521
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:06.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:06.584
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/27/23 21:05:06.6
STEP: Replace a pod template 03/27/23 21:05:06.619
Mar 27 21:05:06.658: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9352" for this suite. 03/27/23 21:05:06.685
------------------------------
• [0.191 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:06.52
    Mar 27 21:05:06.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename podtemplate 03/27/23 21:05:06.521
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:06.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:06.584
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/27/23 21:05:06.6
    STEP: Replace a pod template 03/27/23 21:05:06.619
    Mar 27 21:05:06.658: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:06.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9352" for this suite. 03/27/23 21:05:06.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:06.716
Mar 27 21:05:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:05:06.717
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:06.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:06.785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 03/27/23 21:05:06.8
Mar 27 21:05:06.801: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2458 proxy --unix-socket=/tmp/kubectl-proxy-unix1211497940/test'
STEP: retrieving proxy /api/ output 03/27/23 21:05:06.868
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:06.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2458" for this suite. 03/27/23 21:05:06.89
------------------------------
• [0.205 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:06.716
    Mar 27 21:05:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:05:06.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:06.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:06.785
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 03/27/23 21:05:06.8
    Mar 27 21:05:06.801: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2458 proxy --unix-socket=/tmp/kubectl-proxy-unix1211497940/test'
    STEP: retrieving proxy /api/ output 03/27/23 21:05:06.868
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:06.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2458" for this suite. 03/27/23 21:05:06.89
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:06.922
Mar 27 21:05:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:05:06.924
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.034
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 03/27/23 21:05:07.048
STEP: fetching the ConfigMap 03/27/23 21:05:07.066
STEP: patching the ConfigMap 03/27/23 21:05:07.084
STEP: listing all ConfigMaps in all namespaces with a label selector 03/27/23 21:05:07.103
STEP: deleting the ConfigMap by collection with a label selector 03/27/23 21:05:07.121
STEP: listing all ConfigMaps in test namespace 03/27/23 21:05:07.152
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:07.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6132" for this suite. 03/27/23 21:05:07.199
------------------------------
• [0.303 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:06.922
    Mar 27 21:05:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:05:06.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.034
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 03/27/23 21:05:07.048
    STEP: fetching the ConfigMap 03/27/23 21:05:07.066
    STEP: patching the ConfigMap 03/27/23 21:05:07.084
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/27/23 21:05:07.103
    STEP: deleting the ConfigMap by collection with a label selector 03/27/23 21:05:07.121
    STEP: listing all ConfigMaps in test namespace 03/27/23 21:05:07.152
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:07.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6132" for this suite. 03/27/23 21:05:07.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:07.226
Mar 27 21:05:07.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:05:07.227
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.299
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 03/27/23 21:05:07.314
STEP: Getting a ResourceQuota 03/27/23 21:05:07.361
STEP: Updating a ResourceQuota 03/27/23 21:05:07.374
STEP: Verifying a ResourceQuota was modified 03/27/23 21:05:07.389
STEP: Deleting a ResourceQuota 03/27/23 21:05:07.401
STEP: Verifying the deleted ResourceQuota 03/27/23 21:05:07.423
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:07.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1230" for this suite. 03/27/23 21:05:07.456
------------------------------
• [0.281 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:07.226
    Mar 27 21:05:07.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:05:07.227
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.299
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 03/27/23 21:05:07.314
    STEP: Getting a ResourceQuota 03/27/23 21:05:07.361
    STEP: Updating a ResourceQuota 03/27/23 21:05:07.374
    STEP: Verifying a ResourceQuota was modified 03/27/23 21:05:07.389
    STEP: Deleting a ResourceQuota 03/27/23 21:05:07.401
    STEP: Verifying the deleted ResourceQuota 03/27/23 21:05:07.423
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:07.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1230" for this suite. 03/27/23 21:05:07.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:07.516
Mar 27 21:05:07.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 21:05:07.517
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.588
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/27/23 21:05:07.626
STEP: delete the rc 03/27/23 21:05:12.66
STEP: wait for the rc to be deleted 03/27/23 21:05:12.694
STEP: Gathering metrics 03/27/23 21:05:13.726
W0327 21:05:13.775004      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 21:05:13.775: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 21:05:13.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4356" for this suite. 03/27/23 21:05:13.798
------------------------------
• [SLOW TEST] [6.312 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:07.516
    Mar 27 21:05:07.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 21:05:07.517
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:07.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:07.588
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/27/23 21:05:07.626
    STEP: delete the rc 03/27/23 21:05:12.66
    STEP: wait for the rc to be deleted 03/27/23 21:05:12.694
    STEP: Gathering metrics 03/27/23 21:05:13.726
    W0327 21:05:13.775004      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 21:05:13.775: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:05:13.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4356" for this suite. 03/27/23 21:05:13.798
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:05:13.83
Mar 27 21:05:13.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 21:05:13.831
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:13.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:13.921
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2300 03/27/23 21:05:13.939
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 03/27/23 21:05:13.961
STEP: Creating stateful set ss in namespace statefulset-2300 03/27/23 21:05:13.979
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2300 03/27/23 21:05:13.998
Mar 27 21:05:14.015: INFO: Found 0 stateful pods, waiting for 1
Mar 27 21:05:24.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/27/23 21:05:24.037
Mar 27 21:05:24.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:05:24.331: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:05:24.331: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:05:24.331: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:05:24.350: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 27 21:05:34.371: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:05:34.371: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:05:34.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998406s
Mar 27 21:05:35.453: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.981357845s
Mar 27 21:05:36.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.964310261s
Mar 27 21:05:37.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.946622828s
Mar 27 21:05:38.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.927248607s
Mar 27 21:05:39.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.909073604s
Mar 27 21:05:40.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.888627764s
Mar 27 21:05:41.565: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.871206508s
Mar 27 21:05:42.583: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.852354062s
Mar 27 21:05:43.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 833.945045ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2300 03/27/23 21:05:44.602
Mar 27 21:05:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:05:44.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:05:44.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:05:44.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:05:44.947: INFO: Found 1 stateful pods, waiting for 3
Mar 27 21:05:54.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:05:54.969: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:05:54.969: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/27/23 21:05:54.969
STEP: Scale down will halt with unhealthy stateful pod 03/27/23 21:05:54.969
Mar 27 21:05:55.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:05:55.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:05:55.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:05:55.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:05:55.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:05:55.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:05:55.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:05:55.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:05:55.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:05:55.944: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:05:55.944: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:05:55.944: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:05:55.944: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:05:55.958: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 27 21:06:05.997: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:06:05.997: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:06:05.997: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 27 21:06:06.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998474s
Mar 27 21:06:07.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981523789s
Mar 27 21:06:08.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961759024s
Mar 27 21:06:09.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937090269s
Mar 27 21:06:10.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.916329834s
Mar 27 21:06:11.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897011757s
Mar 27 21:06:12.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.87813704s
Mar 27 21:06:13.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.849052294s
Mar 27 21:06:14.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.828194455s
Mar 27 21:06:15.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 807.160254ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2300 03/27/23 21:06:16.246
Mar 27 21:06:16.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:06:16.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:06:16.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:06:16.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:06:16.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:06:16.868: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:06:16.868: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:06:16.868: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:06:16.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:06:17.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:06:17.136: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:06:17.136: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:06:17.136: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/27/23 21:06:27.207
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 21:06:27.207: INFO: Deleting all statefulset in ns statefulset-2300
Mar 27 21:06:27.221: INFO: Scaling statefulset ss to 0
Mar 27 21:06:27.269: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:06:27.286: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:27.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2300" for this suite. 03/27/23 21:06:27.363
------------------------------
• [SLOW TEST] [73.567 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:05:13.83
    Mar 27 21:05:13.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 21:05:13.831
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:05:13.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:05:13.921
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2300 03/27/23 21:05:13.939
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/27/23 21:05:13.961
    STEP: Creating stateful set ss in namespace statefulset-2300 03/27/23 21:05:13.979
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2300 03/27/23 21:05:13.998
    Mar 27 21:05:14.015: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 21:05:24.037: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/27/23 21:05:24.037
    Mar 27 21:05:24.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:05:24.331: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:05:24.331: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:05:24.331: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:05:24.350: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 27 21:05:34.371: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:05:34.371: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:05:34.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998406s
    Mar 27 21:05:35.453: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.981357845s
    Mar 27 21:05:36.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.964310261s
    Mar 27 21:05:37.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.946622828s
    Mar 27 21:05:38.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.927248607s
    Mar 27 21:05:39.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.909073604s
    Mar 27 21:05:40.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.888627764s
    Mar 27 21:05:41.565: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.871206508s
    Mar 27 21:05:42.583: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.852354062s
    Mar 27 21:05:43.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 833.945045ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2300 03/27/23 21:05:44.602
    Mar 27 21:05:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:05:44.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:05:44.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:05:44.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:05:44.947: INFO: Found 1 stateful pods, waiting for 3
    Mar 27 21:05:54.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:05:54.969: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:05:54.969: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/27/23 21:05:54.969
    STEP: Scale down will halt with unhealthy stateful pod 03/27/23 21:05:54.969
    Mar 27 21:05:55.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:05:55.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:05:55.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:05:55.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:05:55.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:05:55.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:05:55.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:05:55.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:05:55.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:05:55.944: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:05:55.944: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:05:55.944: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:05:55.944: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:05:55.958: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Mar 27 21:06:05.997: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:06:05.997: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:06:05.997: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 27 21:06:06.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998474s
    Mar 27 21:06:07.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981523789s
    Mar 27 21:06:08.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961759024s
    Mar 27 21:06:09.114: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.937090269s
    Mar 27 21:06:10.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.916329834s
    Mar 27 21:06:11.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.897011757s
    Mar 27 21:06:12.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.87813704s
    Mar 27 21:06:13.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.849052294s
    Mar 27 21:06:14.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.828194455s
    Mar 27 21:06:15.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 807.160254ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2300 03/27/23 21:06:16.246
    Mar 27 21:06:16.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:06:16.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:06:16.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:06:16.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:06:16.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:06:16.868: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:06:16.868: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:06:16.868: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:06:16.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-2300 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:06:17.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:06:17.136: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:06:17.136: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:06:17.136: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/27/23 21:06:27.207
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 21:06:27.207: INFO: Deleting all statefulset in ns statefulset-2300
    Mar 27 21:06:27.221: INFO: Scaling statefulset ss to 0
    Mar 27 21:06:27.269: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:06:27.286: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:27.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2300" for this suite. 03/27/23 21:06:27.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:27.399
Mar 27 21:06:27.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:06:27.4
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:27.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:27.467
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 21:06:27.491
Mar 27 21:06:27.525: INFO: Waiting up to 5m0s for pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1" in namespace "emptydir-3043" to be "Succeeded or Failed"
Mar 27 21:06:27.543: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.175135ms
Mar 27 21:06:29.561: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035726685s
Mar 27 21:06:31.562: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037515496s
STEP: Saw pod success 03/27/23 21:06:31.563
Mar 27 21:06:31.563: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1" satisfied condition "Succeeded or Failed"
Mar 27 21:06:31.580: INFO: Trying to get logs from node 10.176.99.177 pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 container test-container: <nil>
STEP: delete the pod 03/27/23 21:06:31.668
Mar 27 21:06:31.710: INFO: Waiting for pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 to disappear
Mar 27 21:06:31.727: INFO: Pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:31.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3043" for this suite. 03/27/23 21:06:31.752
------------------------------
• [4.379 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:27.399
    Mar 27 21:06:27.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:06:27.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:27.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:27.467
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 21:06:27.491
    Mar 27 21:06:27.525: INFO: Waiting up to 5m0s for pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1" in namespace "emptydir-3043" to be "Succeeded or Failed"
    Mar 27 21:06:27.543: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.175135ms
    Mar 27 21:06:29.561: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035726685s
    Mar 27 21:06:31.562: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037515496s
    STEP: Saw pod success 03/27/23 21:06:31.563
    Mar 27 21:06:31.563: INFO: Pod "pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1" satisfied condition "Succeeded or Failed"
    Mar 27 21:06:31.580: INFO: Trying to get logs from node 10.176.99.177 pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 container test-container: <nil>
    STEP: delete the pod 03/27/23 21:06:31.668
    Mar 27 21:06:31.710: INFO: Waiting for pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 to disappear
    Mar 27 21:06:31.727: INFO: Pod pod-58a970a8-6239-4b2c-981c-7e8341bf5ab1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:31.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3043" for this suite. 03/27/23 21:06:31.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:31.779
Mar 27 21:06:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 21:06:31.78
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:31.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:31.854
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 27 21:06:31.870: INFO: Creating simple deployment test-new-deployment
Mar 27 21:06:31.926: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/27/23 21:06:33.998
STEP: updating a scale subresource 03/27/23 21:06:34.014
STEP: verifying the deployment Spec.Replicas was modified 03/27/23 21:06:34.033
STEP: Patch a scale subresource 03/27/23 21:06:34.05
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 21:06:34.163: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9155  19a51fc9-2aae-4d32-b6d6-5da0be1b306a 22918 3 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-27 21:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a060e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-27 21:06:33 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 21:06:34 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 21:06:34.185: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9155  ef574ed6-85f0-4fef-a608-7d0d03c72566 22920 3 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 19a51fc9-2aae-4d32-b6d6-5da0be1b306a 0xc000b769a7 0xc000b769a8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19a51fc9-2aae-4d32-b6d6-5da0be1b306a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b76b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 21:06:34.207: INFO: Pod "test-new-deployment-7f5969cbc7-7q95j" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7q95j test-new-deployment-7f5969cbc7- deployment-9155  7614127d-c9ef-417e-88c2-3f13d5c9b90a 22892 0 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b6a934d96c3b13895a0eac8d5b7b257b28f4a30a610c62686674589c10105288 cni.projectcalico.org/podIP:172.30.85.181/32 cni.projectcalico.org/podIPs:172.30.85.181/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc001c878d7 0xc001c878d8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:06:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vd9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vd9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.181,StartTime:2023-03-27 21:06:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:06:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa6ad2cafd4b77981ea3c4cca74f72d508f8d664afd09cef0961a5a464d3b7b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-8p567" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8p567 test-new-deployment-7f5969cbc7- deployment-9155  c6017159-d3c6-4131-8a14-dc453e84d22e 22919 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc001c87fe7 0xc001c87fe8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4445c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4445c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 21:06:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-m68bm" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m68bm test-new-deployment-7f5969cbc7- deployment-9155  5ea421ca-4600-4031-9d61-77e553daae22 22930 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc00358c6c7 0xc00358c6c8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rx78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rx78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-xvxvx" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xvxvx test-new-deployment-7f5969cbc7- deployment-9155  8ea89e5b-1dff-4a61-b024-d0bfcd0e0d0c 22927 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc00358c830 0xc00358c831}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qncv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qncv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:34.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9155" for this suite. 03/27/23 21:06:34.23
------------------------------
• [2.484 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:31.779
    Mar 27 21:06:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 21:06:31.78
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:31.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:31.854
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 27 21:06:31.870: INFO: Creating simple deployment test-new-deployment
    Mar 27 21:06:31.926: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/27/23 21:06:33.998
    STEP: updating a scale subresource 03/27/23 21:06:34.014
    STEP: verifying the deployment Spec.Replicas was modified 03/27/23 21:06:34.033
    STEP: Patch a scale subresource 03/27/23 21:06:34.05
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 21:06:34.163: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9155  19a51fc9-2aae-4d32-b6d6-5da0be1b306a 22918 3 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-27 21:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a060e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-03-27 21:06:33 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 21:06:34 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 21:06:34.185: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9155  ef574ed6-85f0-4fef-a608-7d0d03c72566 22920 3 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 19a51fc9-2aae-4d32-b6d6-5da0be1b306a 0xc000b769a7 0xc000b769a8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19a51fc9-2aae-4d32-b6d6-5da0be1b306a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b76b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 21:06:34.207: INFO: Pod "test-new-deployment-7f5969cbc7-7q95j" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7q95j test-new-deployment-7f5969cbc7- deployment-9155  7614127d-c9ef-417e-88c2-3f13d5c9b90a 22892 0 2023-03-27 21:06:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b6a934d96c3b13895a0eac8d5b7b257b28f4a30a610c62686674589c10105288 cni.projectcalico.org/podIP:172.30.85.181/32 cni.projectcalico.org/podIPs:172.30.85.181/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc001c878d7 0xc001c878d8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:06:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vd9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vd9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.181,StartTime:2023-03-27 21:06:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:06:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa6ad2cafd4b77981ea3c4cca74f72d508f8d664afd09cef0961a5a464d3b7b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-8p567" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8p567 test-new-deployment-7f5969cbc7- deployment-9155  c6017159-d3c6-4131-8a14-dc453e84d22e 22919 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc001c87fe7 0xc001c87fe8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4445c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4445c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 21:06:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-m68bm" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-m68bm test-new-deployment-7f5969cbc7- deployment-9155  5ea421ca-4600-4031-9d61-77e553daae22 22930 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc00358c6c7 0xc00358c6c8}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rx78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rx78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 21:06:34.208: INFO: Pod "test-new-deployment-7f5969cbc7-xvxvx" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-xvxvx test-new-deployment-7f5969cbc7- deployment-9155  8ea89e5b-1dff-4a61-b024-d0bfcd0e0d0c 22927 0 2023-03-27 21:06:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 ef574ed6-85f0-4fef-a608-7d0d03c72566 0xc00358c830 0xc00358c831}] [] [{kube-controller-manager Update v1 2023-03-27 21:06:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef574ed6-85f0-4fef-a608-7d0d03c72566\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qncv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qncv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:06:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:34.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9155" for this suite. 03/27/23 21:06:34.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:34.269
Mar 27 21:06:34.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 21:06:34.271
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:34.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:34.34
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 03/27/23 21:06:34.358
Mar 27 21:06:34.389: INFO: Waiting up to 5m0s for pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f" in namespace "var-expansion-8597" to be "Succeeded or Failed"
Mar 27 21:06:34.407: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.110909ms
Mar 27 21:06:36.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036343378s
Mar 27 21:06:38.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036746383s
STEP: Saw pod success 03/27/23 21:06:38.426
Mar 27 21:06:38.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f" satisfied condition "Succeeded or Failed"
Mar 27 21:06:38.443: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:06:38.473
Mar 27 21:06:38.513: INFO: Waiting for pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f to disappear
Mar 27 21:06:38.530: INFO: Pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8597" for this suite. 03/27/23 21:06:38.553
------------------------------
• [4.309 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:34.269
    Mar 27 21:06:34.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 21:06:34.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:34.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:34.34
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 03/27/23 21:06:34.358
    Mar 27 21:06:34.389: INFO: Waiting up to 5m0s for pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f" in namespace "var-expansion-8597" to be "Succeeded or Failed"
    Mar 27 21:06:34.407: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.110909ms
    Mar 27 21:06:36.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036343378s
    Mar 27 21:06:38.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036746383s
    STEP: Saw pod success 03/27/23 21:06:38.426
    Mar 27 21:06:38.426: INFO: Pod "var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f" satisfied condition "Succeeded or Failed"
    Mar 27 21:06:38.443: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:06:38.473
    Mar 27 21:06:38.513: INFO: Waiting for pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f to disappear
    Mar 27 21:06:38.530: INFO: Pod var-expansion-abbd8019-6ad6-46fa-b6ef-bacff5e8fa6f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:38.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8597" for this suite. 03/27/23 21:06:38.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:38.58
Mar 27 21:06:38.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:06:38.581
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:38.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:38.647
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:06:38.724
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:06:39.412
STEP: Deploying the webhook pod 03/27/23 21:06:39.451
STEP: Wait for the deployment to be ready 03/27/23 21:06:39.493
Mar 27 21:06:39.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:06:41.582
STEP: Verifying the service has paired with the endpoint 03/27/23 21:06:41.625
Mar 27 21:06:42.625: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/27/23 21:06:42.639
STEP: create a configmap that should be updated by the webhook 03/27/23 21:06:42.716
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:42.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5555" for this suite. 03/27/23 21:06:42.964
STEP: Destroying namespace "webhook-5555-markers" for this suite. 03/27/23 21:06:42.988
------------------------------
• [4.436 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:38.58
    Mar 27 21:06:38.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:06:38.581
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:38.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:38.647
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:06:38.724
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:06:39.412
    STEP: Deploying the webhook pod 03/27/23 21:06:39.451
    STEP: Wait for the deployment to be ready 03/27/23 21:06:39.493
    Mar 27 21:06:39.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:06:41.582
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:06:41.625
    Mar 27 21:06:42.625: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/27/23 21:06:42.639
    STEP: create a configmap that should be updated by the webhook 03/27/23 21:06:42.716
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:42.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5555" for this suite. 03/27/23 21:06:42.964
    STEP: Destroying namespace "webhook-5555-markers" for this suite. 03/27/23 21:06:42.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:43.031
Mar 27 21:06:43.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename endpointslice 03/27/23 21:06:43.032
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:43.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:43.104
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Mar 27 21:06:43.167: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Mar 27 21:06:43.167: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:43.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7838" for this suite. 03/27/23 21:06:43.189
------------------------------
• [0.189 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:43.031
    Mar 27 21:06:43.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename endpointslice 03/27/23 21:06:43.032
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:43.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:43.104
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Mar 27 21:06:43.167: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Mar 27 21:06:43.167: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:43.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7838" for this suite. 03/27/23 21:06:43.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:43.225
Mar 27 21:06:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 21:06:43.226
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:43.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:43.307
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 03/27/23 21:06:43.322
Mar 27 21:06:43.357: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9999" to be "running and ready"
Mar 27 21:06:43.375: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 18.050529ms
Mar 27 21:06:43.375: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:06:45.393: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035516979s
Mar 27 21:06:45.393: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:06:47.392: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.034484882s
Mar 27 21:06:47.392: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 27 21:06:47.392: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/27/23 21:06:47.408
STEP: Then the orphan pod is adopted 03/27/23 21:06:47.424
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:06:48.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9999" for this suite. 03/27/23 21:06:48.501
------------------------------
• [SLOW TEST] [5.303 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:43.225
    Mar 27 21:06:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 21:06:43.226
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:43.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:43.307
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/27/23 21:06:43.322
    Mar 27 21:06:43.357: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9999" to be "running and ready"
    Mar 27 21:06:43.375: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 18.050529ms
    Mar 27 21:06:43.375: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:06:45.393: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035516979s
    Mar 27 21:06:45.393: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:06:47.392: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.034484882s
    Mar 27 21:06:47.392: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 27 21:06:47.392: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/27/23 21:06:47.408
    STEP: Then the orphan pod is adopted 03/27/23 21:06:47.424
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:06:48.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9999" for this suite. 03/27/23 21:06:48.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:06:48.531
Mar 27 21:06:48.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename subpath 03/27/23 21:06:48.531
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:48.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:48.599
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 21:06:48.614
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-jtv4 03/27/23 21:06:48.649
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:06:48.649
Mar 27 21:06:48.688: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jtv4" in namespace "subpath-5396" to be "Succeeded or Failed"
Mar 27 21:06:48.704: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.891827ms
Mar 27 21:06:50.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.034273641s
Mar 27 21:06:52.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 4.033806764s
Mar 27 21:06:54.728: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 6.040407268s
Mar 27 21:06:56.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 8.034900638s
Mar 27 21:06:58.732: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 10.043659129s
Mar 27 21:07:00.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 12.034288994s
Mar 27 21:07:02.735: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 14.047329961s
Mar 27 21:07:04.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 16.035091975s
Mar 27 21:07:06.721: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 18.033575734s
Mar 27 21:07:08.720: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 20.032071276s
Mar 27 21:07:10.724: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=false. Elapsed: 22.035650073s
Mar 27 21:07:12.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035423348s
STEP: Saw pod success 03/27/23 21:07:12.723
Mar 27 21:07:12.724: INFO: Pod "pod-subpath-test-projected-jtv4" satisfied condition "Succeeded or Failed"
Mar 27 21:07:12.741: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-projected-jtv4 container test-container-subpath-projected-jtv4: <nil>
STEP: delete the pod 03/27/23 21:07:12.775
Mar 27 21:07:12.831: INFO: Waiting for pod pod-subpath-test-projected-jtv4 to disappear
Mar 27 21:07:12.847: INFO: Pod pod-subpath-test-projected-jtv4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-jtv4 03/27/23 21:07:12.847
Mar 27 21:07:12.847: INFO: Deleting pod "pod-subpath-test-projected-jtv4" in namespace "subpath-5396"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 27 21:07:12.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5396" for this suite. 03/27/23 21:07:12.887
------------------------------
• [SLOW TEST] [24.381 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:06:48.531
    Mar 27 21:06:48.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename subpath 03/27/23 21:06:48.531
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:06:48.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:06:48.599
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 21:06:48.614
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-jtv4 03/27/23 21:06:48.649
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:06:48.649
    Mar 27 21:06:48.688: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jtv4" in namespace "subpath-5396" to be "Succeeded or Failed"
    Mar 27 21:06:48.704: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.891827ms
    Mar 27 21:06:50.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 2.034273641s
    Mar 27 21:06:52.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 4.033806764s
    Mar 27 21:06:54.728: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 6.040407268s
    Mar 27 21:06:56.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 8.034900638s
    Mar 27 21:06:58.732: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 10.043659129s
    Mar 27 21:07:00.722: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 12.034288994s
    Mar 27 21:07:02.735: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 14.047329961s
    Mar 27 21:07:04.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 16.035091975s
    Mar 27 21:07:06.721: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 18.033575734s
    Mar 27 21:07:08.720: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=true. Elapsed: 20.032071276s
    Mar 27 21:07:10.724: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Running", Reason="", readiness=false. Elapsed: 22.035650073s
    Mar 27 21:07:12.723: INFO: Pod "pod-subpath-test-projected-jtv4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.035423348s
    STEP: Saw pod success 03/27/23 21:07:12.723
    Mar 27 21:07:12.724: INFO: Pod "pod-subpath-test-projected-jtv4" satisfied condition "Succeeded or Failed"
    Mar 27 21:07:12.741: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-projected-jtv4 container test-container-subpath-projected-jtv4: <nil>
    STEP: delete the pod 03/27/23 21:07:12.775
    Mar 27 21:07:12.831: INFO: Waiting for pod pod-subpath-test-projected-jtv4 to disappear
    Mar 27 21:07:12.847: INFO: Pod pod-subpath-test-projected-jtv4 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-jtv4 03/27/23 21:07:12.847
    Mar 27 21:07:12.847: INFO: Deleting pod "pod-subpath-test-projected-jtv4" in namespace "subpath-5396"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:07:12.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5396" for this suite. 03/27/23 21:07:12.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:07:12.916
Mar 27 21:07:12.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sysctl 03/27/23 21:07:12.917
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:12.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:12.993
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/27/23 21:07:13.008
STEP: Watching for error events or started pod 03/27/23 21:07:13.041
STEP: Waiting for pod completion 03/27/23 21:07:15.059
Mar 27 21:07:15.059: INFO: Waiting up to 3m0s for pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb" in namespace "sysctl-6207" to be "completed"
Mar 27 21:07:15.076: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.99588ms
Mar 27 21:07:17.094: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035035686s
Mar 27 21:07:17.094: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/27/23 21:07:17.115
STEP: Getting logs from the pod 03/27/23 21:07:17.115
STEP: Checking that the sysctl is actually updated 03/27/23 21:07:17.149
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:07:17.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6207" for this suite. 03/27/23 21:07:17.172
------------------------------
• [4.282 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:07:12.916
    Mar 27 21:07:12.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sysctl 03/27/23 21:07:12.917
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:12.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:12.993
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/27/23 21:07:13.008
    STEP: Watching for error events or started pod 03/27/23 21:07:13.041
    STEP: Waiting for pod completion 03/27/23 21:07:15.059
    Mar 27 21:07:15.059: INFO: Waiting up to 3m0s for pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb" in namespace "sysctl-6207" to be "completed"
    Mar 27 21:07:15.076: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.99588ms
    Mar 27 21:07:17.094: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035035686s
    Mar 27 21:07:17.094: INFO: Pod "sysctl-5cb3c954-8f5a-4514-90a0-9e80490802bb" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/27/23 21:07:17.115
    STEP: Getting logs from the pod 03/27/23 21:07:17.115
    STEP: Checking that the sysctl is actually updated 03/27/23 21:07:17.149
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:07:17.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6207" for this suite. 03/27/23 21:07:17.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:07:17.199
Mar 27 21:07:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:07:17.2
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:17.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:17.267
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 03/27/23 21:07:17.284
Mar 27 21:07:17.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 create -f -'
Mar 27 21:07:17.586: INFO: stderr: ""
Mar 27 21:07:17.586: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:17.586
Mar 27 21:07:17.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:17.708: INFO: stderr: ""
Mar 27 21:07:17.708: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
Mar 27 21:07:17.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:17.810: INFO: stderr: ""
Mar 27 21:07:17.810: INFO: stdout: ""
Mar 27 21:07:17.810: INFO: update-demo-nautilus-6kxvc is created but not running
Mar 27 21:07:22.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:22.921: INFO: stderr: ""
Mar 27 21:07:22.921: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
Mar 27 21:07:22.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:23.054: INFO: stderr: ""
Mar 27 21:07:23.054: INFO: stdout: "true"
Mar 27 21:07:23.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:07:23.183: INFO: stderr: ""
Mar 27 21:07:23.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:07:23.183: INFO: validating pod update-demo-nautilus-6kxvc
Mar 27 21:07:23.242: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:07:23.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:07:23.242: INFO: update-demo-nautilus-6kxvc is verified up and running
Mar 27 21:07:23.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:23.346: INFO: stderr: ""
Mar 27 21:07:23.346: INFO: stdout: "true"
Mar 27 21:07:23.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:07:23.439: INFO: stderr: ""
Mar 27 21:07:23.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:07:23.439: INFO: validating pod update-demo-nautilus-q5qh4
Mar 27 21:07:23.514: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:07:23.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:07:23.514: INFO: update-demo-nautilus-q5qh4 is verified up and running
STEP: scaling down the replication controller 03/27/23 21:07:23.514
Mar 27 21:07:23.517: INFO: scanned /root for discovery docs: <nil>
Mar 27 21:07:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 27 21:07:24.788: INFO: stderr: ""
Mar 27 21:07:24.788: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:24.788
Mar 27 21:07:24.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:24.900: INFO: stderr: ""
Mar 27 21:07:24.900: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/27/23 21:07:24.9
Mar 27 21:07:29.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:30.004: INFO: stderr: ""
Mar 27 21:07:30.004: INFO: stdout: "update-demo-nautilus-q5qh4 "
Mar 27 21:07:30.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:30.111: INFO: stderr: ""
Mar 27 21:07:30.111: INFO: stdout: "true"
Mar 27 21:07:30.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:07:30.225: INFO: stderr: ""
Mar 27 21:07:30.225: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:07:30.225: INFO: validating pod update-demo-nautilus-q5qh4
Mar 27 21:07:30.248: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:07:30.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:07:30.248: INFO: update-demo-nautilus-q5qh4 is verified up and running
STEP: scaling up the replication controller 03/27/23 21:07:30.248
Mar 27 21:07:30.250: INFO: scanned /root for discovery docs: <nil>
Mar 27 21:07:30.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 27 21:07:31.373: INFO: stderr: ""
Mar 27 21:07:31.373: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:31.373
Mar 27 21:07:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:31.523: INFO: stderr: ""
Mar 27 21:07:31.523: INFO: stdout: "update-demo-nautilus-nv2b5 update-demo-nautilus-q5qh4 "
Mar 27 21:07:31.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:31.647: INFO: stderr: ""
Mar 27 21:07:31.647: INFO: stdout: ""
Mar 27 21:07:31.647: INFO: update-demo-nautilus-nv2b5 is created but not running
Mar 27 21:07:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:07:36.759: INFO: stderr: ""
Mar 27 21:07:36.759: INFO: stdout: "update-demo-nautilus-nv2b5 update-demo-nautilus-q5qh4 "
Mar 27 21:07:36.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:36.865: INFO: stderr: ""
Mar 27 21:07:36.865: INFO: stdout: "true"
Mar 27 21:07:36.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:07:36.970: INFO: stderr: ""
Mar 27 21:07:36.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:07:36.970: INFO: validating pod update-demo-nautilus-nv2b5
Mar 27 21:07:37.037: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:07:37.037: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:07:37.037: INFO: update-demo-nautilus-nv2b5 is verified up and running
Mar 27 21:07:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:07:37.144: INFO: stderr: ""
Mar 27 21:07:37.145: INFO: stdout: "true"
Mar 27 21:07:37.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:07:37.246: INFO: stderr: ""
Mar 27 21:07:37.246: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:07:37.246: INFO: validating pod update-demo-nautilus-q5qh4
Mar 27 21:07:37.277: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:07:37.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:07:37.277: INFO: update-demo-nautilus-q5qh4 is verified up and running
STEP: using delete to clean up resources 03/27/23 21:07:37.277
Mar 27 21:07:37.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 delete --grace-period=0 --force -f -'
Mar 27 21:07:37.400: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:07:37.400: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 27 21:07:37.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get rc,svc -l name=update-demo --no-headers'
Mar 27 21:07:37.556: INFO: stderr: "No resources found in kubectl-1330 namespace.\n"
Mar 27 21:07:37.556: INFO: stdout: ""
Mar 27 21:07:37.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 21:07:37.693: INFO: stderr: ""
Mar 27 21:07:37.693: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:07:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1330" for this suite. 03/27/23 21:07:37.715
------------------------------
• [SLOW TEST] [20.551 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:07:17.199
    Mar 27 21:07:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:07:17.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:17.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:17.267
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 03/27/23 21:07:17.284
    Mar 27 21:07:17.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 create -f -'
    Mar 27 21:07:17.586: INFO: stderr: ""
    Mar 27 21:07:17.586: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:17.586
    Mar 27 21:07:17.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:17.708: INFO: stderr: ""
    Mar 27 21:07:17.708: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
    Mar 27 21:07:17.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:17.810: INFO: stderr: ""
    Mar 27 21:07:17.810: INFO: stdout: ""
    Mar 27 21:07:17.810: INFO: update-demo-nautilus-6kxvc is created but not running
    Mar 27 21:07:22.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:22.921: INFO: stderr: ""
    Mar 27 21:07:22.921: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
    Mar 27 21:07:22.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:23.054: INFO: stderr: ""
    Mar 27 21:07:23.054: INFO: stdout: "true"
    Mar 27 21:07:23.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-6kxvc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:07:23.183: INFO: stderr: ""
    Mar 27 21:07:23.183: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:07:23.183: INFO: validating pod update-demo-nautilus-6kxvc
    Mar 27 21:07:23.242: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:07:23.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:07:23.242: INFO: update-demo-nautilus-6kxvc is verified up and running
    Mar 27 21:07:23.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:23.346: INFO: stderr: ""
    Mar 27 21:07:23.346: INFO: stdout: "true"
    Mar 27 21:07:23.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:07:23.439: INFO: stderr: ""
    Mar 27 21:07:23.439: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:07:23.439: INFO: validating pod update-demo-nautilus-q5qh4
    Mar 27 21:07:23.514: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:07:23.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:07:23.514: INFO: update-demo-nautilus-q5qh4 is verified up and running
    STEP: scaling down the replication controller 03/27/23 21:07:23.514
    Mar 27 21:07:23.517: INFO: scanned /root for discovery docs: <nil>
    Mar 27 21:07:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 27 21:07:24.788: INFO: stderr: ""
    Mar 27 21:07:24.788: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:24.788
    Mar 27 21:07:24.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:24.900: INFO: stderr: ""
    Mar 27 21:07:24.900: INFO: stdout: "update-demo-nautilus-6kxvc update-demo-nautilus-q5qh4 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/27/23 21:07:24.9
    Mar 27 21:07:29.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:30.004: INFO: stderr: ""
    Mar 27 21:07:30.004: INFO: stdout: "update-demo-nautilus-q5qh4 "
    Mar 27 21:07:30.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:30.111: INFO: stderr: ""
    Mar 27 21:07:30.111: INFO: stdout: "true"
    Mar 27 21:07:30.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:07:30.225: INFO: stderr: ""
    Mar 27 21:07:30.225: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:07:30.225: INFO: validating pod update-demo-nautilus-q5qh4
    Mar 27 21:07:30.248: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:07:30.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:07:30.248: INFO: update-demo-nautilus-q5qh4 is verified up and running
    STEP: scaling up the replication controller 03/27/23 21:07:30.248
    Mar 27 21:07:30.250: INFO: scanned /root for discovery docs: <nil>
    Mar 27 21:07:30.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 27 21:07:31.373: INFO: stderr: ""
    Mar 27 21:07:31.373: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:07:31.373
    Mar 27 21:07:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:31.523: INFO: stderr: ""
    Mar 27 21:07:31.523: INFO: stdout: "update-demo-nautilus-nv2b5 update-demo-nautilus-q5qh4 "
    Mar 27 21:07:31.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:31.647: INFO: stderr: ""
    Mar 27 21:07:31.647: INFO: stdout: ""
    Mar 27 21:07:31.647: INFO: update-demo-nautilus-nv2b5 is created but not running
    Mar 27 21:07:36.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:07:36.759: INFO: stderr: ""
    Mar 27 21:07:36.759: INFO: stdout: "update-demo-nautilus-nv2b5 update-demo-nautilus-q5qh4 "
    Mar 27 21:07:36.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:36.865: INFO: stderr: ""
    Mar 27 21:07:36.865: INFO: stdout: "true"
    Mar 27 21:07:36.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-nv2b5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:07:36.970: INFO: stderr: ""
    Mar 27 21:07:36.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:07:36.970: INFO: validating pod update-demo-nautilus-nv2b5
    Mar 27 21:07:37.037: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:07:37.037: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:07:37.037: INFO: update-demo-nautilus-nv2b5 is verified up and running
    Mar 27 21:07:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:07:37.144: INFO: stderr: ""
    Mar 27 21:07:37.145: INFO: stdout: "true"
    Mar 27 21:07:37.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods update-demo-nautilus-q5qh4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:07:37.246: INFO: stderr: ""
    Mar 27 21:07:37.246: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:07:37.246: INFO: validating pod update-demo-nautilus-q5qh4
    Mar 27 21:07:37.277: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:07:37.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:07:37.277: INFO: update-demo-nautilus-q5qh4 is verified up and running
    STEP: using delete to clean up resources 03/27/23 21:07:37.277
    Mar 27 21:07:37.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 delete --grace-period=0 --force -f -'
    Mar 27 21:07:37.400: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:07:37.400: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 27 21:07:37.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get rc,svc -l name=update-demo --no-headers'
    Mar 27 21:07:37.556: INFO: stderr: "No resources found in kubectl-1330 namespace.\n"
    Mar 27 21:07:37.556: INFO: stdout: ""
    Mar 27 21:07:37.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1330 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 21:07:37.693: INFO: stderr: ""
    Mar 27 21:07:37.693: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:07:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1330" for this suite. 03/27/23 21:07:37.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:07:37.751
Mar 27 21:07:37.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:07:37.752
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:37.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:37.836
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:07:37.852
Mar 27 21:07:37.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 27 21:07:37.983: INFO: stderr: ""
Mar 27 21:07:37.983: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/27/23 21:07:37.983
Mar 27 21:07:37.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Mar 27 21:07:38.326: INFO: stderr: ""
Mar 27 21:07:38.326: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:07:38.326
Mar 27 21:07:38.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 delete pods e2e-test-httpd-pod'
Mar 27 21:07:40.624: INFO: stderr: ""
Mar 27 21:07:40.624: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:07:40.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2839" for this suite. 03/27/23 21:07:40.646
------------------------------
• [2.940 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:07:37.751
    Mar 27 21:07:37.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:07:37.752
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:37.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:37.836
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:07:37.852
    Mar 27 21:07:37.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 27 21:07:37.983: INFO: stderr: ""
    Mar 27 21:07:37.983: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/27/23 21:07:37.983
    Mar 27 21:07:37.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Mar 27 21:07:38.326: INFO: stderr: ""
    Mar 27 21:07:38.326: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:07:38.326
    Mar 27 21:07:38.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2839 delete pods e2e-test-httpd-pod'
    Mar 27 21:07:40.624: INFO: stderr: ""
    Mar 27 21:07:40.624: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:07:40.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2839" for this suite. 03/27/23 21:07:40.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:07:40.693
Mar 27 21:07:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:07:40.694
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:40.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:40.76
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-7679 03/27/23 21:07:40.775
STEP: creating service affinity-clusterip-transition in namespace services-7679 03/27/23 21:07:40.776
STEP: creating replication controller affinity-clusterip-transition in namespace services-7679 03/27/23 21:07:40.818
I0327 21:07:40.859984      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7679, replica count: 3
I0327 21:07:43.911209      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 21:07:43.938: INFO: Creating new exec pod
Mar 27 21:07:43.971: INFO: Waiting up to 5m0s for pod "execpod-affinityvwjjm" in namespace "services-7679" to be "running"
Mar 27 21:07:43.989: INFO: Pod "execpod-affinityvwjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 17.413576ms
Mar 27 21:07:46.007: INFO: Pod "execpod-affinityvwjjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.035961224s
Mar 27 21:07:46.007: INFO: Pod "execpod-affinityvwjjm" satisfied condition "running"
Mar 27 21:07:47.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Mar 27 21:07:47.297: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 27 21:07:47.298: INFO: stdout: ""
Mar 27 21:07:47.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c nc -v -z -w 2 172.21.9.79 80'
Mar 27 21:07:47.548: INFO: stderr: "+ nc -v -z -w 2 172.21.9.79 80\nConnection to 172.21.9.79 80 port [tcp/http] succeeded!\n"
Mar 27 21:07:47.548: INFO: stdout: ""
Mar 27 21:07:47.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
Mar 27 21:07:47.900: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
Mar 27 21:07:47.900: INFO: stdout: "\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf"
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:17.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
Mar 27 21:08:18.277: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
Mar 27 21:08:18.277: INFO: stdout: "\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-jdssm"
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
Mar 27 21:08:18.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
Mar 27 21:08:18.673: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
Mar 27 21:08:18.673: INFO: stdout: "\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc"
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
Mar 27 21:08:18.674: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7679, will wait for the garbage collector to delete the pods 03/27/23 21:08:18.728
Mar 27 21:08:18.812: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.962545ms
Mar 27 21:08:18.913: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.026045ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:08:21.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7679" for this suite. 03/27/23 21:08:21.191
------------------------------
• [SLOW TEST] [40.523 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:07:40.693
    Mar 27 21:07:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:07:40.694
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:07:40.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:07:40.76
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-7679 03/27/23 21:07:40.775
    STEP: creating service affinity-clusterip-transition in namespace services-7679 03/27/23 21:07:40.776
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7679 03/27/23 21:07:40.818
    I0327 21:07:40.859984      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7679, replica count: 3
    I0327 21:07:43.911209      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 21:07:43.938: INFO: Creating new exec pod
    Mar 27 21:07:43.971: INFO: Waiting up to 5m0s for pod "execpod-affinityvwjjm" in namespace "services-7679" to be "running"
    Mar 27 21:07:43.989: INFO: Pod "execpod-affinityvwjjm": Phase="Pending", Reason="", readiness=false. Elapsed: 17.413576ms
    Mar 27 21:07:46.007: INFO: Pod "execpod-affinityvwjjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.035961224s
    Mar 27 21:07:46.007: INFO: Pod "execpod-affinityvwjjm" satisfied condition "running"
    Mar 27 21:07:47.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Mar 27 21:07:47.297: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 27 21:07:47.298: INFO: stdout: ""
    Mar 27 21:07:47.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c nc -v -z -w 2 172.21.9.79 80'
    Mar 27 21:07:47.548: INFO: stderr: "+ nc -v -z -w 2 172.21.9.79 80\nConnection to 172.21.9.79 80 port [tcp/http] succeeded!\n"
    Mar 27 21:07:47.548: INFO: stdout: ""
    Mar 27 21:07:47.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
    Mar 27 21:07:47.900: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
    Mar 27 21:07:47.900: INFO: stdout: "\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf"
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.900: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:07:47.901: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:17.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
    Mar 27 21:08:18.277: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
    Mar 27 21:08:18.277: INFO: stdout: "\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-jdssm\naffinity-clusterip-transition-9hdxf\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-jdssm"
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-9hdxf
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.277: INFO: Received response from host: affinity-clusterip-transition-jdssm
    Mar 27 21:08:18.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7679 exec execpod-affinityvwjjm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.79:80/ ; done'
    Mar 27 21:08:18.673: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.79:80/\n"
    Mar 27 21:08:18.673: INFO: stdout: "\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc\naffinity-clusterip-transition-wn7rc"
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Received response from host: affinity-clusterip-transition-wn7rc
    Mar 27 21:08:18.674: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7679, will wait for the garbage collector to delete the pods 03/27/23 21:08:18.728
    Mar 27 21:08:18.812: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.962545ms
    Mar 27 21:08:18.913: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.026045ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:08:21.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7679" for this suite. 03/27/23 21:08:21.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:08:21.22
Mar 27 21:08:21.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename subpath 03/27/23 21:08:21.221
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:21.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:21.288
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 21:08:21.304
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-fz9d 03/27/23 21:08:21.339
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:08:21.34
Mar 27 21:08:21.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fz9d" in namespace "subpath-3936" to be "Succeeded or Failed"
Mar 27 21:08:21.400: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.989361ms
Mar 27 21:08:23.416: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.044615336s
Mar 27 21:08:25.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 4.047921088s
Mar 27 21:08:27.423: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 6.051105234s
Mar 27 21:08:29.432: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 8.060137313s
Mar 27 21:08:31.426: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 10.054303496s
Mar 27 21:08:33.419: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 12.047354222s
Mar 27 21:08:35.418: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.046233328s
Mar 27 21:08:37.419: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 16.047068695s
Mar 27 21:08:39.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 18.048528497s
Mar 27 21:08:41.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 20.048036199s
Mar 27 21:08:43.421: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=false. Elapsed: 22.048867111s
Mar 27 21:08:45.417: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045375703s
STEP: Saw pod success 03/27/23 21:08:45.417
Mar 27 21:08:45.418: INFO: Pod "pod-subpath-test-downwardapi-fz9d" satisfied condition "Succeeded or Failed"
Mar 27 21:08:45.436: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-downwardapi-fz9d container test-container-subpath-downwardapi-fz9d: <nil>
STEP: delete the pod 03/27/23 21:08:45.48
Mar 27 21:08:45.536: INFO: Waiting for pod pod-subpath-test-downwardapi-fz9d to disappear
Mar 27 21:08:45.552: INFO: Pod pod-subpath-test-downwardapi-fz9d no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fz9d 03/27/23 21:08:45.552
Mar 27 21:08:45.552: INFO: Deleting pod "pod-subpath-test-downwardapi-fz9d" in namespace "subpath-3936"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 27 21:08:45.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3936" for this suite. 03/27/23 21:08:45.597
------------------------------
• [SLOW TEST] [24.401 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:08:21.22
    Mar 27 21:08:21.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename subpath 03/27/23 21:08:21.221
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:21.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:21.288
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 21:08:21.304
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-fz9d 03/27/23 21:08:21.339
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:08:21.34
    Mar 27 21:08:21.372: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fz9d" in namespace "subpath-3936" to be "Succeeded or Failed"
    Mar 27 21:08:21.400: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Pending", Reason="", readiness=false. Elapsed: 27.989361ms
    Mar 27 21:08:23.416: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.044615336s
    Mar 27 21:08:25.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 4.047921088s
    Mar 27 21:08:27.423: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 6.051105234s
    Mar 27 21:08:29.432: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 8.060137313s
    Mar 27 21:08:31.426: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 10.054303496s
    Mar 27 21:08:33.419: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 12.047354222s
    Mar 27 21:08:35.418: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.046233328s
    Mar 27 21:08:37.419: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 16.047068695s
    Mar 27 21:08:39.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 18.048528497s
    Mar 27 21:08:41.420: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=true. Elapsed: 20.048036199s
    Mar 27 21:08:43.421: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Running", Reason="", readiness=false. Elapsed: 22.048867111s
    Mar 27 21:08:45.417: INFO: Pod "pod-subpath-test-downwardapi-fz9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045375703s
    STEP: Saw pod success 03/27/23 21:08:45.417
    Mar 27 21:08:45.418: INFO: Pod "pod-subpath-test-downwardapi-fz9d" satisfied condition "Succeeded or Failed"
    Mar 27 21:08:45.436: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-downwardapi-fz9d container test-container-subpath-downwardapi-fz9d: <nil>
    STEP: delete the pod 03/27/23 21:08:45.48
    Mar 27 21:08:45.536: INFO: Waiting for pod pod-subpath-test-downwardapi-fz9d to disappear
    Mar 27 21:08:45.552: INFO: Pod pod-subpath-test-downwardapi-fz9d no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-fz9d 03/27/23 21:08:45.552
    Mar 27 21:08:45.552: INFO: Deleting pod "pod-subpath-test-downwardapi-fz9d" in namespace "subpath-3936"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:08:45.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3936" for this suite. 03/27/23 21:08:45.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:08:45.624
Mar 27 21:08:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 21:08:45.625
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:45.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:45.705
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 03/27/23 21:08:45.736
STEP: Patching the Job 03/27/23 21:08:45.759
STEP: Watching for Job to be patched 03/27/23 21:08:45.8
Mar 27 21:08:45.808: INFO: Event ADDED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 27 21:08:45.808: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 27 21:08:45.809: INFO: Event MODIFIED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/27/23 21:08:45.809
STEP: Watching for Job to be updated 03/27/23 21:08:45.842
Mar 27 21:08:45.850: INFO: Event MODIFIED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:45.850: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/27/23 21:08:45.85
Mar 27 21:08:45.865: INFO: Job: e2e-j95d7 as labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7]
STEP: Waiting for job to complete 03/27/23 21:08:45.865
STEP: Delete a job collection with a labelselector 03/27/23 21:08:55.88
STEP: Watching for Job to be deleted 03/27/23 21:08:55.908
Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.920: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 27 21:08:55.921: INFO: Event DELETED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/27/23 21:08:55.921
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 21:08:55.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5500" for this suite. 03/27/23 21:08:55.959
------------------------------
• [SLOW TEST] [10.372 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:08:45.624
    Mar 27 21:08:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 21:08:45.625
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:45.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:45.705
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 03/27/23 21:08:45.736
    STEP: Patching the Job 03/27/23 21:08:45.759
    STEP: Watching for Job to be patched 03/27/23 21:08:45.8
    Mar 27 21:08:45.808: INFO: Event ADDED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 27 21:08:45.808: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 27 21:08:45.809: INFO: Event MODIFIED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/27/23 21:08:45.809
    STEP: Watching for Job to be updated 03/27/23 21:08:45.842
    Mar 27 21:08:45.850: INFO: Event MODIFIED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:45.850: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/27/23 21:08:45.85
    Mar 27 21:08:45.865: INFO: Job: e2e-j95d7 as labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7]
    STEP: Waiting for job to complete 03/27/23 21:08:45.865
    STEP: Delete a job collection with a labelselector 03/27/23 21:08:55.88
    STEP: Watching for Job to be deleted 03/27/23 21:08:55.908
    Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.916: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.917: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.920: INFO: Event MODIFIED observed for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 27 21:08:55.921: INFO: Event DELETED found for Job e2e-j95d7 in namespace job-5500 with labels: map[e2e-j95d7:patched e2e-job-label:e2e-j95d7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/27/23 21:08:55.921
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:08:55.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5500" for this suite. 03/27/23 21:08:55.959
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:08:55.996
Mar 27 21:08:55.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:08:55.997
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:56.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:56.073
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-6440 03/27/23 21:08:56.088
STEP: creating a selector 03/27/23 21:08:56.088
STEP: Creating the service pods in kubernetes 03/27/23 21:08:56.088
Mar 27 21:08:56.088: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 21:08:56.192: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6440" to be "running and ready"
Mar 27 21:08:56.225: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.951531ms
Mar 27 21:08:56.225: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:08:58.246: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.054475616s
Mar 27 21:08:58.247: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:00.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.052908637s
Mar 27 21:09:00.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:02.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.055824432s
Mar 27 21:09:02.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:04.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.050865768s
Mar 27 21:09:04.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:06.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.052528096s
Mar 27 21:09:06.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:08.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.056121985s
Mar 27 21:09:08.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:10.242: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050418318s
Mar 27 21:09:10.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:12.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051232807s
Mar 27 21:09:12.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:14.244: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05187839s
Mar 27 21:09:14.244: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:16.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052569633s
Mar 27 21:09:16.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:09:18.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05299652s
Mar 27 21:09:18.245: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 21:09:18.245: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 21:09:18.269: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6440" to be "running and ready"
Mar 27 21:09:18.288: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.953464ms
Mar 27 21:09:18.288: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 21:09:18.288: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 21:09:18.306: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6440" to be "running and ready"
Mar 27 21:09:18.324: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.758974ms
Mar 27 21:09:18.324: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 21:09:18.324: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 21:09:18.34
Mar 27 21:09:18.383: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6440" to be "running"
Mar 27 21:09:18.403: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.56085ms
Mar 27 21:09:20.420: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037444851s
Mar 27 21:09:20.420: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 21:09:20.436: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6440" to be "running"
Mar 27 21:09:20.452: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.687044ms
Mar 27 21:09:20.452: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 27 21:09:20.471: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 21:09:20.471: INFO: Going to poll 172.30.56.102 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 21:09:20.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.56.102:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:09:20.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:09:20.487: INFO: ExecWithOptions: Clientset creation
Mar 27 21:09:20.487: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.56.102%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 21:09:20.748: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 27 21:09:20.748: INFO: Going to poll 172.30.85.129 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 21:09:20.780: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.85.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:09:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:09:20.781: INFO: ExecWithOptions: Clientset creation
Mar 27 21:09:20.781: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.85.129%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 21:09:20.976: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 27 21:09:20.976: INFO: Going to poll 172.30.4.81 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 27 21:09:20.995: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.4.81:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:09:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:09:20.996: INFO: ExecWithOptions: Clientset creation
Mar 27 21:09:20.996: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.4.81%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 21:09:21.160: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:21.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6440" for this suite. 03/27/23 21:09:21.184
------------------------------
• [SLOW TEST] [25.218 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:08:55.996
    Mar 27 21:08:55.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:08:55.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:08:56.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:08:56.073
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-6440 03/27/23 21:08:56.088
    STEP: creating a selector 03/27/23 21:08:56.088
    STEP: Creating the service pods in kubernetes 03/27/23 21:08:56.088
    Mar 27 21:08:56.088: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 21:08:56.192: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6440" to be "running and ready"
    Mar 27 21:08:56.225: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.951531ms
    Mar 27 21:08:56.225: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:08:58.246: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.054475616s
    Mar 27 21:08:58.247: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:00.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.052908637s
    Mar 27 21:09:00.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:02.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.055824432s
    Mar 27 21:09:02.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:04.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.050865768s
    Mar 27 21:09:04.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:06.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.052528096s
    Mar 27 21:09:06.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:08.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.056121985s
    Mar 27 21:09:08.248: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:10.242: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050418318s
    Mar 27 21:09:10.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:12.243: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051232807s
    Mar 27 21:09:12.243: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:14.244: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05187839s
    Mar 27 21:09:14.244: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:16.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052569633s
    Mar 27 21:09:16.245: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:09:18.245: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05299652s
    Mar 27 21:09:18.245: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 21:09:18.245: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 21:09:18.269: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6440" to be "running and ready"
    Mar 27 21:09:18.288: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.953464ms
    Mar 27 21:09:18.288: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 21:09:18.288: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 21:09:18.306: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6440" to be "running and ready"
    Mar 27 21:09:18.324: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.758974ms
    Mar 27 21:09:18.324: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 21:09:18.324: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 21:09:18.34
    Mar 27 21:09:18.383: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6440" to be "running"
    Mar 27 21:09:18.403: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.56085ms
    Mar 27 21:09:20.420: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037444851s
    Mar 27 21:09:20.420: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 21:09:20.436: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6440" to be "running"
    Mar 27 21:09:20.452: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.687044ms
    Mar 27 21:09:20.452: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 27 21:09:20.471: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 21:09:20.471: INFO: Going to poll 172.30.56.102 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 21:09:20.486: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.56.102:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:09:20.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:09:20.487: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:09:20.487: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.56.102%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 21:09:20.748: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 27 21:09:20.748: INFO: Going to poll 172.30.85.129 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 21:09:20.780: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.85.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:09:20.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:09:20.781: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:09:20.781: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.85.129%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 21:09:20.976: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 27 21:09:20.976: INFO: Going to poll 172.30.4.81 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 21:09:20.995: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.4.81:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:09:20.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:09:20.996: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:09:20.996: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6440/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.4.81%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 21:09:21.160: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:21.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6440" for this suite. 03/27/23 21:09:21.184
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:21.219
Mar 27 21:09:21.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 21:09:21.22
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:21.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:21.297
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Mar 27 21:09:21.344: INFO: Waiting up to 5m0s for pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40" in namespace "pods-7260" to be "running and ready"
Mar 27 21:09:21.363: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40": Phase="Pending", Reason="", readiness=false. Elapsed: 18.349629ms
Mar 27 21:09:21.363: INFO: The phase of Pod server-envvars-c5101441-0905-4311-9187-f2b6b40aab40 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:09:23.382: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40": Phase="Running", Reason="", readiness=true. Elapsed: 2.037702032s
Mar 27 21:09:23.382: INFO: The phase of Pod server-envvars-c5101441-0905-4311-9187-f2b6b40aab40 is Running (Ready = true)
Mar 27 21:09:23.382: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40" satisfied condition "running and ready"
Mar 27 21:09:23.466: INFO: Waiting up to 5m0s for pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0" in namespace "pods-7260" to be "Succeeded or Failed"
Mar 27 21:09:23.482: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.509384ms
Mar 27 21:09:25.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.033904088s
Mar 27 21:09:27.506: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.040171965s
Mar 27 21:09:29.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033843633s
STEP: Saw pod success 03/27/23 21:09:29.5
Mar 27 21:09:29.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0" satisfied condition "Succeeded or Failed"
Mar 27 21:09:29.521: INFO: Trying to get logs from node 10.176.99.177 pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 container env3cont: <nil>
STEP: delete the pod 03/27/23 21:09:29.556
Mar 27 21:09:29.608: INFO: Waiting for pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 to disappear
Mar 27 21:09:29.625: INFO: Pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:29.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7260" for this suite. 03/27/23 21:09:29.646
------------------------------
• [SLOW TEST] [8.453 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:21.219
    Mar 27 21:09:21.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 21:09:21.22
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:21.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:21.297
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Mar 27 21:09:21.344: INFO: Waiting up to 5m0s for pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40" in namespace "pods-7260" to be "running and ready"
    Mar 27 21:09:21.363: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40": Phase="Pending", Reason="", readiness=false. Elapsed: 18.349629ms
    Mar 27 21:09:21.363: INFO: The phase of Pod server-envvars-c5101441-0905-4311-9187-f2b6b40aab40 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:09:23.382: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40": Phase="Running", Reason="", readiness=true. Elapsed: 2.037702032s
    Mar 27 21:09:23.382: INFO: The phase of Pod server-envvars-c5101441-0905-4311-9187-f2b6b40aab40 is Running (Ready = true)
    Mar 27 21:09:23.382: INFO: Pod "server-envvars-c5101441-0905-4311-9187-f2b6b40aab40" satisfied condition "running and ready"
    Mar 27 21:09:23.466: INFO: Waiting up to 5m0s for pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0" in namespace "pods-7260" to be "Succeeded or Failed"
    Mar 27 21:09:23.482: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.509384ms
    Mar 27 21:09:25.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.033904088s
    Mar 27 21:09:27.506: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.040171965s
    Mar 27 21:09:29.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033843633s
    STEP: Saw pod success 03/27/23 21:09:29.5
    Mar 27 21:09:29.500: INFO: Pod "client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0" satisfied condition "Succeeded or Failed"
    Mar 27 21:09:29.521: INFO: Trying to get logs from node 10.176.99.177 pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 container env3cont: <nil>
    STEP: delete the pod 03/27/23 21:09:29.556
    Mar 27 21:09:29.608: INFO: Waiting for pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 to disappear
    Mar 27 21:09:29.625: INFO: Pod client-envvars-82802b2b-424f-4cc9-a79d-bab296f493e0 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:29.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7260" for this suite. 03/27/23 21:09:29.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:29.674
Mar 27 21:09:29.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:09:29.675
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:29.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:29.748
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-64a86bfa-6a9c-4e8a-ab65-8bda1c2a33db 03/27/23 21:09:29.764
STEP: Creating a pod to test consume configMaps 03/27/23 21:09:29.783
Mar 27 21:09:29.816: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f" in namespace "projected-3916" to be "Succeeded or Failed"
Mar 27 21:09:29.836: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.247632ms
Mar 27 21:09:31.853: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036535366s
Mar 27 21:09:33.854: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038140417s
Mar 27 21:09:35.856: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039978599s
STEP: Saw pod success 03/27/23 21:09:35.856
Mar 27 21:09:35.857: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f" satisfied condition "Succeeded or Failed"
Mar 27 21:09:35.884: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:09:35.923
Mar 27 21:09:35.968: INFO: Waiting for pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f to disappear
Mar 27 21:09:35.984: INFO: Pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3916" for this suite. 03/27/23 21:09:36.007
------------------------------
• [SLOW TEST] [6.361 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:29.674
    Mar 27 21:09:29.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:09:29.675
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:29.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:29.748
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-64a86bfa-6a9c-4e8a-ab65-8bda1c2a33db 03/27/23 21:09:29.764
    STEP: Creating a pod to test consume configMaps 03/27/23 21:09:29.783
    Mar 27 21:09:29.816: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f" in namespace "projected-3916" to be "Succeeded or Failed"
    Mar 27 21:09:29.836: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.247632ms
    Mar 27 21:09:31.853: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036535366s
    Mar 27 21:09:33.854: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038140417s
    Mar 27 21:09:35.856: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039978599s
    STEP: Saw pod success 03/27/23 21:09:35.856
    Mar 27 21:09:35.857: INFO: Pod "pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f" satisfied condition "Succeeded or Failed"
    Mar 27 21:09:35.884: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:09:35.923
    Mar 27 21:09:35.968: INFO: Waiting for pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f to disappear
    Mar 27 21:09:35.984: INFO: Pod pod-projected-configmaps-b29feff2-5dbc-4b73-bc8a-36d56ad8181f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:35.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3916" for this suite. 03/27/23 21:09:36.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:36.039
Mar 27 21:09:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename lease-test 03/27/23 21:09:36.041
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:36.108
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-850" for this suite. 03/27/23 21:09:36.391
------------------------------
• [0.371 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:36.039
    Mar 27 21:09:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename lease-test 03/27/23 21:09:36.041
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:36.108
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-850" for this suite. 03/27/23 21:09:36.391
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:36.41
Mar 27 21:09:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 21:09:36.412
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:36.496
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 03/27/23 21:09:36.509
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.568
STEP: Creating a service in the namespace 03/27/23 21:09:36.58
STEP: Deleting the namespace 03/27/23 21:09:36.613
STEP: Waiting for the namespace to be removed. 03/27/23 21:09:36.635
STEP: Recreating the namespace 03/27/23 21:09:42.649
STEP: Verifying there is no service in the namespace 03/27/23 21:09:42.703
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9456" for this suite. 03/27/23 21:09:42.735
STEP: Destroying namespace "nsdeletetest-6777" for this suite. 03/27/23 21:09:42.759
Mar 27 21:09:42.773: INFO: Namespace nsdeletetest-6777 was already deleted
STEP: Destroying namespace "nsdeletetest-5981" for this suite. 03/27/23 21:09:42.774
------------------------------
• [SLOW TEST] [6.388 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:36.41
    Mar 27 21:09:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 21:09:36.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:36.496
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 03/27/23 21:09:36.509
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:36.568
    STEP: Creating a service in the namespace 03/27/23 21:09:36.58
    STEP: Deleting the namespace 03/27/23 21:09:36.613
    STEP: Waiting for the namespace to be removed. 03/27/23 21:09:36.635
    STEP: Recreating the namespace 03/27/23 21:09:42.649
    STEP: Verifying there is no service in the namespace 03/27/23 21:09:42.703
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:42.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9456" for this suite. 03/27/23 21:09:42.735
    STEP: Destroying namespace "nsdeletetest-6777" for this suite. 03/27/23 21:09:42.759
    Mar 27 21:09:42.773: INFO: Namespace nsdeletetest-6777 was already deleted
    STEP: Destroying namespace "nsdeletetest-5981" for this suite. 03/27/23 21:09:42.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:42.805
Mar 27 21:09:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename containers 03/27/23 21:09:42.806
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:42.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:42.879
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 03/27/23 21:09:42.89
Mar 27 21:09:42.916: INFO: Waiting up to 5m0s for pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012" in namespace "containers-3701" to be "Succeeded or Failed"
Mar 27 21:09:42.927: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Pending", Reason="", readiness=false. Elapsed: 11.05845ms
Mar 27 21:09:44.940: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Running", Reason="", readiness=true. Elapsed: 2.024577802s
Mar 27 21:09:46.943: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Running", Reason="", readiness=false. Elapsed: 4.026899719s
Mar 27 21:09:48.941: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025248215s
STEP: Saw pod success 03/27/23 21:09:48.941
Mar 27 21:09:48.941: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012" satisfied condition "Succeeded or Failed"
Mar 27 21:09:48.953: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:09:49.031
Mar 27 21:09:49.068: INFO: Waiting for pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 to disappear
Mar 27 21:09:49.080: INFO: Pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:49.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3701" for this suite. 03/27/23 21:09:49.11
------------------------------
• [SLOW TEST] [6.336 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:42.805
    Mar 27 21:09:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename containers 03/27/23 21:09:42.806
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:42.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:42.879
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 03/27/23 21:09:42.89
    Mar 27 21:09:42.916: INFO: Waiting up to 5m0s for pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012" in namespace "containers-3701" to be "Succeeded or Failed"
    Mar 27 21:09:42.927: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Pending", Reason="", readiness=false. Elapsed: 11.05845ms
    Mar 27 21:09:44.940: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Running", Reason="", readiness=true. Elapsed: 2.024577802s
    Mar 27 21:09:46.943: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Running", Reason="", readiness=false. Elapsed: 4.026899719s
    Mar 27 21:09:48.941: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025248215s
    STEP: Saw pod success 03/27/23 21:09:48.941
    Mar 27 21:09:48.941: INFO: Pod "client-containers-0438f740-0d1d-4a9c-9909-2b833422f012" satisfied condition "Succeeded or Failed"
    Mar 27 21:09:48.953: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:09:49.031
    Mar 27 21:09:49.068: INFO: Waiting for pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 to disappear
    Mar 27 21:09:49.080: INFO: Pod client-containers-0438f740-0d1d-4a9c-9909-2b833422f012 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:49.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3701" for this suite. 03/27/23 21:09:49.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:49.143
Mar 27 21:09:49.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:09:49.144
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:49.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:49.221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:09:49.296
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:09:49.784
STEP: Deploying the webhook pod 03/27/23 21:09:49.812
STEP: Wait for the deployment to be ready 03/27/23 21:09:49.849
Mar 27 21:09:49.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:09:51.932
STEP: Verifying the service has paired with the endpoint 03/27/23 21:09:51.963
Mar 27 21:09:52.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 03/27/23 21:09:52.978
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.057
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/27/23 21:09:53.108
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.148
STEP: Patching a validating webhook configuration's rules to include the create operation 03/27/23 21:09:53.188
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.215
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:53.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7281" for this suite. 03/27/23 21:09:53.425
STEP: Destroying namespace "webhook-7281-markers" for this suite. 03/27/23 21:09:53.464
------------------------------
• [4.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:49.143
    Mar 27 21:09:49.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:09:49.144
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:49.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:49.221
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:09:49.296
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:09:49.784
    STEP: Deploying the webhook pod 03/27/23 21:09:49.812
    STEP: Wait for the deployment to be ready 03/27/23 21:09:49.849
    Mar 27 21:09:49.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:09:51.932
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:09:51.963
    Mar 27 21:09:52.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 03/27/23 21:09:52.978
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.057
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/27/23 21:09:53.108
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.148
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/27/23 21:09:53.188
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:09:53.215
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:53.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7281" for this suite. 03/27/23 21:09:53.425
    STEP: Destroying namespace "webhook-7281-markers" for this suite. 03/27/23 21:09:53.464
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:53.489
Mar 27 21:09:53.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:09:53.491
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:53.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:53.589
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ba90d9be-8539-4de7-9ff4-480f3af87856 03/27/23 21:09:53.617
STEP: Creating the pod 03/27/23 21:09:53.632
Mar 27 21:09:53.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6" in namespace "projected-3239" to be "running and ready"
Mar 27 21:09:53.669: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.559246ms
Mar 27 21:09:53.669: INFO: The phase of Pod pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:09:55.682: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.024552251s
Mar 27 21:09:55.682: INFO: The phase of Pod pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6 is Running (Ready = true)
Mar 27 21:09:55.682: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-ba90d9be-8539-4de7-9ff4-480f3af87856 03/27/23 21:09:55.723
STEP: waiting to observe update in volume 03/27/23 21:09:55.738
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:09:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3239" for this suite. 03/27/23 21:09:57.811
------------------------------
• [4.346 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:53.489
    Mar 27 21:09:53.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:09:53.491
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:53.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:53.589
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-ba90d9be-8539-4de7-9ff4-480f3af87856 03/27/23 21:09:53.617
    STEP: Creating the pod 03/27/23 21:09:53.632
    Mar 27 21:09:53.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6" in namespace "projected-3239" to be "running and ready"
    Mar 27 21:09:53.669: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.559246ms
    Mar 27 21:09:53.669: INFO: The phase of Pod pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:09:55.682: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.024552251s
    Mar 27 21:09:55.682: INFO: The phase of Pod pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6 is Running (Ready = true)
    Mar 27 21:09:55.682: INFO: Pod "pod-projected-configmaps-0549073e-9206-4a10-85fe-c2973818bfd6" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-ba90d9be-8539-4de7-9ff4-480f3af87856 03/27/23 21:09:55.723
    STEP: waiting to observe update in volume 03/27/23 21:09:55.738
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:09:57.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3239" for this suite. 03/27/23 21:09:57.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:09:57.84
Mar 27 21:09:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename containers 03/27/23 21:09:57.841
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:57.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:57.911
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Mar 27 21:09:57.952: INFO: Waiting up to 5m0s for pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3" in namespace "containers-532" to be "running"
Mar 27 21:09:57.964: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547645ms
Mar 27 21:09:59.976: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.024267826s
Mar 27 21:09:59.976: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:00.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-532" for this suite. 03/27/23 21:10:00.034
------------------------------
• [2.217 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:09:57.84
    Mar 27 21:09:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename containers 03/27/23 21:09:57.841
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:09:57.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:09:57.911
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Mar 27 21:09:57.952: INFO: Waiting up to 5m0s for pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3" in namespace "containers-532" to be "running"
    Mar 27 21:09:57.964: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547645ms
    Mar 27 21:09:59.976: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.024267826s
    Mar 27 21:09:59.976: INFO: Pod "client-containers-184bac35-3805-44f7-a022-d75387e196e3" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:00.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-532" for this suite. 03/27/23 21:10:00.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:00.061
Mar 27 21:10:00.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:10:00.062
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:00.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:00.13
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/27/23 21:10:00.142
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/27/23 21:10:00.142
STEP: creating a pod to probe DNS 03/27/23 21:10:00.143
STEP: submitting the pod to kubernetes 03/27/23 21:10:00.143
Mar 27 21:10:00.167: INFO: Waiting up to 15m0s for pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f" in namespace "dns-8572" to be "running"
Mar 27 21:10:00.178: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.529356ms
Mar 27 21:10:02.193: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f": Phase="Running", Reason="", readiness=true. Elapsed: 2.026161212s
Mar 27 21:10:02.193: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:10:02.193
STEP: looking for the results for each expected name from probers 03/27/23 21:10:02.206
Mar 27 21:10:02.325: INFO: DNS probes using dns-8572/dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f succeeded

STEP: deleting the pod 03/27/23 21:10:02.325
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:02.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8572" for this suite. 03/27/23 21:10:02.403
------------------------------
• [2.366 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:00.061
    Mar 27 21:10:00.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:10:00.062
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:00.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:00.13
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/27/23 21:10:00.142
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/27/23 21:10:00.142
    STEP: creating a pod to probe DNS 03/27/23 21:10:00.143
    STEP: submitting the pod to kubernetes 03/27/23 21:10:00.143
    Mar 27 21:10:00.167: INFO: Waiting up to 15m0s for pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f" in namespace "dns-8572" to be "running"
    Mar 27 21:10:00.178: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.529356ms
    Mar 27 21:10:02.193: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f": Phase="Running", Reason="", readiness=true. Elapsed: 2.026161212s
    Mar 27 21:10:02.193: INFO: Pod "dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:10:02.193
    STEP: looking for the results for each expected name from probers 03/27/23 21:10:02.206
    Mar 27 21:10:02.325: INFO: DNS probes using dns-8572/dns-test-62ddaeb8-15c2-4b61-b11e-2425ef41320f succeeded

    STEP: deleting the pod 03/27/23 21:10:02.325
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:02.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8572" for this suite. 03/27/23 21:10:02.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:02.432
Mar 27 21:10:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 21:10:02.433
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:02.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:02.547
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 27 21:10:02.562: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 27 21:10:02.594: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 21:10:07.607: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 21:10:07.607
Mar 27 21:10:07.607: INFO: Creating deployment "test-rolling-update-deployment"
Mar 27 21:10:07.628: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 27 21:10:07.724: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar 27 21:10:09.760: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 27 21:10:09.776: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 21:10:09.824: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4264  d9c08940-afcc-431f-a7d4-1afbb3a0acff 24427 1 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007745b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 21:10:07 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-27 21:10:09 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 21:10:09.841: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4264  2233bbb8-45c3-4354-b0c9-fb6495d9b250 24416 1 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d9c08940-afcc-431f-a7d4-1afbb3a0acff 0xc0050485f7 0xc0050485f8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c08940-afcc-431f-a7d4-1afbb3a0acff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050486a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 21:10:09.841: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 27 21:10:09.841: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4264  851c37ef-73f1-44a3-9c62-63d82c66c746 24425 2 2023-03-27 21:10:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d9c08940-afcc-431f-a7d4-1afbb3a0acff 0xc0050484c7 0xc0050484c8}] [] [{e2e.test Update apps/v1 2023-03-27 21:10:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c08940-afcc-431f-a7d4-1afbb3a0acff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005048588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 21:10:09.854: INFO: Pod "test-rolling-update-deployment-7549d9f46d-fccr9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-fccr9 test-rolling-update-deployment-7549d9f46d- deployment-4264  9e452390-e1ab-4bae-b3b5-7e3ad2bc08d6 24415 0 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:efe0452bf32fad2d4b6a318bffc39f9c52887e4995a90b793afb3bd9c01a99b1 cni.projectcalico.org/podIP:172.30.85.162/32 cni.projectcalico.org/podIPs:172.30.85.162/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2233bbb8-45c3-4354-b0c9-fb6495d9b250 0xc005048e07 0xc005048e08}] [] [{kube-controller-manager Update v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2233bbb8-45c3-4354-b0c9-fb6495d9b250\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbxq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbxq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.162,StartTime:2023-03-27 21:10:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c1f3b75ce9df10e98956159e526ae22141e0a0fb0f9a3e340f1525a46dc2a2b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:09.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4264" for this suite. 03/27/23 21:10:09.875
------------------------------
• [SLOW TEST] [7.469 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:02.432
    Mar 27 21:10:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 21:10:02.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:02.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:02.547
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 27 21:10:02.562: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 27 21:10:02.594: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 21:10:07.607: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 21:10:07.607
    Mar 27 21:10:07.607: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 27 21:10:07.628: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 27 21:10:07.724: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
    Mar 27 21:10:09.760: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 27 21:10:09.776: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 21:10:09.824: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4264  d9c08940-afcc-431f-a7d4-1afbb3a0acff 24427 1 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007745b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 21:10:07 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-03-27 21:10:09 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 21:10:09.841: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4264  2233bbb8-45c3-4354-b0c9-fb6495d9b250 24416 1 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d9c08940-afcc-431f-a7d4-1afbb3a0acff 0xc0050485f7 0xc0050485f8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c08940-afcc-431f-a7d4-1afbb3a0acff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050486a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 21:10:09.841: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 27 21:10:09.841: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4264  851c37ef-73f1-44a3-9c62-63d82c66c746 24425 2 2023-03-27 21:10:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d9c08940-afcc-431f-a7d4-1afbb3a0acff 0xc0050484c7 0xc0050484c8}] [] [{e2e.test Update apps/v1 2023-03-27 21:10:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9c08940-afcc-431f-a7d4-1afbb3a0acff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005048588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 21:10:09.854: INFO: Pod "test-rolling-update-deployment-7549d9f46d-fccr9" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-fccr9 test-rolling-update-deployment-7549d9f46d- deployment-4264  9e452390-e1ab-4bae-b3b5-7e3ad2bc08d6 24415 0 2023-03-27 21:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:efe0452bf32fad2d4b6a318bffc39f9c52887e4995a90b793afb3bd9c01a99b1 cni.projectcalico.org/podIP:172.30.85.162/32 cni.projectcalico.org/podIPs:172.30.85.162/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2233bbb8-45c3-4354-b0c9-fb6495d9b250 0xc005048e07 0xc005048e08}] [] [{kube-controller-manager Update v1 2023-03-27 21:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2233bbb8-45c3-4354-b0c9-fb6495d9b250\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:10:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbxq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbxq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.162,StartTime:2023-03-27 21:10:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:10:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c1f3b75ce9df10e98956159e526ae22141e0a0fb0f9a3e340f1525a46dc2a2b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:09.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4264" for this suite. 03/27/23 21:10:09.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:09.905
Mar 27 21:10:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:10:09.906
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:09.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:09.972
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-2c12d6bd-6e8a-48f3-9c83-50ee14625085 03/27/23 21:10:09.983
STEP: Creating a pod to test consume configMaps 03/27/23 21:10:10
Mar 27 21:10:10.024: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4" in namespace "projected-4165" to be "Succeeded or Failed"
Mar 27 21:10:10.037: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.793667ms
Mar 27 21:10:12.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.025650675s
Mar 27 21:10:14.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Running", Reason="", readiness=false. Elapsed: 4.025392602s
Mar 27 21:10:16.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025630097s
STEP: Saw pod success 03/27/23 21:10:16.049
Mar 27 21:10:16.050: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4" satisfied condition "Succeeded or Failed"
Mar 27 21:10:16.065: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:10:16.094
Mar 27 21:10:16.126: INFO: Waiting for pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 to disappear
Mar 27 21:10:16.137: INFO: Pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:16.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4165" for this suite. 03/27/23 21:10:16.158
------------------------------
• [SLOW TEST] [6.283 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:09.905
    Mar 27 21:10:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:10:09.906
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:09.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:09.972
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-2c12d6bd-6e8a-48f3-9c83-50ee14625085 03/27/23 21:10:09.983
    STEP: Creating a pod to test consume configMaps 03/27/23 21:10:10
    Mar 27 21:10:10.024: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4" in namespace "projected-4165" to be "Succeeded or Failed"
    Mar 27 21:10:10.037: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.793667ms
    Mar 27 21:10:12.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.025650675s
    Mar 27 21:10:14.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Running", Reason="", readiness=false. Elapsed: 4.025392602s
    Mar 27 21:10:16.049: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025630097s
    STEP: Saw pod success 03/27/23 21:10:16.049
    Mar 27 21:10:16.050: INFO: Pod "pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4" satisfied condition "Succeeded or Failed"
    Mar 27 21:10:16.065: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:10:16.094
    Mar 27 21:10:16.126: INFO: Waiting for pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 to disappear
    Mar 27 21:10:16.137: INFO: Pod pod-projected-configmaps-6846e0a9-7b8b-459f-bb7b-265739ad32f4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:16.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4165" for this suite. 03/27/23 21:10:16.158
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:16.189
Mar 27 21:10:16.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:10:16.19
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:16.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:16.254
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 03/27/23 21:10:16.277
Mar 27 21:10:16.300: INFO: Waiting up to 5m0s for pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680" in namespace "projected-8434" to be "running and ready"
Mar 27 21:10:16.312: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680": Phase="Pending", Reason="", readiness=false. Elapsed: 11.912346ms
Mar 27 21:10:16.312: INFO: The phase of Pod annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:10:18.324: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680": Phase="Running", Reason="", readiness=true. Elapsed: 2.024559648s
Mar 27 21:10:18.324: INFO: The phase of Pod annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680 is Running (Ready = true)
Mar 27 21:10:18.324: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680" satisfied condition "running and ready"
Mar 27 21:10:18.893: INFO: Successfully updated pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:20.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8434" for this suite. 03/27/23 21:10:20.971
------------------------------
• [4.814 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:16.189
    Mar 27 21:10:16.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:10:16.19
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:16.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:16.254
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 03/27/23 21:10:16.277
    Mar 27 21:10:16.300: INFO: Waiting up to 5m0s for pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680" in namespace "projected-8434" to be "running and ready"
    Mar 27 21:10:16.312: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680": Phase="Pending", Reason="", readiness=false. Elapsed: 11.912346ms
    Mar 27 21:10:16.312: INFO: The phase of Pod annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:10:18.324: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680": Phase="Running", Reason="", readiness=true. Elapsed: 2.024559648s
    Mar 27 21:10:18.324: INFO: The phase of Pod annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680 is Running (Ready = true)
    Mar 27 21:10:18.324: INFO: Pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680" satisfied condition "running and ready"
    Mar 27 21:10:18.893: INFO: Successfully updated pod "annotationupdate8f4d6248-fde0-425d-b62c-8c77df432680"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:20.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8434" for this suite. 03/27/23 21:10:20.971
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:21.004
Mar 27 21:10:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:10:21.006
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:21.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:21.071
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 03/27/23 21:10:21.099
STEP: watching for the Service to be added 03/27/23 21:10:21.131
Mar 27 21:10:21.138: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 27 21:10:21.138: INFO: Service test-service-pwckk created
STEP: Getting /status 03/27/23 21:10:21.138
Mar 27 21:10:21.151: INFO: Service test-service-pwckk has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/27/23 21:10:21.151
STEP: watching for the Service to be patched 03/27/23 21:10:21.173
Mar 27 21:10:21.179: INFO: observed Service test-service-pwckk in namespace services-3525 with annotations: map[] & LoadBalancer: {[]}
Mar 27 21:10:21.179: INFO: Found Service test-service-pwckk in namespace services-3525 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 27 21:10:21.179: INFO: Service test-service-pwckk has service status patched
STEP: updating the ServiceStatus 03/27/23 21:10:21.179
Mar 27 21:10:21.209: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/27/23 21:10:21.21
Mar 27 21:10:21.216: INFO: Observed Service test-service-pwckk in namespace services-3525 with annotations: map[] & Conditions: {[]}
Mar 27 21:10:21.216: INFO: Observed event: &Service{ObjectMeta:{test-service-pwckk  services-3525  e633343d-341d-4d66-9ad6-6a265f011fe5 24520 0 2023-03-27 21:10:21 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-27 21:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-27 21:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.43.205,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.43.205],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 27 21:10:21.217: INFO: Found Service test-service-pwckk in namespace services-3525 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 21:10:21.217: INFO: Service test-service-pwckk has service status updated
STEP: patching the service 03/27/23 21:10:21.217
STEP: watching for the Service to be patched 03/27/23 21:10:21.239
Mar 27 21:10:21.246: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
Mar 27 21:10:21.246: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
Mar 27 21:10:21.247: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
Mar 27 21:10:21.247: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service:patched test-service-static:true]
Mar 27 21:10:21.247: INFO: Service test-service-pwckk patched
STEP: deleting the service 03/27/23 21:10:21.247
STEP: watching for the Service to be deleted 03/27/23 21:10:21.294
Mar 27 21:10:21.300: INFO: Observed event: ADDED
Mar 27 21:10:21.300: INFO: Observed event: MODIFIED
Mar 27 21:10:21.300: INFO: Observed event: MODIFIED
Mar 27 21:10:21.301: INFO: Observed event: MODIFIED
Mar 27 21:10:21.301: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 27 21:10:21.301: INFO: Service test-service-pwckk deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3525" for this suite. 03/27/23 21:10:21.321
------------------------------
• [0.340 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:21.004
    Mar 27 21:10:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:10:21.006
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:21.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:21.071
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 03/27/23 21:10:21.099
    STEP: watching for the Service to be added 03/27/23 21:10:21.131
    Mar 27 21:10:21.138: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 27 21:10:21.138: INFO: Service test-service-pwckk created
    STEP: Getting /status 03/27/23 21:10:21.138
    Mar 27 21:10:21.151: INFO: Service test-service-pwckk has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/27/23 21:10:21.151
    STEP: watching for the Service to be patched 03/27/23 21:10:21.173
    Mar 27 21:10:21.179: INFO: observed Service test-service-pwckk in namespace services-3525 with annotations: map[] & LoadBalancer: {[]}
    Mar 27 21:10:21.179: INFO: Found Service test-service-pwckk in namespace services-3525 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 27 21:10:21.179: INFO: Service test-service-pwckk has service status patched
    STEP: updating the ServiceStatus 03/27/23 21:10:21.179
    Mar 27 21:10:21.209: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/27/23 21:10:21.21
    Mar 27 21:10:21.216: INFO: Observed Service test-service-pwckk in namespace services-3525 with annotations: map[] & Conditions: {[]}
    Mar 27 21:10:21.216: INFO: Observed event: &Service{ObjectMeta:{test-service-pwckk  services-3525  e633343d-341d-4d66-9ad6-6a265f011fe5 24520 0 2023-03-27 21:10:21 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-27 21:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-27 21:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.43.205,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.43.205],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 27 21:10:21.217: INFO: Found Service test-service-pwckk in namespace services-3525 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 21:10:21.217: INFO: Service test-service-pwckk has service status updated
    STEP: patching the service 03/27/23 21:10:21.217
    STEP: watching for the Service to be patched 03/27/23 21:10:21.239
    Mar 27 21:10:21.246: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
    Mar 27 21:10:21.246: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
    Mar 27 21:10:21.247: INFO: observed Service test-service-pwckk in namespace services-3525 with labels: map[test-service-static:true]
    Mar 27 21:10:21.247: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service:patched test-service-static:true]
    Mar 27 21:10:21.247: INFO: Service test-service-pwckk patched
    STEP: deleting the service 03/27/23 21:10:21.247
    STEP: watching for the Service to be deleted 03/27/23 21:10:21.294
    Mar 27 21:10:21.300: INFO: Observed event: ADDED
    Mar 27 21:10:21.300: INFO: Observed event: MODIFIED
    Mar 27 21:10:21.300: INFO: Observed event: MODIFIED
    Mar 27 21:10:21.301: INFO: Observed event: MODIFIED
    Mar 27 21:10:21.301: INFO: Found Service test-service-pwckk in namespace services-3525 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 27 21:10:21.301: INFO: Service test-service-pwckk deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3525" for this suite. 03/27/23 21:10:21.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:21.346
Mar 27 21:10:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:10:21.348
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:21.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:21.421
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-4a6db660-be97-4dc1-9f13-54e3c1550aa7 03/27/23 21:10:21.432
STEP: Creating a pod to test consume secrets 03/27/23 21:10:21.447
Mar 27 21:10:21.472: INFO: Waiting up to 5m0s for pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88" in namespace "secrets-7234" to be "Succeeded or Failed"
Mar 27 21:10:21.493: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Pending", Reason="", readiness=false. Elapsed: 20.516343ms
Mar 27 21:10:23.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033336256s
Mar 27 21:10:25.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033099898s
STEP: Saw pod success 03/27/23 21:10:25.506
Mar 27 21:10:25.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88" satisfied condition "Succeeded or Failed"
Mar 27 21:10:25.518: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:10:25.546
Mar 27 21:10:25.575: INFO: Waiting for pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 to disappear
Mar 27 21:10:25.588: INFO: Pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7234" for this suite. 03/27/23 21:10:25.608
------------------------------
• [4.284 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:21.346
    Mar 27 21:10:21.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:10:21.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:21.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:21.421
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-4a6db660-be97-4dc1-9f13-54e3c1550aa7 03/27/23 21:10:21.432
    STEP: Creating a pod to test consume secrets 03/27/23 21:10:21.447
    Mar 27 21:10:21.472: INFO: Waiting up to 5m0s for pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88" in namespace "secrets-7234" to be "Succeeded or Failed"
    Mar 27 21:10:21.493: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Pending", Reason="", readiness=false. Elapsed: 20.516343ms
    Mar 27 21:10:23.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033336256s
    Mar 27 21:10:25.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033099898s
    STEP: Saw pod success 03/27/23 21:10:25.506
    Mar 27 21:10:25.506: INFO: Pod "pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88" satisfied condition "Succeeded or Failed"
    Mar 27 21:10:25.518: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:10:25.546
    Mar 27 21:10:25.575: INFO: Waiting for pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 to disappear
    Mar 27 21:10:25.588: INFO: Pod pod-secrets-827a94dd-8bc2-4c92-a808-bbaa047bcd88 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:25.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7234" for this suite. 03/27/23 21:10:25.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:25.631
Mar 27 21:10:25.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 21:10:25.632
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:25.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:25.717
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/27/23 21:10:25.75
STEP: waiting for Deployment to be created 03/27/23 21:10:25.769
STEP: waiting for all Replicas to be Ready 03/27/23 21:10:25.776
Mar 27 21:10:25.782: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.782: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.797: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.797: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.826: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.826: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.858: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:25.858: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 27 21:10:27.234: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 27 21:10:27.234: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 27 21:10:27.251: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/27/23 21:10:27.251
W0327 21:10:27.276430      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 21:10:27.282: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/27/23 21:10:27.282
Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.306: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.306: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.339: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.339: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:27.368: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:27.369: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:27.391: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:27.391: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:29.293: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:29.293: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:29.345: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
STEP: listing Deployments 03/27/23 21:10:29.345
Mar 27 21:10:29.370: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/27/23 21:10:29.37
Mar 27 21:10:29.402: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/27/23 21:10:29.402
Mar 27 21:10:29.426: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:29.434: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:29.458: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:29.495: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:29.509: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:31.246: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:31.284: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:31.340: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:31.364: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 27 21:10:33.264: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/27/23 21:10:33.406
STEP: fetching the DeploymentStatus 03/27/23 21:10:33.44
Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3
Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3
STEP: deleting the Deployment 03/27/23 21:10:33.466
Mar 27 21:10:33.509: INFO: observed event type MODIFIED
Mar 27 21:10:33.509: INFO: observed event type MODIFIED
Mar 27 21:10:33.509: INFO: observed event type MODIFIED
Mar 27 21:10:33.509: INFO: observed event type MODIFIED
Mar 27 21:10:33.509: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
Mar 27 21:10:33.510: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 21:10:33.532: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 27 21:10:33.550: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7396  f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa 24775 4 2023-03-27 21:10:27 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4ad0a962-7299-470a-8258-5542790087b0 0xc001ea6b37 0xc001ea6b38}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ad0a962-7299-470a-8258-5542790087b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6bc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 27 21:10:33.564: INFO: pod: "test-deployment-7df74c55ff-2zwlc":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-2zwlc test-deployment-7df74c55ff- deployment-7396  02da290f-29c3-4a85-9d95-d65c4ac7d57a 24770 0 2023-03-27 21:10:29 +0000 UTC 2023-03-27 21:10:34 +0000 UTC 0xc001ea70b8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:47948ed80a067aef9c831a54a71b9db0c931f069645c8a9a0a1b3060be6d1faa cni.projectcalico.org/podIP:172.30.56.105/32 cni.projectcalico.org/podIPs:172.30.56.105/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa 0xc001ea7117 0xc001ea7118}] [] [{kube-controller-manager Update v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:10:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.105,StartTime:2023-03-27 21:10:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:10:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://833ed3c4e224e44340ffa050b9f3d05883e7bc2c141e60e971de82fdf05fce21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 27 21:10:33.564: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7396  59aca441-2d99-4901-adb3-7b94e67e8a7e 24663 3 2023-03-27 21:10:25 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4ad0a962-7299-470a-8258-5542790087b0 0xc001ea6c27 0xc001ea6c28}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ad0a962-7299-470a-8258-5542790087b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 21:10:33.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7396" for this suite. 03/27/23 21:10:33.603
------------------------------
• [SLOW TEST] [8.009 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:25.631
    Mar 27 21:10:25.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 21:10:25.632
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:25.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:25.717
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/27/23 21:10:25.75
    STEP: waiting for Deployment to be created 03/27/23 21:10:25.769
    STEP: waiting for all Replicas to be Ready 03/27/23 21:10:25.776
    Mar 27 21:10:25.782: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.782: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.797: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.797: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.826: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.826: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.858: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:25.858: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 27 21:10:27.234: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 27 21:10:27.234: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 27 21:10:27.251: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/27/23 21:10:27.251
    W0327 21:10:27.276430      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 21:10:27.282: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/27/23 21:10:27.282
    Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.288: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.289: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 0
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.290: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.306: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.306: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.339: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.339: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:27.368: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:27.369: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:27.391: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:27.391: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:29.293: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:29.293: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:29.345: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    STEP: listing Deployments 03/27/23 21:10:29.345
    Mar 27 21:10:29.370: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/27/23 21:10:29.37
    Mar 27 21:10:29.402: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/27/23 21:10:29.402
    Mar 27 21:10:29.426: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:29.434: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:29.458: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:29.495: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:29.509: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:31.246: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:31.284: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:31.340: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:31.364: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 27 21:10:33.264: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/27/23 21:10:33.406
    STEP: fetching the DeploymentStatus 03/27/23 21:10:33.44
    Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:33.464: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 1
    Mar 27 21:10:33.465: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3
    Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 2
    Mar 27 21:10:33.466: INFO: observed Deployment test-deployment in namespace deployment-7396 with ReadyReplicas 3
    STEP: deleting the Deployment 03/27/23 21:10:33.466
    Mar 27 21:10:33.509: INFO: observed event type MODIFIED
    Mar 27 21:10:33.509: INFO: observed event type MODIFIED
    Mar 27 21:10:33.509: INFO: observed event type MODIFIED
    Mar 27 21:10:33.509: INFO: observed event type MODIFIED
    Mar 27 21:10:33.509: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    Mar 27 21:10:33.510: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 21:10:33.532: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 27 21:10:33.550: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7396  f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa 24775 4 2023-03-27 21:10:27 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4ad0a962-7299-470a-8258-5542790087b0 0xc001ea6b37 0xc001ea6b38}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ad0a962-7299-470a-8258-5542790087b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6bc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 27 21:10:33.564: INFO: pod: "test-deployment-7df74c55ff-2zwlc":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-2zwlc test-deployment-7df74c55ff- deployment-7396  02da290f-29c3-4a85-9d95-d65c4ac7d57a 24770 0 2023-03-27 21:10:29 +0000 UTC 2023-03-27 21:10:34 +0000 UTC 0xc001ea70b8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:47948ed80a067aef9c831a54a71b9db0c931f069645c8a9a0a1b3060be6d1faa cni.projectcalico.org/podIP:172.30.56.105/32 cni.projectcalico.org/podIPs:172.30.56.105/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa 0xc001ea7117 0xc001ea7118}] [] [{kube-controller-manager Update v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8fb5c02-73d1-4ee2-9a7c-19fdfe792cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:10:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:10:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:10:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.105,StartTime:2023-03-27 21:10:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:10:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://833ed3c4e224e44340ffa050b9f3d05883e7bc2c141e60e971de82fdf05fce21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 27 21:10:33.564: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7396  59aca441-2d99-4901-adb3-7b94e67e8a7e 24663 3 2023-03-27 21:10:25 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4ad0a962-7299-470a-8258-5542790087b0 0xc001ea6c27 0xc001ea6c28}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ad0a962-7299-470a-8258-5542790087b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:10:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6cb0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:10:33.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7396" for this suite. 03/27/23 21:10:33.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:10:33.648
Mar 27 21:10:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-pred 03/27/23 21:10:33.649
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:33.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:33.725
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 27 21:10:33.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 21:10:33.769: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 21:10:33.783: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.175 before test
Mar 27 21:10:33.822: INFO: test-deployment-7b7876f9d6-rh49f from deployment-7396 started at 2023-03-27 21:10:31 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container test-deployment ready: true, restart count 0
Mar 27 21:10:33.822: INFO: test-deployment-7df74c55ff-2zwlc from deployment-7396 started at 2023-03-27 21:10:29 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container test-deployment ready: true, restart count 0
Mar 27 21:10:33.822: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:10:33.822: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:10:33.822: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:10:33.822: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:10:33.822: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:10:33.822: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:10:33.822: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:10:33.822: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:10:33.822: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:10:33.822: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:10:33.822: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container e2e ready: true, restart count 0
Mar 27 21:10:33.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:10:33.822: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:10:33.822: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:10:33.822: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.822: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar 27 21:10:33.822: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.177 before test
Mar 27 21:10:33.853: INFO: test-deployment-7b7876f9d6-5pvgb from deployment-7396 started at 2023-03-27 21:10:29 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container test-deployment ready: true, restart count 0
Mar 27 21:10:33.853: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-4dv5m from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:10:33.853: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:10:33.853: INFO: calico-typha-7f67cb7cc9-r6hcj from kube-system started at 2023-03-27 19:00:01 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:10:33.853: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:10:33.853: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:10:33.853: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:10:33.853: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:10:33.853: INFO: ingress-cluster-healthcheck-b895f86ff-ttch8 from kube-system started at 2023-03-27 19:08:06 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Mar 27 21:10:33.853: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:10:33.853: INFO: metrics-server-6c65f45547-2qssm from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:10:33.853: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:10:33.853: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:10:33.853: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-tqnwf from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:10:33.853: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 21:10:33.853: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:10:33.853: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:10:33.853: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.178 before test
Mar 27 21:10:33.885: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 21:10:33.885: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:10:33.885: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:10:33.885: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:10:33.885: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:10:33.885: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 21:10:33.885: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 27 21:10:33.885: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 27 21:10:33.885: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:10:33.885: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.885: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:10:33.886: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:10:33.886: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 27 21:10:33.886: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:10:33.886: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 27 21:10:33.886: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:10:33.886: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 27 21:10:33.886: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:10:33.886: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:10:33.886: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:10:33.886: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:10:33.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:10:33.886: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:10:33.886
Mar 27 21:10:33.912: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-628" to be "running"
Mar 27 21:10:33.923: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.941403ms
Mar 27 21:10:35.956: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.044132184s
Mar 27 21:10:35.956: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:10:35.974
STEP: Trying to apply a random label on the found node. 03/27/23 21:10:36.027
STEP: verifying the node has the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 95 03/27/23 21:10:36.049
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/27/23 21:10:36.061
Mar 27 21:10:36.076: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-628" to be "not pending"
Mar 27 21:10:36.087: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.548216ms
Mar 27 21:10:38.100: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024673409s
Mar 27 21:10:38.100: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.176.99.177 on the node which pod4 resides and expect not scheduled 03/27/23 21:10:38.1
Mar 27 21:10:38.115: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-628" to be "not pending"
Mar 27 21:10:38.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.777528ms
Mar 27 21:10:40.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024318438s
Mar 27 21:10:42.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025831398s
Mar 27 21:10:44.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02696754s
Mar 27 21:10:46.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024113046s
Mar 27 21:10:48.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025237517s
Mar 27 21:10:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024301648s
Mar 27 21:10:52.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025459252s
Mar 27 21:10:54.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025116499s
Mar 27 21:10:56.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025419252s
Mar 27 21:10:58.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025612488s
Mar 27 21:11:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024895034s
Mar 27 21:11:02.147: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.031398344s
Mar 27 21:11:04.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024736658s
Mar 27 21:11:06.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025680815s
Mar 27 21:11:08.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02623205s
Mar 27 21:11:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024871919s
Mar 27 21:11:12.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.026019878s
Mar 27 21:11:14.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025501447s
Mar 27 21:11:16.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022939296s
Mar 27 21:11:18.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.026301301s
Mar 27 21:11:20.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026993249s
Mar 27 21:11:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025328072s
Mar 27 21:11:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025743195s
Mar 27 21:11:26.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023959824s
Mar 27 21:11:28.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025541162s
Mar 27 21:11:30.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024455122s
Mar 27 21:11:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026052496s
Mar 27 21:11:34.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026335624s
Mar 27 21:11:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024217838s
Mar 27 21:11:38.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025343233s
Mar 27 21:11:40.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023908147s
Mar 27 21:11:42.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023972367s
Mar 27 21:11:44.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025350503s
Mar 27 21:11:46.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025861461s
Mar 27 21:11:48.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.024828548s
Mar 27 21:11:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02408705s
Mar 27 21:11:52.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.028243473s
Mar 27 21:11:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024788739s
Mar 27 21:11:56.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025351432s
Mar 27 21:11:58.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024217004s
Mar 27 21:12:00.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.030434527s
Mar 27 21:12:02.156: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.040347103s
Mar 27 21:12:04.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023783829s
Mar 27 21:12:06.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026087371s
Mar 27 21:12:08.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025313912s
Mar 27 21:12:10.168: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.052879334s
Mar 27 21:12:12.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024417148s
Mar 27 21:12:14.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024871648s
Mar 27 21:12:16.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023323047s
Mar 27 21:12:18.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025132333s
Mar 27 21:12:20.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025939754s
Mar 27 21:12:22.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.024023942s
Mar 27 21:12:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025126503s
Mar 27 21:12:26.150: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.034711467s
Mar 27 21:12:28.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.024464497s
Mar 27 21:12:30.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023650836s
Mar 27 21:12:32.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025960254s
Mar 27 21:12:34.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025672009s
Mar 27 21:12:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024750102s
Mar 27 21:12:38.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024512932s
Mar 27 21:12:40.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.024396855s
Mar 27 21:12:42.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02813391s
Mar 27 21:12:44.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.024505577s
Mar 27 21:12:46.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.024728997s
Mar 27 21:12:48.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.02662639s
Mar 27 21:12:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.02427052s
Mar 27 21:12:52.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.024918842s
Mar 27 21:12:54.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.027565708s
Mar 27 21:12:56.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.02335166s
Mar 27 21:12:58.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02376737s
Mar 27 21:13:00.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.025643508s
Mar 27 21:13:02.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024701551s
Mar 27 21:13:04.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.028129958s
Mar 27 21:13:06.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.023712191s
Mar 27 21:13:08.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024116356s
Mar 27 21:13:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.024456142s
Mar 27 21:13:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025546682s
Mar 27 21:13:14.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023895491s
Mar 27 21:13:16.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.026970464s
Mar 27 21:13:18.175: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.059209937s
Mar 27 21:13:20.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024390047s
Mar 27 21:13:22.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.023552969s
Mar 27 21:13:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.024398603s
Mar 27 21:13:26.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022609312s
Mar 27 21:13:28.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027985873s
Mar 27 21:13:30.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.023788576s
Mar 27 21:13:32.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.025697371s
Mar 27 21:13:34.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.028454931s
Mar 27 21:13:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.024495014s
Mar 27 21:13:38.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.024993102s
Mar 27 21:13:40.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.025944114s
Mar 27 21:13:42.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024055775s
Mar 27 21:13:44.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.023789303s
Mar 27 21:13:46.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025060174s
Mar 27 21:13:48.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.030002562s
Mar 27 21:13:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024722078s
Mar 27 21:13:52.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.027406544s
Mar 27 21:13:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.024536369s
Mar 27 21:13:56.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023670311s
Mar 27 21:13:58.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.02494936s
Mar 27 21:14:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024286511s
Mar 27 21:14:02.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.025894518s
Mar 27 21:14:04.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.029385722s
Mar 27 21:14:06.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.02442266s
Mar 27 21:14:08.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024401692s
Mar 27 21:14:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.024285682s
Mar 27 21:14:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024992459s
Mar 27 21:14:14.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.026048125s
Mar 27 21:14:16.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024889612s
Mar 27 21:14:18.157: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.04183975s
Mar 27 21:14:20.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.024071032s
Mar 27 21:14:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.024989987s
Mar 27 21:14:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.024192693s
Mar 27 21:14:26.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022677855s
Mar 27 21:14:28.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.026215028s
Mar 27 21:14:30.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024454421s
Mar 27 21:14:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.026474377s
Mar 27 21:14:34.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.024779031s
Mar 27 21:14:36.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.022807181s
Mar 27 21:14:38.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.024630089s
Mar 27 21:14:40.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026019089s
Mar 27 21:14:42.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.025386979s
Mar 27 21:14:44.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024452923s
Mar 27 21:14:46.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023666957s
Mar 27 21:14:48.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.024011231s
Mar 27 21:14:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.024311645s
Mar 27 21:14:52.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.026343877s
Mar 27 21:14:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.024076878s
Mar 27 21:14:56.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.024790912s
Mar 27 21:14:58.156: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.040828441s
Mar 27 21:15:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024600667s
Mar 27 21:15:02.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.03047188s
Mar 27 21:15:04.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.028824403s
Mar 27 21:15:06.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.035528119s
Mar 27 21:15:08.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.023975009s
Mar 27 21:15:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.024447029s
Mar 27 21:15:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02538772s
Mar 27 21:15:14.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.025357085s
Mar 27 21:15:16.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.023710256s
Mar 27 21:15:18.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.023707344s
Mar 27 21:15:20.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.023591085s
Mar 27 21:15:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.025051289s
Mar 27 21:15:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024396627s
Mar 27 21:15:26.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.02887389s
Mar 27 21:15:28.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024226007s
Mar 27 21:15:30.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.02580665s
Mar 27 21:15:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.026684769s
Mar 27 21:15:34.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.024580127s
Mar 27 21:15:36.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.025586075s
Mar 27 21:15:38.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035379732s
Mar 27 21:15:38.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.067796088s
STEP: removing the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 off the node 10.176.99.177 03/27/23 21:15:38.183
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 03/27/23 21:15:38.229
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:15:38.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-628" for this suite. 03/27/23 21:15:38.262
------------------------------
• [SLOW TEST] [304.638 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:10:33.648
    Mar 27 21:10:33.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-pred 03/27/23 21:10:33.649
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:10:33.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:10:33.725
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 27 21:10:33.735: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 21:10:33.769: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 21:10:33.783: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.175 before test
    Mar 27 21:10:33.822: INFO: test-deployment-7b7876f9d6-rh49f from deployment-7396 started at 2023-03-27 21:10:31 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container test-deployment ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: test-deployment-7df74c55ff-2zwlc from deployment-7396 started at 2023-03-27 21:10:29 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container test-deployment ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.822: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Mar 27 21:10:33.822: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.177 before test
    Mar 27 21:10:33.853: INFO: test-deployment-7b7876f9d6-5pvgb from deployment-7396 started at 2023-03-27 21:10:29 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container test-deployment ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-4dv5m from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: calico-typha-7f67cb7cc9-r6hcj from kube-system started at 2023-03-27 19:00:01 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: ingress-cluster-healthcheck-b895f86ff-ttch8 from kube-system started at 2023-03-27 19:08:06 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: metrics-server-6c65f45547-2qssm from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-tqnwf from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:10:33.853: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.178 before test
    Mar 27 21:10:33.885: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:10:33.885: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.885: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:10:33.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:10:33.886: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:10:33.886
    Mar 27 21:10:33.912: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-628" to be "running"
    Mar 27 21:10:33.923: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.941403ms
    Mar 27 21:10:35.956: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.044132184s
    Mar 27 21:10:35.956: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:10:35.974
    STEP: Trying to apply a random label on the found node. 03/27/23 21:10:36.027
    STEP: verifying the node has the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 95 03/27/23 21:10:36.049
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/27/23 21:10:36.061
    Mar 27 21:10:36.076: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-628" to be "not pending"
    Mar 27 21:10:36.087: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.548216ms
    Mar 27 21:10:38.100: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.024673409s
    Mar 27 21:10:38.100: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.176.99.177 on the node which pod4 resides and expect not scheduled 03/27/23 21:10:38.1
    Mar 27 21:10:38.115: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-628" to be "not pending"
    Mar 27 21:10:38.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.777528ms
    Mar 27 21:10:40.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024318438s
    Mar 27 21:10:42.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025831398s
    Mar 27 21:10:44.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02696754s
    Mar 27 21:10:46.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024113046s
    Mar 27 21:10:48.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025237517s
    Mar 27 21:10:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024301648s
    Mar 27 21:10:52.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025459252s
    Mar 27 21:10:54.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025116499s
    Mar 27 21:10:56.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025419252s
    Mar 27 21:10:58.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025612488s
    Mar 27 21:11:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024895034s
    Mar 27 21:11:02.147: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.031398344s
    Mar 27 21:11:04.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024736658s
    Mar 27 21:11:06.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.025680815s
    Mar 27 21:11:08.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02623205s
    Mar 27 21:11:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024871919s
    Mar 27 21:11:12.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.026019878s
    Mar 27 21:11:14.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025501447s
    Mar 27 21:11:16.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022939296s
    Mar 27 21:11:18.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.026301301s
    Mar 27 21:11:20.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026993249s
    Mar 27 21:11:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025328072s
    Mar 27 21:11:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025743195s
    Mar 27 21:11:26.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023959824s
    Mar 27 21:11:28.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025541162s
    Mar 27 21:11:30.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024455122s
    Mar 27 21:11:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026052496s
    Mar 27 21:11:34.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026335624s
    Mar 27 21:11:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024217838s
    Mar 27 21:11:38.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025343233s
    Mar 27 21:11:40.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023908147s
    Mar 27 21:11:42.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023972367s
    Mar 27 21:11:44.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025350503s
    Mar 27 21:11:46.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025861461s
    Mar 27 21:11:48.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.024828548s
    Mar 27 21:11:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.02408705s
    Mar 27 21:11:52.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.028243473s
    Mar 27 21:11:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024788739s
    Mar 27 21:11:56.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025351432s
    Mar 27 21:11:58.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024217004s
    Mar 27 21:12:00.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.030434527s
    Mar 27 21:12:02.156: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.040347103s
    Mar 27 21:12:04.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.023783829s
    Mar 27 21:12:06.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026087371s
    Mar 27 21:12:08.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025313912s
    Mar 27 21:12:10.168: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.052879334s
    Mar 27 21:12:12.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024417148s
    Mar 27 21:12:14.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024871648s
    Mar 27 21:12:16.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.023323047s
    Mar 27 21:12:18.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025132333s
    Mar 27 21:12:20.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025939754s
    Mar 27 21:12:22.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.024023942s
    Mar 27 21:12:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025126503s
    Mar 27 21:12:26.150: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.034711467s
    Mar 27 21:12:28.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.024464497s
    Mar 27 21:12:30.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023650836s
    Mar 27 21:12:32.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025960254s
    Mar 27 21:12:34.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025672009s
    Mar 27 21:12:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024750102s
    Mar 27 21:12:38.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024512932s
    Mar 27 21:12:40.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.024396855s
    Mar 27 21:12:42.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02813391s
    Mar 27 21:12:44.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.024505577s
    Mar 27 21:12:46.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.024728997s
    Mar 27 21:12:48.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.02662639s
    Mar 27 21:12:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.02427052s
    Mar 27 21:12:52.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.024918842s
    Mar 27 21:12:54.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.027565708s
    Mar 27 21:12:56.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.02335166s
    Mar 27 21:12:58.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.02376737s
    Mar 27 21:13:00.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.025643508s
    Mar 27 21:13:02.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024701551s
    Mar 27 21:13:04.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.028129958s
    Mar 27 21:13:06.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.023712191s
    Mar 27 21:13:08.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.024116356s
    Mar 27 21:13:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.024456142s
    Mar 27 21:13:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025546682s
    Mar 27 21:13:14.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.023895491s
    Mar 27 21:13:16.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.026970464s
    Mar 27 21:13:18.175: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.059209937s
    Mar 27 21:13:20.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024390047s
    Mar 27 21:13:22.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.023552969s
    Mar 27 21:13:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.024398603s
    Mar 27 21:13:26.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022609312s
    Mar 27 21:13:28.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027985873s
    Mar 27 21:13:30.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.023788576s
    Mar 27 21:13:32.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.025697371s
    Mar 27 21:13:34.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.028454931s
    Mar 27 21:13:36.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.024495014s
    Mar 27 21:13:38.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.024993102s
    Mar 27 21:13:40.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.025944114s
    Mar 27 21:13:42.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024055775s
    Mar 27 21:13:44.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.023789303s
    Mar 27 21:13:46.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025060174s
    Mar 27 21:13:48.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.030002562s
    Mar 27 21:13:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024722078s
    Mar 27 21:13:52.143: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.027406544s
    Mar 27 21:13:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.024536369s
    Mar 27 21:13:56.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023670311s
    Mar 27 21:13:58.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.02494936s
    Mar 27 21:14:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.024286511s
    Mar 27 21:14:02.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.025894518s
    Mar 27 21:14:04.145: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.029385722s
    Mar 27 21:14:06.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.02442266s
    Mar 27 21:14:08.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024401692s
    Mar 27 21:14:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.024285682s
    Mar 27 21:14:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.024992459s
    Mar 27 21:14:14.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.026048125s
    Mar 27 21:14:16.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024889612s
    Mar 27 21:14:18.157: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.04183975s
    Mar 27 21:14:20.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.024071032s
    Mar 27 21:14:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.024989987s
    Mar 27 21:14:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.024192693s
    Mar 27 21:14:26.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022677855s
    Mar 27 21:14:28.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.026215028s
    Mar 27 21:14:30.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024454421s
    Mar 27 21:14:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.026474377s
    Mar 27 21:14:34.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.024779031s
    Mar 27 21:14:36.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.022807181s
    Mar 27 21:14:38.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.024630089s
    Mar 27 21:14:40.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026019089s
    Mar 27 21:14:42.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.025386979s
    Mar 27 21:14:44.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024452923s
    Mar 27 21:14:46.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.023666957s
    Mar 27 21:14:48.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.024011231s
    Mar 27 21:14:50.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.024311645s
    Mar 27 21:14:52.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.026343877s
    Mar 27 21:14:54.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.024076878s
    Mar 27 21:14:56.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.024790912s
    Mar 27 21:14:58.156: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.040828441s
    Mar 27 21:15:00.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024600667s
    Mar 27 21:15:02.146: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.03047188s
    Mar 27 21:15:04.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.028824403s
    Mar 27 21:15:06.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.035528119s
    Mar 27 21:15:08.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.023975009s
    Mar 27 21:15:10.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.024447029s
    Mar 27 21:15:12.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02538772s
    Mar 27 21:15:14.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.025357085s
    Mar 27 21:15:16.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.023710256s
    Mar 27 21:15:18.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.023707344s
    Mar 27 21:15:20.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.023591085s
    Mar 27 21:15:22.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.025051289s
    Mar 27 21:15:24.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024396627s
    Mar 27 21:15:26.144: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.02887389s
    Mar 27 21:15:28.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024226007s
    Mar 27 21:15:30.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.02580665s
    Mar 27 21:15:32.142: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.026684769s
    Mar 27 21:15:34.140: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.024580127s
    Mar 27 21:15:36.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.025586075s
    Mar 27 21:15:38.151: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.035379732s
    Mar 27 21:15:38.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.067796088s
    STEP: removing the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 off the node 10.176.99.177 03/27/23 21:15:38.183
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e3b66605-8364-4180-9e7f-543c62ee4b01 03/27/23 21:15:38.229
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:15:38.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-628" for this suite. 03/27/23 21:15:38.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:15:38.287
Mar 27 21:15:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename init-container 03/27/23 21:15:38.288
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:15:38.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:15:38.351
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 03/27/23 21:15:38.364
Mar 27 21:15:38.364: INFO: PodSpec: initContainers in spec.initContainers
Mar 27 21:16:23.148: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a5664f74-8028-40e2-9ab5-44220e65ac52", GenerateName:"", Namespace:"init-container-9416", SelfLink:"", UID:"05c5874c-e9e2-4323-ad1b-4def36c70a2d", ResourceVersion:"25380", Generation:0, CreationTimestamp:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"364478290"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"cb40670f7cbae7b7a831e2897fc2211e18e71398478d617f1ac1a80641adf486", "cni.projectcalico.org/podIP":"172.30.85.172/32", "cni.projectcalico.org/podIPs":"172.30.85.172/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce498), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 15, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce4c8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 16, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce4f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-frfm8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003df5a40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0061478b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.176.99.177", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000d0bab0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006147940)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006147960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006147968), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00614796c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00153b750), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.176.99.177", PodIP:"172.30.85.172", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.85.172"}}, StartTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d0bb90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d0bce0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://06cdd5fdac1e5a5c421190ac21a7b7105c0ed668d9ae520573851767d7f4efcf", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003df5ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003df5aa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0061479e4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9416" for this suite. 03/27/23 21:16:23.169
------------------------------
• [SLOW TEST] [44.907 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:15:38.287
    Mar 27 21:15:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename init-container 03/27/23 21:15:38.288
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:15:38.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:15:38.351
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 03/27/23 21:15:38.364
    Mar 27 21:15:38.364: INFO: PodSpec: initContainers in spec.initContainers
    Mar 27 21:16:23.148: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a5664f74-8028-40e2-9ab5-44220e65ac52", GenerateName:"", Namespace:"init-container-9416", SelfLink:"", UID:"05c5874c-e9e2-4323-ad1b-4def36c70a2d", ResourceVersion:"25380", Generation:0, CreationTimestamp:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"364478290"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"cb40670f7cbae7b7a831e2897fc2211e18e71398478d617f1ac1a80641adf486", "cni.projectcalico.org/podIP":"172.30.85.172/32", "cni.projectcalico.org/podIPs":"172.30.85.172/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce498), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 15, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce4c8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 27, 21, 16, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004bce4f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-frfm8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003df5a40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frfm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0061478b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.176.99.177", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000d0bab0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006147940)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006147960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006147968), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00614796c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00153b750), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.176.99.177", PodIP:"172.30.85.172", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.85.172"}}, StartTime:time.Date(2023, time.March, 27, 21, 15, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d0bb90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d0bce0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://06cdd5fdac1e5a5c421190ac21a7b7105c0ed668d9ae520573851767d7f4efcf", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003df5ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003df5aa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0061479e4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9416" for this suite. 03/27/23 21:16:23.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:23.195
Mar 27 21:16:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 21:16:23.197
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:23.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:23.268
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 27 21:16:23.333: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 21:16:28.348: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 21:16:28.348
STEP: Scaling up "test-rs" replicaset  03/27/23 21:16:28.348
Mar 27 21:16:28.392: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/27/23 21:16:28.392
W0327 21:16:28.447996      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 21:16:28.453: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 21:16:28.479: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 21:16:28.493: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
Mar 27 21:16:30.241: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 2, AvailableReplicas 2
Mar 27 21:16:30.570: INFO: observed Replicaset test-rs in namespace replicaset-9706 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:30.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9706" for this suite. 03/27/23 21:16:30.59
------------------------------
• [SLOW TEST] [7.421 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:23.195
    Mar 27 21:16:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 21:16:23.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:23.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:23.268
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 27 21:16:23.333: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 21:16:28.348: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 21:16:28.348
    STEP: Scaling up "test-rs" replicaset  03/27/23 21:16:28.348
    Mar 27 21:16:28.392: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/27/23 21:16:28.392
    W0327 21:16:28.447996      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 21:16:28.453: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 21:16:28.479: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 21:16:28.493: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 1, AvailableReplicas 1
    Mar 27 21:16:30.241: INFO: observed ReplicaSet test-rs in namespace replicaset-9706 with ReadyReplicas 2, AvailableReplicas 2
    Mar 27 21:16:30.570: INFO: observed Replicaset test-rs in namespace replicaset-9706 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:30.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9706" for this suite. 03/27/23 21:16:30.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:30.617
Mar 27 21:16:30.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:16:30.618
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:30.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:30.691
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 27 21:16:30.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:31.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6139" for this suite. 03/27/23 21:16:31.796
------------------------------
• [1.203 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:30.617
    Mar 27 21:16:30.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:16:30.618
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:30.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:30.691
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 27 21:16:30.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:31.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6139" for this suite. 03/27/23 21:16:31.796
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:31.821
Mar 27 21:16:31.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:16:31.822
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:31.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:31.887
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/27/23 21:16:31.899
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_tcp@PTR;sleep 1; done
 03/27/23 21:16:31.95
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_tcp@PTR;sleep 1; done
 03/27/23 21:16:31.95
STEP: creating a pod to probe DNS 03/27/23 21:16:31.95
STEP: submitting the pod to kubernetes 03/27/23 21:16:31.95
Mar 27 21:16:31.978: INFO: Waiting up to 15m0s for pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925" in namespace "dns-8597" to be "running"
Mar 27 21:16:31.991: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Pending", Reason="", readiness=false. Elapsed: 13.095339ms
Mar 27 21:16:34.005: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027440336s
Mar 27 21:16:36.004: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Running", Reason="", readiness=true. Elapsed: 4.02625673s
Mar 27 21:16:36.004: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:16:36.005
STEP: looking for the results for each expected name from probers 03/27/23 21:16:36.018
Mar 27 21:16:36.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local from pod dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925: the server could not find the requested resource (get pods dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925)
Mar 27 21:16:36.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local from pod dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925: the server could not find the requested resource (get pods dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925)
Mar 27 21:16:36.329: INFO: Lookups using dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925 failed for: [jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local]

Mar 27 21:16:41.623: INFO: DNS probes using dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925 succeeded

STEP: deleting the pod 03/27/23 21:16:41.623
STEP: deleting the test service 03/27/23 21:16:41.654
STEP: deleting the test headless service 03/27/23 21:16:41.718
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8597" for this suite. 03/27/23 21:16:41.777
------------------------------
• [SLOW TEST] [9.982 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:31.821
    Mar 27 21:16:31.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:16:31.822
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:31.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:31.887
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/27/23 21:16:31.899
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_tcp@PTR;sleep 1; done
     03/27/23 21:16:31.95
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8597.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8597.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8597.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.23.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.23.166_tcp@PTR;sleep 1; done
     03/27/23 21:16:31.95
    STEP: creating a pod to probe DNS 03/27/23 21:16:31.95
    STEP: submitting the pod to kubernetes 03/27/23 21:16:31.95
    Mar 27 21:16:31.978: INFO: Waiting up to 15m0s for pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925" in namespace "dns-8597" to be "running"
    Mar 27 21:16:31.991: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Pending", Reason="", readiness=false. Elapsed: 13.095339ms
    Mar 27 21:16:34.005: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027440336s
    Mar 27 21:16:36.004: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925": Phase="Running", Reason="", readiness=true. Elapsed: 4.02625673s
    Mar 27 21:16:36.004: INFO: Pod "dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:16:36.005
    STEP: looking for the results for each expected name from probers 03/27/23 21:16:36.018
    Mar 27 21:16:36.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local from pod dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925: the server could not find the requested resource (get pods dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925)
    Mar 27 21:16:36.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local from pod dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925: the server could not find the requested resource (get pods dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925)
    Mar 27 21:16:36.329: INFO: Lookups using dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925 failed for: [jessie_udp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8597.svc.cluster.local]

    Mar 27 21:16:41.623: INFO: DNS probes using dns-8597/dns-test-e85b16ee-ac08-43b3-80d7-d41c99bed925 succeeded

    STEP: deleting the pod 03/27/23 21:16:41.623
    STEP: deleting the test service 03/27/23 21:16:41.654
    STEP: deleting the test headless service 03/27/23 21:16:41.718
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8597" for this suite. 03/27/23 21:16:41.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:41.804
Mar 27 21:16:41.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:16:41.806
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:41.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:41.877
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 03/27/23 21:16:41.89
Mar 27 21:16:41.890: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 27 21:16:41.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:42.889: INFO: stderr: ""
Mar 27 21:16:42.889: INFO: stdout: "service/agnhost-replica created\n"
Mar 27 21:16:42.889: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 27 21:16:42.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:43.799: INFO: stderr: ""
Mar 27 21:16:43.799: INFO: stdout: "service/agnhost-primary created\n"
Mar 27 21:16:43.799: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 27 21:16:43.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:44.743: INFO: stderr: ""
Mar 27 21:16:44.743: INFO: stdout: "service/frontend created\n"
Mar 27 21:16:44.744: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 27 21:16:44.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:45.064: INFO: stderr: ""
Mar 27 21:16:45.064: INFO: stdout: "deployment.apps/frontend created\n"
Mar 27 21:16:45.064: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 27 21:16:45.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:45.391: INFO: stderr: ""
Mar 27 21:16:45.391: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 27 21:16:45.392: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 27 21:16:45.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
Mar 27 21:16:45.732: INFO: stderr: ""
Mar 27 21:16:45.733: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/27/23 21:16:45.733
Mar 27 21:16:45.733: INFO: Waiting for all frontend pods to be Running.
Mar 27 21:16:50.784: INFO: Waiting for frontend to serve content.
Mar 27 21:16:51.853: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Mar 27 21:16:56.919: INFO: Trying to add a new entry to the guestbook.
Mar 27 21:16:56.950: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/27/23 21:16:57.025
Mar 27 21:16:57.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.185: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 21:16:57.185
Mar 27 21:16:57.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.331: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.332: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 21:16:57.332
Mar 27 21:16:57.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.509: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.509: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 21:16:57.509
Mar 27 21:16:57.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.629: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.629: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 21:16:57.629
Mar 27 21:16:57.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.761: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/27/23 21:16:57.761
Mar 27 21:16:57.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
Mar 27 21:16:57.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:16:57.929: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:57.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1983" for this suite. 03/27/23 21:16:57.952
------------------------------
• [SLOW TEST] [16.175 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:41.804
    Mar 27 21:16:41.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:16:41.806
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:41.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:41.877
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 03/27/23 21:16:41.89
    Mar 27 21:16:41.890: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 27 21:16:41.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:42.889: INFO: stderr: ""
    Mar 27 21:16:42.889: INFO: stdout: "service/agnhost-replica created\n"
    Mar 27 21:16:42.889: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 27 21:16:42.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:43.799: INFO: stderr: ""
    Mar 27 21:16:43.799: INFO: stdout: "service/agnhost-primary created\n"
    Mar 27 21:16:43.799: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 27 21:16:43.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:44.743: INFO: stderr: ""
    Mar 27 21:16:44.743: INFO: stdout: "service/frontend created\n"
    Mar 27 21:16:44.744: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 27 21:16:44.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:45.064: INFO: stderr: ""
    Mar 27 21:16:45.064: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 27 21:16:45.064: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 27 21:16:45.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:45.391: INFO: stderr: ""
    Mar 27 21:16:45.391: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 27 21:16:45.392: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 27 21:16:45.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 create -f -'
    Mar 27 21:16:45.732: INFO: stderr: ""
    Mar 27 21:16:45.733: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/27/23 21:16:45.733
    Mar 27 21:16:45.733: INFO: Waiting for all frontend pods to be Running.
    Mar 27 21:16:50.784: INFO: Waiting for frontend to serve content.
    Mar 27 21:16:51.853: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
    Mar 27 21:16:56.919: INFO: Trying to add a new entry to the guestbook.
    Mar 27 21:16:56.950: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/27/23 21:16:57.025
    Mar 27 21:16:57.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.185: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 21:16:57.185
    Mar 27 21:16:57.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.331: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.332: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 21:16:57.332
    Mar 27 21:16:57.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.509: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.509: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 21:16:57.509
    Mar 27 21:16:57.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.629: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.629: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 21:16:57.629
    Mar 27 21:16:57.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.761: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.761: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/27/23 21:16:57.761
    Mar 27 21:16:57.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1983 delete --grace-period=0 --force -f -'
    Mar 27 21:16:57.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:16:57.929: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:57.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1983" for this suite. 03/27/23 21:16:57.952
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:57.981
Mar 27 21:16:57.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sysctl 03/27/23 21:16:57.983
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:58.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:58.06
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/27/23 21:16:58.085
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:16:58.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1334" for this suite. 03/27/23 21:16:58.119
------------------------------
• [0.162 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:57.981
    Mar 27 21:16:57.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sysctl 03/27/23 21:16:57.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:58.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:58.06
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/27/23 21:16:58.085
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:16:58.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1334" for this suite. 03/27/23 21:16:58.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:16:58.143
Mar 27 21:16:58.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:16:58.145
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:58.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:58.214
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Mar 27 21:16:58.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:17:00.633
Mar 27 21:17:00.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 create -f -'
Mar 27 21:17:01.514: INFO: stderr: ""
Mar 27 21:17:01.514: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 27 21:17:01.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 delete e2e-test-crd-publish-openapi-9170-crds test-cr'
Mar 27 21:17:01.649: INFO: stderr: ""
Mar 27 21:17:01.649: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 27 21:17:01.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 apply -f -'
Mar 27 21:17:02.047: INFO: stderr: ""
Mar 27 21:17:02.047: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 27 21:17:02.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 delete e2e-test-crd-publish-openapi-9170-crds test-cr'
Mar 27 21:17:02.223: INFO: stderr: ""
Mar 27 21:17:02.223: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/27/23 21:17:02.223
Mar 27 21:17:02.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 explain e2e-test-crd-publish-openapi-9170-crds'
Mar 27 21:17:02.805: INFO: stderr: ""
Mar 27 21:17:02.805: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9170-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:05.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6808" for this suite. 03/27/23 21:17:05.122
------------------------------
• [SLOW TEST] [7.005 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:16:58.143
    Mar 27 21:16:58.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:16:58.145
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:16:58.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:16:58.214
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Mar 27 21:16:58.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:17:00.633
    Mar 27 21:17:00.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 create -f -'
    Mar 27 21:17:01.514: INFO: stderr: ""
    Mar 27 21:17:01.514: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 27 21:17:01.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 delete e2e-test-crd-publish-openapi-9170-crds test-cr'
    Mar 27 21:17:01.649: INFO: stderr: ""
    Mar 27 21:17:01.649: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 27 21:17:01.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 apply -f -'
    Mar 27 21:17:02.047: INFO: stderr: ""
    Mar 27 21:17:02.047: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 27 21:17:02.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 --namespace=crd-publish-openapi-6808 delete e2e-test-crd-publish-openapi-9170-crds test-cr'
    Mar 27 21:17:02.223: INFO: stderr: ""
    Mar 27 21:17:02.223: INFO: stdout: "e2e-test-crd-publish-openapi-9170-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/27/23 21:17:02.223
    Mar 27 21:17:02.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-6808 explain e2e-test-crd-publish-openapi-9170-crds'
    Mar 27 21:17:02.805: INFO: stderr: ""
    Mar 27 21:17:02.805: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9170-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:05.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6808" for this suite. 03/27/23 21:17:05.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:05.151
Mar 27 21:17:05.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:17:05.152
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:05.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:05.197
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 03/27/23 21:17:05.205
STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:17:05.221
STEP: Creating a ResourceQuota with not terminating scope 03/27/23 21:17:07.234
STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:17:07.25
STEP: Creating a long running pod 03/27/23 21:17:09.265
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/27/23 21:17:09.301
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/27/23 21:17:11.317
STEP: Deleting the pod 03/27/23 21:17:13.363
STEP: Ensuring resource quota status released the pod usage 03/27/23 21:17:13.41
STEP: Creating a terminating pod 03/27/23 21:17:15.438
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/27/23 21:17:15.491
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/27/23 21:17:17.517
STEP: Deleting the pod 03/27/23 21:17:19.533
STEP: Ensuring resource quota status released the pod usage 03/27/23 21:17:19.57
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:21.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2013" for this suite. 03/27/23 21:17:21.607
------------------------------
• [SLOW TEST] [16.490 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:05.151
    Mar 27 21:17:05.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:17:05.152
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:05.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:05.197
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 03/27/23 21:17:05.205
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:17:05.221
    STEP: Creating a ResourceQuota with not terminating scope 03/27/23 21:17:07.234
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:17:07.25
    STEP: Creating a long running pod 03/27/23 21:17:09.265
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/27/23 21:17:09.301
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/27/23 21:17:11.317
    STEP: Deleting the pod 03/27/23 21:17:13.363
    STEP: Ensuring resource quota status released the pod usage 03/27/23 21:17:13.41
    STEP: Creating a terminating pod 03/27/23 21:17:15.438
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/27/23 21:17:15.491
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/27/23 21:17:17.517
    STEP: Deleting the pod 03/27/23 21:17:19.533
    STEP: Ensuring resource quota status released the pod usage 03/27/23 21:17:19.57
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:21.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2013" for this suite. 03/27/23 21:17:21.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:21.646
Mar 27 21:17:21.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename tables 03/27/23 21:17:21.647
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:21.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:21.694
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:21.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-5294" for this suite. 03/27/23 21:17:21.728
------------------------------
• [0.106 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:21.646
    Mar 27 21:17:21.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename tables 03/27/23 21:17:21.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:21.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:21.694
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:21.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-5294" for this suite. 03/27/23 21:17:21.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:21.755
Mar 27 21:17:21.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:17:21.756
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:21.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:21.819
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/27/23 21:17:21.828
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local;sleep 1; done
 03/27/23 21:17:21.845
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local;sleep 1; done
 03/27/23 21:17:21.845
STEP: creating a pod to probe DNS 03/27/23 21:17:21.845
STEP: submitting the pod to kubernetes 03/27/23 21:17:21.845
Mar 27 21:17:21.869: INFO: Waiting up to 15m0s for pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879" in namespace "dns-4519" to be "running"
Mar 27 21:17:21.883: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879": Phase="Pending", Reason="", readiness=false. Elapsed: 13.296376ms
Mar 27 21:17:23.898: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879": Phase="Running", Reason="", readiness=true. Elapsed: 2.028588709s
Mar 27 21:17:23.898: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:17:23.898
STEP: looking for the results for each expected name from probers 03/27/23 21:17:23.911
Mar 27 21:17:23.952: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:23.968: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:23.984: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.028: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.048: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.067: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.085: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:24.085: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:29.107: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.125: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.143: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.175: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.208: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:29.225: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:34.114: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.150: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.169: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.191: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.208: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.255: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.283: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:34.284: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:39.108: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.136: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.153: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.171: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.193: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.212: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.229: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.245: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:39.245: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:44.106: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.124: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.141: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.176: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.213: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.233: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:44.233: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:49.106: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.123: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.140: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.176: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.211: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.233: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
Mar 27 21:17:49.233: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

Mar 27 21:17:54.228: INFO: DNS probes using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 succeeded

STEP: deleting the pod 03/27/23 21:17:54.228
STEP: deleting the test headless service 03/27/23 21:17:54.279
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:54.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4519" for this suite. 03/27/23 21:17:54.325
------------------------------
• [SLOW TEST] [32.595 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:21.755
    Mar 27 21:17:21.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:17:21.756
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:21.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:21.819
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/27/23 21:17:21.828
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local;sleep 1; done
     03/27/23 21:17:21.845
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4519.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local;sleep 1; done
     03/27/23 21:17:21.845
    STEP: creating a pod to probe DNS 03/27/23 21:17:21.845
    STEP: submitting the pod to kubernetes 03/27/23 21:17:21.845
    Mar 27 21:17:21.869: INFO: Waiting up to 15m0s for pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879" in namespace "dns-4519" to be "running"
    Mar 27 21:17:21.883: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879": Phase="Pending", Reason="", readiness=false. Elapsed: 13.296376ms
    Mar 27 21:17:23.898: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879": Phase="Running", Reason="", readiness=true. Elapsed: 2.028588709s
    Mar 27 21:17:23.898: INFO: Pod "dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:17:23.898
    STEP: looking for the results for each expected name from probers 03/27/23 21:17:23.911
    Mar 27 21:17:23.952: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:23.968: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:23.984: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.001: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.028: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.048: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.067: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.085: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:24.085: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:29.107: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.125: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.143: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.175: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.208: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:29.225: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:34.114: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.150: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.169: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.191: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.208: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.255: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.283: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:34.284: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:39.108: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.136: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.153: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.171: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.193: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.212: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.229: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.245: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:39.245: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:44.106: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.124: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.141: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.176: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.192: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.213: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.233: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:44.233: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:49.106: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.123: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.140: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.159: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.176: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.193: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.211: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.233: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local from pod dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879: the server could not find the requested resource (get pods dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879)
    Mar 27 21:17:49.233: INFO: Lookups using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4519.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4519.svc.cluster.local jessie_udp@dns-test-service-2.dns-4519.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4519.svc.cluster.local]

    Mar 27 21:17:54.228: INFO: DNS probes using dns-4519/dns-test-3c0e0656-808e-4687-89aa-8fbdeecc8879 succeeded

    STEP: deleting the pod 03/27/23 21:17:54.228
    STEP: deleting the test headless service 03/27/23 21:17:54.279
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:54.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4519" for this suite. 03/27/23 21:17:54.325
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:54.353
Mar 27 21:17:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 21:17:54.354
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:54.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:54.401
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-ncgtw" 03/27/23 21:17:54.409
Mar 27 21:17:54.449: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-ncgtw-8580" 03/27/23 21:17:54.449
Mar 27 21:17:54.479: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-ncgtw-8580" 03/27/23 21:17:54.479
Mar 27 21:17:54.524: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7955" for this suite. 03/27/23 21:17:54.548
STEP: Destroying namespace "e2e-ns-ncgtw-8580" for this suite. 03/27/23 21:17:54.573
------------------------------
• [0.245 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:54.353
    Mar 27 21:17:54.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 21:17:54.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:54.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:54.401
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-ncgtw" 03/27/23 21:17:54.409
    Mar 27 21:17:54.449: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-ncgtw-8580" 03/27/23 21:17:54.449
    Mar 27 21:17:54.479: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-ncgtw-8580" 03/27/23 21:17:54.479
    Mar 27 21:17:54.524: INFO: Namespace "e2e-ns-ncgtw-8580" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7955" for this suite. 03/27/23 21:17:54.548
    STEP: Destroying namespace "e2e-ns-ncgtw-8580" for this suite. 03/27/23 21:17:54.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:54.607
Mar 27 21:17:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:17:54.609
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:54.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:54.656
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-79000b70-e73a-4779-af5c-4f4cc14a737f 03/27/23 21:17:54.664
STEP: Creating a pod to test consume configMaps 03/27/23 21:17:54.681
Mar 27 21:17:54.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235" in namespace "configmap-7634" to be "Succeeded or Failed"
Mar 27 21:17:54.718: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Pending", Reason="", readiness=false. Elapsed: 15.307584ms
Mar 27 21:17:56.733: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029964685s
Mar 27 21:17:58.732: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028913194s
STEP: Saw pod success 03/27/23 21:17:58.732
Mar 27 21:17:58.732: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235" satisfied condition "Succeeded or Failed"
Mar 27 21:17:58.750: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:17:58.828
Mar 27 21:17:58.864: INFO: Waiting for pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 to disappear
Mar 27 21:17:58.878: INFO: Pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:17:58.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7634" for this suite. 03/27/23 21:17:58.898
------------------------------
• [4.316 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:54.607
    Mar 27 21:17:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:17:54.609
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:54.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:54.656
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-79000b70-e73a-4779-af5c-4f4cc14a737f 03/27/23 21:17:54.664
    STEP: Creating a pod to test consume configMaps 03/27/23 21:17:54.681
    Mar 27 21:17:54.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235" in namespace "configmap-7634" to be "Succeeded or Failed"
    Mar 27 21:17:54.718: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Pending", Reason="", readiness=false. Elapsed: 15.307584ms
    Mar 27 21:17:56.733: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029964685s
    Mar 27 21:17:58.732: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028913194s
    STEP: Saw pod success 03/27/23 21:17:58.732
    Mar 27 21:17:58.732: INFO: Pod "pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235" satisfied condition "Succeeded or Failed"
    Mar 27 21:17:58.750: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:17:58.828
    Mar 27 21:17:58.864: INFO: Waiting for pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 to disappear
    Mar 27 21:17:58.878: INFO: Pod pod-configmaps-2596caa5-66e4-43ec-8e50-0fa5546db235 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:17:58.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7634" for this suite. 03/27/23 21:17:58.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:17:58.927
Mar 27 21:17:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:17:58.928
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:58.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:58.976
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/27/23 21:17:58.984
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:17:58.994
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:17:58.994
STEP: creating a pod to probe DNS 03/27/23 21:17:58.995
STEP: submitting the pod to kubernetes 03/27/23 21:17:58.995
Mar 27 21:17:59.021: INFO: Waiting up to 15m0s for pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1" in namespace "dns-1384" to be "running"
Mar 27 21:17:59.035: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.252806ms
Mar 27 21:18:01.049: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028625241s
Mar 27 21:18:01.049: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:18:01.05
STEP: looking for the results for each expected name from probers 03/27/23 21:18:01.063
Mar 27 21:18:01.112: INFO: DNS probes using dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1 succeeded

STEP: deleting the pod 03/27/23 21:18:01.112
STEP: changing the externalName to bar.example.com 03/27/23 21:18:01.16
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:18:01.18
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:18:01.18
STEP: creating a second pod to probe DNS 03/27/23 21:18:01.181
STEP: submitting the pod to kubernetes 03/27/23 21:18:01.181
Mar 27 21:18:01.198: INFO: Waiting up to 15m0s for pod "dns-test-05257ac0-e074-4436-9658-ca5256655804" in namespace "dns-1384" to be "running"
Mar 27 21:18:01.213: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Pending", Reason="", readiness=false. Elapsed: 15.093949ms
Mar 27 21:18:03.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029658854s
Mar 27 21:18:05.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Running", Reason="", readiness=true. Elapsed: 4.029860759s
Mar 27 21:18:05.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:18:05.228
STEP: looking for the results for each expected name from probers 03/27/23 21:18:05.242
Mar 27 21:18:05.280: INFO: File wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local from pod  dns-1384/dns-test-05257ac0-e074-4436-9658-ca5256655804 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 27 21:18:05.297: INFO: Lookups using dns-1384/dns-test-05257ac0-e074-4436-9658-ca5256655804 failed for: [wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local]

Mar 27 21:18:10.334: INFO: DNS probes using dns-test-05257ac0-e074-4436-9658-ca5256655804 succeeded

STEP: deleting the pod 03/27/23 21:18:10.334
STEP: changing the service to type=ClusterIP 03/27/23 21:18:10.393
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:18:10.421
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
 03/27/23 21:18:10.424
STEP: creating a third pod to probe DNS 03/27/23 21:18:10.424
STEP: submitting the pod to kubernetes 03/27/23 21:18:10.455
Mar 27 21:18:10.473: INFO: Waiting up to 15m0s for pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420" in namespace "dns-1384" to be "running"
Mar 27 21:18:10.518: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420": Phase="Pending", Reason="", readiness=false. Elapsed: 44.769068ms
Mar 27 21:18:12.536: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420": Phase="Running", Reason="", readiness=true. Elapsed: 2.062383298s
Mar 27 21:18:12.536: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:18:12.536
STEP: looking for the results for each expected name from probers 03/27/23 21:18:12.567
Mar 27 21:18:12.663: INFO: DNS probes using dns-test-516faed6-2f28-4f0f-b55f-c98a22189420 succeeded

STEP: deleting the pod 03/27/23 21:18:12.663
STEP: deleting the test externalName service 03/27/23 21:18:12.728
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:18:12.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1384" for this suite. 03/27/23 21:18:12.782
------------------------------
• [SLOW TEST] [13.885 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:17:58.927
    Mar 27 21:17:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:17:58.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:17:58.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:17:58.976
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/27/23 21:17:58.984
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:17:58.994
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:17:58.994
    STEP: creating a pod to probe DNS 03/27/23 21:17:58.995
    STEP: submitting the pod to kubernetes 03/27/23 21:17:58.995
    Mar 27 21:17:59.021: INFO: Waiting up to 15m0s for pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1" in namespace "dns-1384" to be "running"
    Mar 27 21:17:59.035: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.252806ms
    Mar 27 21:18:01.049: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028625241s
    Mar 27 21:18:01.049: INFO: Pod "dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:18:01.05
    STEP: looking for the results for each expected name from probers 03/27/23 21:18:01.063
    Mar 27 21:18:01.112: INFO: DNS probes using dns-test-61f30ca4-20a4-4c48-b80a-e2ba7e9397a1 succeeded

    STEP: deleting the pod 03/27/23 21:18:01.112
    STEP: changing the externalName to bar.example.com 03/27/23 21:18:01.16
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:18:01.18
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:18:01.18
    STEP: creating a second pod to probe DNS 03/27/23 21:18:01.181
    STEP: submitting the pod to kubernetes 03/27/23 21:18:01.181
    Mar 27 21:18:01.198: INFO: Waiting up to 15m0s for pod "dns-test-05257ac0-e074-4436-9658-ca5256655804" in namespace "dns-1384" to be "running"
    Mar 27 21:18:01.213: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Pending", Reason="", readiness=false. Elapsed: 15.093949ms
    Mar 27 21:18:03.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029658854s
    Mar 27 21:18:05.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804": Phase="Running", Reason="", readiness=true. Elapsed: 4.029860759s
    Mar 27 21:18:05.228: INFO: Pod "dns-test-05257ac0-e074-4436-9658-ca5256655804" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:18:05.228
    STEP: looking for the results for each expected name from probers 03/27/23 21:18:05.242
    Mar 27 21:18:05.280: INFO: File wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local from pod  dns-1384/dns-test-05257ac0-e074-4436-9658-ca5256655804 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Mar 27 21:18:05.297: INFO: Lookups using dns-1384/dns-test-05257ac0-e074-4436-9658-ca5256655804 failed for: [wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local]

    Mar 27 21:18:10.334: INFO: DNS probes using dns-test-05257ac0-e074-4436-9658-ca5256655804 succeeded

    STEP: deleting the pod 03/27/23 21:18:10.334
    STEP: changing the service to type=ClusterIP 03/27/23 21:18:10.393
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:18:10.421
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1384.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1384.svc.cluster.local; sleep 1; done
     03/27/23 21:18:10.424
    STEP: creating a third pod to probe DNS 03/27/23 21:18:10.424
    STEP: submitting the pod to kubernetes 03/27/23 21:18:10.455
    Mar 27 21:18:10.473: INFO: Waiting up to 15m0s for pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420" in namespace "dns-1384" to be "running"
    Mar 27 21:18:10.518: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420": Phase="Pending", Reason="", readiness=false. Elapsed: 44.769068ms
    Mar 27 21:18:12.536: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420": Phase="Running", Reason="", readiness=true. Elapsed: 2.062383298s
    Mar 27 21:18:12.536: INFO: Pod "dns-test-516faed6-2f28-4f0f-b55f-c98a22189420" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:18:12.536
    STEP: looking for the results for each expected name from probers 03/27/23 21:18:12.567
    Mar 27 21:18:12.663: INFO: DNS probes using dns-test-516faed6-2f28-4f0f-b55f-c98a22189420 succeeded

    STEP: deleting the pod 03/27/23 21:18:12.663
    STEP: deleting the test externalName service 03/27/23 21:18:12.728
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:18:12.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1384" for this suite. 03/27/23 21:18:12.782
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:18:12.812
Mar 27 21:18:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:18:12.814
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:12.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:12.857
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 21:18:12.866
Mar 27 21:18:12.886: INFO: Waiting up to 5m0s for pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a" in namespace "emptydir-2964" to be "Succeeded or Failed"
Mar 27 21:18:12.899: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.171946ms
Mar 27 21:18:14.914: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028337756s
Mar 27 21:18:16.915: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029382796s
STEP: Saw pod success 03/27/23 21:18:16.915
Mar 27 21:18:16.916: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a" satisfied condition "Succeeded or Failed"
Mar 27 21:18:16.930: INFO: Trying to get logs from node 10.176.99.177 pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a container test-container: <nil>
STEP: delete the pod 03/27/23 21:18:16.96
Mar 27 21:18:17.010: INFO: Waiting for pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a to disappear
Mar 27 21:18:17.023: INFO: Pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:18:17.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2964" for this suite. 03/27/23 21:18:17.043
------------------------------
• [4.254 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:18:12.812
    Mar 27 21:18:12.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:18:12.814
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:12.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:12.857
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 21:18:12.866
    Mar 27 21:18:12.886: INFO: Waiting up to 5m0s for pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a" in namespace "emptydir-2964" to be "Succeeded or Failed"
    Mar 27 21:18:12.899: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.171946ms
    Mar 27 21:18:14.914: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028337756s
    Mar 27 21:18:16.915: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029382796s
    STEP: Saw pod success 03/27/23 21:18:16.915
    Mar 27 21:18:16.916: INFO: Pod "pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a" satisfied condition "Succeeded or Failed"
    Mar 27 21:18:16.930: INFO: Trying to get logs from node 10.176.99.177 pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a container test-container: <nil>
    STEP: delete the pod 03/27/23 21:18:16.96
    Mar 27 21:18:17.010: INFO: Waiting for pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a to disappear
    Mar 27 21:18:17.023: INFO: Pod pod-fa404efa-9fd8-4c37-b1c7-1c5a04e4970a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:18:17.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2964" for this suite. 03/27/23 21:18:17.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:18:17.069
Mar 27 21:18:17.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:18:17.07
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:17.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:17.121
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-4656ce49-15dc-473e-958c-e6c554b3df1d 03/27/23 21:18:17.129
STEP: Creating a pod to test consume configMaps 03/27/23 21:18:17.145
Mar 27 21:18:17.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce" in namespace "configmap-2014" to be "Succeeded or Failed"
Mar 27 21:18:17.180: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Pending", Reason="", readiness=false. Elapsed: 13.013715ms
Mar 27 21:18:19.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.02815811s
Mar 27 21:18:21.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Running", Reason="", readiness=false. Elapsed: 4.027733917s
Mar 27 21:18:23.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027466319s
STEP: Saw pod success 03/27/23 21:18:23.195
Mar 27 21:18:23.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce" satisfied condition "Succeeded or Failed"
Mar 27 21:18:23.209: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:18:23.24
Mar 27 21:18:23.288: INFO: Waiting for pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce to disappear
Mar 27 21:18:23.302: INFO: Pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:18:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2014" for this suite. 03/27/23 21:18:23.322
------------------------------
• [SLOW TEST] [6.278 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:18:17.069
    Mar 27 21:18:17.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:18:17.07
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:17.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:17.121
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-4656ce49-15dc-473e-958c-e6c554b3df1d 03/27/23 21:18:17.129
    STEP: Creating a pod to test consume configMaps 03/27/23 21:18:17.145
    Mar 27 21:18:17.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce" in namespace "configmap-2014" to be "Succeeded or Failed"
    Mar 27 21:18:17.180: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Pending", Reason="", readiness=false. Elapsed: 13.013715ms
    Mar 27 21:18:19.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.02815811s
    Mar 27 21:18:21.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Running", Reason="", readiness=false. Elapsed: 4.027733917s
    Mar 27 21:18:23.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027466319s
    STEP: Saw pod success 03/27/23 21:18:23.195
    Mar 27 21:18:23.195: INFO: Pod "pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce" satisfied condition "Succeeded or Failed"
    Mar 27 21:18:23.209: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:18:23.24
    Mar 27 21:18:23.288: INFO: Waiting for pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce to disappear
    Mar 27 21:18:23.302: INFO: Pod pod-configmaps-194194d7-fd6f-43e7-9a1a-4f934a3861ce no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:18:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2014" for this suite. 03/27/23 21:18:23.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:18:23.352
Mar 27 21:18:23.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 21:18:23.354
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:23.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:23.401
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 03/27/23 21:18:23.409
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:23.447
STEP: Creating a pod in the namespace 03/27/23 21:18:23.456
STEP: Waiting for the pod to have running status 03/27/23 21:18:23.478
Mar 27 21:18:23.478: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7389" to be "running"
Mar 27 21:18:23.491: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.283486ms
Mar 27 21:18:25.508: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029953775s
Mar 27 21:18:25.508: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/27/23 21:18:25.508
STEP: Waiting for the namespace to be removed. 03/27/23 21:18:25.534
STEP: Recreating the namespace 03/27/23 21:18:37.547
STEP: Verifying there are no pods in the namespace 03/27/23 21:18:37.588
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:18:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-221" for this suite. 03/27/23 21:18:37.624
STEP: Destroying namespace "nsdeletetest-7389" for this suite. 03/27/23 21:18:37.648
Mar 27 21:18:37.661: INFO: Namespace nsdeletetest-7389 was already deleted
STEP: Destroying namespace "nsdeletetest-4353" for this suite. 03/27/23 21:18:37.661
------------------------------
• [SLOW TEST] [14.332 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:18:23.352
    Mar 27 21:18:23.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 21:18:23.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:23.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:23.401
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 03/27/23 21:18:23.409
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:23.447
    STEP: Creating a pod in the namespace 03/27/23 21:18:23.456
    STEP: Waiting for the pod to have running status 03/27/23 21:18:23.478
    Mar 27 21:18:23.478: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7389" to be "running"
    Mar 27 21:18:23.491: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.283486ms
    Mar 27 21:18:25.508: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029953775s
    Mar 27 21:18:25.508: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/27/23 21:18:25.508
    STEP: Waiting for the namespace to be removed. 03/27/23 21:18:25.534
    STEP: Recreating the namespace 03/27/23 21:18:37.547
    STEP: Verifying there are no pods in the namespace 03/27/23 21:18:37.588
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:18:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-221" for this suite. 03/27/23 21:18:37.624
    STEP: Destroying namespace "nsdeletetest-7389" for this suite. 03/27/23 21:18:37.648
    Mar 27 21:18:37.661: INFO: Namespace nsdeletetest-7389 was already deleted
    STEP: Destroying namespace "nsdeletetest-4353" for this suite. 03/27/23 21:18:37.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:18:37.691
Mar 27 21:18:37.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:18:37.692
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:37.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:37.746
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:18:37.796
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:18:38.487
STEP: Deploying the webhook pod 03/27/23 21:18:38.512
STEP: Wait for the deployment to be ready 03/27/23 21:18:38.589
Mar 27 21:18:38.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 27 21:18:40.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/27/23 21:18:42.684
STEP: Verifying the service has paired with the endpoint 03/27/23 21:18:42.705
Mar 27 21:18:43.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 21:18:43.714
STEP: create a pod 03/27/23 21:18:43.77
Mar 27 21:18:43.786: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3745" to be "running"
Mar 27 21:18:43.799: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.903196ms
Mar 27 21:18:45.813: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026642809s
Mar 27 21:18:45.813: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/27/23 21:18:45.813
Mar 27 21:18:45.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=webhook-3745 attach --namespace=webhook-3745 to-be-attached-pod -i -c=container1'
Mar 27 21:18:45.967: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:18:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3745" for this suite. 03/27/23 21:18:46.119
STEP: Destroying namespace "webhook-3745-markers" for this suite. 03/27/23 21:18:46.145
------------------------------
• [SLOW TEST] [8.488 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:18:37.691
    Mar 27 21:18:37.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:18:37.692
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:37.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:37.746
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:18:37.796
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:18:38.487
    STEP: Deploying the webhook pod 03/27/23 21:18:38.512
    STEP: Wait for the deployment to be ready 03/27/23 21:18:38.589
    Mar 27 21:18:38.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 27 21:18:40.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 21, 18, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/27/23 21:18:42.684
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:18:42.705
    Mar 27 21:18:43.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 21:18:43.714
    STEP: create a pod 03/27/23 21:18:43.77
    Mar 27 21:18:43.786: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3745" to be "running"
    Mar 27 21:18:43.799: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.903196ms
    Mar 27 21:18:45.813: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026642809s
    Mar 27 21:18:45.813: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/27/23 21:18:45.813
    Mar 27 21:18:45.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=webhook-3745 attach --namespace=webhook-3745 to-be-attached-pod -i -c=container1'
    Mar 27 21:18:45.967: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:18:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3745" for this suite. 03/27/23 21:18:46.119
    STEP: Destroying namespace "webhook-3745-markers" for this suite. 03/27/23 21:18:46.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:18:46.187
Mar 27 21:18:46.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:18:46.188
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:46.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:46.234
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:18:46.294
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:18:47.194
STEP: Deploying the webhook pod 03/27/23 21:18:47.211
STEP: Wait for the deployment to be ready 03/27/23 21:18:47.255
Mar 27 21:18:47.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:18:49.335
STEP: Verifying the service has paired with the endpoint 03/27/23 21:18:49.355
Mar 27 21:18:50.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 21:18:50.365
STEP: create a pod that should be denied by the webhook 03/27/23 21:18:50.411
STEP: create a pod that causes the webhook to hang 03/27/23 21:18:50.443
STEP: create a configmap that should be denied by the webhook 03/27/23 21:19:00.464
STEP: create a configmap that should be admitted by the webhook 03/27/23 21:19:00.5
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 21:19:00.535
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 21:19:00.57
STEP: create a namespace that bypass the webhook 03/27/23 21:19:00.59
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/27/23 21:19:00.615
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:00.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1472" for this suite. 03/27/23 21:19:00.829
STEP: Destroying namespace "webhook-1472-markers" for this suite. 03/27/23 21:19:00.855
------------------------------
• [SLOW TEST] [14.692 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:18:46.187
    Mar 27 21:18:46.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:18:46.188
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:18:46.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:18:46.234
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:18:46.294
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:18:47.194
    STEP: Deploying the webhook pod 03/27/23 21:18:47.211
    STEP: Wait for the deployment to be ready 03/27/23 21:18:47.255
    Mar 27 21:18:47.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:18:49.335
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:18:49.355
    Mar 27 21:18:50.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 03/27/23 21:18:50.365
    STEP: create a pod that should be denied by the webhook 03/27/23 21:18:50.411
    STEP: create a pod that causes the webhook to hang 03/27/23 21:18:50.443
    STEP: create a configmap that should be denied by the webhook 03/27/23 21:19:00.464
    STEP: create a configmap that should be admitted by the webhook 03/27/23 21:19:00.5
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 21:19:00.535
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/27/23 21:19:00.57
    STEP: create a namespace that bypass the webhook 03/27/23 21:19:00.59
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/27/23 21:19:00.615
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:00.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1472" for this suite. 03/27/23 21:19:00.829
    STEP: Destroying namespace "webhook-1472-markers" for this suite. 03/27/23 21:19:00.855
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:00.879
Mar 27 21:19:00.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:19:00.88
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:00.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:00.926
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-606dd14f-9750-4273-a7cd-1f15f106006e 03/27/23 21:19:00.935
STEP: Creating a pod to test consume secrets 03/27/23 21:19:00.95
Mar 27 21:19:00.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff" in namespace "projected-3545" to be "Succeeded or Failed"
Mar 27 21:19:00.985: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.577178ms
Mar 27 21:19:03.000: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029136638s
Mar 27 21:19:04.999: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027729693s
STEP: Saw pod success 03/27/23 21:19:04.999
Mar 27 21:19:04.999: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff" satisfied condition "Succeeded or Failed"
Mar 27 21:19:05.013: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:19:05.043
Mar 27 21:19:05.089: INFO: Waiting for pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff to disappear
Mar 27 21:19:05.101: INFO: Pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:05.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3545" for this suite. 03/27/23 21:19:05.121
------------------------------
• [4.266 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:00.879
    Mar 27 21:19:00.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:19:00.88
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:00.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:00.926
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-606dd14f-9750-4273-a7cd-1f15f106006e 03/27/23 21:19:00.935
    STEP: Creating a pod to test consume secrets 03/27/23 21:19:00.95
    Mar 27 21:19:00.971: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff" in namespace "projected-3545" to be "Succeeded or Failed"
    Mar 27 21:19:00.985: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.577178ms
    Mar 27 21:19:03.000: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029136638s
    Mar 27 21:19:04.999: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027729693s
    STEP: Saw pod success 03/27/23 21:19:04.999
    Mar 27 21:19:04.999: INFO: Pod "pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff" satisfied condition "Succeeded or Failed"
    Mar 27 21:19:05.013: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:19:05.043
    Mar 27 21:19:05.089: INFO: Waiting for pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff to disappear
    Mar 27 21:19:05.101: INFO: Pod pod-projected-secrets-693e7d5a-0496-47ef-a3b6-e6609ab1b2ff no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:05.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3545" for this suite. 03/27/23 21:19:05.121
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:05.146
Mar 27 21:19:05.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:19:05.147
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:05.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:05.196
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-f5933bcd-3cc9-4b6f-a8ad-22ed702c52a7 03/27/23 21:19:05.204
STEP: Creating a pod to test consume secrets 03/27/23 21:19:05.219
Mar 27 21:19:05.239: INFO: Waiting up to 5m0s for pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad" in namespace "secrets-1435" to be "Succeeded or Failed"
Mar 27 21:19:05.252: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Pending", Reason="", readiness=false. Elapsed: 13.094551ms
Mar 27 21:19:07.268: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028625165s
Mar 27 21:19:09.267: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02836841s
STEP: Saw pod success 03/27/23 21:19:09.268
Mar 27 21:19:09.268: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad" satisfied condition "Succeeded or Failed"
Mar 27 21:19:09.281: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:19:09.31
Mar 27 21:19:09.347: INFO: Waiting for pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad to disappear
Mar 27 21:19:09.364: INFO: Pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1435" for this suite. 03/27/23 21:19:09.39
------------------------------
• [4.270 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:05.146
    Mar 27 21:19:05.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:19:05.147
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:05.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:05.196
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-f5933bcd-3cc9-4b6f-a8ad-22ed702c52a7 03/27/23 21:19:05.204
    STEP: Creating a pod to test consume secrets 03/27/23 21:19:05.219
    Mar 27 21:19:05.239: INFO: Waiting up to 5m0s for pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad" in namespace "secrets-1435" to be "Succeeded or Failed"
    Mar 27 21:19:05.252: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Pending", Reason="", readiness=false. Elapsed: 13.094551ms
    Mar 27 21:19:07.268: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028625165s
    Mar 27 21:19:09.267: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02836841s
    STEP: Saw pod success 03/27/23 21:19:09.268
    Mar 27 21:19:09.268: INFO: Pod "pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad" satisfied condition "Succeeded or Failed"
    Mar 27 21:19:09.281: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:19:09.31
    Mar 27 21:19:09.347: INFO: Waiting for pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad to disappear
    Mar 27 21:19:09.364: INFO: Pod pod-secrets-c6f0a1c0-a9b2-4b43-ae6f-4a8bc6e065ad no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:09.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1435" for this suite. 03/27/23 21:19:09.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:09.416
Mar 27 21:19:09.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 21:19:09.418
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:09.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:09.47
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 21:19:09.551
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:19:09.568
Mar 27 21:19:09.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:19:09.599: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:19:10.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:19:10.635: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:19:11.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:19:11.635: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:19:12.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:19:12.633: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/27/23 21:19:12.647
Mar 27 21:19:12.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:19:12.718: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 21:19:13.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:19:13.756: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 21:19:14.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:19:14.752: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 21:19:15.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:19:15.753: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 21:19:16.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:19:16.753: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:19:16.767
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2007, will wait for the garbage collector to delete the pods 03/27/23 21:19:16.767
Mar 27 21:19:16.858: INFO: Deleting DaemonSet.extensions daemon-set took: 25.714599ms
Mar 27 21:19:16.959: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.909257ms
Mar 27 21:19:19.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:19:19.173: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 21:19:19.187: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26800"},"items":null}

Mar 27 21:19:19.204: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26800"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2007" for this suite. 03/27/23 21:19:19.284
------------------------------
• [SLOW TEST] [9.892 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:09.416
    Mar 27 21:19:09.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 21:19:09.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:09.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:09.47
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 21:19:09.551
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:19:09.568
    Mar 27 21:19:09.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:19:09.599: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:19:10.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:19:10.635: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:19:11.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:19:11.635: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:19:12.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:19:12.633: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/27/23 21:19:12.647
    Mar 27 21:19:12.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:19:12.718: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 21:19:13.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:19:13.756: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 21:19:14.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:19:14.752: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 21:19:15.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:19:15.753: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 21:19:16.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:19:16.753: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:19:16.767
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2007, will wait for the garbage collector to delete the pods 03/27/23 21:19:16.767
    Mar 27 21:19:16.858: INFO: Deleting DaemonSet.extensions daemon-set took: 25.714599ms
    Mar 27 21:19:16.959: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.909257ms
    Mar 27 21:19:19.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:19:19.173: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 21:19:19.187: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26800"},"items":null}

    Mar 27 21:19:19.204: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26800"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2007" for this suite. 03/27/23 21:19:19.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:19.315
Mar 27 21:19:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:19:19.316
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:19.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:19.365
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-6407 03/27/23 21:19:19.373
STEP: creating replication controller nodeport-test in namespace services-6407 03/27/23 21:19:19.416
I0327 21:19:19.432770      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6407, replica count: 2
I0327 21:19:22.483752      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 21:19:22.483: INFO: Creating new exec pod
Mar 27 21:19:22.500: INFO: Waiting up to 5m0s for pod "execpodfmn4p" in namespace "services-6407" to be "running"
Mar 27 21:19:22.512: INFO: Pod "execpodfmn4p": Phase="Pending", Reason="", readiness=false. Elapsed: 12.365239ms
Mar 27 21:19:24.526: INFO: Pod "execpodfmn4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.026487511s
Mar 27 21:19:24.526: INFO: Pod "execpodfmn4p" satisfied condition "running"
Mar 27 21:19:25.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Mar 27 21:19:25.870: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 27 21:19:25.870: INFO: stdout: ""
Mar 27 21:19:25.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 172.21.201.75 80'
Mar 27 21:19:26.127: INFO: stderr: "+ nc -v -z -w 2 172.21.201.75 80\nConnection to 172.21.201.75 80 port [tcp/http] succeeded!\n"
Mar 27 21:19:26.127: INFO: stdout: ""
Mar 27 21:19:26.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 30278'
Mar 27 21:19:26.388: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 30278\nConnection to 10.176.99.177 30278 port [tcp/*] succeeded!\n"
Mar 27 21:19:26.388: INFO: stdout: ""
Mar 27 21:19:26.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 10.176.99.178 30278'
Mar 27 21:19:26.627: INFO: stderr: "+ nc -v -z -w 2 10.176.99.178 30278\nConnection to 10.176.99.178 30278 port [tcp/*] succeeded!\n"
Mar 27 21:19:26.627: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:26.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6407" for this suite. 03/27/23 21:19:26.647
------------------------------
• [SLOW TEST] [7.358 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:19.315
    Mar 27 21:19:19.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:19:19.316
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:19.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:19.365
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-6407 03/27/23 21:19:19.373
    STEP: creating replication controller nodeport-test in namespace services-6407 03/27/23 21:19:19.416
    I0327 21:19:19.432770      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6407, replica count: 2
    I0327 21:19:22.483752      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 21:19:22.483: INFO: Creating new exec pod
    Mar 27 21:19:22.500: INFO: Waiting up to 5m0s for pod "execpodfmn4p" in namespace "services-6407" to be "running"
    Mar 27 21:19:22.512: INFO: Pod "execpodfmn4p": Phase="Pending", Reason="", readiness=false. Elapsed: 12.365239ms
    Mar 27 21:19:24.526: INFO: Pod "execpodfmn4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.026487511s
    Mar 27 21:19:24.526: INFO: Pod "execpodfmn4p" satisfied condition "running"
    Mar 27 21:19:25.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Mar 27 21:19:25.870: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 27 21:19:25.870: INFO: stdout: ""
    Mar 27 21:19:25.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 172.21.201.75 80'
    Mar 27 21:19:26.127: INFO: stderr: "+ nc -v -z -w 2 172.21.201.75 80\nConnection to 172.21.201.75 80 port [tcp/http] succeeded!\n"
    Mar 27 21:19:26.127: INFO: stdout: ""
    Mar 27 21:19:26.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 30278'
    Mar 27 21:19:26.388: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 30278\nConnection to 10.176.99.177 30278 port [tcp/*] succeeded!\n"
    Mar 27 21:19:26.388: INFO: stdout: ""
    Mar 27 21:19:26.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6407 exec execpodfmn4p -- /bin/sh -x -c nc -v -z -w 2 10.176.99.178 30278'
    Mar 27 21:19:26.627: INFO: stderr: "+ nc -v -z -w 2 10.176.99.178 30278\nConnection to 10.176.99.178 30278 port [tcp/*] succeeded!\n"
    Mar 27 21:19:26.627: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:26.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6407" for this suite. 03/27/23 21:19:26.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:26.675
Mar 27 21:19:26.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:19:26.677
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:26.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:26.721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:19:26.772
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:19:27.044
STEP: Deploying the webhook pod 03/27/23 21:19:27.068
STEP: Wait for the deployment to be ready 03/27/23 21:19:27.102
Mar 27 21:19:27.132: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:19:29.177
STEP: Verifying the service has paired with the endpoint 03/27/23 21:19:29.199
Mar 27 21:19:30.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Mar 27 21:19:30.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/27/23 21:19:30.735
STEP: Creating a custom resource that should be denied by the webhook 03/27/23 21:19:30.778
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/27/23 21:19:32.855
STEP: Updating the custom resource with disallowed data should be denied 03/27/23 21:19:32.875
STEP: Deleting the custom resource should be denied 03/27/23 21:19:32.917
STEP: Remove the offending key and value from the custom resource data 03/27/23 21:19:32.949
STEP: Deleting the updated custom resource should be successful 03/27/23 21:19:32.986
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-561" for this suite. 03/27/23 21:19:33.717
STEP: Destroying namespace "webhook-561-markers" for this suite. 03/27/23 21:19:33.791
------------------------------
• [SLOW TEST] [7.145 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:26.675
    Mar 27 21:19:26.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:19:26.677
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:26.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:26.721
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:19:26.772
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:19:27.044
    STEP: Deploying the webhook pod 03/27/23 21:19:27.068
    STEP: Wait for the deployment to be ready 03/27/23 21:19:27.102
    Mar 27 21:19:27.132: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:19:29.177
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:19:29.199
    Mar 27 21:19:30.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Mar 27 21:19:30.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/27/23 21:19:30.735
    STEP: Creating a custom resource that should be denied by the webhook 03/27/23 21:19:30.778
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/27/23 21:19:32.855
    STEP: Updating the custom resource with disallowed data should be denied 03/27/23 21:19:32.875
    STEP: Deleting the custom resource should be denied 03/27/23 21:19:32.917
    STEP: Remove the offending key and value from the custom resource data 03/27/23 21:19:32.949
    STEP: Deleting the updated custom resource should be successful 03/27/23 21:19:32.986
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:33.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-561" for this suite. 03/27/23 21:19:33.717
    STEP: Destroying namespace "webhook-561-markers" for this suite. 03/27/23 21:19:33.791
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:33.821
Mar 27 21:19:33.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:19:33.823
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:33.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:33.877
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-de2e85c1-08b3-43d7-8deb-2d9535a9266a 03/27/23 21:19:33.885
STEP: Creating a pod to test consume secrets 03/27/23 21:19:33.901
Mar 27 21:19:33.923: INFO: Waiting up to 5m0s for pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3" in namespace "secrets-3586" to be "Succeeded or Failed"
Mar 27 21:19:33.937: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.780429ms
Mar 27 21:19:35.950: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027110672s
Mar 27 21:19:37.952: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028690508s
STEP: Saw pod success 03/27/23 21:19:37.952
Mar 27 21:19:37.952: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3" satisfied condition "Succeeded or Failed"
Mar 27 21:19:37.965: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:19:37.995
Mar 27 21:19:38.056: INFO: Waiting for pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 to disappear
Mar 27 21:19:38.068: INFO: Pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3586" for this suite. 03/27/23 21:19:38.088
------------------------------
• [4.291 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:33.821
    Mar 27 21:19:33.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:19:33.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:33.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:33.877
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-de2e85c1-08b3-43d7-8deb-2d9535a9266a 03/27/23 21:19:33.885
    STEP: Creating a pod to test consume secrets 03/27/23 21:19:33.901
    Mar 27 21:19:33.923: INFO: Waiting up to 5m0s for pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3" in namespace "secrets-3586" to be "Succeeded or Failed"
    Mar 27 21:19:33.937: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.780429ms
    Mar 27 21:19:35.950: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027110672s
    Mar 27 21:19:37.952: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028690508s
    STEP: Saw pod success 03/27/23 21:19:37.952
    Mar 27 21:19:37.952: INFO: Pod "pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3" satisfied condition "Succeeded or Failed"
    Mar 27 21:19:37.965: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:19:37.995
    Mar 27 21:19:38.056: INFO: Waiting for pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 to disappear
    Mar 27 21:19:38.068: INFO: Pod pod-secrets-ee172755-7e92-4dfd-b861-5e7023ecfdf3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3586" for this suite. 03/27/23 21:19:38.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:38.117
Mar 27 21:19:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:19:38.118
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:38.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:38.163
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:19:38.173
Mar 27 21:19:38.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662" in namespace "downward-api-8554" to be "Succeeded or Failed"
Mar 27 21:19:38.211: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Pending", Reason="", readiness=false. Elapsed: 13.83714ms
Mar 27 21:19:40.226: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02876969s
Mar 27 21:19:42.229: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031945713s
STEP: Saw pod success 03/27/23 21:19:42.229
Mar 27 21:19:42.230: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662" satisfied condition "Succeeded or Failed"
Mar 27 21:19:42.243: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 container client-container: <nil>
STEP: delete the pod 03/27/23 21:19:42.272
Mar 27 21:19:42.307: INFO: Waiting for pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 to disappear
Mar 27 21:19:42.320: INFO: Pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:42.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8554" for this suite. 03/27/23 21:19:42.341
------------------------------
• [4.250 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:38.117
    Mar 27 21:19:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:19:38.118
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:38.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:38.163
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:19:38.173
    Mar 27 21:19:38.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662" in namespace "downward-api-8554" to be "Succeeded or Failed"
    Mar 27 21:19:38.211: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Pending", Reason="", readiness=false. Elapsed: 13.83714ms
    Mar 27 21:19:40.226: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02876969s
    Mar 27 21:19:42.229: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031945713s
    STEP: Saw pod success 03/27/23 21:19:42.229
    Mar 27 21:19:42.230: INFO: Pod "downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662" satisfied condition "Succeeded or Failed"
    Mar 27 21:19:42.243: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:19:42.272
    Mar 27 21:19:42.307: INFO: Waiting for pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 to disappear
    Mar 27 21:19:42.320: INFO: Pod downwardapi-volume-c07d8dfe-eaad-491c-9df5-7155d7a08662 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:42.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8554" for this suite. 03/27/23 21:19:42.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:42.367
Mar 27 21:19:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 21:19:42.368
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:42.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:42.414
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 27 21:19:42.423: INFO: Creating ReplicaSet my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d
Mar 27 21:19:42.462: INFO: Pod name my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Found 0 pods out of 1
Mar 27 21:19:47.476: INFO: Pod name my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Found 1 pods out of 1
Mar 27 21:19:47.476: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d" is running
Mar 27 21:19:47.476: INFO: Waiting up to 5m0s for pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" in namespace "replicaset-8928" to be "running"
Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn": Phase="Running", Reason="", readiness=true. Elapsed: 13.363181ms
Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" satisfied condition "running"
Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:42 +0000 UTC Reason: Message:}])
Mar 27 21:19:47.490: INFO: Trying to dial the pod
Mar 27 21:19:52.553: INFO: Controller my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Got expected result from replica 1 [my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn]: "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:52.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8928" for this suite. 03/27/23 21:19:52.573
------------------------------
• [SLOW TEST] [10.231 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:42.367
    Mar 27 21:19:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 21:19:42.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:42.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:42.414
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 27 21:19:42.423: INFO: Creating ReplicaSet my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d
    Mar 27 21:19:42.462: INFO: Pod name my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Found 0 pods out of 1
    Mar 27 21:19:47.476: INFO: Pod name my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Found 1 pods out of 1
    Mar 27 21:19:47.476: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d" is running
    Mar 27 21:19:47.476: INFO: Waiting up to 5m0s for pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" in namespace "replicaset-8928" to be "running"
    Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn": Phase="Running", Reason="", readiness=true. Elapsed: 13.363181ms
    Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" satisfied condition "running"
    Mar 27 21:19:47.489: INFO: Pod "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-27 21:19:42 +0000 UTC Reason: Message:}])
    Mar 27 21:19:47.490: INFO: Trying to dial the pod
    Mar 27 21:19:52.553: INFO: Controller my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d: Got expected result from replica 1 [my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn]: "my-hostname-basic-abb1261f-807e-41f5-af4b-97602eabaa3d-jbtjn", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:52.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8928" for this suite. 03/27/23 21:19:52.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:52.604
Mar 27 21:19:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:19:52.605
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:52.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:52.655
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:52.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5339" for this suite. 03/27/23 21:19:52.695
------------------------------
• [0.115 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:52.604
    Mar 27 21:19:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:19:52.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:52.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:52.655
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:52.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5339" for this suite. 03/27/23 21:19:52.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:52.721
Mar 27 21:19:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:19:52.722
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:52.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:52.766
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:19:52.775
Mar 27 21:19:52.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-7211 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Mar 27 21:19:52.923: INFO: stderr: ""
Mar 27 21:19:52.923: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 21:19:52.923
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Mar 27 21:19:52.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-7211 delete pods e2e-test-httpd-pod'
Mar 27 21:19:55.979: INFO: stderr: ""
Mar 27 21:19:55.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:19:55.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7211" for this suite. 03/27/23 21:19:56.033
------------------------------
• [3.366 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:52.721
    Mar 27 21:19:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:19:52.722
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:52.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:52.766
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:19:52.775
    Mar 27 21:19:52.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-7211 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Mar 27 21:19:52.923: INFO: stderr: ""
    Mar 27 21:19:52.923: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 21:19:52.923
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Mar 27 21:19:52.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-7211 delete pods e2e-test-httpd-pod'
    Mar 27 21:19:55.979: INFO: stderr: ""
    Mar 27 21:19:55.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:19:55.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7211" for this suite. 03/27/23 21:19:56.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:19:56.088
Mar 27 21:19:56.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 21:19:56.09
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:56.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:56.221
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 03/27/23 21:19:56.288
STEP: submitting the pod to kubernetes 03/27/23 21:19:56.288
Mar 27 21:19:56.309: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" in namespace "pods-3929" to be "running and ready"
Mar 27 21:19:56.335: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.721794ms
Mar 27 21:19:56.335: INFO: The phase of Pod pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:19:58.354: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.04573947s
Mar 27 21:19:58.354: INFO: The phase of Pod pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5 is Running (Ready = true)
Mar 27 21:19:58.354: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/27/23 21:19:58.368
STEP: updating the pod 03/27/23 21:19:58.385
Mar 27 21:19:58.919: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5"
Mar 27 21:19:58.919: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" in namespace "pods-3929" to be "terminated with reason DeadlineExceeded"
Mar 27 21:19:58.933: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 13.771861ms
Mar 27 21:20:00.948: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029041838s
Mar 27 21:20:02.947: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.028603621s
Mar 27 21:20:02.947: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:02.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3929" for this suite. 03/27/23 21:20:02.968
------------------------------
• [SLOW TEST] [6.905 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:19:56.088
    Mar 27 21:19:56.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 21:19:56.09
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:19:56.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:19:56.221
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 03/27/23 21:19:56.288
    STEP: submitting the pod to kubernetes 03/27/23 21:19:56.288
    Mar 27 21:19:56.309: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" in namespace "pods-3929" to be "running and ready"
    Mar 27 21:19:56.335: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.721794ms
    Mar 27 21:19:56.335: INFO: The phase of Pod pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:19:58.354: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.04573947s
    Mar 27 21:19:58.354: INFO: The phase of Pod pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5 is Running (Ready = true)
    Mar 27 21:19:58.354: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/27/23 21:19:58.368
    STEP: updating the pod 03/27/23 21:19:58.385
    Mar 27 21:19:58.919: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5"
    Mar 27 21:19:58.919: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" in namespace "pods-3929" to be "terminated with reason DeadlineExceeded"
    Mar 27 21:19:58.933: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 13.771861ms
    Mar 27 21:20:00.948: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029041838s
    Mar 27 21:20:02.947: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.028603621s
    Mar 27 21:20:02.947: INFO: Pod "pod-update-activedeadlineseconds-2aefa4ab-2f81-4cb8-b3eb-60b0ffcb48f5" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:02.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3929" for this suite. 03/27/23 21:20:02.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:02.994
Mar 27 21:20:02.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:20:02.996
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:03.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:03.054
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-36896f85-4278-4d05-badf-cc574b121b0e 03/27/23 21:20:03.08
STEP: Creating the pod 03/27/23 21:20:03.097
Mar 27 21:20:03.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49" in namespace "configmap-3100" to be "running"
Mar 27 21:20:03.135: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49": Phase="Pending", Reason="", readiness=false. Elapsed: 13.392316ms
Mar 27 21:20:05.150: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49": Phase="Running", Reason="", readiness=false. Elapsed: 2.028780541s
Mar 27 21:20:05.150: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49" satisfied condition "running"
STEP: Waiting for pod with text data 03/27/23 21:20:05.15
STEP: Waiting for pod with binary data 03/27/23 21:20:05.18
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:05.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3100" for this suite. 03/27/23 21:20:05.231
------------------------------
• [2.263 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:02.994
    Mar 27 21:20:02.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:20:02.996
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:03.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:03.054
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-36896f85-4278-4d05-badf-cc574b121b0e 03/27/23 21:20:03.08
    STEP: Creating the pod 03/27/23 21:20:03.097
    Mar 27 21:20:03.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49" in namespace "configmap-3100" to be "running"
    Mar 27 21:20:03.135: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49": Phase="Pending", Reason="", readiness=false. Elapsed: 13.392316ms
    Mar 27 21:20:05.150: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49": Phase="Running", Reason="", readiness=false. Elapsed: 2.028780541s
    Mar 27 21:20:05.150: INFO: Pod "pod-configmaps-d29d3fde-9db4-461d-9531-55153837de49" satisfied condition "running"
    STEP: Waiting for pod with text data 03/27/23 21:20:05.15
    STEP: Waiting for pod with binary data 03/27/23 21:20:05.18
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:05.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3100" for this suite. 03/27/23 21:20:05.231
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:05.259
Mar 27 21:20:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 21:20:05.26
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:05.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:05.36
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 27 21:20:05.394: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 27 21:20:10.411: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 21:20:10.411
Mar 27 21:20:10.412: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/27/23 21:20:10.452
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 21:20:12.560: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4710  6d86d31e-c6ce-4f7a-81e4-58c46c1334c3 27345 1 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e1f178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 21:20:10 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-27 21:20:11 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 21:20:12.596: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4710  0369f4e0-bbe7-4333-96e4-daa809db45d7 27335 1 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6d86d31e-c6ce-4f7a-81e4-58c46c1334c3 0xc005e1f547 0xc005e1f548}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d86d31e-c6ce-4f7a-81e4-58c46c1334c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e1f5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 21:20:12.609: INFO: Pod "test-cleanup-deployment-7698ff6f6b-qmcf7" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-qmcf7 test-cleanup-deployment-7698ff6f6b- deployment-4710  d499d80a-2037-4160-a1fb-ad4dda9dc4a3 27334 0 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:d805bbc36e46f4552bdd61856fdae980a6b6e1459ea6146f1f18ce79182d34b3 cni.projectcalico.org/podIP:172.30.56.112/32 cni.projectcalico.org/podIPs:172.30.56.112/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0369f4e0-bbe7-4333-96e4-daa809db45d7 0xc005637247 0xc005637248}] [] [{kube-controller-manager Update v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0369f4e0-bbe7-4333-96e4-daa809db45d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j88ps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j88ps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.112,StartTime:2023-03-27 21:20:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:20:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://658a6ff09596c399389c334d607426e7514576d3646edd4234939537f201b3b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:12.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4710" for this suite. 03/27/23 21:20:12.63
------------------------------
• [SLOW TEST] [7.404 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:05.259
    Mar 27 21:20:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 21:20:05.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:05.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:05.36
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 27 21:20:05.394: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 27 21:20:10.411: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 21:20:10.411
    Mar 27 21:20:10.412: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/27/23 21:20:10.452
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 21:20:12.560: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4710  6d86d31e-c6ce-4f7a-81e4-58c46c1334c3 27345 1 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e1f178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 21:20:10 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7698ff6f6b" has successfully progressed.,LastUpdateTime:2023-03-27 21:20:11 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 21:20:12.596: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4710  0369f4e0-bbe7-4333-96e4-daa809db45d7 27335 1 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6d86d31e-c6ce-4f7a-81e4-58c46c1334c3 0xc005e1f547 0xc005e1f548}] [] [{kube-controller-manager Update apps/v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d86d31e-c6ce-4f7a-81e4-58c46c1334c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e1f5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 21:20:12.609: INFO: Pod "test-cleanup-deployment-7698ff6f6b-qmcf7" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-qmcf7 test-cleanup-deployment-7698ff6f6b- deployment-4710  d499d80a-2037-4160-a1fb-ad4dda9dc4a3 27334 0 2023-03-27 21:20:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[cni.projectcalico.org/containerID:d805bbc36e46f4552bdd61856fdae980a6b6e1459ea6146f1f18ce79182d34b3 cni.projectcalico.org/podIP:172.30.56.112/32 cni.projectcalico.org/podIPs:172.30.56.112/32] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 0369f4e0-bbe7-4333-96e4-daa809db45d7 0xc005637247 0xc005637248}] [] [{kube-controller-manager Update v1 2023-03-27 21:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0369f4e0-bbe7-4333-96e4-daa809db45d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 21:20:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j88ps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j88ps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 21:20:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.112,StartTime:2023-03-27 21:20:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 21:20:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://658a6ff09596c399389c334d607426e7514576d3646edd4234939537f201b3b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:12.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4710" for this suite. 03/27/23 21:20:12.63
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:12.669
Mar 27 21:20:12.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:20:12.67
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:12.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:12.716
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 03/27/23 21:20:12.725
Mar 27 21:20:12.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-6520 create -f -'
Mar 27 21:20:13.603: INFO: stderr: ""
Mar 27 21:20:13.603: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 21:20:13.603
Mar 27 21:20:14.618: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:20:14.618: INFO: Found 0 / 1
Mar 27 21:20:15.617: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:20:15.617: INFO: Found 1 / 1
Mar 27 21:20:15.617: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/27/23 21:20:15.617
Mar 27 21:20:15.632: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:20:15.632: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 21:20:15.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-6520 patch pod agnhost-primary-hjlhm -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 27 21:20:15.781: INFO: stderr: ""
Mar 27 21:20:15.781: INFO: stdout: "pod/agnhost-primary-hjlhm patched\n"
STEP: checking annotations 03/27/23 21:20:15.781
Mar 27 21:20:15.795: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:20:15.795: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:15.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6520" for this suite. 03/27/23 21:20:15.815
------------------------------
• [3.170 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:12.669
    Mar 27 21:20:12.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:20:12.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:12.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:12.716
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 03/27/23 21:20:12.725
    Mar 27 21:20:12.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-6520 create -f -'
    Mar 27 21:20:13.603: INFO: stderr: ""
    Mar 27 21:20:13.603: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 21:20:13.603
    Mar 27 21:20:14.618: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:20:14.618: INFO: Found 0 / 1
    Mar 27 21:20:15.617: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:20:15.617: INFO: Found 1 / 1
    Mar 27 21:20:15.617: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/27/23 21:20:15.617
    Mar 27 21:20:15.632: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:20:15.632: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 21:20:15.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-6520 patch pod agnhost-primary-hjlhm -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 27 21:20:15.781: INFO: stderr: ""
    Mar 27 21:20:15.781: INFO: stdout: "pod/agnhost-primary-hjlhm patched\n"
    STEP: checking annotations 03/27/23 21:20:15.781
    Mar 27 21:20:15.795: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:20:15.795: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:15.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6520" for this suite. 03/27/23 21:20:15.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:15.841
Mar 27 21:20:15.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:20:15.842
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:15.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:15.888
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 03/27/23 21:20:15.896
Mar 27 21:20:15.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 create -f -'
Mar 27 21:20:16.156: INFO: stderr: ""
Mar 27 21:20:16.156: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:20:16.156
Mar 27 21:20:16.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:20:16.267: INFO: stderr: ""
Mar 27 21:20:16.267: INFO: stdout: "update-demo-nautilus-6wzzk update-demo-nautilus-djp8r "
Mar 27 21:20:16.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:20:16.373: INFO: stderr: ""
Mar 27 21:20:16.373: INFO: stdout: ""
Mar 27 21:20:16.373: INFO: update-demo-nautilus-6wzzk is created but not running
Mar 27 21:20:21.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 27 21:20:21.493: INFO: stderr: ""
Mar 27 21:20:21.493: INFO: stdout: "update-demo-nautilus-6wzzk update-demo-nautilus-djp8r "
Mar 27 21:20:21.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:20:21.600: INFO: stderr: ""
Mar 27 21:20:21.600: INFO: stdout: "true"
Mar 27 21:20:21.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:20:21.701: INFO: stderr: ""
Mar 27 21:20:21.701: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:20:21.701: INFO: validating pod update-demo-nautilus-6wzzk
Mar 27 21:20:21.737: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:20:21.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:20:21.737: INFO: update-demo-nautilus-6wzzk is verified up and running
Mar 27 21:20:21.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-djp8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 27 21:20:21.841: INFO: stderr: ""
Mar 27 21:20:21.841: INFO: stdout: "true"
Mar 27 21:20:21.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-djp8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 27 21:20:21.937: INFO: stderr: ""
Mar 27 21:20:21.937: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Mar 27 21:20:21.937: INFO: validating pod update-demo-nautilus-djp8r
Mar 27 21:20:21.966: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 27 21:20:21.966: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 27 21:20:21.966: INFO: update-demo-nautilus-djp8r is verified up and running
STEP: using delete to clean up resources 03/27/23 21:20:21.966
Mar 27 21:20:21.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 delete --grace-period=0 --force -f -'
Mar 27 21:20:22.085: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:20:22.085: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 27 21:20:22.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get rc,svc -l name=update-demo --no-headers'
Mar 27 21:20:22.229: INFO: stderr: "No resources found in kubectl-4367 namespace.\n"
Mar 27 21:20:22.229: INFO: stdout: ""
Mar 27 21:20:22.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 21:20:22.368: INFO: stderr: ""
Mar 27 21:20:22.368: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:22.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4367" for this suite. 03/27/23 21:20:22.404
------------------------------
• [SLOW TEST] [6.592 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:15.841
    Mar 27 21:20:15.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:20:15.842
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:15.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:15.888
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 03/27/23 21:20:15.896
    Mar 27 21:20:15.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 create -f -'
    Mar 27 21:20:16.156: INFO: stderr: ""
    Mar 27 21:20:16.156: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/27/23 21:20:16.156
    Mar 27 21:20:16.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:20:16.267: INFO: stderr: ""
    Mar 27 21:20:16.267: INFO: stdout: "update-demo-nautilus-6wzzk update-demo-nautilus-djp8r "
    Mar 27 21:20:16.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:20:16.373: INFO: stderr: ""
    Mar 27 21:20:16.373: INFO: stdout: ""
    Mar 27 21:20:16.373: INFO: update-demo-nautilus-6wzzk is created but not running
    Mar 27 21:20:21.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 27 21:20:21.493: INFO: stderr: ""
    Mar 27 21:20:21.493: INFO: stdout: "update-demo-nautilus-6wzzk update-demo-nautilus-djp8r "
    Mar 27 21:20:21.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:20:21.600: INFO: stderr: ""
    Mar 27 21:20:21.600: INFO: stdout: "true"
    Mar 27 21:20:21.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-6wzzk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:20:21.701: INFO: stderr: ""
    Mar 27 21:20:21.701: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:20:21.701: INFO: validating pod update-demo-nautilus-6wzzk
    Mar 27 21:20:21.737: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:20:21.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:20:21.737: INFO: update-demo-nautilus-6wzzk is verified up and running
    Mar 27 21:20:21.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-djp8r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 27 21:20:21.841: INFO: stderr: ""
    Mar 27 21:20:21.841: INFO: stdout: "true"
    Mar 27 21:20:21.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods update-demo-nautilus-djp8r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 27 21:20:21.937: INFO: stderr: ""
    Mar 27 21:20:21.937: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Mar 27 21:20:21.937: INFO: validating pod update-demo-nautilus-djp8r
    Mar 27 21:20:21.966: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 27 21:20:21.966: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 27 21:20:21.966: INFO: update-demo-nautilus-djp8r is verified up and running
    STEP: using delete to clean up resources 03/27/23 21:20:21.966
    Mar 27 21:20:21.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 delete --grace-period=0 --force -f -'
    Mar 27 21:20:22.085: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:20:22.085: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 27 21:20:22.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get rc,svc -l name=update-demo --no-headers'
    Mar 27 21:20:22.229: INFO: stderr: "No resources found in kubectl-4367 namespace.\n"
    Mar 27 21:20:22.229: INFO: stdout: ""
    Mar 27 21:20:22.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-4367 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 21:20:22.368: INFO: stderr: ""
    Mar 27 21:20:22.368: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:22.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4367" for this suite. 03/27/23 21:20:22.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:22.435
Mar 27 21:20:22.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:20:22.436
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:22.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:22.512
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8184 03/27/23 21:20:22.524
STEP: changing the ExternalName service to type=ClusterIP 03/27/23 21:20:22.535
STEP: creating replication controller externalname-service in namespace services-8184 03/27/23 21:20:22.571
I0327 21:20:22.582636      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8184, replica count: 2
I0327 21:20:25.634259      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 21:20:25.634: INFO: Creating new exec pod
Mar 27 21:20:25.650: INFO: Waiting up to 5m0s for pod "execpodthjcv" in namespace "services-8184" to be "running"
Mar 27 21:20:25.664: INFO: Pod "execpodthjcv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.18855ms
Mar 27 21:20:27.679: INFO: Pod "execpodthjcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.029242858s
Mar 27 21:20:27.679: INFO: Pod "execpodthjcv" satisfied condition "running"
Mar 27 21:20:28.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-8184 exec execpodthjcv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 27 21:20:28.954: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 27 21:20:28.954: INFO: stdout: ""
Mar 27 21:20:28.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-8184 exec execpodthjcv -- /bin/sh -x -c nc -v -z -w 2 172.21.75.181 80'
Mar 27 21:20:29.247: INFO: stderr: "+ nc -v -z -w 2 172.21.75.181 80\nConnection to 172.21.75.181 80 port [tcp/http] succeeded!\n"
Mar 27 21:20:29.247: INFO: stdout: ""
Mar 27 21:20:29.247: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:29.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8184" for this suite. 03/27/23 21:20:29.3
------------------------------
• [SLOW TEST] [6.888 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:22.435
    Mar 27 21:20:22.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:20:22.436
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:22.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:22.512
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8184 03/27/23 21:20:22.524
    STEP: changing the ExternalName service to type=ClusterIP 03/27/23 21:20:22.535
    STEP: creating replication controller externalname-service in namespace services-8184 03/27/23 21:20:22.571
    I0327 21:20:22.582636      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8184, replica count: 2
    I0327 21:20:25.634259      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 21:20:25.634: INFO: Creating new exec pod
    Mar 27 21:20:25.650: INFO: Waiting up to 5m0s for pod "execpodthjcv" in namespace "services-8184" to be "running"
    Mar 27 21:20:25.664: INFO: Pod "execpodthjcv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.18855ms
    Mar 27 21:20:27.679: INFO: Pod "execpodthjcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.029242858s
    Mar 27 21:20:27.679: INFO: Pod "execpodthjcv" satisfied condition "running"
    Mar 27 21:20:28.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-8184 exec execpodthjcv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 27 21:20:28.954: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 27 21:20:28.954: INFO: stdout: ""
    Mar 27 21:20:28.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-8184 exec execpodthjcv -- /bin/sh -x -c nc -v -z -w 2 172.21.75.181 80'
    Mar 27 21:20:29.247: INFO: stderr: "+ nc -v -z -w 2 172.21.75.181 80\nConnection to 172.21.75.181 80 port [tcp/http] succeeded!\n"
    Mar 27 21:20:29.247: INFO: stdout: ""
    Mar 27 21:20:29.247: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:29.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8184" for this suite. 03/27/23 21:20:29.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:29.325
Mar 27 21:20:29.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:20:29.326
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:29.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:29.372
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/27/23 21:20:29.381
Mar 27 21:20:29.403: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7277  854e16a7-a7a5-484f-953b-ea81a5a53dc3 27568 0 2023-03-27 21:20:29 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-27 21:20:29 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94pcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94pcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 21:20:29.403: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7277" to be "running and ready"
Mar 27 21:20:29.417: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 13.624651ms
Mar 27 21:20:29.417: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:20:31.432: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.028731312s
Mar 27 21:20:31.432: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 27 21:20:31.432: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/27/23 21:20:31.432
Mar 27 21:20:31.432: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7277 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:20:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:20:31.433: INFO: ExecWithOptions: Clientset creation
Mar 27 21:20:31.433: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-7277/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/27/23 21:20:31.621
Mar 27 21:20:31.622: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7277 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:20:31.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:20:31.623: INFO: ExecWithOptions: Clientset creation
Mar 27 21:20:31.623: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-7277/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 21:20:31.901: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:31.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7277" for this suite. 03/27/23 21:20:31.973
------------------------------
• [2.672 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:29.325
    Mar 27 21:20:29.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:20:29.326
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:29.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:29.372
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/27/23 21:20:29.381
    Mar 27 21:20:29.403: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7277  854e16a7-a7a5-484f-953b-ea81a5a53dc3 27568 0 2023-03-27 21:20:29 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-27 21:20:29 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94pcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94pcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 21:20:29.403: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7277" to be "running and ready"
    Mar 27 21:20:29.417: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 13.624651ms
    Mar 27 21:20:29.417: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:20:31.432: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.028731312s
    Mar 27 21:20:31.432: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 27 21:20:31.432: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/27/23 21:20:31.432
    Mar 27 21:20:31.432: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7277 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:20:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:20:31.433: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:20:31.433: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-7277/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/27/23 21:20:31.621
    Mar 27 21:20:31.622: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7277 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:20:31.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:20:31.623: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:20:31.623: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-7277/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 21:20:31.901: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:31.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7277" for this suite. 03/27/23 21:20:31.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:32.01
Mar 27 21:20:32.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:20:32.012
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:32.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:32.059
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:32.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-322" for this suite. 03/27/23 21:20:32.166
------------------------------
• [0.178 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:32.01
    Mar 27 21:20:32.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:20:32.012
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:32.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:32.059
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:32.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-322" for this suite. 03/27/23 21:20:32.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:32.192
Mar 27 21:20:32.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 21:20:32.193
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:32.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:32.243
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/27/23 21:20:32.252
STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 21:20:32.268
STEP: delete the deployment 03/27/23 21:20:32.809
STEP: wait for all rs to be garbage collected 03/27/23 21:20:32.835
STEP: expected 0 rs, got 1 rs 03/27/23 21:20:32.866
STEP: expected 0 pods, got 2 pods 03/27/23 21:20:32.882
STEP: Gathering metrics 03/27/23 21:20:33.423
W0327 21:20:33.452472      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 21:20:33.452: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:33.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4126" for this suite. 03/27/23 21:20:33.473
------------------------------
• [1.332 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:32.192
    Mar 27 21:20:32.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 21:20:32.193
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:32.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:32.243
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/27/23 21:20:32.252
    STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 21:20:32.268
    STEP: delete the deployment 03/27/23 21:20:32.809
    STEP: wait for all rs to be garbage collected 03/27/23 21:20:32.835
    STEP: expected 0 rs, got 1 rs 03/27/23 21:20:32.866
    STEP: expected 0 pods, got 2 pods 03/27/23 21:20:32.882
    STEP: Gathering metrics 03/27/23 21:20:33.423
    W0327 21:20:33.452472      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 21:20:33.452: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:33.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4126" for this suite. 03/27/23 21:20:33.473
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:33.524
Mar 27 21:20:33.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:20:33.525
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:33.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:33.576
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-92730e5f-58f4-477d-962d-12360be87227 03/27/23 21:20:33.584
STEP: Creating a pod to test consume secrets 03/27/23 21:20:33.6
Mar 27 21:20:33.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893" in namespace "projected-4359" to be "Succeeded or Failed"
Mar 27 21:20:33.641: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Pending", Reason="", readiness=false. Elapsed: 14.285245ms
Mar 27 21:20:35.658: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031282994s
Mar 27 21:20:37.655: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028898729s
STEP: Saw pod success 03/27/23 21:20:37.655
Mar 27 21:20:37.656: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893" satisfied condition "Succeeded or Failed"
Mar 27 21:20:37.677: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:20:37.73
Mar 27 21:20:37.776: INFO: Waiting for pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 to disappear
Mar 27 21:20:37.789: INFO: Pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:37.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4359" for this suite. 03/27/23 21:20:37.839
------------------------------
• [4.369 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:33.524
    Mar 27 21:20:33.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:20:33.525
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:33.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:33.576
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-92730e5f-58f4-477d-962d-12360be87227 03/27/23 21:20:33.584
    STEP: Creating a pod to test consume secrets 03/27/23 21:20:33.6
    Mar 27 21:20:33.626: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893" in namespace "projected-4359" to be "Succeeded or Failed"
    Mar 27 21:20:33.641: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Pending", Reason="", readiness=false. Elapsed: 14.285245ms
    Mar 27 21:20:35.658: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031282994s
    Mar 27 21:20:37.655: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028898729s
    STEP: Saw pod success 03/27/23 21:20:37.655
    Mar 27 21:20:37.656: INFO: Pod "pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893" satisfied condition "Succeeded or Failed"
    Mar 27 21:20:37.677: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:20:37.73
    Mar 27 21:20:37.776: INFO: Waiting for pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 to disappear
    Mar 27 21:20:37.789: INFO: Pod pod-projected-secrets-a647f0a3-18b1-4eef-b03f-f18cc036f893 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:37.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4359" for this suite. 03/27/23 21:20:37.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:37.9
Mar 27 21:20:37.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 21:20:37.901
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:37.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:37.968
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 03/27/23 21:20:37.976
Mar 27 21:20:37.997: INFO: Waiting up to 5m0s for pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424" in namespace "var-expansion-8031" to be "Succeeded or Failed"
Mar 27 21:20:38.012: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 14.336343ms
Mar 27 21:20:40.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028611715s
Mar 27 21:20:42.027: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029393499s
Mar 27 21:20:44.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028413624s
STEP: Saw pod success 03/27/23 21:20:44.026
Mar 27 21:20:44.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424" satisfied condition "Succeeded or Failed"
Mar 27 21:20:44.045: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:20:44.075
Mar 27 21:20:44.124: INFO: Waiting for pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 to disappear
Mar 27 21:20:44.136: INFO: Pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 21:20:44.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8031" for this suite. 03/27/23 21:20:44.157
------------------------------
• [SLOW TEST] [6.283 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:37.9
    Mar 27 21:20:37.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 21:20:37.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:37.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:37.968
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 03/27/23 21:20:37.976
    Mar 27 21:20:37.997: INFO: Waiting up to 5m0s for pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424" in namespace "var-expansion-8031" to be "Succeeded or Failed"
    Mar 27 21:20:38.012: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 14.336343ms
    Mar 27 21:20:40.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028611715s
    Mar 27 21:20:42.027: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029393499s
    Mar 27 21:20:44.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028413624s
    STEP: Saw pod success 03/27/23 21:20:44.026
    Mar 27 21:20:44.026: INFO: Pod "var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424" satisfied condition "Succeeded or Failed"
    Mar 27 21:20:44.045: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:20:44.075
    Mar 27 21:20:44.124: INFO: Waiting for pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 to disappear
    Mar 27 21:20:44.136: INFO: Pod var-expansion-943abee8-33c8-4d81-8810-29ff7fabf424 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:20:44.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8031" for this suite. 03/27/23 21:20:44.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:20:44.187
Mar 27 21:20:44.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 21:20:44.188
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:44.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:44.236
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9919 03/27/23 21:20:44.249
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-9919 03/27/23 21:20:44.258
Mar 27 21:20:44.292: INFO: Found 0 stateful pods, waiting for 1
Mar 27 21:20:54.307: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/27/23 21:20:54.327
STEP: updating a scale subresource 03/27/23 21:20:54.335
STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 21:20:54.346
STEP: Patch a scale subresource 03/27/23 21:20:54.353
STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 21:20:54.364
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 21:20:54.376: INFO: Deleting all statefulset in ns statefulset-9919
Mar 27 21:20:54.387: INFO: Scaling statefulset ss to 0
Mar 27 21:21:04.516: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:21:04.524: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:21:04.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9919" for this suite. 03/27/23 21:21:04.576
------------------------------
• [SLOW TEST] [20.414 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:20:44.187
    Mar 27 21:20:44.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 21:20:44.188
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:20:44.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:20:44.236
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9919 03/27/23 21:20:44.249
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-9919 03/27/23 21:20:44.258
    Mar 27 21:20:44.292: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 21:20:54.307: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/27/23 21:20:54.327
    STEP: updating a scale subresource 03/27/23 21:20:54.335
    STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 21:20:54.346
    STEP: Patch a scale subresource 03/27/23 21:20:54.353
    STEP: verifying the statefulset Spec.Replicas was modified 03/27/23 21:20:54.364
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 21:20:54.376: INFO: Deleting all statefulset in ns statefulset-9919
    Mar 27 21:20:54.387: INFO: Scaling statefulset ss to 0
    Mar 27 21:21:04.516: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:21:04.524: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:21:04.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9919" for this suite. 03/27/23 21:21:04.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:21:04.607
Mar 27 21:21:04.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:21:04.608
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:04.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:04.697
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Mar 27 21:21:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:21:06.551
Mar 27 21:21:06.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 create -f -'
Mar 27 21:21:07.287: INFO: stderr: ""
Mar 27 21:21:07.287: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 27 21:21:07.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 delete e2e-test-crd-publish-openapi-2493-crds test-cr'
Mar 27 21:21:07.499: INFO: stderr: ""
Mar 27 21:21:07.499: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 27 21:21:07.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 apply -f -'
Mar 27 21:21:08.323: INFO: stderr: ""
Mar 27 21:21:08.323: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 27 21:21:08.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 delete e2e-test-crd-publish-openapi-2493-crds test-cr'
Mar 27 21:21:08.509: INFO: stderr: ""
Mar 27 21:21:08.509: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/27/23 21:21:08.509
Mar 27 21:21:08.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 explain e2e-test-crd-publish-openapi-2493-crds'
Mar 27 21:21:09.543: INFO: stderr: ""
Mar 27 21:21:09.543: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2493-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:21:11.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7469" for this suite. 03/27/23 21:21:11.729
------------------------------
• [SLOW TEST] [7.146 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:21:04.607
    Mar 27 21:21:04.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:21:04.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:04.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:04.697
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Mar 27 21:21:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:21:06.551
    Mar 27 21:21:06.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 create -f -'
    Mar 27 21:21:07.287: INFO: stderr: ""
    Mar 27 21:21:07.287: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 27 21:21:07.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 delete e2e-test-crd-publish-openapi-2493-crds test-cr'
    Mar 27 21:21:07.499: INFO: stderr: ""
    Mar 27 21:21:07.499: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 27 21:21:07.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 apply -f -'
    Mar 27 21:21:08.323: INFO: stderr: ""
    Mar 27 21:21:08.323: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 27 21:21:08.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 --namespace=crd-publish-openapi-7469 delete e2e-test-crd-publish-openapi-2493-crds test-cr'
    Mar 27 21:21:08.509: INFO: stderr: ""
    Mar 27 21:21:08.509: INFO: stdout: "e2e-test-crd-publish-openapi-2493-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/27/23 21:21:08.509
    Mar 27 21:21:08.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7469 explain e2e-test-crd-publish-openapi-2493-crds'
    Mar 27 21:21:09.543: INFO: stderr: ""
    Mar 27 21:21:09.543: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2493-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:21:11.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7469" for this suite. 03/27/23 21:21:11.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:21:11.757
Mar 27 21:21:11.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:21:11.758
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:11.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:11.825
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 03/27/23 21:21:11.836
Mar 27 21:21:11.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: rename a version 03/27/23 21:21:16.777
STEP: check the new version name is served 03/27/23 21:21:16.822
STEP: check the old version name is removed 03/27/23 21:21:19.524
STEP: check the other version is not changed 03/27/23 21:21:20.424
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:21:24.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9699" for this suite. 03/27/23 21:21:24.471
------------------------------
• [SLOW TEST] [12.736 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:21:11.757
    Mar 27 21:21:11.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:21:11.758
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:11.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:11.825
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 03/27/23 21:21:11.836
    Mar 27 21:21:11.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: rename a version 03/27/23 21:21:16.777
    STEP: check the new version name is served 03/27/23 21:21:16.822
    STEP: check the old version name is removed 03/27/23 21:21:19.524
    STEP: check the other version is not changed 03/27/23 21:21:20.424
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:21:24.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9699" for this suite. 03/27/23 21:21:24.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:21:24.495
Mar 27 21:21:24.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 21:21:24.496
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:24.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:24.562
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 in namespace container-probe-4517 03/27/23 21:21:24.574
Mar 27 21:21:24.597: INFO: Waiting up to 5m0s for pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5" in namespace "container-probe-4517" to be "not pending"
Mar 27 21:21:24.608: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.671844ms
Mar 27 21:21:26.620: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022572117s
Mar 27 21:21:26.620: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5" satisfied condition "not pending"
Mar 27 21:21:26.620: INFO: Started pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 in namespace container-probe-4517
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:21:26.62
Mar 27 21:21:26.632: INFO: Initial restart count of pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 is 0
STEP: deleting the pod 03/27/23 21:25:28.415
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:28.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4517" for this suite. 03/27/23 21:25:28.461
------------------------------
• [SLOW TEST] [243.987 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:21:24.495
    Mar 27 21:21:24.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 21:21:24.496
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:21:24.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:21:24.562
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 in namespace container-probe-4517 03/27/23 21:21:24.574
    Mar 27 21:21:24.597: INFO: Waiting up to 5m0s for pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5" in namespace "container-probe-4517" to be "not pending"
    Mar 27 21:21:24.608: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.671844ms
    Mar 27 21:21:26.620: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.022572117s
    Mar 27 21:21:26.620: INFO: Pod "busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5" satisfied condition "not pending"
    Mar 27 21:21:26.620: INFO: Started pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 in namespace container-probe-4517
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:21:26.62
    Mar 27 21:21:26.632: INFO: Initial restart count of pod busybox-8563ffbd-ea4a-45c4-b06b-9d04fec8b6c5 is 0
    STEP: deleting the pod 03/27/23 21:25:28.415
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:28.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4517" for this suite. 03/27/23 21:25:28.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:28.493
Mar 27 21:25:28.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename watch 03/27/23 21:25:28.494
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:28.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:28.553
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/27/23 21:25:28.564
STEP: starting a background goroutine to produce watch events 03/27/23 21:25:28.576
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/27/23 21:25:28.576
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:31.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2183" for this suite. 03/27/23 21:25:31.362
------------------------------
• [2.923 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:28.493
    Mar 27 21:25:28.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename watch 03/27/23 21:25:28.494
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:28.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:28.553
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/27/23 21:25:28.564
    STEP: starting a background goroutine to produce watch events 03/27/23 21:25:28.576
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/27/23 21:25:28.576
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:31.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2183" for this suite. 03/27/23 21:25:31.362
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:31.416
Mar 27 21:25:31.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-webhook 03/27/23 21:25:31.419
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:31.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:31.48
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/27/23 21:25:31.49
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 21:25:32.014
STEP: Deploying the custom resource conversion webhook pod 03/27/23 21:25:32.041
STEP: Wait for the deployment to be ready 03/27/23 21:25:32.074
Mar 27 21:25:32.118: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:25:34.164
STEP: Verifying the service has paired with the endpoint 03/27/23 21:25:34.197
Mar 27 21:25:35.198: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 27 21:25:35.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Creating a v1 custom resource 03/27/23 21:25:37.985
STEP: Create a v2 custom resource 03/27/23 21:25:38.043
STEP: List CRs in v1 03/27/23 21:25:38.161
STEP: List CRs in v2 03/27/23 21:25:38.184
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:38.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4476" for this suite. 03/27/23 21:25:38.909
------------------------------
• [SLOW TEST] [7.513 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:31.416
    Mar 27 21:25:31.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-webhook 03/27/23 21:25:31.419
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:31.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:31.48
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/27/23 21:25:31.49
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 21:25:32.014
    STEP: Deploying the custom resource conversion webhook pod 03/27/23 21:25:32.041
    STEP: Wait for the deployment to be ready 03/27/23 21:25:32.074
    Mar 27 21:25:32.118: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:25:34.164
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:25:34.197
    Mar 27 21:25:35.198: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 27 21:25:35.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Creating a v1 custom resource 03/27/23 21:25:37.985
    STEP: Create a v2 custom resource 03/27/23 21:25:38.043
    STEP: List CRs in v1 03/27/23 21:25:38.161
    STEP: List CRs in v2 03/27/23 21:25:38.184
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:38.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4476" for this suite. 03/27/23 21:25:38.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:38.93
Mar 27 21:25:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:25:38.932
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:38.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:38.993
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 03/27/23 21:25:39.005
Mar 27 21:25:39.028: INFO: Waiting up to 5m0s for pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767" in namespace "downward-api-6522" to be "running and ready"
Mar 27 21:25:39.039: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767": Phase="Pending", Reason="", readiness=false. Elapsed: 11.341549ms
Mar 27 21:25:39.040: INFO: The phase of Pod labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:25:41.053: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767": Phase="Running", Reason="", readiness=true. Elapsed: 2.024667401s
Mar 27 21:25:41.053: INFO: The phase of Pod labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767 is Running (Ready = true)
Mar 27 21:25:41.053: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767" satisfied condition "running and ready"
Mar 27 21:25:41.668: INFO: Successfully updated pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:43.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6522" for this suite. 03/27/23 21:25:43.749
------------------------------
• [4.839 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:38.93
    Mar 27 21:25:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:25:38.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:38.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:38.993
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 03/27/23 21:25:39.005
    Mar 27 21:25:39.028: INFO: Waiting up to 5m0s for pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767" in namespace "downward-api-6522" to be "running and ready"
    Mar 27 21:25:39.039: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767": Phase="Pending", Reason="", readiness=false. Elapsed: 11.341549ms
    Mar 27 21:25:39.040: INFO: The phase of Pod labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:25:41.053: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767": Phase="Running", Reason="", readiness=true. Elapsed: 2.024667401s
    Mar 27 21:25:41.053: INFO: The phase of Pod labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767 is Running (Ready = true)
    Mar 27 21:25:41.053: INFO: Pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767" satisfied condition "running and ready"
    Mar 27 21:25:41.668: INFO: Successfully updated pod "labelsupdatef70cd429-82da-4238-b6fc-c5c1ec87f767"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:43.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6522" for this suite. 03/27/23 21:25:43.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:43.773
Mar 27 21:25:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 21:25:43.774
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:43.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:43.834
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 03/27/23 21:25:43.844
STEP: patching the Namespace 03/27/23 21:25:43.902
STEP: get the Namespace and ensuring it has the label 03/27/23 21:25:43.919
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:43.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6700" for this suite. 03/27/23 21:25:43.95
STEP: Destroying namespace "nspatchtest-2b848830-c58a-4d21-a948-27ae27047e4f-8493" for this suite. 03/27/23 21:25:43.972
------------------------------
• [0.222 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:43.773
    Mar 27 21:25:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 21:25:43.774
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:43.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:43.834
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 03/27/23 21:25:43.844
    STEP: patching the Namespace 03/27/23 21:25:43.902
    STEP: get the Namespace and ensuring it has the label 03/27/23 21:25:43.919
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:43.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6700" for this suite. 03/27/23 21:25:43.95
    STEP: Destroying namespace "nspatchtest-2b848830-c58a-4d21-a948-27ae27047e4f-8493" for this suite. 03/27/23 21:25:43.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:44.011
Mar 27 21:25:44.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename events 03/27/23 21:25:44.012
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:44.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:44.085
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/27/23 21:25:44.095
STEP: listing events in all namespaces 03/27/23 21:25:44.114
STEP: listing events in test namespace 03/27/23 21:25:44.141
STEP: listing events with field selection filtering on source 03/27/23 21:25:44.153
STEP: listing events with field selection filtering on reportingController 03/27/23 21:25:44.168
STEP: getting the test event 03/27/23 21:25:44.179
STEP: patching the test event 03/27/23 21:25:44.191
STEP: getting the test event 03/27/23 21:25:44.218
STEP: updating the test event 03/27/23 21:25:44.229
STEP: getting the test event 03/27/23 21:25:44.248
STEP: deleting the test event 03/27/23 21:25:44.259
STEP: listing events in all namespaces 03/27/23 21:25:44.285
STEP: listing events in test namespace 03/27/23 21:25:44.313
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-726" for this suite. 03/27/23 21:25:44.344
------------------------------
• [0.360 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:44.011
    Mar 27 21:25:44.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename events 03/27/23 21:25:44.012
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:44.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:44.085
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/27/23 21:25:44.095
    STEP: listing events in all namespaces 03/27/23 21:25:44.114
    STEP: listing events in test namespace 03/27/23 21:25:44.141
    STEP: listing events with field selection filtering on source 03/27/23 21:25:44.153
    STEP: listing events with field selection filtering on reportingController 03/27/23 21:25:44.168
    STEP: getting the test event 03/27/23 21:25:44.179
    STEP: patching the test event 03/27/23 21:25:44.191
    STEP: getting the test event 03/27/23 21:25:44.218
    STEP: updating the test event 03/27/23 21:25:44.229
    STEP: getting the test event 03/27/23 21:25:44.248
    STEP: deleting the test event 03/27/23 21:25:44.259
    STEP: listing events in all namespaces 03/27/23 21:25:44.285
    STEP: listing events in test namespace 03/27/23 21:25:44.313
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:44.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-726" for this suite. 03/27/23 21:25:44.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:44.374
Mar 27 21:25:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:25:44.376
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:44.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:44.437
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:25:44.502
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:25:45.05
STEP: Deploying the webhook pod 03/27/23 21:25:45.067
STEP: Wait for the deployment to be ready 03/27/23 21:25:45.1
Mar 27 21:25:45.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:25:47.18
STEP: Verifying the service has paired with the endpoint 03/27/23 21:25:47.211
Mar 27 21:25:48.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/27/23 21:25:48.225
STEP: create a namespace for the webhook 03/27/23 21:25:48.295
STEP: create a configmap should be unconditionally rejected by the webhook 03/27/23 21:25:48.324
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:48.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5561" for this suite. 03/27/23 21:25:48.569
STEP: Destroying namespace "webhook-5561-markers" for this suite. 03/27/23 21:25:48.595
------------------------------
• [4.243 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:44.374
    Mar 27 21:25:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:25:44.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:44.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:44.437
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:25:44.502
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:25:45.05
    STEP: Deploying the webhook pod 03/27/23 21:25:45.067
    STEP: Wait for the deployment to be ready 03/27/23 21:25:45.1
    Mar 27 21:25:45.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:25:47.18
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:25:47.211
    Mar 27 21:25:48.213: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/27/23 21:25:48.225
    STEP: create a namespace for the webhook 03/27/23 21:25:48.295
    STEP: create a configmap should be unconditionally rejected by the webhook 03/27/23 21:25:48.324
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:48.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5561" for this suite. 03/27/23 21:25:48.569
    STEP: Destroying namespace "webhook-5561-markers" for this suite. 03/27/23 21:25:48.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:48.624
Mar 27 21:25:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:25:48.625
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:48.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:48.688
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 03/27/23 21:25:48.699
Mar 27 21:25:48.700: INFO: namespace kubectl-3131
Mar 27 21:25:48.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 create -f -'
Mar 27 21:25:49.746: INFO: stderr: ""
Mar 27 21:25:49.746: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/27/23 21:25:49.746
Mar 27 21:25:50.759: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:25:50.759: INFO: Found 0 / 1
Mar 27 21:25:51.759: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:25:51.759: INFO: Found 0 / 1
Mar 27 21:25:52.757: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:25:52.758: INFO: Found 1 / 1
Mar 27 21:25:52.758: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 27 21:25:52.770: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 27 21:25:52.770: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 27 21:25:52.770: INFO: wait on agnhost-primary startup in kubectl-3131 
Mar 27 21:25:52.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 logs agnhost-primary-l2hjq agnhost-primary'
Mar 27 21:25:52.960: INFO: stderr: ""
Mar 27 21:25:52.960: INFO: stdout: "Paused\n"
STEP: exposing RC 03/27/23 21:25:52.96
Mar 27 21:25:52.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 27 21:25:53.139: INFO: stderr: ""
Mar 27 21:25:53.139: INFO: stdout: "service/rm2 exposed\n"
Mar 27 21:25:53.152: INFO: Service rm2 in namespace kubectl-3131 found.
STEP: exposing service 03/27/23 21:25:55.178
Mar 27 21:25:55.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 27 21:25:55.326: INFO: stderr: ""
Mar 27 21:25:55.326: INFO: stdout: "service/rm3 exposed\n"
Mar 27 21:25:55.338: INFO: Service rm3 in namespace kubectl-3131 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:25:57.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3131" for this suite. 03/27/23 21:25:57.382
------------------------------
• [SLOW TEST] [8.781 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:48.624
    Mar 27 21:25:48.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:25:48.625
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:48.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:48.688
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 03/27/23 21:25:48.699
    Mar 27 21:25:48.700: INFO: namespace kubectl-3131
    Mar 27 21:25:48.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 create -f -'
    Mar 27 21:25:49.746: INFO: stderr: ""
    Mar 27 21:25:49.746: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/27/23 21:25:49.746
    Mar 27 21:25:50.759: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:25:50.759: INFO: Found 0 / 1
    Mar 27 21:25:51.759: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:25:51.759: INFO: Found 0 / 1
    Mar 27 21:25:52.757: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:25:52.758: INFO: Found 1 / 1
    Mar 27 21:25:52.758: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 27 21:25:52.770: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 27 21:25:52.770: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 27 21:25:52.770: INFO: wait on agnhost-primary startup in kubectl-3131 
    Mar 27 21:25:52.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 logs agnhost-primary-l2hjq agnhost-primary'
    Mar 27 21:25:52.960: INFO: stderr: ""
    Mar 27 21:25:52.960: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/27/23 21:25:52.96
    Mar 27 21:25:52.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 27 21:25:53.139: INFO: stderr: ""
    Mar 27 21:25:53.139: INFO: stdout: "service/rm2 exposed\n"
    Mar 27 21:25:53.152: INFO: Service rm2 in namespace kubectl-3131 found.
    STEP: exposing service 03/27/23 21:25:55.178
    Mar 27 21:25:55.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3131 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 27 21:25:55.326: INFO: stderr: ""
    Mar 27 21:25:55.326: INFO: stdout: "service/rm3 exposed\n"
    Mar 27 21:25:55.338: INFO: Service rm3 in namespace kubectl-3131 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:25:57.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3131" for this suite. 03/27/23 21:25:57.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:25:57.409
Mar 27 21:25:57.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename init-container 03/27/23 21:25:57.41
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:57.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:57.472
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 03/27/23 21:25:57.483
Mar 27 21:25:57.483: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:01.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7557" for this suite. 03/27/23 21:26:02.017
------------------------------
• [4.632 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:25:57.409
    Mar 27 21:25:57.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename init-container 03/27/23 21:25:57.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:25:57.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:25:57.472
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 03/27/23 21:25:57.483
    Mar 27 21:25:57.483: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:01.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7557" for this suite. 03/27/23 21:26:02.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:02.044
Mar 27 21:26:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 21:26:02.045
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:02.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:02.128
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/27/23 21:26:02.139
STEP: Verify that the required pods have come up 03/27/23 21:26:02.158
Mar 27 21:26:02.177: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 27 21:26:07.193: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/27/23 21:26:07.193
Mar 27 21:26:07.208: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/27/23 21:26:07.208
STEP: DeleteCollection of the ReplicaSets 03/27/23 21:26:07.23
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/27/23 21:26:07.265
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:07.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6046" for this suite. 03/27/23 21:26:07.304
------------------------------
• [SLOW TEST] [5.300 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:02.044
    Mar 27 21:26:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 21:26:02.045
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:02.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:02.128
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/27/23 21:26:02.139
    STEP: Verify that the required pods have come up 03/27/23 21:26:02.158
    Mar 27 21:26:02.177: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 27 21:26:07.193: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/27/23 21:26:07.193
    Mar 27 21:26:07.208: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/27/23 21:26:07.208
    STEP: DeleteCollection of the ReplicaSets 03/27/23 21:26:07.23
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/27/23 21:26:07.265
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:07.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6046" for this suite. 03/27/23 21:26:07.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:07.349
Mar 27 21:26:07.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 21:26:07.35
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:07.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:07.43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Mar 27 21:26:07.467: INFO: Waiting up to 2m0s for pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" in namespace "var-expansion-6103" to be "container 0 failed with reason CreateContainerConfigError"
Mar 27 21:26:07.479: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495": Phase="Pending", Reason="", readiness=false. Elapsed: 11.317254ms
Mar 27 21:26:09.494: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026055427s
Mar 27 21:26:09.494: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 27 21:26:09.494: INFO: Deleting pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" in namespace "var-expansion-6103"
Mar 27 21:26:09.514: INFO: Wait up to 5m0s for pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:13.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6103" for this suite. 03/27/23 21:26:13.555
------------------------------
• [SLOW TEST] [6.229 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:07.349
    Mar 27 21:26:07.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 21:26:07.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:07.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:07.43
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Mar 27 21:26:07.467: INFO: Waiting up to 2m0s for pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" in namespace "var-expansion-6103" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 27 21:26:07.479: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495": Phase="Pending", Reason="", readiness=false. Elapsed: 11.317254ms
    Mar 27 21:26:09.494: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026055427s
    Mar 27 21:26:09.494: INFO: Pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 27 21:26:09.494: INFO: Deleting pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" in namespace "var-expansion-6103"
    Mar 27 21:26:09.514: INFO: Wait up to 5m0s for pod "var-expansion-df21175d-096f-4e76-ae7e-349180ddf495" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:13.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6103" for this suite. 03/27/23 21:26:13.555
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:13.579
Mar 27 21:26:13.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename limitrange 03/27/23 21:26:13.58
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:13.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:13.654
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-smcqw" in namespace "limitrange-473" 03/27/23 21:26:13.665
STEP: Creating another limitRange in another namespace 03/27/23 21:26:13.679
Mar 27 21:26:13.729: INFO: Namespace "e2e-limitrange-smcqw-3152" created
Mar 27 21:26:13.729: INFO: Creating LimitRange "e2e-limitrange-smcqw" in namespace "e2e-limitrange-smcqw-3152"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-smcqw" 03/27/23 21:26:13.746
Mar 27 21:26:13.757: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-smcqw" in "limitrange-473" namespace 03/27/23 21:26:13.757
Mar 27 21:26:13.777: INFO: LimitRange "e2e-limitrange-smcqw" has been patched
STEP: Delete LimitRange "e2e-limitrange-smcqw" by Collection with labelSelector: "e2e-limitrange-smcqw=patched" 03/27/23 21:26:13.777
STEP: Confirm that the limitRange "e2e-limitrange-smcqw" has been deleted 03/27/23 21:26:13.802
Mar 27 21:26:13.803: INFO: Requesting list of LimitRange to confirm quantity
Mar 27 21:26:13.814: INFO: Found 0 LimitRange with label "e2e-limitrange-smcqw=patched"
Mar 27 21:26:13.814: INFO: LimitRange "e2e-limitrange-smcqw" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-smcqw" 03/27/23 21:26:13.814
Mar 27 21:26:13.825: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:13.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-473" for this suite. 03/27/23 21:26:13.842
STEP: Destroying namespace "e2e-limitrange-smcqw-3152" for this suite. 03/27/23 21:26:13.878
------------------------------
• [0.319 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:13.579
    Mar 27 21:26:13.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename limitrange 03/27/23 21:26:13.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:13.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:13.654
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-smcqw" in namespace "limitrange-473" 03/27/23 21:26:13.665
    STEP: Creating another limitRange in another namespace 03/27/23 21:26:13.679
    Mar 27 21:26:13.729: INFO: Namespace "e2e-limitrange-smcqw-3152" created
    Mar 27 21:26:13.729: INFO: Creating LimitRange "e2e-limitrange-smcqw" in namespace "e2e-limitrange-smcqw-3152"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-smcqw" 03/27/23 21:26:13.746
    Mar 27 21:26:13.757: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-smcqw" in "limitrange-473" namespace 03/27/23 21:26:13.757
    Mar 27 21:26:13.777: INFO: LimitRange "e2e-limitrange-smcqw" has been patched
    STEP: Delete LimitRange "e2e-limitrange-smcqw" by Collection with labelSelector: "e2e-limitrange-smcqw=patched" 03/27/23 21:26:13.777
    STEP: Confirm that the limitRange "e2e-limitrange-smcqw" has been deleted 03/27/23 21:26:13.802
    Mar 27 21:26:13.803: INFO: Requesting list of LimitRange to confirm quantity
    Mar 27 21:26:13.814: INFO: Found 0 LimitRange with label "e2e-limitrange-smcqw=patched"
    Mar 27 21:26:13.814: INFO: LimitRange "e2e-limitrange-smcqw" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-smcqw" 03/27/23 21:26:13.814
    Mar 27 21:26:13.825: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:13.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-473" for this suite. 03/27/23 21:26:13.842
    STEP: Destroying namespace "e2e-limitrange-smcqw-3152" for this suite. 03/27/23 21:26:13.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:13.9
Mar 27 21:26:13.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:26:13.901
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:13.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:13.958
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 27 21:26:13.992: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037" in namespace "kubelet-test-4307" to be "running and ready"
Mar 27 21:26:14.003: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Pending", Reason="", readiness=false. Elapsed: 10.569519ms
Mar 27 21:26:14.003: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:26:16.016: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024114603s
Mar 27 21:26:16.016: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:26:18.017: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Running", Reason="", readiness=true. Elapsed: 4.024584027s
Mar 27 21:26:18.017: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Running (Ready = true)
Mar 27 21:26:18.017: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:18.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4307" for this suite. 03/27/23 21:26:18.08
------------------------------
• [4.200 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:13.9
    Mar 27 21:26:13.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:26:13.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:13.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:13.958
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 27 21:26:13.992: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037" in namespace "kubelet-test-4307" to be "running and ready"
    Mar 27 21:26:14.003: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Pending", Reason="", readiness=false. Elapsed: 10.569519ms
    Mar 27 21:26:14.003: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:26:16.016: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024114603s
    Mar 27 21:26:16.016: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:26:18.017: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037": Phase="Running", Reason="", readiness=true. Elapsed: 4.024584027s
    Mar 27 21:26:18.017: INFO: The phase of Pod busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037 is Running (Ready = true)
    Mar 27 21:26:18.017: INFO: Pod "busybox-scheduling-a846e142-dfa1-4f29-8103-d04ef0b24037" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:18.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4307" for this suite. 03/27/23 21:26:18.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:18.106
Mar 27 21:26:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:26:18.108
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:18.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:18.176
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 03/27/23 21:26:18.187
Mar 27 21:26:18.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 create -f -'
Mar 27 21:26:18.486: INFO: stderr: ""
Mar 27 21:26:18.486: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/27/23 21:26:18.486
Mar 27 21:26:18.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 diff -f -'
Mar 27 21:26:18.831: INFO: rc: 1
Mar 27 21:26:18.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 delete -f -'
Mar 27 21:26:18.953: INFO: stderr: ""
Mar 27 21:26:18.953: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-119" for this suite. 03/27/23 21:26:18.973
------------------------------
• [0.895 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:18.106
    Mar 27 21:26:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:26:18.108
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:18.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:18.176
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 03/27/23 21:26:18.187
    Mar 27 21:26:18.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 create -f -'
    Mar 27 21:26:18.486: INFO: stderr: ""
    Mar 27 21:26:18.486: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/27/23 21:26:18.486
    Mar 27 21:26:18.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 diff -f -'
    Mar 27 21:26:18.831: INFO: rc: 1
    Mar 27 21:26:18.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-119 delete -f -'
    Mar 27 21:26:18.953: INFO: stderr: ""
    Mar 27 21:26:18.953: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:18.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-119" for this suite. 03/27/23 21:26:18.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:19.006
Mar 27 21:26:19.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:26:19.009
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:19.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:19.069
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 27 21:26:19.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8265" for this suite. 03/27/23 21:26:22.445
------------------------------
• [3.470 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:19.006
    Mar 27 21:26:19.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:26:19.009
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:19.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:19.069
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 27 21:26:19.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8265" for this suite. 03/27/23 21:26:22.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:22.485
Mar 27 21:26:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:26:22.486
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:22.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:22.547
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 21:26:22.557
Mar 27 21:26:22.581: INFO: Waiting up to 5m0s for pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76" in namespace "emptydir-9226" to be "Succeeded or Failed"
Mar 27 21:26:22.592: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Pending", Reason="", readiness=false. Elapsed: 11.051519ms
Mar 27 21:26:24.605: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023680678s
Mar 27 21:26:26.637: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055905441s
STEP: Saw pod success 03/27/23 21:26:26.637
Mar 27 21:26:26.637: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76" satisfied condition "Succeeded or Failed"
Mar 27 21:26:26.671: INFO: Trying to get logs from node 10.176.99.177 pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 container test-container: <nil>
STEP: delete the pod 03/27/23 21:26:26.706
Mar 27 21:26:26.741: INFO: Waiting for pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 to disappear
Mar 27 21:26:26.759: INFO: Pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:26.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9226" for this suite. 03/27/23 21:26:26.779
------------------------------
• [4.317 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:22.485
    Mar 27 21:26:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:26:22.486
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:22.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:22.547
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 21:26:22.557
    Mar 27 21:26:22.581: INFO: Waiting up to 5m0s for pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76" in namespace "emptydir-9226" to be "Succeeded or Failed"
    Mar 27 21:26:22.592: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Pending", Reason="", readiness=false. Elapsed: 11.051519ms
    Mar 27 21:26:24.605: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023680678s
    Mar 27 21:26:26.637: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055905441s
    STEP: Saw pod success 03/27/23 21:26:26.637
    Mar 27 21:26:26.637: INFO: Pod "pod-0f626666-4fa1-418e-bac5-9fa3443a5b76" satisfied condition "Succeeded or Failed"
    Mar 27 21:26:26.671: INFO: Trying to get logs from node 10.176.99.177 pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 container test-container: <nil>
    STEP: delete the pod 03/27/23 21:26:26.706
    Mar 27 21:26:26.741: INFO: Waiting for pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 to disappear
    Mar 27 21:26:26.759: INFO: Pod pod-0f626666-4fa1-418e-bac5-9fa3443a5b76 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:26.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9226" for this suite. 03/27/23 21:26:26.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:26.803
Mar 27 21:26:26.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename subpath 03/27/23 21:26:26.804
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:26.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:26.866
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 21:26:26.876
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-ngzg 03/27/23 21:26:26.907
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:26:26.907
Mar 27 21:26:26.934: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ngzg" in namespace "subpath-3768" to be "Succeeded or Failed"
Mar 27 21:26:26.947: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.62658ms
Mar 27 21:26:28.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.026322239s
Mar 27 21:26:31.001: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 4.066520795s
Mar 27 21:26:32.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 6.025468216s
Mar 27 21:26:34.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 8.025588089s
Mar 27 21:26:36.959: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 10.024426427s
Mar 27 21:26:38.962: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 12.027719712s
Mar 27 21:26:40.992: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 14.05787248s
Mar 27 21:26:42.968: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 16.033604642s
Mar 27 21:26:44.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 18.02568273s
Mar 27 21:26:46.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 20.025662672s
Mar 27 21:26:48.961: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=false. Elapsed: 22.026526784s
Mar 27 21:26:50.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.025664741s
STEP: Saw pod success 03/27/23 21:26:50.96
Mar 27 21:26:50.960: INFO: Pod "pod-subpath-test-configmap-ngzg" satisfied condition "Succeeded or Failed"
Mar 27 21:26:50.972: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-configmap-ngzg container test-container-subpath-configmap-ngzg: <nil>
STEP: delete the pod 03/27/23 21:26:50.998
Mar 27 21:26:51.032: INFO: Waiting for pod pod-subpath-test-configmap-ngzg to disappear
Mar 27 21:26:51.042: INFO: Pod pod-subpath-test-configmap-ngzg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ngzg 03/27/23 21:26:51.042
Mar 27 21:26:51.043: INFO: Deleting pod "pod-subpath-test-configmap-ngzg" in namespace "subpath-3768"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:51.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3768" for this suite. 03/27/23 21:26:51.07
------------------------------
• [SLOW TEST] [24.289 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:26.803
    Mar 27 21:26:26.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename subpath 03/27/23 21:26:26.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:26.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:26.866
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 21:26:26.876
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-ngzg 03/27/23 21:26:26.907
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:26:26.907
    Mar 27 21:26:26.934: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ngzg" in namespace "subpath-3768" to be "Succeeded or Failed"
    Mar 27 21:26:26.947: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.62658ms
    Mar 27 21:26:28.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.026322239s
    Mar 27 21:26:31.001: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 4.066520795s
    Mar 27 21:26:32.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 6.025468216s
    Mar 27 21:26:34.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 8.025588089s
    Mar 27 21:26:36.959: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 10.024426427s
    Mar 27 21:26:38.962: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 12.027719712s
    Mar 27 21:26:40.992: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 14.05787248s
    Mar 27 21:26:42.968: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 16.033604642s
    Mar 27 21:26:44.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 18.02568273s
    Mar 27 21:26:46.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=true. Elapsed: 20.025662672s
    Mar 27 21:26:48.961: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Running", Reason="", readiness=false. Elapsed: 22.026526784s
    Mar 27 21:26:50.960: INFO: Pod "pod-subpath-test-configmap-ngzg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.025664741s
    STEP: Saw pod success 03/27/23 21:26:50.96
    Mar 27 21:26:50.960: INFO: Pod "pod-subpath-test-configmap-ngzg" satisfied condition "Succeeded or Failed"
    Mar 27 21:26:50.972: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-configmap-ngzg container test-container-subpath-configmap-ngzg: <nil>
    STEP: delete the pod 03/27/23 21:26:50.998
    Mar 27 21:26:51.032: INFO: Waiting for pod pod-subpath-test-configmap-ngzg to disappear
    Mar 27 21:26:51.042: INFO: Pod pod-subpath-test-configmap-ngzg no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-ngzg 03/27/23 21:26:51.042
    Mar 27 21:26:51.043: INFO: Deleting pod "pod-subpath-test-configmap-ngzg" in namespace "subpath-3768"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:51.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3768" for this suite. 03/27/23 21:26:51.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:51.094
Mar 27 21:26:51.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:26:51.095
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:51.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:51.156
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 03/27/23 21:26:51.167
Mar 27 21:26:51.190: INFO: Waiting up to 5m0s for pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1" in namespace "downward-api-3679" to be "Succeeded or Failed"
Mar 27 21:26:51.201: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.247315ms
Mar 27 21:26:53.212: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021843876s
Mar 27 21:26:55.212: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022136135s
STEP: Saw pod success 03/27/23 21:26:55.213
Mar 27 21:26:55.213: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1" satisfied condition "Succeeded or Failed"
Mar 27 21:26:55.223: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:26:55.249
Mar 27 21:26:55.276: INFO: Waiting for pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 to disappear
Mar 27 21:26:55.286: INFO: Pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:55.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3679" for this suite. 03/27/23 21:26:55.306
------------------------------
• [4.233 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:51.094
    Mar 27 21:26:51.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:26:51.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:51.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:51.156
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 03/27/23 21:26:51.167
    Mar 27 21:26:51.190: INFO: Waiting up to 5m0s for pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1" in namespace "downward-api-3679" to be "Succeeded or Failed"
    Mar 27 21:26:51.201: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.247315ms
    Mar 27 21:26:53.212: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021843876s
    Mar 27 21:26:55.212: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022136135s
    STEP: Saw pod success 03/27/23 21:26:55.213
    Mar 27 21:26:55.213: INFO: Pod "downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1" satisfied condition "Succeeded or Failed"
    Mar 27 21:26:55.223: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:26:55.249
    Mar 27 21:26:55.276: INFO: Waiting for pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 to disappear
    Mar 27 21:26:55.286: INFO: Pod downward-api-1e3bbf3c-b6d2-427c-bb49-6739363cb3e1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:55.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3679" for this suite. 03/27/23 21:26:55.306
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:55.327
Mar 27 21:26:55.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:26:55.328
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:55.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:55.387
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:26:55.398
Mar 27 21:26:55.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69" in namespace "projected-6225" to be "Succeeded or Failed"
Mar 27 21:26:55.436: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.543513ms
Mar 27 21:26:57.449: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023818131s
Mar 27 21:26:59.448: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022870756s
STEP: Saw pod success 03/27/23 21:26:59.448
Mar 27 21:26:59.448: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69" satisfied condition "Succeeded or Failed"
Mar 27 21:26:59.460: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 container client-container: <nil>
STEP: delete the pod 03/27/23 21:26:59.49
Mar 27 21:26:59.524: INFO: Waiting for pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 to disappear
Mar 27 21:26:59.534: INFO: Pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:26:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6225" for this suite. 03/27/23 21:26:59.552
------------------------------
• [4.248 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:55.327
    Mar 27 21:26:55.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:26:55.328
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:55.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:55.387
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:26:55.398
    Mar 27 21:26:55.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69" in namespace "projected-6225" to be "Succeeded or Failed"
    Mar 27 21:26:55.436: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.543513ms
    Mar 27 21:26:57.449: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023818131s
    Mar 27 21:26:59.448: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022870756s
    STEP: Saw pod success 03/27/23 21:26:59.448
    Mar 27 21:26:59.448: INFO: Pod "downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69" satisfied condition "Succeeded or Failed"
    Mar 27 21:26:59.460: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:26:59.49
    Mar 27 21:26:59.524: INFO: Waiting for pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 to disappear
    Mar 27 21:26:59.534: INFO: Pod downwardapi-volume-0c1b8c62-4bdd-4240-b7de-b73d29690b69 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:26:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6225" for this suite. 03/27/23 21:26:59.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:26:59.584
Mar 27 21:26:59.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:26:59.585
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:59.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:59.665
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 03/27/23 21:26:59.677
STEP: Counting existing ResourceQuota 03/27/23 21:27:04.69
STEP: Creating a ResourceQuota 03/27/23 21:27:09.724
STEP: Ensuring resource quota status is calculated 03/27/23 21:27:09.749
STEP: Creating a Secret 03/27/23 21:27:11.766
STEP: Ensuring resource quota status captures secret creation 03/27/23 21:27:11.8
STEP: Deleting a secret 03/27/23 21:27:13.816
STEP: Ensuring resource quota status released usage 03/27/23 21:27:13.834
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:27:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4366" for this suite. 03/27/23 21:27:15.868
------------------------------
• [SLOW TEST] [16.306 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:26:59.584
    Mar 27 21:26:59.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:26:59.585
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:26:59.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:26:59.665
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 03/27/23 21:26:59.677
    STEP: Counting existing ResourceQuota 03/27/23 21:27:04.69
    STEP: Creating a ResourceQuota 03/27/23 21:27:09.724
    STEP: Ensuring resource quota status is calculated 03/27/23 21:27:09.749
    STEP: Creating a Secret 03/27/23 21:27:11.766
    STEP: Ensuring resource quota status captures secret creation 03/27/23 21:27:11.8
    STEP: Deleting a secret 03/27/23 21:27:13.816
    STEP: Ensuring resource quota status released usage 03/27/23 21:27:13.834
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:27:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4366" for this suite. 03/27/23 21:27:15.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:27:15.898
Mar 27 21:27:15.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-runtime 03/27/23 21:27:15.899
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:15.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:15.959
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 03/27/23 21:27:15.969
STEP: wait for the container to reach Failed 03/27/23 21:27:15.992
STEP: get the container status 03/27/23 21:27:20.048
STEP: the container should be terminated 03/27/23 21:27:20.06
STEP: the termination message should be set 03/27/23 21:27:20.06
Mar 27 21:27:20.060: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/27/23 21:27:20.06
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 27 21:27:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4454" for this suite. 03/27/23 21:27:20.125
------------------------------
• [4.249 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:27:15.898
    Mar 27 21:27:15.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-runtime 03/27/23 21:27:15.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:15.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:15.959
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 03/27/23 21:27:15.969
    STEP: wait for the container to reach Failed 03/27/23 21:27:15.992
    STEP: get the container status 03/27/23 21:27:20.048
    STEP: the container should be terminated 03/27/23 21:27:20.06
    STEP: the termination message should be set 03/27/23 21:27:20.06
    Mar 27 21:27:20.060: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/27/23 21:27:20.06
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:27:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4454" for this suite. 03/27/23 21:27:20.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:27:20.148
Mar 27 21:27:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename prestop 03/27/23 21:27:20.149
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:20.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:20.226
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6383 03/27/23 21:27:20.236
STEP: Waiting for pods to come up. 03/27/23 21:27:20.26
Mar 27 21:27:20.260: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6383" to be "running"
Mar 27 21:27:20.271: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 10.96226ms
Mar 27 21:27:22.285: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.024812664s
Mar 27 21:27:22.285: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6383 03/27/23 21:27:22.296
Mar 27 21:27:22.312: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6383" to be "running"
Mar 27 21:27:22.332: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 19.995334ms
Mar 27 21:27:24.343: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.031105925s
Mar 27 21:27:24.343: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/27/23 21:27:24.343
Mar 27 21:27:29.415: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/27/23 21:27:29.415
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Mar 27 21:27:29.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6383" for this suite. 03/27/23 21:27:29.465
------------------------------
• [SLOW TEST] [9.340 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:27:20.148
    Mar 27 21:27:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename prestop 03/27/23 21:27:20.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:20.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:20.226
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6383 03/27/23 21:27:20.236
    STEP: Waiting for pods to come up. 03/27/23 21:27:20.26
    Mar 27 21:27:20.260: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6383" to be "running"
    Mar 27 21:27:20.271: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 10.96226ms
    Mar 27 21:27:22.285: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.024812664s
    Mar 27 21:27:22.285: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6383 03/27/23 21:27:22.296
    Mar 27 21:27:22.312: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6383" to be "running"
    Mar 27 21:27:22.332: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 19.995334ms
    Mar 27 21:27:24.343: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.031105925s
    Mar 27 21:27:24.343: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/27/23 21:27:24.343
    Mar 27 21:27:29.415: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/27/23 21:27:29.415
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:27:29.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6383" for this suite. 03/27/23 21:27:29.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:27:29.488
Mar 27 21:27:29.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 21:27:29.489
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:29.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:29.548
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Mar 27 21:27:29.629: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:27:29.642
Mar 27 21:27:29.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:27:29.669: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:27:30.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:27:30.721: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:27:31.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:27:31.700: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/27/23 21:27:31.75
STEP: Check that daemon pods images are updated. 03/27/23 21:27:31.786
Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-grh2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:32.830: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:32.830: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:33.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:33.827: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:34.828: INFO: Pod daemon-set-5z9x5 is not available
Mar 27 21:27:34.828: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:34.828: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:35.827: INFO: Pod daemon-set-5z9x5 is not available
Mar 27 21:27:35.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:35.828: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:36.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:37.828: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:37.829: INFO: Pod daemon-set-p9bkf is not available
Mar 27 21:27:38.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Mar 27 21:27:38.828: INFO: Pod daemon-set-p9bkf is not available
Mar 27 21:27:40.827: INFO: Pod daemon-set-q6tnw is not available
STEP: Check that daemon pods are still running on every node of the cluster. 03/27/23 21:27:40.845
Mar 27 21:27:40.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:27:40.872: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:27:41.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:27:41.902: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:27:42.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:27:42.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:27:42.97
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6353, will wait for the garbage collector to delete the pods 03/27/23 21:27:42.97
Mar 27 21:27:43.072: INFO: Deleting DaemonSet.extensions daemon-set took: 39.23303ms
Mar 27 21:27:43.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.744037ms
Mar 27 21:27:45.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:27:45.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 21:27:45.297: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29503"},"items":null}

Mar 27 21:27:45.320: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:27:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6353" for this suite. 03/27/23 21:27:45.388
------------------------------
• [SLOW TEST] [15.923 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:27:29.488
    Mar 27 21:27:29.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 21:27:29.489
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:29.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:29.548
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Mar 27 21:27:29.629: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:27:29.642
    Mar 27 21:27:29.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:27:29.669: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:27:30.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:27:30.721: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:27:31.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:27:31.700: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/27/23 21:27:31.75
    STEP: Check that daemon pods images are updated. 03/27/23 21:27:31.786
    Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-grh2f. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:31.799: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:32.830: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:32.830: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:33.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:33.827: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:34.828: INFO: Pod daemon-set-5z9x5 is not available
    Mar 27 21:27:34.828: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:34.828: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:35.827: INFO: Pod daemon-set-5z9x5 is not available
    Mar 27 21:27:35.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:35.828: INFO: Wrong image for pod: daemon-set-kh9rg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:36.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:37.828: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:37.829: INFO: Pod daemon-set-p9bkf is not available
    Mar 27 21:27:38.827: INFO: Wrong image for pod: daemon-set-frp6c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Mar 27 21:27:38.828: INFO: Pod daemon-set-p9bkf is not available
    Mar 27 21:27:40.827: INFO: Pod daemon-set-q6tnw is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 03/27/23 21:27:40.845
    Mar 27 21:27:40.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:27:40.872: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:27:41.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:27:41.902: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:27:42.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:27:42.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:27:42.97
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6353, will wait for the garbage collector to delete the pods 03/27/23 21:27:42.97
    Mar 27 21:27:43.072: INFO: Deleting DaemonSet.extensions daemon-set took: 39.23303ms
    Mar 27 21:27:43.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.744037ms
    Mar 27 21:27:45.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:27:45.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 21:27:45.297: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29503"},"items":null}

    Mar 27 21:27:45.320: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29503"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:27:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6353" for this suite. 03/27/23 21:27:45.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:27:45.414
Mar 27 21:27:45.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:27:45.415
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:45.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:45.484
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Mar 27 21:27:45.508: INFO: Got root ca configmap in namespace "svcaccounts-3750"
Mar 27 21:27:45.530: INFO: Deleted root ca configmap in namespace "svcaccounts-3750"
STEP: waiting for a new root ca configmap created 03/27/23 21:27:46.032
Mar 27 21:27:46.045: INFO: Recreated root ca configmap in namespace "svcaccounts-3750"
Mar 27 21:27:46.061: INFO: Updated root ca configmap in namespace "svcaccounts-3750"
STEP: waiting for the root ca configmap reconciled 03/27/23 21:27:46.562
Mar 27 21:27:46.576: INFO: Reconciled root ca configmap in namespace "svcaccounts-3750"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 21:27:46.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3750" for this suite. 03/27/23 21:27:46.594
------------------------------
• [1.202 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:27:45.414
    Mar 27 21:27:45.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:27:45.415
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:45.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:45.484
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Mar 27 21:27:45.508: INFO: Got root ca configmap in namespace "svcaccounts-3750"
    Mar 27 21:27:45.530: INFO: Deleted root ca configmap in namespace "svcaccounts-3750"
    STEP: waiting for a new root ca configmap created 03/27/23 21:27:46.032
    Mar 27 21:27:46.045: INFO: Recreated root ca configmap in namespace "svcaccounts-3750"
    Mar 27 21:27:46.061: INFO: Updated root ca configmap in namespace "svcaccounts-3750"
    STEP: waiting for the root ca configmap reconciled 03/27/23 21:27:46.562
    Mar 27 21:27:46.576: INFO: Reconciled root ca configmap in namespace "svcaccounts-3750"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:27:46.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3750" for this suite. 03/27/23 21:27:46.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:27:46.617
Mar 27 21:27:46.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 21:27:46.619
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:46.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:46.68
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/27/23 21:27:46.705
STEP: delete the rc 03/27/23 21:27:51.739
STEP: wait for the rc to be deleted 03/27/23 21:27:51.778
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/27/23 21:27:56.795
STEP: Gathering metrics 03/27/23 21:28:26.834
W0327 21:28:26.860924      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 21:28:26.861: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 27 21:28:26.861: INFO: Deleting pod "simpletest.rc-227jd" in namespace "gc-9590"
Mar 27 21:28:26.899: INFO: Deleting pod "simpletest.rc-269vt" in namespace "gc-9590"
Mar 27 21:28:26.939: INFO: Deleting pod "simpletest.rc-2chk6" in namespace "gc-9590"
Mar 27 21:28:26.969: INFO: Deleting pod "simpletest.rc-2n9xn" in namespace "gc-9590"
Mar 27 21:28:27.007: INFO: Deleting pod "simpletest.rc-45crw" in namespace "gc-9590"
Mar 27 21:28:27.044: INFO: Deleting pod "simpletest.rc-4jmc8" in namespace "gc-9590"
Mar 27 21:28:27.078: INFO: Deleting pod "simpletest.rc-55tct" in namespace "gc-9590"
Mar 27 21:28:27.106: INFO: Deleting pod "simpletest.rc-5mrrm" in namespace "gc-9590"
Mar 27 21:28:27.141: INFO: Deleting pod "simpletest.rc-5vw92" in namespace "gc-9590"
Mar 27 21:28:27.176: INFO: Deleting pod "simpletest.rc-67wj8" in namespace "gc-9590"
Mar 27 21:28:27.212: INFO: Deleting pod "simpletest.rc-687d6" in namespace "gc-9590"
Mar 27 21:28:27.252: INFO: Deleting pod "simpletest.rc-6ks5h" in namespace "gc-9590"
Mar 27 21:28:27.291: INFO: Deleting pod "simpletest.rc-6p7ph" in namespace "gc-9590"
Mar 27 21:28:27.327: INFO: Deleting pod "simpletest.rc-6thfc" in namespace "gc-9590"
Mar 27 21:28:27.365: INFO: Deleting pod "simpletest.rc-7hprd" in namespace "gc-9590"
Mar 27 21:28:27.413: INFO: Deleting pod "simpletest.rc-7p87z" in namespace "gc-9590"
Mar 27 21:28:27.451: INFO: Deleting pod "simpletest.rc-7x7gh" in namespace "gc-9590"
Mar 27 21:28:27.494: INFO: Deleting pod "simpletest.rc-8gxtm" in namespace "gc-9590"
Mar 27 21:28:27.537: INFO: Deleting pod "simpletest.rc-8tcvk" in namespace "gc-9590"
Mar 27 21:28:27.577: INFO: Deleting pod "simpletest.rc-9gjjl" in namespace "gc-9590"
Mar 27 21:28:27.604: INFO: Deleting pod "simpletest.rc-9lmzm" in namespace "gc-9590"
Mar 27 21:28:27.637: INFO: Deleting pod "simpletest.rc-9tdrb" in namespace "gc-9590"
Mar 27 21:28:27.665: INFO: Deleting pod "simpletest.rc-b6rdb" in namespace "gc-9590"
Mar 27 21:28:27.702: INFO: Deleting pod "simpletest.rc-bbgml" in namespace "gc-9590"
Mar 27 21:28:27.738: INFO: Deleting pod "simpletest.rc-bbgxt" in namespace "gc-9590"
Mar 27 21:28:27.774: INFO: Deleting pod "simpletest.rc-bdt72" in namespace "gc-9590"
Mar 27 21:28:27.811: INFO: Deleting pod "simpletest.rc-bghnj" in namespace "gc-9590"
Mar 27 21:28:27.849: INFO: Deleting pod "simpletest.rc-bjnkn" in namespace "gc-9590"
Mar 27 21:28:27.887: INFO: Deleting pod "simpletest.rc-bnmgr" in namespace "gc-9590"
Mar 27 21:28:27.925: INFO: Deleting pod "simpletest.rc-c67rl" in namespace "gc-9590"
Mar 27 21:28:27.978: INFO: Deleting pod "simpletest.rc-cm2x6" in namespace "gc-9590"
Mar 27 21:28:28.005: INFO: Deleting pod "simpletest.rc-cm77z" in namespace "gc-9590"
Mar 27 21:28:28.042: INFO: Deleting pod "simpletest.rc-cq49n" in namespace "gc-9590"
Mar 27 21:28:28.080: INFO: Deleting pod "simpletest.rc-csg8w" in namespace "gc-9590"
Mar 27 21:28:28.112: INFO: Deleting pod "simpletest.rc-cswfz" in namespace "gc-9590"
Mar 27 21:28:28.158: INFO: Deleting pod "simpletest.rc-ctfhr" in namespace "gc-9590"
Mar 27 21:28:28.189: INFO: Deleting pod "simpletest.rc-ctpkh" in namespace "gc-9590"
Mar 27 21:28:28.225: INFO: Deleting pod "simpletest.rc-d4rgs" in namespace "gc-9590"
Mar 27 21:28:28.259: INFO: Deleting pod "simpletest.rc-d9chc" in namespace "gc-9590"
Mar 27 21:28:28.320: INFO: Deleting pod "simpletest.rc-dgnpb" in namespace "gc-9590"
Mar 27 21:28:28.351: INFO: Deleting pod "simpletest.rc-dlgtz" in namespace "gc-9590"
Mar 27 21:28:28.384: INFO: Deleting pod "simpletest.rc-f4fb4" in namespace "gc-9590"
Mar 27 21:28:28.416: INFO: Deleting pod "simpletest.rc-fdghg" in namespace "gc-9590"
Mar 27 21:28:28.454: INFO: Deleting pod "simpletest.rc-fj9xv" in namespace "gc-9590"
Mar 27 21:28:28.488: INFO: Deleting pod "simpletest.rc-ggc64" in namespace "gc-9590"
Mar 27 21:28:28.526: INFO: Deleting pod "simpletest.rc-gww7z" in namespace "gc-9590"
Mar 27 21:28:28.568: INFO: Deleting pod "simpletest.rc-h268r" in namespace "gc-9590"
Mar 27 21:28:28.608: INFO: Deleting pod "simpletest.rc-hl5r8" in namespace "gc-9590"
Mar 27 21:28:28.658: INFO: Deleting pod "simpletest.rc-j2qkr" in namespace "gc-9590"
Mar 27 21:28:28.689: INFO: Deleting pod "simpletest.rc-jh8hw" in namespace "gc-9590"
Mar 27 21:28:28.721: INFO: Deleting pod "simpletest.rc-jhqsw" in namespace "gc-9590"
Mar 27 21:28:28.756: INFO: Deleting pod "simpletest.rc-k7rmt" in namespace "gc-9590"
Mar 27 21:28:28.786: INFO: Deleting pod "simpletest.rc-knfvv" in namespace "gc-9590"
Mar 27 21:28:28.823: INFO: Deleting pod "simpletest.rc-knzvq" in namespace "gc-9590"
Mar 27 21:28:28.856: INFO: Deleting pod "simpletest.rc-kr5hv" in namespace "gc-9590"
Mar 27 21:28:28.888: INFO: Deleting pod "simpletest.rc-ll72m" in namespace "gc-9590"
Mar 27 21:28:28.919: INFO: Deleting pod "simpletest.rc-lsv5v" in namespace "gc-9590"
Mar 27 21:28:28.958: INFO: Deleting pod "simpletest.rc-lzmff" in namespace "gc-9590"
Mar 27 21:28:29.009: INFO: Deleting pod "simpletest.rc-m4q86" in namespace "gc-9590"
Mar 27 21:28:29.053: INFO: Deleting pod "simpletest.rc-mhr78" in namespace "gc-9590"
Mar 27 21:28:29.090: INFO: Deleting pod "simpletest.rc-mprdc" in namespace "gc-9590"
Mar 27 21:28:29.119: INFO: Deleting pod "simpletest.rc-mrqb9" in namespace "gc-9590"
Mar 27 21:28:29.153: INFO: Deleting pod "simpletest.rc-mvs97" in namespace "gc-9590"
Mar 27 21:28:29.200: INFO: Deleting pod "simpletest.rc-mxbsw" in namespace "gc-9590"
Mar 27 21:28:29.237: INFO: Deleting pod "simpletest.rc-nf8p6" in namespace "gc-9590"
Mar 27 21:28:29.273: INFO: Deleting pod "simpletest.rc-nlr8s" in namespace "gc-9590"
Mar 27 21:28:29.324: INFO: Deleting pod "simpletest.rc-nlw7t" in namespace "gc-9590"
Mar 27 21:28:29.361: INFO: Deleting pod "simpletest.rc-p72l4" in namespace "gc-9590"
Mar 27 21:28:29.438: INFO: Deleting pod "simpletest.rc-pt6db" in namespace "gc-9590"
Mar 27 21:28:29.477: INFO: Deleting pod "simpletest.rc-qb4hw" in namespace "gc-9590"
Mar 27 21:28:29.516: INFO: Deleting pod "simpletest.rc-qhrcw" in namespace "gc-9590"
Mar 27 21:28:29.554: INFO: Deleting pod "simpletest.rc-qjm45" in namespace "gc-9590"
Mar 27 21:28:29.593: INFO: Deleting pod "simpletest.rc-rcl86" in namespace "gc-9590"
Mar 27 21:28:29.622: INFO: Deleting pod "simpletest.rc-rg8rw" in namespace "gc-9590"
Mar 27 21:28:29.656: INFO: Deleting pod "simpletest.rc-rqsq4" in namespace "gc-9590"
Mar 27 21:28:29.708: INFO: Deleting pod "simpletest.rc-rs66g" in namespace "gc-9590"
Mar 27 21:28:29.743: INFO: Deleting pod "simpletest.rc-rxlln" in namespace "gc-9590"
Mar 27 21:28:29.776: INFO: Deleting pod "simpletest.rc-s77vg" in namespace "gc-9590"
Mar 27 21:28:29.841: INFO: Deleting pod "simpletest.rc-scd5m" in namespace "gc-9590"
Mar 27 21:28:29.923: INFO: Deleting pod "simpletest.rc-scth5" in namespace "gc-9590"
Mar 27 21:28:29.983: INFO: Deleting pod "simpletest.rc-sxngz" in namespace "gc-9590"
Mar 27 21:28:30.018: INFO: Deleting pod "simpletest.rc-tbczb" in namespace "gc-9590"
Mar 27 21:28:30.066: INFO: Deleting pod "simpletest.rc-thlnm" in namespace "gc-9590"
Mar 27 21:28:30.100: INFO: Deleting pod "simpletest.rc-tkq6p" in namespace "gc-9590"
Mar 27 21:28:30.132: INFO: Deleting pod "simpletest.rc-tqnrm" in namespace "gc-9590"
Mar 27 21:28:30.173: INFO: Deleting pod "simpletest.rc-txj45" in namespace "gc-9590"
Mar 27 21:28:30.211: INFO: Deleting pod "simpletest.rc-v5qk4" in namespace "gc-9590"
Mar 27 21:28:30.258: INFO: Deleting pod "simpletest.rc-vp7mb" in namespace "gc-9590"
Mar 27 21:28:30.313: INFO: Deleting pod "simpletest.rc-w8bl7" in namespace "gc-9590"
Mar 27 21:28:30.347: INFO: Deleting pod "simpletest.rc-wzvzk" in namespace "gc-9590"
Mar 27 21:28:30.379: INFO: Deleting pod "simpletest.rc-xk2qh" in namespace "gc-9590"
Mar 27 21:28:30.410: INFO: Deleting pod "simpletest.rc-xkzvt" in namespace "gc-9590"
Mar 27 21:28:30.452: INFO: Deleting pod "simpletest.rc-xlchm" in namespace "gc-9590"
Mar 27 21:28:30.492: INFO: Deleting pod "simpletest.rc-xswdt" in namespace "gc-9590"
Mar 27 21:28:30.527: INFO: Deleting pod "simpletest.rc-xvh2q" in namespace "gc-9590"
Mar 27 21:28:30.590: INFO: Deleting pod "simpletest.rc-zdbsv" in namespace "gc-9590"
Mar 27 21:28:30.681: INFO: Deleting pod "simpletest.rc-zjnsf" in namespace "gc-9590"
Mar 27 21:28:30.756: INFO: Deleting pod "simpletest.rc-zkc8s" in namespace "gc-9590"
Mar 27 21:28:30.792: INFO: Deleting pod "simpletest.rc-zttsd" in namespace "gc-9590"
Mar 27 21:28:30.828: INFO: Deleting pod "simpletest.rc-zvffg" in namespace "gc-9590"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 21:28:30.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9590" for this suite. 03/27/23 21:28:30.899
------------------------------
• [SLOW TEST] [44.304 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:27:46.617
    Mar 27 21:27:46.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 21:27:46.619
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:27:46.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:27:46.68
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/27/23 21:27:46.705
    STEP: delete the rc 03/27/23 21:27:51.739
    STEP: wait for the rc to be deleted 03/27/23 21:27:51.778
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/27/23 21:27:56.795
    STEP: Gathering metrics 03/27/23 21:28:26.834
    W0327 21:28:26.860924      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 21:28:26.861: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 27 21:28:26.861: INFO: Deleting pod "simpletest.rc-227jd" in namespace "gc-9590"
    Mar 27 21:28:26.899: INFO: Deleting pod "simpletest.rc-269vt" in namespace "gc-9590"
    Mar 27 21:28:26.939: INFO: Deleting pod "simpletest.rc-2chk6" in namespace "gc-9590"
    Mar 27 21:28:26.969: INFO: Deleting pod "simpletest.rc-2n9xn" in namespace "gc-9590"
    Mar 27 21:28:27.007: INFO: Deleting pod "simpletest.rc-45crw" in namespace "gc-9590"
    Mar 27 21:28:27.044: INFO: Deleting pod "simpletest.rc-4jmc8" in namespace "gc-9590"
    Mar 27 21:28:27.078: INFO: Deleting pod "simpletest.rc-55tct" in namespace "gc-9590"
    Mar 27 21:28:27.106: INFO: Deleting pod "simpletest.rc-5mrrm" in namespace "gc-9590"
    Mar 27 21:28:27.141: INFO: Deleting pod "simpletest.rc-5vw92" in namespace "gc-9590"
    Mar 27 21:28:27.176: INFO: Deleting pod "simpletest.rc-67wj8" in namespace "gc-9590"
    Mar 27 21:28:27.212: INFO: Deleting pod "simpletest.rc-687d6" in namespace "gc-9590"
    Mar 27 21:28:27.252: INFO: Deleting pod "simpletest.rc-6ks5h" in namespace "gc-9590"
    Mar 27 21:28:27.291: INFO: Deleting pod "simpletest.rc-6p7ph" in namespace "gc-9590"
    Mar 27 21:28:27.327: INFO: Deleting pod "simpletest.rc-6thfc" in namespace "gc-9590"
    Mar 27 21:28:27.365: INFO: Deleting pod "simpletest.rc-7hprd" in namespace "gc-9590"
    Mar 27 21:28:27.413: INFO: Deleting pod "simpletest.rc-7p87z" in namespace "gc-9590"
    Mar 27 21:28:27.451: INFO: Deleting pod "simpletest.rc-7x7gh" in namespace "gc-9590"
    Mar 27 21:28:27.494: INFO: Deleting pod "simpletest.rc-8gxtm" in namespace "gc-9590"
    Mar 27 21:28:27.537: INFO: Deleting pod "simpletest.rc-8tcvk" in namespace "gc-9590"
    Mar 27 21:28:27.577: INFO: Deleting pod "simpletest.rc-9gjjl" in namespace "gc-9590"
    Mar 27 21:28:27.604: INFO: Deleting pod "simpletest.rc-9lmzm" in namespace "gc-9590"
    Mar 27 21:28:27.637: INFO: Deleting pod "simpletest.rc-9tdrb" in namespace "gc-9590"
    Mar 27 21:28:27.665: INFO: Deleting pod "simpletest.rc-b6rdb" in namespace "gc-9590"
    Mar 27 21:28:27.702: INFO: Deleting pod "simpletest.rc-bbgml" in namespace "gc-9590"
    Mar 27 21:28:27.738: INFO: Deleting pod "simpletest.rc-bbgxt" in namespace "gc-9590"
    Mar 27 21:28:27.774: INFO: Deleting pod "simpletest.rc-bdt72" in namespace "gc-9590"
    Mar 27 21:28:27.811: INFO: Deleting pod "simpletest.rc-bghnj" in namespace "gc-9590"
    Mar 27 21:28:27.849: INFO: Deleting pod "simpletest.rc-bjnkn" in namespace "gc-9590"
    Mar 27 21:28:27.887: INFO: Deleting pod "simpletest.rc-bnmgr" in namespace "gc-9590"
    Mar 27 21:28:27.925: INFO: Deleting pod "simpletest.rc-c67rl" in namespace "gc-9590"
    Mar 27 21:28:27.978: INFO: Deleting pod "simpletest.rc-cm2x6" in namespace "gc-9590"
    Mar 27 21:28:28.005: INFO: Deleting pod "simpletest.rc-cm77z" in namespace "gc-9590"
    Mar 27 21:28:28.042: INFO: Deleting pod "simpletest.rc-cq49n" in namespace "gc-9590"
    Mar 27 21:28:28.080: INFO: Deleting pod "simpletest.rc-csg8w" in namespace "gc-9590"
    Mar 27 21:28:28.112: INFO: Deleting pod "simpletest.rc-cswfz" in namespace "gc-9590"
    Mar 27 21:28:28.158: INFO: Deleting pod "simpletest.rc-ctfhr" in namespace "gc-9590"
    Mar 27 21:28:28.189: INFO: Deleting pod "simpletest.rc-ctpkh" in namespace "gc-9590"
    Mar 27 21:28:28.225: INFO: Deleting pod "simpletest.rc-d4rgs" in namespace "gc-9590"
    Mar 27 21:28:28.259: INFO: Deleting pod "simpletest.rc-d9chc" in namespace "gc-9590"
    Mar 27 21:28:28.320: INFO: Deleting pod "simpletest.rc-dgnpb" in namespace "gc-9590"
    Mar 27 21:28:28.351: INFO: Deleting pod "simpletest.rc-dlgtz" in namespace "gc-9590"
    Mar 27 21:28:28.384: INFO: Deleting pod "simpletest.rc-f4fb4" in namespace "gc-9590"
    Mar 27 21:28:28.416: INFO: Deleting pod "simpletest.rc-fdghg" in namespace "gc-9590"
    Mar 27 21:28:28.454: INFO: Deleting pod "simpletest.rc-fj9xv" in namespace "gc-9590"
    Mar 27 21:28:28.488: INFO: Deleting pod "simpletest.rc-ggc64" in namespace "gc-9590"
    Mar 27 21:28:28.526: INFO: Deleting pod "simpletest.rc-gww7z" in namespace "gc-9590"
    Mar 27 21:28:28.568: INFO: Deleting pod "simpletest.rc-h268r" in namespace "gc-9590"
    Mar 27 21:28:28.608: INFO: Deleting pod "simpletest.rc-hl5r8" in namespace "gc-9590"
    Mar 27 21:28:28.658: INFO: Deleting pod "simpletest.rc-j2qkr" in namespace "gc-9590"
    Mar 27 21:28:28.689: INFO: Deleting pod "simpletest.rc-jh8hw" in namespace "gc-9590"
    Mar 27 21:28:28.721: INFO: Deleting pod "simpletest.rc-jhqsw" in namespace "gc-9590"
    Mar 27 21:28:28.756: INFO: Deleting pod "simpletest.rc-k7rmt" in namespace "gc-9590"
    Mar 27 21:28:28.786: INFO: Deleting pod "simpletest.rc-knfvv" in namespace "gc-9590"
    Mar 27 21:28:28.823: INFO: Deleting pod "simpletest.rc-knzvq" in namespace "gc-9590"
    Mar 27 21:28:28.856: INFO: Deleting pod "simpletest.rc-kr5hv" in namespace "gc-9590"
    Mar 27 21:28:28.888: INFO: Deleting pod "simpletest.rc-ll72m" in namespace "gc-9590"
    Mar 27 21:28:28.919: INFO: Deleting pod "simpletest.rc-lsv5v" in namespace "gc-9590"
    Mar 27 21:28:28.958: INFO: Deleting pod "simpletest.rc-lzmff" in namespace "gc-9590"
    Mar 27 21:28:29.009: INFO: Deleting pod "simpletest.rc-m4q86" in namespace "gc-9590"
    Mar 27 21:28:29.053: INFO: Deleting pod "simpletest.rc-mhr78" in namespace "gc-9590"
    Mar 27 21:28:29.090: INFO: Deleting pod "simpletest.rc-mprdc" in namespace "gc-9590"
    Mar 27 21:28:29.119: INFO: Deleting pod "simpletest.rc-mrqb9" in namespace "gc-9590"
    Mar 27 21:28:29.153: INFO: Deleting pod "simpletest.rc-mvs97" in namespace "gc-9590"
    Mar 27 21:28:29.200: INFO: Deleting pod "simpletest.rc-mxbsw" in namespace "gc-9590"
    Mar 27 21:28:29.237: INFO: Deleting pod "simpletest.rc-nf8p6" in namespace "gc-9590"
    Mar 27 21:28:29.273: INFO: Deleting pod "simpletest.rc-nlr8s" in namespace "gc-9590"
    Mar 27 21:28:29.324: INFO: Deleting pod "simpletest.rc-nlw7t" in namespace "gc-9590"
    Mar 27 21:28:29.361: INFO: Deleting pod "simpletest.rc-p72l4" in namespace "gc-9590"
    Mar 27 21:28:29.438: INFO: Deleting pod "simpletest.rc-pt6db" in namespace "gc-9590"
    Mar 27 21:28:29.477: INFO: Deleting pod "simpletest.rc-qb4hw" in namespace "gc-9590"
    Mar 27 21:28:29.516: INFO: Deleting pod "simpletest.rc-qhrcw" in namespace "gc-9590"
    Mar 27 21:28:29.554: INFO: Deleting pod "simpletest.rc-qjm45" in namespace "gc-9590"
    Mar 27 21:28:29.593: INFO: Deleting pod "simpletest.rc-rcl86" in namespace "gc-9590"
    Mar 27 21:28:29.622: INFO: Deleting pod "simpletest.rc-rg8rw" in namespace "gc-9590"
    Mar 27 21:28:29.656: INFO: Deleting pod "simpletest.rc-rqsq4" in namespace "gc-9590"
    Mar 27 21:28:29.708: INFO: Deleting pod "simpletest.rc-rs66g" in namespace "gc-9590"
    Mar 27 21:28:29.743: INFO: Deleting pod "simpletest.rc-rxlln" in namespace "gc-9590"
    Mar 27 21:28:29.776: INFO: Deleting pod "simpletest.rc-s77vg" in namespace "gc-9590"
    Mar 27 21:28:29.841: INFO: Deleting pod "simpletest.rc-scd5m" in namespace "gc-9590"
    Mar 27 21:28:29.923: INFO: Deleting pod "simpletest.rc-scth5" in namespace "gc-9590"
    Mar 27 21:28:29.983: INFO: Deleting pod "simpletest.rc-sxngz" in namespace "gc-9590"
    Mar 27 21:28:30.018: INFO: Deleting pod "simpletest.rc-tbczb" in namespace "gc-9590"
    Mar 27 21:28:30.066: INFO: Deleting pod "simpletest.rc-thlnm" in namespace "gc-9590"
    Mar 27 21:28:30.100: INFO: Deleting pod "simpletest.rc-tkq6p" in namespace "gc-9590"
    Mar 27 21:28:30.132: INFO: Deleting pod "simpletest.rc-tqnrm" in namespace "gc-9590"
    Mar 27 21:28:30.173: INFO: Deleting pod "simpletest.rc-txj45" in namespace "gc-9590"
    Mar 27 21:28:30.211: INFO: Deleting pod "simpletest.rc-v5qk4" in namespace "gc-9590"
    Mar 27 21:28:30.258: INFO: Deleting pod "simpletest.rc-vp7mb" in namespace "gc-9590"
    Mar 27 21:28:30.313: INFO: Deleting pod "simpletest.rc-w8bl7" in namespace "gc-9590"
    Mar 27 21:28:30.347: INFO: Deleting pod "simpletest.rc-wzvzk" in namespace "gc-9590"
    Mar 27 21:28:30.379: INFO: Deleting pod "simpletest.rc-xk2qh" in namespace "gc-9590"
    Mar 27 21:28:30.410: INFO: Deleting pod "simpletest.rc-xkzvt" in namespace "gc-9590"
    Mar 27 21:28:30.452: INFO: Deleting pod "simpletest.rc-xlchm" in namespace "gc-9590"
    Mar 27 21:28:30.492: INFO: Deleting pod "simpletest.rc-xswdt" in namespace "gc-9590"
    Mar 27 21:28:30.527: INFO: Deleting pod "simpletest.rc-xvh2q" in namespace "gc-9590"
    Mar 27 21:28:30.590: INFO: Deleting pod "simpletest.rc-zdbsv" in namespace "gc-9590"
    Mar 27 21:28:30.681: INFO: Deleting pod "simpletest.rc-zjnsf" in namespace "gc-9590"
    Mar 27 21:28:30.756: INFO: Deleting pod "simpletest.rc-zkc8s" in namespace "gc-9590"
    Mar 27 21:28:30.792: INFO: Deleting pod "simpletest.rc-zttsd" in namespace "gc-9590"
    Mar 27 21:28:30.828: INFO: Deleting pod "simpletest.rc-zvffg" in namespace "gc-9590"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:28:30.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9590" for this suite. 03/27/23 21:28:30.899
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:28:30.923
Mar 27 21:28:30.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 21:28:30.928
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:30.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:30.999
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Mar 27 21:28:31.108: INFO: Create a RollingUpdate DaemonSet
Mar 27 21:28:31.122: INFO: Check that daemon pods launch on every node of the cluster
Mar 27 21:28:31.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:28:31.155: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:28:32.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:28:32.194: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:28:33.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:28:33.191: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:28:34.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:28:34.187: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:28:35.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 21:28:35.244: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:28:36.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:28:36.184: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 27 21:28:36.184: INFO: Update the DaemonSet to trigger a rollout
Mar 27 21:28:36.209: INFO: Updating DaemonSet daemon-set
Mar 27 21:28:40.269: INFO: Roll back the DaemonSet before rollout is complete
Mar 27 21:28:40.294: INFO: Updating DaemonSet daemon-set
Mar 27 21:28:40.294: INFO: Make sure DaemonSet rollback is complete
Mar 27 21:28:40.306: INFO: Wrong image for pod: daemon-set-z9tlq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Mar 27 21:28:40.306: INFO: Pod daemon-set-z9tlq is not available
Mar 27 21:28:44.356: INFO: Pod daemon-set-7l25g is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:28:44.399
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4609, will wait for the garbage collector to delete the pods 03/27/23 21:28:44.399
Mar 27 21:28:44.509: INFO: Deleting DaemonSet.extensions daemon-set took: 47.732555ms
Mar 27 21:28:44.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.716433ms
Mar 27 21:28:47.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:28:47.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 21:28:47.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31883"},"items":null}

Mar 27 21:28:47.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31884"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:28:47.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4609" for this suite. 03/27/23 21:28:47.24
------------------------------
• [SLOW TEST] [16.382 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:28:30.923
    Mar 27 21:28:30.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 21:28:30.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:30.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:30.999
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Mar 27 21:28:31.108: INFO: Create a RollingUpdate DaemonSet
    Mar 27 21:28:31.122: INFO: Check that daemon pods launch on every node of the cluster
    Mar 27 21:28:31.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:28:31.155: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:28:32.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:28:32.194: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:28:33.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:28:33.191: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:28:34.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:28:34.187: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:28:35.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 21:28:35.244: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:28:36.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:28:36.184: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 27 21:28:36.184: INFO: Update the DaemonSet to trigger a rollout
    Mar 27 21:28:36.209: INFO: Updating DaemonSet daemon-set
    Mar 27 21:28:40.269: INFO: Roll back the DaemonSet before rollout is complete
    Mar 27 21:28:40.294: INFO: Updating DaemonSet daemon-set
    Mar 27 21:28:40.294: INFO: Make sure DaemonSet rollback is complete
    Mar 27 21:28:40.306: INFO: Wrong image for pod: daemon-set-z9tlq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Mar 27 21:28:40.306: INFO: Pod daemon-set-z9tlq is not available
    Mar 27 21:28:44.356: INFO: Pod daemon-set-7l25g is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:28:44.399
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4609, will wait for the garbage collector to delete the pods 03/27/23 21:28:44.399
    Mar 27 21:28:44.509: INFO: Deleting DaemonSet.extensions daemon-set took: 47.732555ms
    Mar 27 21:28:44.609: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.716433ms
    Mar 27 21:28:47.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:28:47.022: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 21:28:47.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31883"},"items":null}

    Mar 27 21:28:47.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31884"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:28:47.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4609" for this suite. 03/27/23 21:28:47.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:28:47.312
Mar 27 21:28:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:28:47.314
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:47.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:47.445
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 03/27/23 21:28:47.46
Mar 27 21:28:47.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 27 21:28:47.593: INFO: stderr: ""
Mar 27 21:28:47.593: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 03/27/23 21:28:47.594
Mar 27 21:28:47.594: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 27 21:28:47.594: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1771" to be "running and ready, or succeeded"
Mar 27 21:28:47.613: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.541067ms
Mar 27 21:28:47.614: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.176.99.177' to be 'Running' but was 'Pending'
Mar 27 21:28:49.631: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.037150978s
Mar 27 21:28:49.631: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 27 21:28:49.631: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/27/23 21:28:49.631
Mar 27 21:28:49.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator'
Mar 27 21:28:49.820: INFO: stderr: ""
Mar 27 21:28:49.820: INFO: stdout: "I0327 21:28:48.742156       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/t4jk 348\nI0327 21:28:48.942336       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/vrz 335\nI0327 21:28:49.142781       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/vg7 497\nI0327 21:28:49.343217       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rrvb 598\nI0327 21:28:49.542607       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9g6z 201\nI0327 21:28:49.742986       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/j6m 539\n"
STEP: limiting log lines 03/27/23 21:28:49.82
Mar 27 21:28:49.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --tail=1'
Mar 27 21:28:49.978: INFO: stderr: ""
Mar 27 21:28:49.978: INFO: stdout: "I0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\n"
Mar 27 21:28:49.979: INFO: got output "I0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\n"
STEP: limiting log bytes 03/27/23 21:28:49.979
Mar 27 21:28:49.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --limit-bytes=1'
Mar 27 21:28:50.155: INFO: stderr: ""
Mar 27 21:28:50.155: INFO: stdout: "I"
Mar 27 21:28:50.155: INFO: got output "I"
STEP: exposing timestamps 03/27/23 21:28:50.155
Mar 27 21:28:50.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 27 21:28:50.308: INFO: stderr: ""
Mar 27 21:28:50.308: INFO: stdout: "2023-03-27T21:28:50.142844063Z I0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\n"
Mar 27 21:28:50.308: INFO: got output "2023-03-27T21:28:50.142844063Z I0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\n"
STEP: restricting to a time range 03/27/23 21:28:50.308
Mar 27 21:28:52.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --since=1s'
Mar 27 21:28:52.945: INFO: stderr: ""
Mar 27 21:28:52.945: INFO: stdout: "I0327 21:28:51.942875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/wd6 507\nI0327 21:28:52.142208       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8hhx 417\nI0327 21:28:52.342611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/6t8c 536\nI0327 21:28:52.542963       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfg 356\nI0327 21:28:52.742547       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xr7l 409\n"
Mar 27 21:28:52.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --since=24h'
Mar 27 21:28:53.090: INFO: stderr: ""
Mar 27 21:28:53.090: INFO: stdout: "I0327 21:28:48.742156       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/t4jk 348\nI0327 21:28:48.942336       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/vrz 335\nI0327 21:28:49.142781       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/vg7 497\nI0327 21:28:49.343217       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rrvb 598\nI0327 21:28:49.542607       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9g6z 201\nI0327 21:28:49.742986       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/j6m 539\nI0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\nI0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\nI0327 21:28:50.343138       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cbf 323\nI0327 21:28:50.542297       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/jtnv 281\nI0327 21:28:50.742678       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/cbx4 458\nI0327 21:28:50.943035       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/v2n6 497\nI0327 21:28:51.142342       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/wz5g 367\nI0327 21:28:51.342759       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/86t 525\nI0327 21:28:51.543179       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/mpt6 359\nI0327 21:28:51.742550       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xrz 210\nI0327 21:28:51.942875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/wd6 507\nI0327 21:28:52.142208       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8hhx 417\nI0327 21:28:52.342611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/6t8c 536\nI0327 21:28:52.542963       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfg 356\nI0327 21:28:52.742547       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xr7l 409\nI0327 21:28:52.942913       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/dkk 338\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Mar 27 21:28:53.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 delete pod logs-generator'
Mar 27 21:28:54.636: INFO: stderr: ""
Mar 27 21:28:54.636: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:28:54.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1771" for this suite. 03/27/23 21:28:54.659
------------------------------
• [SLOW TEST] [7.374 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:28:47.312
    Mar 27 21:28:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:28:47.314
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:47.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:47.445
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 03/27/23 21:28:47.46
    Mar 27 21:28:47.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 27 21:28:47.593: INFO: stderr: ""
    Mar 27 21:28:47.593: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 03/27/23 21:28:47.594
    Mar 27 21:28:47.594: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 27 21:28:47.594: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1771" to be "running and ready, or succeeded"
    Mar 27 21:28:47.613: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.541067ms
    Mar 27 21:28:47.614: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.176.99.177' to be 'Running' but was 'Pending'
    Mar 27 21:28:49.631: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.037150978s
    Mar 27 21:28:49.631: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 27 21:28:49.631: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/27/23 21:28:49.631
    Mar 27 21:28:49.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator'
    Mar 27 21:28:49.820: INFO: stderr: ""
    Mar 27 21:28:49.820: INFO: stdout: "I0327 21:28:48.742156       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/t4jk 348\nI0327 21:28:48.942336       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/vrz 335\nI0327 21:28:49.142781       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/vg7 497\nI0327 21:28:49.343217       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rrvb 598\nI0327 21:28:49.542607       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9g6z 201\nI0327 21:28:49.742986       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/j6m 539\n"
    STEP: limiting log lines 03/27/23 21:28:49.82
    Mar 27 21:28:49.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --tail=1'
    Mar 27 21:28:49.978: INFO: stderr: ""
    Mar 27 21:28:49.978: INFO: stdout: "I0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\n"
    Mar 27 21:28:49.979: INFO: got output "I0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\n"
    STEP: limiting log bytes 03/27/23 21:28:49.979
    Mar 27 21:28:49.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --limit-bytes=1'
    Mar 27 21:28:50.155: INFO: stderr: ""
    Mar 27 21:28:50.155: INFO: stdout: "I"
    Mar 27 21:28:50.155: INFO: got output "I"
    STEP: exposing timestamps 03/27/23 21:28:50.155
    Mar 27 21:28:50.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 27 21:28:50.308: INFO: stderr: ""
    Mar 27 21:28:50.308: INFO: stdout: "2023-03-27T21:28:50.142844063Z I0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\n"
    Mar 27 21:28:50.308: INFO: got output "2023-03-27T21:28:50.142844063Z I0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\n"
    STEP: restricting to a time range 03/27/23 21:28:50.308
    Mar 27 21:28:52.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --since=1s'
    Mar 27 21:28:52.945: INFO: stderr: ""
    Mar 27 21:28:52.945: INFO: stdout: "I0327 21:28:51.942875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/wd6 507\nI0327 21:28:52.142208       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8hhx 417\nI0327 21:28:52.342611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/6t8c 536\nI0327 21:28:52.542963       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfg 356\nI0327 21:28:52.742547       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xr7l 409\n"
    Mar 27 21:28:52.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 logs logs-generator logs-generator --since=24h'
    Mar 27 21:28:53.090: INFO: stderr: ""
    Mar 27 21:28:53.090: INFO: stdout: "I0327 21:28:48.742156       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/t4jk 348\nI0327 21:28:48.942336       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/vrz 335\nI0327 21:28:49.142781       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/vg7 497\nI0327 21:28:49.343217       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rrvb 598\nI0327 21:28:49.542607       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9g6z 201\nI0327 21:28:49.742986       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/j6m 539\nI0327 21:28:49.942327       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/9z2 317\nI0327 21:28:50.142685       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/cj8s 565\nI0327 21:28:50.343138       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/cbf 323\nI0327 21:28:50.542297       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/jtnv 281\nI0327 21:28:50.742678       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/cbx4 458\nI0327 21:28:50.943035       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/v2n6 497\nI0327 21:28:51.142342       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/wz5g 367\nI0327 21:28:51.342759       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/86t 525\nI0327 21:28:51.543179       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/mpt6 359\nI0327 21:28:51.742550       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xrz 210\nI0327 21:28:51.942875       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/wd6 507\nI0327 21:28:52.142208       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8hhx 417\nI0327 21:28:52.342611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/6t8c 536\nI0327 21:28:52.542963       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfg 356\nI0327 21:28:52.742547       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xr7l 409\nI0327 21:28:52.942913       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/dkk 338\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Mar 27 21:28:53.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1771 delete pod logs-generator'
    Mar 27 21:28:54.636: INFO: stderr: ""
    Mar 27 21:28:54.636: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:28:54.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1771" for this suite. 03/27/23 21:28:54.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:28:54.689
Mar 27 21:28:54.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context 03/27/23 21:28:54.69
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:54.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:54.752
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 21:28:54.765
Mar 27 21:28:54.797: INFO: Waiting up to 5m0s for pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c" in namespace "security-context-4151" to be "Succeeded or Failed"
Mar 27 21:28:54.834: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Pending", Reason="", readiness=false. Elapsed: 37.344118ms
Mar 27 21:28:56.852: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055313953s
Mar 27 21:28:58.858: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061437425s
STEP: Saw pod success 03/27/23 21:28:58.858
Mar 27 21:28:58.859: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c" satisfied condition "Succeeded or Failed"
Mar 27 21:28:58.876: INFO: Trying to get logs from node 10.176.99.177 pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c container test-container: <nil>
STEP: delete the pod 03/27/23 21:28:58.909
Mar 27 21:28:58.952: INFO: Waiting for pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c to disappear
Mar 27 21:28:58.968: INFO: Pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 21:28:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4151" for this suite. 03/27/23 21:28:58.991
------------------------------
• [4.326 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:28:54.689
    Mar 27 21:28:54.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context 03/27/23 21:28:54.69
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:54.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:54.752
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/27/23 21:28:54.765
    Mar 27 21:28:54.797: INFO: Waiting up to 5m0s for pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c" in namespace "security-context-4151" to be "Succeeded or Failed"
    Mar 27 21:28:54.834: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Pending", Reason="", readiness=false. Elapsed: 37.344118ms
    Mar 27 21:28:56.852: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055313953s
    Mar 27 21:28:58.858: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061437425s
    STEP: Saw pod success 03/27/23 21:28:58.858
    Mar 27 21:28:58.859: INFO: Pod "security-context-67be01e4-a003-46ad-9acd-a91267c9c21c" satisfied condition "Succeeded or Failed"
    Mar 27 21:28:58.876: INFO: Trying to get logs from node 10.176.99.177 pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c container test-container: <nil>
    STEP: delete the pod 03/27/23 21:28:58.909
    Mar 27 21:28:58.952: INFO: Waiting for pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c to disappear
    Mar 27 21:28:58.968: INFO: Pod security-context-67be01e4-a003-46ad-9acd-a91267c9c21c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:28:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4151" for this suite. 03/27/23 21:28:58.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:28:59.017
Mar 27 21:28:59.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename conformance-tests 03/27/23 21:28:59.018
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:59.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:59.082
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/27/23 21:28:59.097
Mar 27 21:28:59.097: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Mar 27 21:28:59.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8059" for this suite. 03/27/23 21:28:59.15
------------------------------
• [0.156 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:28:59.017
    Mar 27 21:28:59.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename conformance-tests 03/27/23 21:28:59.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:59.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:59.082
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/27/23 21:28:59.097
    Mar 27 21:28:59.097: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:28:59.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8059" for this suite. 03/27/23 21:28:59.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:28:59.176
Mar 27 21:28:59.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-runtime 03/27/23 21:28:59.177
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:59.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:59.242
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 03/27/23 21:28:59.255
STEP: wait for the container to reach Succeeded 03/27/23 21:28:59.292
STEP: get the container status 03/27/23 21:29:03.381
STEP: the container should be terminated 03/27/23 21:29:03.398
STEP: the termination message should be set 03/27/23 21:29:03.398
Mar 27 21:29:03.398: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/27/23 21:29:03.398
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:03.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7402" for this suite. 03/27/23 21:29:03.483
------------------------------
• [4.330 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:28:59.176
    Mar 27 21:28:59.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-runtime 03/27/23 21:28:59.177
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:28:59.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:28:59.242
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 03/27/23 21:28:59.255
    STEP: wait for the container to reach Succeeded 03/27/23 21:28:59.292
    STEP: get the container status 03/27/23 21:29:03.381
    STEP: the container should be terminated 03/27/23 21:29:03.398
    STEP: the termination message should be set 03/27/23 21:29:03.398
    Mar 27 21:29:03.398: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/27/23 21:29:03.398
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:03.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7402" for this suite. 03/27/23 21:29:03.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:03.51
Mar 27 21:29:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:29:03.512
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:03.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:03.574
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 03/27/23 21:29:03.584
Mar 27 21:29:03.629: INFO: Waiting up to 5m0s for pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542" in namespace "projected-3667" to be "running and ready"
Mar 27 21:29:03.646: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542": Phase="Pending", Reason="", readiness=false. Elapsed: 17.068805ms
Mar 27 21:29:03.646: INFO: The phase of Pod labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:29:05.664: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542": Phase="Running", Reason="", readiness=true. Elapsed: 2.035095971s
Mar 27 21:29:05.664: INFO: The phase of Pod labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542 is Running (Ready = true)
Mar 27 21:29:05.664: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542" satisfied condition "running and ready"
Mar 27 21:29:06.260: INFO: Successfully updated pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3667" for this suite. 03/27/23 21:29:10.416
------------------------------
• [SLOW TEST] [6.930 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:03.51
    Mar 27 21:29:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:29:03.512
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:03.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:03.574
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 03/27/23 21:29:03.584
    Mar 27 21:29:03.629: INFO: Waiting up to 5m0s for pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542" in namespace "projected-3667" to be "running and ready"
    Mar 27 21:29:03.646: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542": Phase="Pending", Reason="", readiness=false. Elapsed: 17.068805ms
    Mar 27 21:29:03.646: INFO: The phase of Pod labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:29:05.664: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542": Phase="Running", Reason="", readiness=true. Elapsed: 2.035095971s
    Mar 27 21:29:05.664: INFO: The phase of Pod labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542 is Running (Ready = true)
    Mar 27 21:29:05.664: INFO: Pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542" satisfied condition "running and ready"
    Mar 27 21:29:06.260: INFO: Successfully updated pod "labelsupdate8b5c2d53-f513-4b77-bdfe-62dc78563542"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3667" for this suite. 03/27/23 21:29:10.416
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:10.443
Mar 27 21:29:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 21:29:10.445
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:10.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:10.511
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/27/23 21:29:10.524
STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 21:29:10.551
STEP: delete the deployment 03/27/23 21:29:10.702
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/27/23 21:29:10.748
STEP: Gathering metrics 03/27/23 21:29:11.349
W0327 21:29:11.390600      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 21:29:11.390: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:11.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8000" for this suite. 03/27/23 21:29:11.412
------------------------------
• [1.002 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:10.443
    Mar 27 21:29:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 21:29:10.445
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:10.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:10.511
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/27/23 21:29:10.524
    STEP: Wait for the Deployment to create new ReplicaSet 03/27/23 21:29:10.551
    STEP: delete the deployment 03/27/23 21:29:10.702
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/27/23 21:29:10.748
    STEP: Gathering metrics 03/27/23 21:29:11.349
    W0327 21:29:11.390600      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 21:29:11.390: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:11.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8000" for this suite. 03/27/23 21:29:11.412
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:11.448
Mar 27 21:29:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename endpointslice 03/27/23 21:29:11.456
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:11.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:11.529
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:11.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7352" for this suite. 03/27/23 21:29:11.75
------------------------------
• [0.332 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:11.448
    Mar 27 21:29:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename endpointslice 03/27/23 21:29:11.456
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:11.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:11.529
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:11.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7352" for this suite. 03/27/23 21:29:11.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:11.787
Mar 27 21:29:11.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:29:11.789
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:11.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:11.851
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:29:11.866
Mar 27 21:29:11.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5" in namespace "downward-api-3536" to be "Succeeded or Failed"
Mar 27 21:29:11.918: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574197ms
Mar 27 21:29:13.936: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03600103s
Mar 27 21:29:15.937: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03640366s
STEP: Saw pod success 03/27/23 21:29:15.937
Mar 27 21:29:15.937: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5" satisfied condition "Succeeded or Failed"
Mar 27 21:29:15.958: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 container client-container: <nil>
STEP: delete the pod 03/27/23 21:29:15.991
Mar 27 21:29:16.032: INFO: Waiting for pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 to disappear
Mar 27 21:29:16.048: INFO: Pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3536" for this suite. 03/27/23 21:29:16.069
------------------------------
• [4.308 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:11.787
    Mar 27 21:29:11.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:29:11.789
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:11.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:11.851
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:29:11.866
    Mar 27 21:29:11.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5" in namespace "downward-api-3536" to be "Succeeded or Failed"
    Mar 27 21:29:11.918: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.574197ms
    Mar 27 21:29:13.936: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03600103s
    Mar 27 21:29:15.937: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03640366s
    STEP: Saw pod success 03/27/23 21:29:15.937
    Mar 27 21:29:15.937: INFO: Pod "downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5" satisfied condition "Succeeded or Failed"
    Mar 27 21:29:15.958: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:29:15.991
    Mar 27 21:29:16.032: INFO: Waiting for pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 to disappear
    Mar 27 21:29:16.048: INFO: Pod downwardapi-volume-ed2773ae-a918-4530-9cbe-55e9cb970cc5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3536" for this suite. 03/27/23 21:29:16.069
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:16.095
Mar 27 21:29:16.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:29:16.098
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:16.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:16.162
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-2259 03/27/23 21:29:16.175
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[] 03/27/23 21:29:16.221
Mar 27 21:29:16.261: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2259 03/27/23 21:29:16.261
Mar 27 21:29:16.292: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2259" to be "running and ready"
Mar 27 21:29:16.309: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.217648ms
Mar 27 21:29:16.309: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:29:18.325: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033085836s
Mar 27 21:29:18.325: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 21:29:18.325: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod1:[80]] 03/27/23 21:29:18.342
Mar 27 21:29:18.396: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/27/23 21:29:18.397
Mar 27 21:29:18.397: INFO: Creating new exec pod
Mar 27 21:29:18.418: INFO: Waiting up to 5m0s for pod "execpodkx7sq" in namespace "services-2259" to be "running"
Mar 27 21:29:18.434: INFO: Pod "execpodkx7sq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.744168ms
Mar 27 21:29:20.458: INFO: Pod "execpodkx7sq": Phase="Running", Reason="", readiness=true. Elapsed: 2.039620782s
Mar 27 21:29:20.458: INFO: Pod "execpodkx7sq" satisfied condition "running"
Mar 27 21:29:21.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 27 21:29:22.749: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:22.750: INFO: stdout: ""
Mar 27 21:29:22.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
Mar 27 21:29:23.050: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:23.050: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-2259 03/27/23 21:29:23.05
Mar 27 21:29:23.072: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2259" to be "running and ready"
Mar 27 21:29:23.091: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.981556ms
Mar 27 21:29:23.091: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:29:25.123: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.050579457s
Mar 27 21:29:25.123: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 21:29:25.123: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod1:[80] pod2:[80]] 03/27/23 21:29:25.14
Mar 27 21:29:25.220: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/27/23 21:29:25.22
Mar 27 21:29:26.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 27 21:29:26.464: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:26.464: INFO: stdout: ""
Mar 27 21:29:26.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
Mar 27 21:29:26.766: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:26.766: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2259 03/27/23 21:29:26.766
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod2:[80]] 03/27/23 21:29:26.809
Mar 27 21:29:26.865: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/27/23 21:29:26.865
Mar 27 21:29:27.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Mar 27 21:29:28.159: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:28.159: INFO: stdout: ""
Mar 27 21:29:28.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
Mar 27 21:29:28.442: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
Mar 27 21:29:28.442: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-2259 03/27/23 21:29:28.442
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[] 03/27/23 21:29:28.498
Mar 27 21:29:28.534: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:28.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2259" for this suite. 03/27/23 21:29:28.626
------------------------------
• [SLOW TEST] [12.561 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:16.095
    Mar 27 21:29:16.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:29:16.098
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:16.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:16.162
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-2259 03/27/23 21:29:16.175
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[] 03/27/23 21:29:16.221
    Mar 27 21:29:16.261: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2259 03/27/23 21:29:16.261
    Mar 27 21:29:16.292: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2259" to be "running and ready"
    Mar 27 21:29:16.309: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.217648ms
    Mar 27 21:29:16.309: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:29:18.325: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033085836s
    Mar 27 21:29:18.325: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 21:29:18.325: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod1:[80]] 03/27/23 21:29:18.342
    Mar 27 21:29:18.396: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/27/23 21:29:18.397
    Mar 27 21:29:18.397: INFO: Creating new exec pod
    Mar 27 21:29:18.418: INFO: Waiting up to 5m0s for pod "execpodkx7sq" in namespace "services-2259" to be "running"
    Mar 27 21:29:18.434: INFO: Pod "execpodkx7sq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.744168ms
    Mar 27 21:29:20.458: INFO: Pod "execpodkx7sq": Phase="Running", Reason="", readiness=true. Elapsed: 2.039620782s
    Mar 27 21:29:20.458: INFO: Pod "execpodkx7sq" satisfied condition "running"
    Mar 27 21:29:21.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 27 21:29:22.749: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:22.750: INFO: stdout: ""
    Mar 27 21:29:22.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
    Mar 27 21:29:23.050: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:23.050: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-2259 03/27/23 21:29:23.05
    Mar 27 21:29:23.072: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2259" to be "running and ready"
    Mar 27 21:29:23.091: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.981556ms
    Mar 27 21:29:23.091: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:29:25.123: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.050579457s
    Mar 27 21:29:25.123: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 21:29:25.123: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod1:[80] pod2:[80]] 03/27/23 21:29:25.14
    Mar 27 21:29:25.220: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/27/23 21:29:25.22
    Mar 27 21:29:26.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 27 21:29:26.464: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:26.464: INFO: stdout: ""
    Mar 27 21:29:26.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
    Mar 27 21:29:26.766: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:26.766: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2259 03/27/23 21:29:26.766
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[pod2:[80]] 03/27/23 21:29:26.809
    Mar 27 21:29:26.865: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/27/23 21:29:26.865
    Mar 27 21:29:27.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Mar 27 21:29:28.159: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:28.159: INFO: stdout: ""
    Mar 27 21:29:28.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-2259 exec execpodkx7sq -- /bin/sh -x -c nc -v -z -w 2 172.21.141.129 80'
    Mar 27 21:29:28.442: INFO: stderr: "+ nc -v -z -w 2 172.21.141.129 80\nConnection to 172.21.141.129 80 port [tcp/http] succeeded!\n"
    Mar 27 21:29:28.442: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-2259 03/27/23 21:29:28.442
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2259 to expose endpoints map[] 03/27/23 21:29:28.498
    Mar 27 21:29:28.534: INFO: successfully validated that service endpoint-test2 in namespace services-2259 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:28.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2259" for this suite. 03/27/23 21:29:28.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:28.656
Mar 27 21:29:28.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:29:28.657
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:28.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:28.758
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 03/27/23 21:29:28.77
STEP: Creating a ResourceQuota 03/27/23 21:29:33.784
STEP: Ensuring resource quota status is calculated 03/27/23 21:29:33.8
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:35.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9594" for this suite. 03/27/23 21:29:35.838
------------------------------
• [SLOW TEST] [7.209 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:28.656
    Mar 27 21:29:28.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:29:28.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:28.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:28.758
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 03/27/23 21:29:28.77
    STEP: Creating a ResourceQuota 03/27/23 21:29:33.784
    STEP: Ensuring resource quota status is calculated 03/27/23 21:29:33.8
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:35.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9594" for this suite. 03/27/23 21:29:35.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:35.873
Mar 27 21:29:35.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:29:35.875
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:35.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:35.936
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 03/27/23 21:29:35.952
STEP: Creating a ResourceQuota 03/27/23 21:29:40.966
STEP: Ensuring resource quota status is calculated 03/27/23 21:29:40.981
STEP: Creating a ReplicaSet 03/27/23 21:29:42.996
STEP: Ensuring resource quota status captures replicaset creation 03/27/23 21:29:43.03
STEP: Deleting a ReplicaSet 03/27/23 21:29:45.045
STEP: Ensuring resource quota status released usage 03/27/23 21:29:45.074
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:29:47.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6090" for this suite. 03/27/23 21:29:47.11
------------------------------
• [SLOW TEST] [11.264 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:35.873
    Mar 27 21:29:35.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:29:35.875
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:35.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:35.936
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 03/27/23 21:29:35.952
    STEP: Creating a ResourceQuota 03/27/23 21:29:40.966
    STEP: Ensuring resource quota status is calculated 03/27/23 21:29:40.981
    STEP: Creating a ReplicaSet 03/27/23 21:29:42.996
    STEP: Ensuring resource quota status captures replicaset creation 03/27/23 21:29:43.03
    STEP: Deleting a ReplicaSet 03/27/23 21:29:45.045
    STEP: Ensuring resource quota status released usage 03/27/23 21:29:45.074
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:29:47.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6090" for this suite. 03/27/23 21:29:47.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:29:47.141
Mar 27 21:29:47.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-watch 03/27/23 21:29:47.142
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:47.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:47.213
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 27 21:29:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Creating first CR  03/27/23 21:29:49.9
Mar 27 21:29:49.920: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:29:49Z]] name:name1 resourceVersion:32598 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/27/23 21:29:59.921
Mar 27 21:29:59.941: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:29:59Z]] name:name2 resourceVersion:32616 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/27/23 21:30:09.942
Mar 27 21:30:09.967: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:09Z]] name:name1 resourceVersion:32629 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/27/23 21:30:19.968
Mar 27 21:30:19.992: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:19Z]] name:name2 resourceVersion:32646 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/27/23 21:30:29.993
Mar 27 21:30:30.025: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:09Z]] name:name1 resourceVersion:32657 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/27/23 21:30:40.025
Mar 27 21:30:40.056: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:19Z]] name:name2 resourceVersion:32671 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:30:50.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-4351" for this suite. 03/27/23 21:30:50.617
------------------------------
• [SLOW TEST] [63.504 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:29:47.141
    Mar 27 21:29:47.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-watch 03/27/23 21:29:47.142
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:29:47.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:29:47.213
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 27 21:29:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Creating first CR  03/27/23 21:29:49.9
    Mar 27 21:29:49.920: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:29:49Z]] name:name1 resourceVersion:32598 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/27/23 21:29:59.921
    Mar 27 21:29:59.941: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:29:59Z]] name:name2 resourceVersion:32616 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/27/23 21:30:09.942
    Mar 27 21:30:09.967: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:09Z]] name:name1 resourceVersion:32629 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/27/23 21:30:19.968
    Mar 27 21:30:19.992: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:19Z]] name:name2 resourceVersion:32646 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/27/23 21:30:29.993
    Mar 27 21:30:30.025: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:09Z]] name:name1 resourceVersion:32657 uid:f2bfaca0-0a4f-467d-93ed-d56f61c24945] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/27/23 21:30:40.025
    Mar 27 21:30:40.056: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-27T21:29:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-27T21:30:19Z]] name:name2 resourceVersion:32671 uid:1d0c585b-011c-43d1-bbae-eea56a54430d] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:30:50.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-4351" for this suite. 03/27/23 21:30:50.617
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:30:50.646
Mar 27 21:30:50.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:30:50.647
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:30:50.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:30:50.707
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-b2206a81-b2e5-4288-afd9-87f1acdf1d2e 03/27/23 21:30:50.723
STEP: Creating a pod to test consume secrets 03/27/23 21:30:50.741
Mar 27 21:30:50.773: INFO: Waiting up to 5m0s for pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a" in namespace "secrets-108" to be "Succeeded or Failed"
Mar 27 21:30:50.790: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.347052ms
Mar 27 21:30:52.806: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033132703s
Mar 27 21:30:54.808: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Running", Reason="", readiness=false. Elapsed: 4.034849486s
Mar 27 21:30:56.808: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034948969s
STEP: Saw pod success 03/27/23 21:30:56.808
Mar 27 21:30:56.809: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a" satisfied condition "Succeeded or Failed"
Mar 27 21:30:56.826: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a container secret-env-test: <nil>
STEP: delete the pod 03/27/23 21:30:56.918
Mar 27 21:30:56.971: INFO: Waiting for pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a to disappear
Mar 27 21:30:56.990: INFO: Pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:30:56.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-108" for this suite. 03/27/23 21:30:57.026
------------------------------
• [SLOW TEST] [6.406 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:30:50.646
    Mar 27 21:30:50.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:30:50.647
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:30:50.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:30:50.707
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-b2206a81-b2e5-4288-afd9-87f1acdf1d2e 03/27/23 21:30:50.723
    STEP: Creating a pod to test consume secrets 03/27/23 21:30:50.741
    Mar 27 21:30:50.773: INFO: Waiting up to 5m0s for pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a" in namespace "secrets-108" to be "Succeeded or Failed"
    Mar 27 21:30:50.790: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.347052ms
    Mar 27 21:30:52.806: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.033132703s
    Mar 27 21:30:54.808: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Running", Reason="", readiness=false. Elapsed: 4.034849486s
    Mar 27 21:30:56.808: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034948969s
    STEP: Saw pod success 03/27/23 21:30:56.808
    Mar 27 21:30:56.809: INFO: Pod "pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a" satisfied condition "Succeeded or Failed"
    Mar 27 21:30:56.826: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a container secret-env-test: <nil>
    STEP: delete the pod 03/27/23 21:30:56.918
    Mar 27 21:30:56.971: INFO: Waiting for pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a to disappear
    Mar 27 21:30:56.990: INFO: Pod pod-secrets-fc6bc607-044c-4446-bd1a-0b61adc56b2a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:30:56.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-108" for this suite. 03/27/23 21:30:57.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:30:57.057
Mar 27 21:30:57.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:30:57.058
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:30:57.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:30:57.126
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 27 21:30:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:04.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2433" for this suite. 03/27/23 21:31:04.063
------------------------------
• [SLOW TEST] [7.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:30:57.057
    Mar 27 21:30:57.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:30:57.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:30:57.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:30:57.126
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 27 21:30:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:04.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2433" for this suite. 03/27/23 21:31:04.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:04.092
Mar 27 21:31:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:31:04.094
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:04.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:04.158
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 03/27/23 21:31:04.175
Mar 27 21:31:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2829 cluster-info'
Mar 27 21:31:04.292: INFO: stderr: ""
Mar 27 21:31:04.292: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:04.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2829" for this suite. 03/27/23 21:31:04.317
------------------------------
• [0.252 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:04.092
    Mar 27 21:31:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:31:04.094
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:04.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:04.158
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 03/27/23 21:31:04.175
    Mar 27 21:31:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2829 cluster-info'
    Mar 27 21:31:04.292: INFO: stderr: ""
    Mar 27 21:31:04.292: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:04.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2829" for this suite. 03/27/23 21:31:04.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:04.347
Mar 27 21:31:04.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:31:04.348
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:04.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:04.421
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4070/configmap-test-f90d6275-05c6-4954-af84-a3ead87f2825 03/27/23 21:31:04.436
STEP: Creating a pod to test consume configMaps 03/27/23 21:31:04.455
Mar 27 21:31:04.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b" in namespace "configmap-4070" to be "Succeeded or Failed"
Mar 27 21:31:04.508: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.262868ms
Mar 27 21:31:06.526: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037124708s
Mar 27 21:31:08.525: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036883423s
STEP: Saw pod success 03/27/23 21:31:08.525
Mar 27 21:31:08.526: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b" satisfied condition "Succeeded or Failed"
Mar 27 21:31:08.544: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b container env-test: <nil>
STEP: delete the pod 03/27/23 21:31:08.589
Mar 27 21:31:08.638: INFO: Waiting for pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b to disappear
Mar 27 21:31:08.654: INFO: Pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:08.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4070" for this suite. 03/27/23 21:31:08.676
------------------------------
• [4.354 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:04.347
    Mar 27 21:31:04.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:31:04.348
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:04.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:04.421
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4070/configmap-test-f90d6275-05c6-4954-af84-a3ead87f2825 03/27/23 21:31:04.436
    STEP: Creating a pod to test consume configMaps 03/27/23 21:31:04.455
    Mar 27 21:31:04.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b" in namespace "configmap-4070" to be "Succeeded or Failed"
    Mar 27 21:31:04.508: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.262868ms
    Mar 27 21:31:06.526: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037124708s
    Mar 27 21:31:08.525: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036883423s
    STEP: Saw pod success 03/27/23 21:31:08.525
    Mar 27 21:31:08.526: INFO: Pod "pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b" satisfied condition "Succeeded or Failed"
    Mar 27 21:31:08.544: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b container env-test: <nil>
    STEP: delete the pod 03/27/23 21:31:08.589
    Mar 27 21:31:08.638: INFO: Waiting for pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b to disappear
    Mar 27 21:31:08.654: INFO: Pod pod-configmaps-002a2716-0671-46fb-85ca-fb70f622040b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:08.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4070" for this suite. 03/27/23 21:31:08.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:08.703
Mar 27 21:31:08.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:31:08.705
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:08.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:08.769
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:31:08.831
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:31:09.462
STEP: Deploying the webhook pod 03/27/23 21:31:09.492
STEP: Wait for the deployment to be ready 03/27/23 21:31:09.534
Mar 27 21:31:09.574: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:31:11.63
STEP: Verifying the service has paired with the endpoint 03/27/23 21:31:11.673
Mar 27 21:31:12.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/27/23 21:31:12.7
STEP: create a pod that should be updated by the webhook 03/27/23 21:31:12.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4203" for this suite. 03/27/23 21:31:13.064
STEP: Destroying namespace "webhook-4203-markers" for this suite. 03/27/23 21:31:13.091
------------------------------
• [4.415 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:08.703
    Mar 27 21:31:08.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:31:08.705
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:08.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:08.769
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:31:08.831
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:31:09.462
    STEP: Deploying the webhook pod 03/27/23 21:31:09.492
    STEP: Wait for the deployment to be ready 03/27/23 21:31:09.534
    Mar 27 21:31:09.574: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:31:11.63
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:31:11.673
    Mar 27 21:31:12.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/27/23 21:31:12.7
    STEP: create a pod that should be updated by the webhook 03/27/23 21:31:12.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4203" for this suite. 03/27/23 21:31:13.064
    STEP: Destroying namespace "webhook-4203-markers" for this suite. 03/27/23 21:31:13.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:13.121
Mar 27 21:31:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 21:31:13.123
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:13.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:13.186
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Mar 27 21:31:13.200: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/27/23 21:31:14.244
STEP: Checking rc "condition-test" has the desired failure condition set 03/27/23 21:31:14.261
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/27/23 21:31:15.288
Mar 27 21:31:15.320: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/27/23 21:31:15.32
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5538" for this suite. 03/27/23 21:31:15.352
------------------------------
• [2.260 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:13.121
    Mar 27 21:31:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 21:31:13.123
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:13.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:13.186
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Mar 27 21:31:13.200: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/27/23 21:31:14.244
    STEP: Checking rc "condition-test" has the desired failure condition set 03/27/23 21:31:14.261
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/27/23 21:31:15.288
    Mar 27 21:31:15.320: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/27/23 21:31:15.32
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5538" for this suite. 03/27/23 21:31:15.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:15.382
Mar 27 21:31:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:31:15.383
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:15.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:15.447
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/27/23 21:31:15.461
STEP: getting /apis/node.k8s.io 03/27/23 21:31:15.476
STEP: getting /apis/node.k8s.io/v1 03/27/23 21:31:15.509
STEP: creating 03/27/23 21:31:15.516
STEP: watching 03/27/23 21:31:15.569
Mar 27 21:31:15.569: INFO: starting watch
STEP: getting 03/27/23 21:31:15.591
STEP: listing 03/27/23 21:31:15.605
STEP: patching 03/27/23 21:31:15.622
STEP: updating 03/27/23 21:31:15.638
Mar 27 21:31:15.654: INFO: waiting for watch events with expected annotations
STEP: deleting 03/27/23 21:31:15.654
STEP: deleting a collection 03/27/23 21:31:15.704
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1680" for this suite. 03/27/23 21:31:15.784
------------------------------
• [0.426 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:15.382
    Mar 27 21:31:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:31:15.383
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:15.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:15.447
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/27/23 21:31:15.461
    STEP: getting /apis/node.k8s.io 03/27/23 21:31:15.476
    STEP: getting /apis/node.k8s.io/v1 03/27/23 21:31:15.509
    STEP: creating 03/27/23 21:31:15.516
    STEP: watching 03/27/23 21:31:15.569
    Mar 27 21:31:15.569: INFO: starting watch
    STEP: getting 03/27/23 21:31:15.591
    STEP: listing 03/27/23 21:31:15.605
    STEP: patching 03/27/23 21:31:15.622
    STEP: updating 03/27/23 21:31:15.638
    Mar 27 21:31:15.654: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/27/23 21:31:15.654
    STEP: deleting a collection 03/27/23 21:31:15.704
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:15.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1680" for this suite. 03/27/23 21:31:15.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:15.81
Mar 27 21:31:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:31:15.811
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:15.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:15.875
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 03/27/23 21:31:15.89
STEP: Getting a ResourceQuota 03/27/23 21:31:15.906
STEP: Listing all ResourceQuotas with LabelSelector 03/27/23 21:31:15.919
STEP: Patching the ResourceQuota 03/27/23 21:31:15.932
STEP: Deleting a Collection of ResourceQuotas 03/27/23 21:31:15.954
STEP: Verifying the deleted ResourceQuota 03/27/23 21:31:15.981
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:15.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3758" for this suite. 03/27/23 21:31:16.019
------------------------------
• [0.235 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:15.81
    Mar 27 21:31:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:31:15.811
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:15.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:15.875
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 03/27/23 21:31:15.89
    STEP: Getting a ResourceQuota 03/27/23 21:31:15.906
    STEP: Listing all ResourceQuotas with LabelSelector 03/27/23 21:31:15.919
    STEP: Patching the ResourceQuota 03/27/23 21:31:15.932
    STEP: Deleting a Collection of ResourceQuotas 03/27/23 21:31:15.954
    STEP: Verifying the deleted ResourceQuota 03/27/23 21:31:15.981
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:15.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3758" for this suite. 03/27/23 21:31:16.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:16.054
Mar 27 21:31:16.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 21:31:16.055
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:16.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:16.128
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 03/27/23 21:31:16.228
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:31:16.245
Mar 27 21:31:16.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:31:16.283: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:31:17.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:31:17.321: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:31:18.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:31:18.413: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:31:19.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:31:19.323: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/27/23 21:31:19.336
Mar 27 21:31:19.421: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:31:19.421: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 21:31:20.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:31:20.468: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 21:31:21.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:31:21.494: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/27/23 21:31:21.494
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:31:21.572
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3028, will wait for the garbage collector to delete the pods 03/27/23 21:31:21.572
Mar 27 21:31:21.670: INFO: Deleting DaemonSet.extensions daemon-set took: 23.352112ms
Mar 27 21:31:21.771: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.36556ms
Mar 27 21:31:24.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:31:24.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 21:31:24.200: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33168"},"items":null}

Mar 27 21:31:24.217: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33168"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:24.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3028" for this suite. 03/27/23 21:31:24.313
------------------------------
• [SLOW TEST] [8.283 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:16.054
    Mar 27 21:31:16.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 21:31:16.055
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:16.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:16.128
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 03/27/23 21:31:16.228
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:31:16.245
    Mar 27 21:31:16.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:31:16.283: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:31:17.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:31:17.321: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:31:18.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:31:18.413: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:31:19.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:31:19.323: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/27/23 21:31:19.336
    Mar 27 21:31:19.421: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:31:19.421: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 21:31:20.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:31:20.468: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 21:31:21.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:31:21.494: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/27/23 21:31:21.494
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:31:21.572
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3028, will wait for the garbage collector to delete the pods 03/27/23 21:31:21.572
    Mar 27 21:31:21.670: INFO: Deleting DaemonSet.extensions daemon-set took: 23.352112ms
    Mar 27 21:31:21.771: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.36556ms
    Mar 27 21:31:24.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:31:24.187: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 21:31:24.200: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33168"},"items":null}

    Mar 27 21:31:24.217: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33168"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:24.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3028" for this suite. 03/27/23 21:31:24.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:24.349
Mar 27 21:31:24.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:31:24.35
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:24.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:24.414
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 03/27/23 21:31:24.427
Mar 27 21:31:24.463: INFO: Waiting up to 5m0s for pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9" in namespace "downward-api-1050" to be "Succeeded or Failed"
Mar 27 21:31:24.503: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.882186ms
Mar 27 21:31:26.523: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046523562s
Mar 27 21:31:28.522: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046217995s
STEP: Saw pod success 03/27/23 21:31:28.522
Mar 27 21:31:28.523: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9" satisfied condition "Succeeded or Failed"
Mar 27 21:31:28.541: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:31:28.58
Mar 27 21:31:28.639: INFO: Waiting for pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 to disappear
Mar 27 21:31:28.656: INFO: Pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:28.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1050" for this suite. 03/27/23 21:31:28.679
------------------------------
• [4.367 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:24.349
    Mar 27 21:31:24.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:31:24.35
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:24.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:24.414
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 03/27/23 21:31:24.427
    Mar 27 21:31:24.463: INFO: Waiting up to 5m0s for pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9" in namespace "downward-api-1050" to be "Succeeded or Failed"
    Mar 27 21:31:24.503: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.882186ms
    Mar 27 21:31:26.523: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046523562s
    Mar 27 21:31:28.522: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046217995s
    STEP: Saw pod success 03/27/23 21:31:28.522
    Mar 27 21:31:28.523: INFO: Pod "downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9" satisfied condition "Succeeded or Failed"
    Mar 27 21:31:28.541: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:31:28.58
    Mar 27 21:31:28.639: INFO: Waiting for pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 to disappear
    Mar 27 21:31:28.656: INFO: Pod downward-api-28f31160-e314-4b83-8f5f-653f2b7ba2f9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:28.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1050" for this suite. 03/27/23 21:31:28.679
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:28.716
Mar 27 21:31:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 21:31:28.717
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:28.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:28.777
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/27/23 21:31:28.791
STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:29.704
Mar 27 21:31:29.753: INFO: Pod name wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565: Found 0 pods out of 5
Mar 27 21:31:34.797: INFO: Pod name wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 21:31:34.798
Mar 27 21:31:34.798: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:34.816: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l": Phase="Running", Reason="", readiness=true. Elapsed: 18.262184ms
Mar 27 21:31:34.816: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l" satisfied condition "running"
Mar 27 21:31:34.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:34.840: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg": Phase="Running", Reason="", readiness=true. Elapsed: 23.616417ms
Mar 27 21:31:34.840: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg" satisfied condition "running"
Mar 27 21:31:34.840: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:34.857: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k": Phase="Running", Reason="", readiness=true. Elapsed: 16.941504ms
Mar 27 21:31:34.857: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k" satisfied condition "running"
Mar 27 21:31:34.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:34.874: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7": Phase="Running", Reason="", readiness=true. Elapsed: 17.023078ms
Mar 27 21:31:34.874: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7" satisfied condition "running"
Mar 27 21:31:34.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:34.895: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss": Phase="Running", Reason="", readiness=true. Elapsed: 20.488502ms
Mar 27 21:31:34.895: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:34.895
Mar 27 21:31:34.984: INFO: Deleting ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 took: 22.712255ms
Mar 27 21:31:35.084: INFO: Terminating ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 pods took: 100.389273ms
STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:37.705
Mar 27 21:31:37.759: INFO: Pod name wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc: Found 1 pods out of 5
Mar 27 21:31:42.796: INFO: Pod name wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 21:31:42.796
Mar 27 21:31:42.796: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:42.814: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5": Phase="Running", Reason="", readiness=true. Elapsed: 18.224204ms
Mar 27 21:31:42.814: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5" satisfied condition "running"
Mar 27 21:31:42.814: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:42.833: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6": Phase="Running", Reason="", readiness=true. Elapsed: 18.397332ms
Mar 27 21:31:42.833: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6" satisfied condition "running"
Mar 27 21:31:42.833: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:42.851: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8": Phase="Running", Reason="", readiness=true. Elapsed: 18.133648ms
Mar 27 21:31:42.851: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8" satisfied condition "running"
Mar 27 21:31:42.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:42.869: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw": Phase="Running", Reason="", readiness=true. Elapsed: 18.426602ms
Mar 27 21:31:42.869: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw" satisfied condition "running"
Mar 27 21:31:42.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:42.887: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch": Phase="Running", Reason="", readiness=true. Elapsed: 17.325863ms
Mar 27 21:31:42.887: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:42.887
Mar 27 21:31:42.977: INFO: Deleting ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc took: 22.938742ms
Mar 27 21:31:43.078: INFO: Terminating ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc pods took: 100.724653ms
STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:45.9
Mar 27 21:31:45.946: INFO: Pod name wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4: Found 1 pods out of 5
Mar 27 21:31:50.994: INFO: Pod name wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/27/23 21:31:50.994
Mar 27 21:31:50.995: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:51.013: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5": Phase="Running", Reason="", readiness=true. Elapsed: 18.612972ms
Mar 27 21:31:51.013: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5" satisfied condition "running"
Mar 27 21:31:51.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:51.032: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.370847ms
Mar 27 21:31:51.032: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj" satisfied condition "running"
Mar 27 21:31:51.032: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:51.057: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n": Phase="Running", Reason="", readiness=true. Elapsed: 24.718805ms
Mar 27 21:31:51.057: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n" satisfied condition "running"
Mar 27 21:31:51.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:51.075: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh": Phase="Running", Reason="", readiness=true. Elapsed: 17.768382ms
Mar 27 21:31:51.075: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh" satisfied condition "running"
Mar 27 21:31:51.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn" in namespace "emptydir-wrapper-794" to be "running"
Mar 27 21:31:51.093: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn": Phase="Running", Reason="", readiness=true. Elapsed: 18.234402ms
Mar 27 21:31:51.093: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:51.093
Mar 27 21:31:51.186: INFO: Deleting ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 took: 26.193952ms
Mar 27 21:31:51.287: INFO: Terminating ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 pods took: 101.089495ms
STEP: Cleaning up the configMaps 03/27/23 21:31:53.688
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:31:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-794" for this suite. 03/27/23 21:31:55.014
------------------------------
• [SLOW TEST] [26.323 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:28.716
    Mar 27 21:31:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 21:31:28.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:28.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:28.777
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/27/23 21:31:28.791
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:29.704
    Mar 27 21:31:29.753: INFO: Pod name wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565: Found 0 pods out of 5
    Mar 27 21:31:34.797: INFO: Pod name wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 21:31:34.798
    Mar 27 21:31:34.798: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:34.816: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l": Phase="Running", Reason="", readiness=true. Elapsed: 18.262184ms
    Mar 27 21:31:34.816: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-dxv8l" satisfied condition "running"
    Mar 27 21:31:34.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:34.840: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg": Phase="Running", Reason="", readiness=true. Elapsed: 23.616417ms
    Mar 27 21:31:34.840: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-ksbrg" satisfied condition "running"
    Mar 27 21:31:34.840: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:34.857: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k": Phase="Running", Reason="", readiness=true. Elapsed: 16.941504ms
    Mar 27 21:31:34.857: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-n877k" satisfied condition "running"
    Mar 27 21:31:34.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:34.874: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7": Phase="Running", Reason="", readiness=true. Elapsed: 17.023078ms
    Mar 27 21:31:34.874: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-rvhl7" satisfied condition "running"
    Mar 27 21:31:34.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:34.895: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss": Phase="Running", Reason="", readiness=true. Elapsed: 20.488502ms
    Mar 27 21:31:34.895: INFO: Pod "wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565-t7nss" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:34.895
    Mar 27 21:31:34.984: INFO: Deleting ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 took: 22.712255ms
    Mar 27 21:31:35.084: INFO: Terminating ReplicationController wrapped-volume-race-d08ea784-4804-4373-87cc-843732dbe565 pods took: 100.389273ms
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:37.705
    Mar 27 21:31:37.759: INFO: Pod name wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc: Found 1 pods out of 5
    Mar 27 21:31:42.796: INFO: Pod name wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 21:31:42.796
    Mar 27 21:31:42.796: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:42.814: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5": Phase="Running", Reason="", readiness=true. Elapsed: 18.224204ms
    Mar 27 21:31:42.814: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-blds5" satisfied condition "running"
    Mar 27 21:31:42.814: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:42.833: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6": Phase="Running", Reason="", readiness=true. Elapsed: 18.397332ms
    Mar 27 21:31:42.833: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-dtzv6" satisfied condition "running"
    Mar 27 21:31:42.833: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:42.851: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8": Phase="Running", Reason="", readiness=true. Elapsed: 18.133648ms
    Mar 27 21:31:42.851: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-j6rs8" satisfied condition "running"
    Mar 27 21:31:42.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:42.869: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw": Phase="Running", Reason="", readiness=true. Elapsed: 18.426602ms
    Mar 27 21:31:42.869: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wshcw" satisfied condition "running"
    Mar 27 21:31:42.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:42.887: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch": Phase="Running", Reason="", readiness=true. Elapsed: 17.325863ms
    Mar 27 21:31:42.887: INFO: Pod "wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc-wzmch" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:42.887
    Mar 27 21:31:42.977: INFO: Deleting ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc took: 22.938742ms
    Mar 27 21:31:43.078: INFO: Terminating ReplicationController wrapped-volume-race-aa340b10-e248-48a0-8306-deb28c5dccbc pods took: 100.724653ms
    STEP: Creating RC which spawns configmap-volume pods 03/27/23 21:31:45.9
    Mar 27 21:31:45.946: INFO: Pod name wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4: Found 1 pods out of 5
    Mar 27 21:31:50.994: INFO: Pod name wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/27/23 21:31:50.994
    Mar 27 21:31:50.995: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:51.013: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5": Phase="Running", Reason="", readiness=true. Elapsed: 18.612972ms
    Mar 27 21:31:51.013: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-9pkz5" satisfied condition "running"
    Mar 27 21:31:51.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:51.032: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.370847ms
    Mar 27 21:31:51.032: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-fbvxj" satisfied condition "running"
    Mar 27 21:31:51.032: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:51.057: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n": Phase="Running", Reason="", readiness=true. Elapsed: 24.718805ms
    Mar 27 21:31:51.057: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-hc58n" satisfied condition "running"
    Mar 27 21:31:51.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:51.075: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh": Phase="Running", Reason="", readiness=true. Elapsed: 17.768382ms
    Mar 27 21:31:51.075: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-r8vlh" satisfied condition "running"
    Mar 27 21:31:51.075: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn" in namespace "emptydir-wrapper-794" to be "running"
    Mar 27 21:31:51.093: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn": Phase="Running", Reason="", readiness=true. Elapsed: 18.234402ms
    Mar 27 21:31:51.093: INFO: Pod "wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4-rndwn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 in namespace emptydir-wrapper-794, will wait for the garbage collector to delete the pods 03/27/23 21:31:51.093
    Mar 27 21:31:51.186: INFO: Deleting ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 took: 26.193952ms
    Mar 27 21:31:51.287: INFO: Terminating ReplicationController wrapped-volume-race-74ecc0f1-84a0-4183-9639-9c038ad560f4 pods took: 101.089495ms
    STEP: Cleaning up the configMaps 03/27/23 21:31:53.688
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:31:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-794" for this suite. 03/27/23 21:31:55.014
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:31:55.039
Mar 27 21:31:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename cronjob 03/27/23 21:31:55.041
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:55.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:55.116
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/27/23 21:31:55.132
STEP: Ensuring a job is scheduled 03/27/23 21:31:55.152
STEP: Ensuring exactly one is scheduled 03/27/23 21:32:01.17
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 21:32:01.188
STEP: Ensuring no more jobs are scheduled 03/27/23 21:32:01.2
STEP: Removing cronjob 03/27/23 21:37:01.229
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 27 21:37:01.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4947" for this suite. 03/27/23 21:37:01.275
------------------------------
• [SLOW TEST] [306.271 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:31:55.039
    Mar 27 21:31:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename cronjob 03/27/23 21:31:55.041
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:31:55.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:31:55.116
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/27/23 21:31:55.132
    STEP: Ensuring a job is scheduled 03/27/23 21:31:55.152
    STEP: Ensuring exactly one is scheduled 03/27/23 21:32:01.17
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 21:32:01.188
    STEP: Ensuring no more jobs are scheduled 03/27/23 21:32:01.2
    STEP: Removing cronjob 03/27/23 21:37:01.229
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:37:01.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4947" for this suite. 03/27/23 21:37:01.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:37:01.318
Mar 27 21:37:01.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename containers 03/27/23 21:37:01.32
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:37:01.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:37:01.388
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 03/27/23 21:37:01.399
Mar 27 21:37:01.423: INFO: Waiting up to 5m0s for pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f" in namespace "containers-3461" to be "Succeeded or Failed"
Mar 27 21:37:01.437: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.161566ms
Mar 27 21:37:03.450: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026387246s
Mar 27 21:37:05.453: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02935156s
STEP: Saw pod success 03/27/23 21:37:05.453
Mar 27 21:37:05.453: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f" satisfied condition "Succeeded or Failed"
Mar 27 21:37:05.466: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-d9506adb-2635-4f94-a591-7d966821229f container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:37:05.551
Mar 27 21:37:05.582: INFO: Waiting for pod client-containers-d9506adb-2635-4f94-a591-7d966821229f to disappear
Mar 27 21:37:05.594: INFO: Pod client-containers-d9506adb-2635-4f94-a591-7d966821229f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 27 21:37:05.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3461" for this suite. 03/27/23 21:37:05.616
------------------------------
• [4.324 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:37:01.318
    Mar 27 21:37:01.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename containers 03/27/23 21:37:01.32
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:37:01.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:37:01.388
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 03/27/23 21:37:01.399
    Mar 27 21:37:01.423: INFO: Waiting up to 5m0s for pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f" in namespace "containers-3461" to be "Succeeded or Failed"
    Mar 27 21:37:01.437: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.161566ms
    Mar 27 21:37:03.450: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026387246s
    Mar 27 21:37:05.453: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02935156s
    STEP: Saw pod success 03/27/23 21:37:05.453
    Mar 27 21:37:05.453: INFO: Pod "client-containers-d9506adb-2635-4f94-a591-7d966821229f" satisfied condition "Succeeded or Failed"
    Mar 27 21:37:05.466: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-d9506adb-2635-4f94-a591-7d966821229f container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:37:05.551
    Mar 27 21:37:05.582: INFO: Waiting for pod client-containers-d9506adb-2635-4f94-a591-7d966821229f to disappear
    Mar 27 21:37:05.594: INFO: Pod client-containers-d9506adb-2635-4f94-a591-7d966821229f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:37:05.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3461" for this suite. 03/27/23 21:37:05.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:37:05.643
Mar 27 21:37:05.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename cronjob 03/27/23 21:37:05.644
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:37:05.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:37:05.708
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/27/23 21:37:05.72
STEP: Ensuring more than one job is running at a time 03/27/23 21:37:05.737
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/27/23 21:39:01.765
STEP: Removing cronjob 03/27/23 21:39:01.782
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7664" for this suite. 03/27/23 21:39:01.829
------------------------------
• [SLOW TEST] [116.225 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:37:05.643
    Mar 27 21:37:05.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename cronjob 03/27/23 21:37:05.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:37:05.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:37:05.708
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/27/23 21:37:05.72
    STEP: Ensuring more than one job is running at a time 03/27/23 21:37:05.737
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/27/23 21:39:01.765
    STEP: Removing cronjob 03/27/23 21:39:01.782
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7664" for this suite. 03/27/23 21:39:01.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:01.869
Mar 27 21:39:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:39:01.871
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:01.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:01.951
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7863 03/27/23 21:39:01.962
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[] 03/27/23 21:39:01.997
Mar 27 21:39:02.031: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7863 03/27/23 21:39:02.031
Mar 27 21:39:02.060: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7863" to be "running and ready"
Mar 27 21:39:02.073: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199587ms
Mar 27 21:39:02.073: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:39:04.087: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026598797s
Mar 27 21:39:04.087: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 27 21:39:04.087: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod1:[100]] 03/27/23 21:39:04.1
Mar 27 21:39:04.142: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7863 03/27/23 21:39:04.142
Mar 27 21:39:04.158: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7863" to be "running and ready"
Mar 27 21:39:04.170: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.697993ms
Mar 27 21:39:04.170: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:39:06.184: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025522205s
Mar 27 21:39:06.184: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 27 21:39:06.184: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod1:[100] pod2:[101]] 03/27/23 21:39:06.2
Mar 27 21:39:06.278: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/27/23 21:39:06.278
Mar 27 21:39:06.278: INFO: Creating new exec pod
Mar 27 21:39:06.293: INFO: Waiting up to 5m0s for pod "execpodlt8cm" in namespace "services-7863" to be "running"
Mar 27 21:39:06.304: INFO: Pod "execpodlt8cm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.764518ms
Mar 27 21:39:08.320: INFO: Pod "execpodlt8cm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026266545s
Mar 27 21:39:10.325: INFO: Pod "execpodlt8cm": Phase="Running", Reason="", readiness=true. Elapsed: 4.031653913s
Mar 27 21:39:10.325: INFO: Pod "execpodlt8cm" satisfied condition "running"
Mar 27 21:39:11.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Mar 27 21:39:11.664: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 27 21:39:11.664: INFO: stdout: ""
Mar 27 21:39:11.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 172.21.66.148 80'
Mar 27 21:39:11.963: INFO: stderr: "+ nc -v -z -w 2 172.21.66.148 80\nConnection to 172.21.66.148 80 port [tcp/http] succeeded!\n"
Mar 27 21:39:11.963: INFO: stdout: ""
Mar 27 21:39:11.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Mar 27 21:39:12.276: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 27 21:39:12.276: INFO: stdout: ""
Mar 27 21:39:12.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 172.21.66.148 81'
Mar 27 21:39:12.572: INFO: stderr: "+ nc -v -z -w 2 172.21.66.148 81\nConnection to 172.21.66.148 81 port [tcp/*] succeeded!\n"
Mar 27 21:39:12.572: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7863 03/27/23 21:39:12.572
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod2:[101]] 03/27/23 21:39:12.602
Mar 27 21:39:12.648: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7863 03/27/23 21:39:12.648
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[] 03/27/23 21:39:12.688
Mar 27 21:39:13.761: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:13.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7863" for this suite. 03/27/23 21:39:13.835
------------------------------
• [SLOW TEST] [11.990 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:01.869
    Mar 27 21:39:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:39:01.871
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:01.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:01.951
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7863 03/27/23 21:39:01.962
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[] 03/27/23 21:39:01.997
    Mar 27 21:39:02.031: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7863 03/27/23 21:39:02.031
    Mar 27 21:39:02.060: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7863" to be "running and ready"
    Mar 27 21:39:02.073: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199587ms
    Mar 27 21:39:02.073: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:39:04.087: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026598797s
    Mar 27 21:39:04.087: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 27 21:39:04.087: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod1:[100]] 03/27/23 21:39:04.1
    Mar 27 21:39:04.142: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7863 03/27/23 21:39:04.142
    Mar 27 21:39:04.158: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7863" to be "running and ready"
    Mar 27 21:39:04.170: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.697993ms
    Mar 27 21:39:04.170: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:39:06.184: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025522205s
    Mar 27 21:39:06.184: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 27 21:39:06.184: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod1:[100] pod2:[101]] 03/27/23 21:39:06.2
    Mar 27 21:39:06.278: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/27/23 21:39:06.278
    Mar 27 21:39:06.278: INFO: Creating new exec pod
    Mar 27 21:39:06.293: INFO: Waiting up to 5m0s for pod "execpodlt8cm" in namespace "services-7863" to be "running"
    Mar 27 21:39:06.304: INFO: Pod "execpodlt8cm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.764518ms
    Mar 27 21:39:08.320: INFO: Pod "execpodlt8cm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026266545s
    Mar 27 21:39:10.325: INFO: Pod "execpodlt8cm": Phase="Running", Reason="", readiness=true. Elapsed: 4.031653913s
    Mar 27 21:39:10.325: INFO: Pod "execpodlt8cm" satisfied condition "running"
    Mar 27 21:39:11.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Mar 27 21:39:11.664: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 27 21:39:11.664: INFO: stdout: ""
    Mar 27 21:39:11.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 172.21.66.148 80'
    Mar 27 21:39:11.963: INFO: stderr: "+ nc -v -z -w 2 172.21.66.148 80\nConnection to 172.21.66.148 80 port [tcp/http] succeeded!\n"
    Mar 27 21:39:11.963: INFO: stdout: ""
    Mar 27 21:39:11.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Mar 27 21:39:12.276: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 27 21:39:12.276: INFO: stdout: ""
    Mar 27 21:39:12.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-7863 exec execpodlt8cm -- /bin/sh -x -c nc -v -z -w 2 172.21.66.148 81'
    Mar 27 21:39:12.572: INFO: stderr: "+ nc -v -z -w 2 172.21.66.148 81\nConnection to 172.21.66.148 81 port [tcp/*] succeeded!\n"
    Mar 27 21:39:12.572: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7863 03/27/23 21:39:12.572
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[pod2:[101]] 03/27/23 21:39:12.602
    Mar 27 21:39:12.648: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7863 03/27/23 21:39:12.648
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7863 to expose endpoints map[] 03/27/23 21:39:12.688
    Mar 27 21:39:13.761: INFO: successfully validated that service multi-endpoint-test in namespace services-7863 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:13.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7863" for this suite. 03/27/23 21:39:13.835
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:13.863
Mar 27 21:39:13.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:39:13.865
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:13.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:13.93
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:39:13.984
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:39:14.888
STEP: Deploying the webhook pod 03/27/23 21:39:14.917
STEP: Wait for the deployment to be ready 03/27/23 21:39:14.956
Mar 27 21:39:14.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:39:17.041
STEP: Verifying the service has paired with the endpoint 03/27/23 21:39:17.074
Mar 27 21:39:18.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 03/27/23 21:39:18.286
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:39:18.398
STEP: Deleting the collection of validation webhooks 03/27/23 21:39:18.523
STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:39:18.815
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:18.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6925" for this suite. 03/27/23 21:39:18.999
STEP: Destroying namespace "webhook-6925-markers" for this suite. 03/27/23 21:39:19.022
------------------------------
• [SLOW TEST] [5.187 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:13.863
    Mar 27 21:39:13.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:39:13.865
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:13.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:13.93
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:39:13.984
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:39:14.888
    STEP: Deploying the webhook pod 03/27/23 21:39:14.917
    STEP: Wait for the deployment to be ready 03/27/23 21:39:14.956
    Mar 27 21:39:14.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:39:17.041
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:39:17.074
    Mar 27 21:39:18.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 03/27/23 21:39:18.286
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:39:18.398
    STEP: Deleting the collection of validation webhooks 03/27/23 21:39:18.523
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/27/23 21:39:18.815
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:18.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6925" for this suite. 03/27/23 21:39:18.999
    STEP: Destroying namespace "webhook-6925-markers" for this suite. 03/27/23 21:39:19.022
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:19.051
Mar 27 21:39:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:39:19.052
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:19.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:19.119
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-46693486-7f3b-4edd-bad5-9fddd9ecda35 03/27/23 21:39:19.15
STEP: Creating the pod 03/27/23 21:39:19.168
Mar 27 21:39:19.194: INFO: Waiting up to 5m0s for pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe" in namespace "configmap-4299" to be "running and ready"
Mar 27 21:39:19.205: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.436823ms
Mar 27 21:39:19.205: INFO: The phase of Pod pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:39:21.219: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025731561s
Mar 27 21:39:21.219: INFO: The phase of Pod pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe is Running (Ready = true)
Mar 27 21:39:21.219: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-46693486-7f3b-4edd-bad5-9fddd9ecda35 03/27/23 21:39:21.308
STEP: waiting to observe update in volume 03/27/23 21:39:21.344
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:23.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4299" for this suite. 03/27/23 21:39:23.489
------------------------------
• [4.464 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:19.051
    Mar 27 21:39:19.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:39:19.052
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:19.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:19.119
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-46693486-7f3b-4edd-bad5-9fddd9ecda35 03/27/23 21:39:19.15
    STEP: Creating the pod 03/27/23 21:39:19.168
    Mar 27 21:39:19.194: INFO: Waiting up to 5m0s for pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe" in namespace "configmap-4299" to be "running and ready"
    Mar 27 21:39:19.205: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.436823ms
    Mar 27 21:39:19.205: INFO: The phase of Pod pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:39:21.219: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025731561s
    Mar 27 21:39:21.219: INFO: The phase of Pod pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe is Running (Ready = true)
    Mar 27 21:39:21.219: INFO: Pod "pod-configmaps-a247fa7a-3d61-4c78-af77-cd0a211a2ebe" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-46693486-7f3b-4edd-bad5-9fddd9ecda35 03/27/23 21:39:21.308
    STEP: waiting to observe update in volume 03/27/23 21:39:21.344
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:23.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4299" for this suite. 03/27/23 21:39:23.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:23.521
Mar 27 21:39:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:39:23.522
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:23.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:23.583
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 03/27/23 21:39:23.597
Mar 27 21:39:23.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-9128 api-versions'
Mar 27 21:39:23.782: INFO: stderr: ""
Mar 27 21:39:23.782: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:23.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9128" for this suite. 03/27/23 21:39:23.805
------------------------------
• [0.308 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:23.521
    Mar 27 21:39:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:39:23.522
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:23.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:23.583
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 03/27/23 21:39:23.597
    Mar 27 21:39:23.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-9128 api-versions'
    Mar 27 21:39:23.782: INFO: stderr: ""
    Mar 27 21:39:23.782: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:23.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9128" for this suite. 03/27/23 21:39:23.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:23.834
Mar 27 21:39:23.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:39:23.835
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:23.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:23.912
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-36f60617-4a5e-42dd-a584-e63c94287961 03/27/23 21:39:23.926
STEP: Creating secret with name secret-projected-all-test-volume-22828d91-d36a-4b1a-a97c-26f01568f164 03/27/23 21:39:23.944
STEP: Creating a pod to test Check all projections for projected volume plugin 03/27/23 21:39:23.962
Mar 27 21:39:23.998: INFO: Waiting up to 5m0s for pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb" in namespace "projected-4391" to be "Succeeded or Failed"
Mar 27 21:39:24.015: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.667779ms
Mar 27 21:39:26.051: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052447838s
Mar 27 21:39:28.037: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038411458s
Mar 27 21:39:30.032: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034089639s
STEP: Saw pod success 03/27/23 21:39:30.032
Mar 27 21:39:30.032: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb" satisfied condition "Succeeded or Failed"
Mar 27 21:39:30.049: INFO: Trying to get logs from node 10.176.99.175 pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb container projected-all-volume-test: <nil>
STEP: delete the pod 03/27/23 21:39:30.148
Mar 27 21:39:30.190: INFO: Waiting for pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb to disappear
Mar 27 21:39:30.207: INFO: Pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Mar 27 21:39:30.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4391" for this suite. 03/27/23 21:39:30.229
------------------------------
• [SLOW TEST] [6.420 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:23.834
    Mar 27 21:39:23.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:39:23.835
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:23.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:23.912
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-36f60617-4a5e-42dd-a584-e63c94287961 03/27/23 21:39:23.926
    STEP: Creating secret with name secret-projected-all-test-volume-22828d91-d36a-4b1a-a97c-26f01568f164 03/27/23 21:39:23.944
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/27/23 21:39:23.962
    Mar 27 21:39:23.998: INFO: Waiting up to 5m0s for pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb" in namespace "projected-4391" to be "Succeeded or Failed"
    Mar 27 21:39:24.015: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.667779ms
    Mar 27 21:39:26.051: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052447838s
    Mar 27 21:39:28.037: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038411458s
    Mar 27 21:39:30.032: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034089639s
    STEP: Saw pod success 03/27/23 21:39:30.032
    Mar 27 21:39:30.032: INFO: Pod "projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb" satisfied condition "Succeeded or Failed"
    Mar 27 21:39:30.049: INFO: Trying to get logs from node 10.176.99.175 pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb container projected-all-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:39:30.148
    Mar 27 21:39:30.190: INFO: Waiting for pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb to disappear
    Mar 27 21:39:30.207: INFO: Pod projected-volume-213d67bd-3b9d-4035-8921-b8bd4aa080eb no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:39:30.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4391" for this suite. 03/27/23 21:39:30.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:39:30.255
Mar 27 21:39:30.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 21:39:30.257
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:30.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:30.318
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/27/23 21:39:30.333
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3850;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3850;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +notcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_tcp@PTR;sleep 1; done
 03/27/23 21:39:30.389
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3850;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3850;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +notcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_tcp@PTR;sleep 1; done
 03/27/23 21:39:30.389
STEP: creating a pod to probe DNS 03/27/23 21:39:30.389
STEP: submitting the pod to kubernetes 03/27/23 21:39:30.39
Mar 27 21:39:30.425: INFO: Waiting up to 15m0s for pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9" in namespace "dns-3850" to be "running"
Mar 27 21:39:30.443: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.774658ms
Mar 27 21:39:32.468: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042785585s
Mar 27 21:39:32.468: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9" satisfied condition "running"
STEP: retrieving the pod 03/27/23 21:39:32.468
STEP: looking for the results for each expected name from probers 03/27/23 21:39:32.486
Mar 27 21:39:32.533: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.554: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.580: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.623: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.645: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.805: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.846: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.897: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:32.954: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:33.067: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc jessie_udp@_http._tcp.dns-test-service.dns-3850.svc]

Mar 27 21:39:38.095: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.116: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.141: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.227: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.360: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.379: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.401: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.447: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.468: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:38.645: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

Mar 27 21:39:43.090: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.203: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.373: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.395: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.416: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.438: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.462: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.485: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:43.621: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

Mar 27 21:39:48.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.179: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.231: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.477: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.502: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.544: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.610: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.633: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:48.771: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

Mar 27 21:39:53.094: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.116: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.137: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.159: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.204: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.359: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.382: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.405: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.427: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.463: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.485: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:53.621: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

Mar 27 21:39:58.092: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.358: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.380: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.402: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.450: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
Mar 27 21:39:58.607: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

Mar 27 21:40:03.645: INFO: DNS probes using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 succeeded

STEP: deleting the pod 03/27/23 21:40:03.645
STEP: deleting the test service 03/27/23 21:40:03.7
STEP: deleting the test headless service 03/27/23 21:40:03.765
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 21:40:03.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3850" for this suite. 03/27/23 21:40:03.825
------------------------------
• [SLOW TEST] [33.593 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:39:30.255
    Mar 27 21:39:30.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 21:39:30.257
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:39:30.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:39:30.318
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/27/23 21:39:30.333
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3850;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3850;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +notcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_tcp@PTR;sleep 1; done
     03/27/23 21:39:30.389
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3850;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3850;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3850.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3850.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3850.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3850.svc;check="$$(dig +notcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.221.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.221.41_tcp@PTR;sleep 1; done
     03/27/23 21:39:30.389
    STEP: creating a pod to probe DNS 03/27/23 21:39:30.389
    STEP: submitting the pod to kubernetes 03/27/23 21:39:30.39
    Mar 27 21:39:30.425: INFO: Waiting up to 15m0s for pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9" in namespace "dns-3850" to be "running"
    Mar 27 21:39:30.443: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.774658ms
    Mar 27 21:39:32.468: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042785585s
    Mar 27 21:39:32.468: INFO: Pod "dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 21:39:32.468
    STEP: looking for the results for each expected name from probers 03/27/23 21:39:32.486
    Mar 27 21:39:32.533: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.554: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.580: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.623: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.645: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.668: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.690: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.805: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.825: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.846: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.897: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.932: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:32.954: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:33.067: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc jessie_udp@_http._tcp.dns-test-service.dns-3850.svc]

    Mar 27 21:39:38.095: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.116: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.141: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.163: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.184: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.227: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.360: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.379: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.401: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.447: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.468: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:38.645: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc wheezy_udp@_http._tcp.dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

    Mar 27 21:39:43.090: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.203: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.373: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.395: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.416: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.438: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.462: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.485: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:43.621: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

    Mar 27 21:39:48.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.179: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.231: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.477: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.502: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.544: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.610: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.633: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:48.771: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

    Mar 27 21:39:53.094: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.116: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.137: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.159: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.204: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.359: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.382: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.405: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.427: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.463: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.485: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:53.621: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

    Mar 27 21:39:58.092: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.113: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.358: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.380: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.402: INFO: Unable to read jessie_udp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850 from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.450: INFO: Unable to read jessie_udp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-3850.svc from pod dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9: the server could not find the requested resource (get pods dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9)
    Mar 27 21:39:58.607: INFO: Lookups using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3850 wheezy_tcp@dns-test-service.dns-3850 wheezy_udp@dns-test-service.dns-3850.svc wheezy_tcp@dns-test-service.dns-3850.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3850 jessie_tcp@dns-test-service.dns-3850 jessie_udp@dns-test-service.dns-3850.svc jessie_tcp@dns-test-service.dns-3850.svc]

    Mar 27 21:40:03.645: INFO: DNS probes using dns-3850/dns-test-0920f581-745a-44d3-81d2-9b1066ab3ba9 succeeded

    STEP: deleting the pod 03/27/23 21:40:03.645
    STEP: deleting the test service 03/27/23 21:40:03.7
    STEP: deleting the test headless service 03/27/23 21:40:03.765
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:40:03.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3850" for this suite. 03/27/23 21:40:03.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:40:03.852
Mar 27 21:40:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:40:03.853
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:03.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:03.916
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  03/27/23 21:40:03.931
Mar 27 21:40:03.973: INFO: Waiting up to 5m0s for pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9" in namespace "svcaccounts-7105" to be "Succeeded or Failed"
Mar 27 21:40:03.990: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.08472ms
Mar 27 21:40:06.009: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035617819s
Mar 27 21:40:08.008: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034830243s
STEP: Saw pod success 03/27/23 21:40:08.008
Mar 27 21:40:08.009: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9" satisfied condition "Succeeded or Failed"
Mar 27 21:40:08.032: INFO: Trying to get logs from node 10.176.99.177 pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:40:08.063
Mar 27 21:40:08.120: INFO: Waiting for pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 to disappear
Mar 27 21:40:08.138: INFO: Pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 21:40:08.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7105" for this suite. 03/27/23 21:40:08.16
------------------------------
• [4.331 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:40:03.852
    Mar 27 21:40:03.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:40:03.853
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:03.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:03.916
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  03/27/23 21:40:03.931
    Mar 27 21:40:03.973: INFO: Waiting up to 5m0s for pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9" in namespace "svcaccounts-7105" to be "Succeeded or Failed"
    Mar 27 21:40:03.990: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.08472ms
    Mar 27 21:40:06.009: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035617819s
    Mar 27 21:40:08.008: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034830243s
    STEP: Saw pod success 03/27/23 21:40:08.008
    Mar 27 21:40:08.009: INFO: Pod "test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9" satisfied condition "Succeeded or Failed"
    Mar 27 21:40:08.032: INFO: Trying to get logs from node 10.176.99.177 pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:40:08.063
    Mar 27 21:40:08.120: INFO: Waiting for pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 to disappear
    Mar 27 21:40:08.138: INFO: Pod test-pod-c4fed61c-7375-4e11-bd99-c27c183227a9 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:40:08.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7105" for this suite. 03/27/23 21:40:08.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:40:08.194
Mar 27 21:40:08.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:40:08.195
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:08.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:08.257
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-8191d204-dd9f-4186-a6aa-30f54b2dac0f 03/27/23 21:40:08.272
STEP: Creating a pod to test consume configMaps 03/27/23 21:40:08.29
Mar 27 21:40:08.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46" in namespace "projected-8255" to be "Succeeded or Failed"
Mar 27 21:40:08.339: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048407ms
Mar 27 21:40:10.357: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033507007s
Mar 27 21:40:12.357: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033861852s
STEP: Saw pod success 03/27/23 21:40:12.357
Mar 27 21:40:12.358: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46" satisfied condition "Succeeded or Failed"
Mar 27 21:40:12.379: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:40:12.412
Mar 27 21:40:12.470: INFO: Waiting for pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 to disappear
Mar 27 21:40:12.492: INFO: Pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:40:12.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8255" for this suite. 03/27/23 21:40:12.514
------------------------------
• [4.345 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:40:08.194
    Mar 27 21:40:08.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:40:08.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:08.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:08.257
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-8191d204-dd9f-4186-a6aa-30f54b2dac0f 03/27/23 21:40:08.272
    STEP: Creating a pod to test consume configMaps 03/27/23 21:40:08.29
    Mar 27 21:40:08.323: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46" in namespace "projected-8255" to be "Succeeded or Failed"
    Mar 27 21:40:08.339: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048407ms
    Mar 27 21:40:10.357: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033507007s
    Mar 27 21:40:12.357: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033861852s
    STEP: Saw pod success 03/27/23 21:40:12.357
    Mar 27 21:40:12.358: INFO: Pod "pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46" satisfied condition "Succeeded or Failed"
    Mar 27 21:40:12.379: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:40:12.412
    Mar 27 21:40:12.470: INFO: Waiting for pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 to disappear
    Mar 27 21:40:12.492: INFO: Pod pod-projected-configmaps-ebaa077f-2844-404a-b22d-71c286554b46 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:40:12.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8255" for this suite. 03/27/23 21:40:12.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:40:12.541
Mar 27 21:40:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:40:12.542
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:12.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:12.616
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:40:12.628
Mar 27 21:40:12.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12" in namespace "projected-1608" to be "Succeeded or Failed"
Mar 27 21:40:12.677: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Pending", Reason="", readiness=false. Elapsed: 16.040235ms
Mar 27 21:40:14.695: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034319466s
Mar 27 21:40:16.695: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034646715s
STEP: Saw pod success 03/27/23 21:40:16.696
Mar 27 21:40:16.696: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12" satisfied condition "Succeeded or Failed"
Mar 27 21:40:16.721: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 container client-container: <nil>
STEP: delete the pod 03/27/23 21:40:16.755
Mar 27 21:40:16.811: INFO: Waiting for pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 to disappear
Mar 27 21:40:16.859: INFO: Pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:40:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1608" for this suite. 03/27/23 21:40:16.884
------------------------------
• [4.367 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:40:12.541
    Mar 27 21:40:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:40:12.542
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:12.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:12.616
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:40:12.628
    Mar 27 21:40:12.661: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12" in namespace "projected-1608" to be "Succeeded or Failed"
    Mar 27 21:40:12.677: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Pending", Reason="", readiness=false. Elapsed: 16.040235ms
    Mar 27 21:40:14.695: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034319466s
    Mar 27 21:40:16.695: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034646715s
    STEP: Saw pod success 03/27/23 21:40:16.696
    Mar 27 21:40:16.696: INFO: Pod "downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12" satisfied condition "Succeeded or Failed"
    Mar 27 21:40:16.721: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:40:16.755
    Mar 27 21:40:16.811: INFO: Waiting for pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 to disappear
    Mar 27 21:40:16.859: INFO: Pod downwardapi-volume-38bfa8aa-0b8d-4bde-bd0c-1bafb41c8d12 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:40:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1608" for this suite. 03/27/23 21:40:16.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:40:16.91
Mar 27 21:40:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename taint-multiple-pods 03/27/23 21:40:16.911
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:16.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:16.978
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Mar 27 21:40:16.992: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 21:41:17.084: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Mar 27 21:41:17.102: INFO: Starting informer...
STEP: Starting pods... 03/27/23 21:41:17.102
Mar 27 21:41:17.384: INFO: Pod1 is running on 10.176.99.177. Tainting Node
Mar 27 21:41:17.639: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5754" to be "running"
Mar 27 21:41:17.653: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.748077ms
Mar 27 21:41:19.668: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028899003s
Mar 27 21:41:19.668: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 27 21:41:19.668: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5754" to be "running"
Mar 27 21:41:19.682: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 13.548159ms
Mar 27 21:41:19.683: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 27 21:41:19.683: INFO: Pod2 is running on 10.176.99.177. Tainting Node
STEP: Trying to apply a taint on the Node 03/27/23 21:41:19.683
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:41:19.717
STEP: Waiting for Pod1 and Pod2 to be deleted 03/27/23 21:41:19.727
Mar 27 21:41:25.766: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 27 21:41:45.817: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:41:45.858
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:41:45.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-5754" for this suite. 03/27/23 21:41:45.888
------------------------------
• [SLOW TEST] [89.000 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:40:16.91
    Mar 27 21:40:16.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename taint-multiple-pods 03/27/23 21:40:16.911
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:40:16.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:40:16.978
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Mar 27 21:40:16.992: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 21:41:17.084: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Mar 27 21:41:17.102: INFO: Starting informer...
    STEP: Starting pods... 03/27/23 21:41:17.102
    Mar 27 21:41:17.384: INFO: Pod1 is running on 10.176.99.177. Tainting Node
    Mar 27 21:41:17.639: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5754" to be "running"
    Mar 27 21:41:17.653: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.748077ms
    Mar 27 21:41:19.668: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.028899003s
    Mar 27 21:41:19.668: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 27 21:41:19.668: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5754" to be "running"
    Mar 27 21:41:19.682: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 13.548159ms
    Mar 27 21:41:19.683: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 27 21:41:19.683: INFO: Pod2 is running on 10.176.99.177. Tainting Node
    STEP: Trying to apply a taint on the Node 03/27/23 21:41:19.683
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:41:19.717
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/27/23 21:41:19.727
    Mar 27 21:41:25.766: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 27 21:41:45.817: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:41:45.858
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:41:45.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-5754" for this suite. 03/27/23 21:41:45.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:41:45.911
Mar 27 21:41:45.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:41:45.913
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:41:45.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:41:45.968
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 03/27/23 21:41:45.977
STEP: Creating a ResourceQuota 03/27/23 21:41:50.99
STEP: Ensuring resource quota status is calculated 03/27/23 21:41:51.003
STEP: Creating a Pod that fits quota 03/27/23 21:41:53.02
STEP: Ensuring ResourceQuota status captures the pod usage 03/27/23 21:41:53.074
STEP: Not allowing a pod to be created that exceeds remaining quota 03/27/23 21:41:55.086
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/27/23 21:41:55.093
STEP: Ensuring a pod cannot update its resource requirements 03/27/23 21:41:55.099
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/27/23 21:41:55.116
STEP: Deleting the pod 03/27/23 21:41:57.131
STEP: Ensuring resource quota status released the pod usage 03/27/23 21:41:57.169
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:41:59.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7550" for this suite. 03/27/23 21:41:59.197
------------------------------
• [SLOW TEST] [13.307 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:41:45.911
    Mar 27 21:41:45.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:41:45.913
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:41:45.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:41:45.968
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 03/27/23 21:41:45.977
    STEP: Creating a ResourceQuota 03/27/23 21:41:50.99
    STEP: Ensuring resource quota status is calculated 03/27/23 21:41:51.003
    STEP: Creating a Pod that fits quota 03/27/23 21:41:53.02
    STEP: Ensuring ResourceQuota status captures the pod usage 03/27/23 21:41:53.074
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/27/23 21:41:55.086
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/27/23 21:41:55.093
    STEP: Ensuring a pod cannot update its resource requirements 03/27/23 21:41:55.099
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/27/23 21:41:55.116
    STEP: Deleting the pod 03/27/23 21:41:57.131
    STEP: Ensuring resource quota status released the pod usage 03/27/23 21:41:57.169
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:41:59.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7550" for this suite. 03/27/23 21:41:59.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:41:59.22
Mar 27 21:41:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:41:59.221
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:41:59.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:41:59.298
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:41:59.307
Mar 27 21:41:59.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616" in namespace "projected-5455" to be "Succeeded or Failed"
Mar 27 21:41:59.355: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Pending", Reason="", readiness=false. Elapsed: 14.319114ms
Mar 27 21:42:01.370: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029312257s
Mar 27 21:42:03.371: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030225803s
STEP: Saw pod success 03/27/23 21:42:03.371
Mar 27 21:42:03.371: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616" satisfied condition "Succeeded or Failed"
Mar 27 21:42:03.386: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 container client-container: <nil>
STEP: delete the pod 03/27/23 21:42:03.466
Mar 27 21:42:03.513: INFO: Waiting for pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 to disappear
Mar 27 21:42:03.525: INFO: Pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:03.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5455" for this suite. 03/27/23 21:42:03.54
------------------------------
• [4.342 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:41:59.22
    Mar 27 21:41:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:41:59.221
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:41:59.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:41:59.298
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:41:59.307
    Mar 27 21:41:59.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616" in namespace "projected-5455" to be "Succeeded or Failed"
    Mar 27 21:41:59.355: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Pending", Reason="", readiness=false. Elapsed: 14.319114ms
    Mar 27 21:42:01.370: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029312257s
    Mar 27 21:42:03.371: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030225803s
    STEP: Saw pod success 03/27/23 21:42:03.371
    Mar 27 21:42:03.371: INFO: Pod "downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616" satisfied condition "Succeeded or Failed"
    Mar 27 21:42:03.386: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:42:03.466
    Mar 27 21:42:03.513: INFO: Waiting for pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 to disappear
    Mar 27 21:42:03.525: INFO: Pod downwardapi-volume-0d463a9f-2391-4e50-a768-a1a54f82e616 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:03.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5455" for this suite. 03/27/23 21:42:03.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:03.564
Mar 27 21:42:03.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename csistoragecapacity 03/27/23 21:42:03.566
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:03.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:03.616
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/27/23 21:42:03.625
STEP: getting /apis/storage.k8s.io 03/27/23 21:42:03.633
STEP: getting /apis/storage.k8s.io/v1 03/27/23 21:42:03.636
STEP: creating 03/27/23 21:42:03.64
STEP: watching 03/27/23 21:42:03.693
Mar 27 21:42:03.693: INFO: starting watch
STEP: getting 03/27/23 21:42:03.712
STEP: listing in namespace 03/27/23 21:42:03.726
STEP: listing across namespaces 03/27/23 21:42:03.738
STEP: patching 03/27/23 21:42:03.748
STEP: updating 03/27/23 21:42:03.762
Mar 27 21:42:03.776: INFO: waiting for watch events with expected annotations in namespace
Mar 27 21:42:03.776: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/27/23 21:42:03.777
STEP: deleting a collection 03/27/23 21:42:03.817
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:03.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-4507" for this suite. 03/27/23 21:42:03.899
------------------------------
• [0.357 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:03.564
    Mar 27 21:42:03.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename csistoragecapacity 03/27/23 21:42:03.566
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:03.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:03.616
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/27/23 21:42:03.625
    STEP: getting /apis/storage.k8s.io 03/27/23 21:42:03.633
    STEP: getting /apis/storage.k8s.io/v1 03/27/23 21:42:03.636
    STEP: creating 03/27/23 21:42:03.64
    STEP: watching 03/27/23 21:42:03.693
    Mar 27 21:42:03.693: INFO: starting watch
    STEP: getting 03/27/23 21:42:03.712
    STEP: listing in namespace 03/27/23 21:42:03.726
    STEP: listing across namespaces 03/27/23 21:42:03.738
    STEP: patching 03/27/23 21:42:03.748
    STEP: updating 03/27/23 21:42:03.762
    Mar 27 21:42:03.776: INFO: waiting for watch events with expected annotations in namespace
    Mar 27 21:42:03.776: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/27/23 21:42:03.777
    STEP: deleting a collection 03/27/23 21:42:03.817
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:03.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-4507" for this suite. 03/27/23 21:42:03.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:03.924
Mar 27 21:42:03.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:42:03.926
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:03.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:03.989
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 03/27/23 21:42:03.997
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3088" for this suite. 03/27/23 21:42:04.028
------------------------------
• [0.128 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:03.924
    Mar 27 21:42:03.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:42:03.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:03.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:03.989
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 03/27/23 21:42:03.997
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3088" for this suite. 03/27/23 21:42:04.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:04.056
Mar 27 21:42:04.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 21:42:04.058
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:04.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:04.109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 03/27/23 21:42:04.12
STEP: When the matched label of one of its pods change 03/27/23 21:42:04.132
Mar 27 21:42:04.147: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 27 21:42:09.162: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/27/23 21:42:09.202
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:10.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4669" for this suite. 03/27/23 21:42:10.25
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:04.056
    Mar 27 21:42:04.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 21:42:04.058
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:04.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:04.109
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 03/27/23 21:42:04.12
    STEP: When the matched label of one of its pods change 03/27/23 21:42:04.132
    Mar 27 21:42:04.147: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 27 21:42:09.162: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/27/23 21:42:09.202
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:10.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4669" for this suite. 03/27/23 21:42:10.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:10.28
Mar 27 21:42:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:42:10.281
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:10.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:10.333
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 27 21:42:10.386: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9875 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:10.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9875" for this suite. 03/27/23 21:42:10.437
------------------------------
• [0.178 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:10.28
    Mar 27 21:42:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 21:42:10.281
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:10.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:10.333
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 27 21:42:10.386: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9875 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:10.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9875" for this suite. 03/27/23 21:42:10.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:10.468
Mar 27 21:42:10.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:42:10.469
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:10.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:10.533
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1950 03/27/23 21:42:10.545
STEP: creating a selector 03/27/23 21:42:10.545
STEP: Creating the service pods in kubernetes 03/27/23 21:42:10.546
Mar 27 21:42:10.546: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 21:42:10.670: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1950" to be "running and ready"
Mar 27 21:42:10.693: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.00198ms
Mar 27 21:42:10.693: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:42:12.712: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.04159824s
Mar 27 21:42:12.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:14.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.039734788s
Mar 27 21:42:14.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:16.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.038564544s
Mar 27 21:42:16.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:18.707: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.03621966s
Mar 27 21:42:18.707: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:20.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038073843s
Mar 27 21:42:20.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:22.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03780648s
Mar 27 21:42:22.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:24.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.038579922s
Mar 27 21:42:24.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:26.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.040566712s
Mar 27 21:42:26.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:28.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.037837734s
Mar 27 21:42:28.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:30.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.040745926s
Mar 27 21:42:30.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:42:32.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.03890329s
Mar 27 21:42:32.709: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 21:42:32.709: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 21:42:32.734: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1950" to be "running and ready"
Mar 27 21:42:32.748: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.380296ms
Mar 27 21:42:32.748: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 21:42:32.748: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 21:42:32.763: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1950" to be "running and ready"
Mar 27 21:42:32.776: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.839662ms
Mar 27 21:42:32.776: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 21:42:32.776: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 21:42:32.79
Mar 27 21:42:32.809: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1950" to be "running"
Mar 27 21:42:32.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684098ms
Mar 27 21:42:34.837: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02883147s
Mar 27 21:42:36.836: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.02788714s
Mar 27 21:42:36.837: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 21:42:36.851: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 21:42:36.851: INFO: Breadth first check of 172.30.56.111 on host 10.176.99.175...
Mar 27 21:42:36.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.56.111&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:42:36.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:42:36.864: INFO: ExecWithOptions: Clientset creation
Mar 27 21:42:36.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.56.111%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:42:37.013: INFO: Waiting for responses: map[]
Mar 27 21:42:37.013: INFO: reached 172.30.56.111 after 0/1 tries
Mar 27 21:42:37.013: INFO: Breadth first check of 172.30.85.146 on host 10.176.99.177...
Mar 27 21:42:37.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.85.146&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:42:37.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:42:37.028: INFO: ExecWithOptions: Clientset creation
Mar 27 21:42:37.028: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.85.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:42:37.222: INFO: Waiting for responses: map[]
Mar 27 21:42:37.222: INFO: reached 172.30.85.146 after 0/1 tries
Mar 27 21:42:37.222: INFO: Breadth first check of 172.30.4.93 on host 10.176.99.178...
Mar 27 21:42:37.236: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.4.93&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:42:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:42:37.237: INFO: ExecWithOptions: Clientset creation
Mar 27 21:42:37.237: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.4.93%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:42:37.377: INFO: Waiting for responses: map[]
Mar 27 21:42:37.378: INFO: reached 172.30.4.93 after 0/1 tries
Mar 27 21:42:37.378: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:37.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1950" for this suite. 03/27/23 21:42:37.405
------------------------------
• [SLOW TEST] [26.969 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:10.468
    Mar 27 21:42:10.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:42:10.469
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:10.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:10.533
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1950 03/27/23 21:42:10.545
    STEP: creating a selector 03/27/23 21:42:10.545
    STEP: Creating the service pods in kubernetes 03/27/23 21:42:10.546
    Mar 27 21:42:10.546: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 21:42:10.670: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1950" to be "running and ready"
    Mar 27 21:42:10.693: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.00198ms
    Mar 27 21:42:10.693: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:42:12.712: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.04159824s
    Mar 27 21:42:12.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:14.710: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.039734788s
    Mar 27 21:42:14.710: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:16.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.038564544s
    Mar 27 21:42:16.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:18.707: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.03621966s
    Mar 27 21:42:18.707: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:20.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038073843s
    Mar 27 21:42:20.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:22.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03780648s
    Mar 27 21:42:22.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:24.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.038579922s
    Mar 27 21:42:24.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:26.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.040566712s
    Mar 27 21:42:26.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:28.708: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.037837734s
    Mar 27 21:42:28.708: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:30.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.040745926s
    Mar 27 21:42:30.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:42:32.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.03890329s
    Mar 27 21:42:32.709: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 21:42:32.709: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 21:42:32.734: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1950" to be "running and ready"
    Mar 27 21:42:32.748: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.380296ms
    Mar 27 21:42:32.748: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 21:42:32.748: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 21:42:32.763: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1950" to be "running and ready"
    Mar 27 21:42:32.776: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 12.839662ms
    Mar 27 21:42:32.776: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 21:42:32.776: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 21:42:32.79
    Mar 27 21:42:32.809: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1950" to be "running"
    Mar 27 21:42:32.822: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684098ms
    Mar 27 21:42:34.837: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02883147s
    Mar 27 21:42:36.836: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.02788714s
    Mar 27 21:42:36.837: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 21:42:36.851: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 21:42:36.851: INFO: Breadth first check of 172.30.56.111 on host 10.176.99.175...
    Mar 27 21:42:36.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.56.111&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:42:36.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:42:36.864: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:42:36.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.56.111%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:42:37.013: INFO: Waiting for responses: map[]
    Mar 27 21:42:37.013: INFO: reached 172.30.56.111 after 0/1 tries
    Mar 27 21:42:37.013: INFO: Breadth first check of 172.30.85.146 on host 10.176.99.177...
    Mar 27 21:42:37.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.85.146&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:42:37.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:42:37.028: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:42:37.028: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.85.146%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:42:37.222: INFO: Waiting for responses: map[]
    Mar 27 21:42:37.222: INFO: reached 172.30.85.146 after 0/1 tries
    Mar 27 21:42:37.222: INFO: Breadth first check of 172.30.4.93 on host 10.176.99.178...
    Mar 27 21:42:37.236: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.177:9080/dial?request=hostname&protocol=http&host=172.30.4.93&port=8083&tries=1'] Namespace:pod-network-test-1950 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:42:37.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:42:37.237: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:42:37.237: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1950/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.4.93%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:42:37.377: INFO: Waiting for responses: map[]
    Mar 27 21:42:37.378: INFO: reached 172.30.4.93 after 0/1 tries
    Mar 27 21:42:37.378: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:37.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1950" for this suite. 03/27/23 21:42:37.405
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:37.439
Mar 27 21:42:37.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename events 03/27/23 21:42:37.442
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:37.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:37.496
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/27/23 21:42:37.505
STEP: listing all events in all namespaces 03/27/23 21:42:37.52
STEP: patching the test event 03/27/23 21:42:37.542
STEP: fetching the test event 03/27/23 21:42:37.568
STEP: updating the test event 03/27/23 21:42:37.581
STEP: getting the test event 03/27/23 21:42:37.618
STEP: deleting the test event 03/27/23 21:42:37.632
STEP: listing all events in all namespaces 03/27/23 21:42:37.671
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:37.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4247" for this suite. 03/27/23 21:42:37.706
------------------------------
• [0.289 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:37.439
    Mar 27 21:42:37.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename events 03/27/23 21:42:37.442
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:37.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:37.496
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/27/23 21:42:37.505
    STEP: listing all events in all namespaces 03/27/23 21:42:37.52
    STEP: patching the test event 03/27/23 21:42:37.542
    STEP: fetching the test event 03/27/23 21:42:37.568
    STEP: updating the test event 03/27/23 21:42:37.581
    STEP: getting the test event 03/27/23 21:42:37.618
    STEP: deleting the test event 03/27/23 21:42:37.632
    STEP: listing all events in all namespaces 03/27/23 21:42:37.671
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:37.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4247" for this suite. 03/27/23 21:42:37.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:37.732
Mar 27 21:42:37.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:42:37.733
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:37.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:37.789
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 03/27/23 21:42:37.797
Mar 27 21:42:37.798: INFO: Creating e2e-svc-a-qvqvh
Mar 27 21:42:37.835: INFO: Creating e2e-svc-b-hkk6r
Mar 27 21:42:37.874: INFO: Creating e2e-svc-c-9hw4q
STEP: deleting service collection 03/27/23 21:42:37.921
Mar 27 21:42:38.032: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:38.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8794" for this suite. 03/27/23 21:42:38.048
------------------------------
• [0.339 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:37.732
    Mar 27 21:42:37.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:42:37.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:37.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:37.789
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 03/27/23 21:42:37.797
    Mar 27 21:42:37.798: INFO: Creating e2e-svc-a-qvqvh
    Mar 27 21:42:37.835: INFO: Creating e2e-svc-b-hkk6r
    Mar 27 21:42:37.874: INFO: Creating e2e-svc-c-9hw4q
    STEP: deleting service collection 03/27/23 21:42:37.921
    Mar 27 21:42:38.032: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:38.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8794" for this suite. 03/27/23 21:42:38.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:38.074
Mar 27 21:42:38.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 21:42:38.075
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:38.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:38.129
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Mar 27 21:42:38.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: creating the pod 03/27/23 21:42:38.139
STEP: submitting the pod to kubernetes 03/27/23 21:42:38.139
Mar 27 21:42:38.168: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c" in namespace "pods-9526" to be "running and ready"
Mar 27 21:42:38.182: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.913406ms
Mar 27 21:42:38.182: INFO: The phase of Pod pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:42:40.197: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.028242657s
Mar 27 21:42:40.197: INFO: The phase of Pod pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c is Running (Ready = true)
Mar 27 21:42:40.197: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:40.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9526" for this suite. 03/27/23 21:42:40.339
------------------------------
• [2.287 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:38.074
    Mar 27 21:42:38.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 21:42:38.075
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:38.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:38.129
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Mar 27 21:42:38.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: creating the pod 03/27/23 21:42:38.139
    STEP: submitting the pod to kubernetes 03/27/23 21:42:38.139
    Mar 27 21:42:38.168: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c" in namespace "pods-9526" to be "running and ready"
    Mar 27 21:42:38.182: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.913406ms
    Mar 27 21:42:38.182: INFO: The phase of Pod pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:42:40.197: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.028242657s
    Mar 27 21:42:40.197: INFO: The phase of Pod pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c is Running (Ready = true)
    Mar 27 21:42:40.197: INFO: Pod "pod-logs-websocket-05006e72-ef00-4810-9004-02a6157c9c2c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:40.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9526" for this suite. 03/27/23 21:42:40.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:40.366
Mar 27 21:42:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 21:42:40.368
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:40.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:40.424
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/27/23 21:42:40.451
STEP: Verify that the required pods have come up. 03/27/23 21:42:40.467
Mar 27 21:42:40.479: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 27 21:42:45.496: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 21:42:45.496
STEP: Getting /status 03/27/23 21:42:45.496
Mar 27 21:42:45.511: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/27/23 21:42:45.511
Mar 27 21:42:45.549: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/27/23 21:42:45.549
Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: ADDED
Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.556: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.556: INFO: Found replicaset test-rs in namespace replicaset-8527 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 21:42:45.556: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/27/23 21:42:45.556
Mar 27 21:42:45.556: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 21:42:45.592: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/27/23 21:42:45.592
Mar 27 21:42:45.597: INFO: Observed &ReplicaSet event: ADDED
Mar 27 21:42:45.598: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.598: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.600: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.600: INFO: Observed replicaset test-rs in namespace replicaset-8527 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 21:42:45.601: INFO: Observed &ReplicaSet event: MODIFIED
Mar 27 21:42:45.601: INFO: Found replicaset test-rs in namespace replicaset-8527 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 27 21:42:45.601: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:45.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8527" for this suite. 03/27/23 21:42:45.622
------------------------------
• [SLOW TEST] [5.285 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:40.366
    Mar 27 21:42:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 21:42:40.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:40.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:40.424
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/27/23 21:42:40.451
    STEP: Verify that the required pods have come up. 03/27/23 21:42:40.467
    Mar 27 21:42:40.479: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 27 21:42:45.496: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 21:42:45.496
    STEP: Getting /status 03/27/23 21:42:45.496
    Mar 27 21:42:45.511: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/27/23 21:42:45.511
    Mar 27 21:42:45.549: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/27/23 21:42:45.549
    Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: ADDED
    Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.555: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.556: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.556: INFO: Found replicaset test-rs in namespace replicaset-8527 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 21:42:45.556: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/27/23 21:42:45.556
    Mar 27 21:42:45.556: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 21:42:45.592: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/27/23 21:42:45.592
    Mar 27 21:42:45.597: INFO: Observed &ReplicaSet event: ADDED
    Mar 27 21:42:45.598: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.598: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.600: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.600: INFO: Observed replicaset test-rs in namespace replicaset-8527 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 21:42:45.601: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 27 21:42:45.601: INFO: Found replicaset test-rs in namespace replicaset-8527 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 27 21:42:45.601: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:45.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8527" for this suite. 03/27/23 21:42:45.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:45.655
Mar 27 21:42:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:42:45.656
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:45.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:45.718
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 03/27/23 21:42:45.726
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/27/23 21:42:45.741
STEP: patching the secret 03/27/23 21:42:45.761
STEP: deleting the secret using a LabelSelector 03/27/23 21:42:45.788
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/27/23 21:42:45.817
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:45.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-857" for this suite. 03/27/23 21:42:45.851
------------------------------
• [0.218 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:45.655
    Mar 27 21:42:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:42:45.656
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:45.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:45.718
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 03/27/23 21:42:45.726
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/27/23 21:42:45.741
    STEP: patching the secret 03/27/23 21:42:45.761
    STEP: deleting the secret using a LabelSelector 03/27/23 21:42:45.788
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/27/23 21:42:45.817
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:45.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-857" for this suite. 03/27/23 21:42:45.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:45.874
Mar 27 21:42:45.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:42:45.876
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:45.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:45.938
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:42:45.948
Mar 27 21:42:45.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9" in namespace "downward-api-9810" to be "Succeeded or Failed"
Mar 27 21:42:45.998: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.93345ms
Mar 27 21:42:48.015: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037246292s
Mar 27 21:42:50.013: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034333105s
STEP: Saw pod success 03/27/23 21:42:50.013
Mar 27 21:42:50.013: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9" satisfied condition "Succeeded or Failed"
Mar 27 21:42:50.030: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 container client-container: <nil>
STEP: delete the pod 03/27/23 21:42:50.059
Mar 27 21:42:50.110: INFO: Waiting for pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 to disappear
Mar 27 21:42:50.123: INFO: Pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9810" for this suite. 03/27/23 21:42:50.142
------------------------------
• [4.291 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:45.874
    Mar 27 21:42:45.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:42:45.876
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:45.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:45.938
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:42:45.948
    Mar 27 21:42:45.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9" in namespace "downward-api-9810" to be "Succeeded or Failed"
    Mar 27 21:42:45.998: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.93345ms
    Mar 27 21:42:48.015: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037246292s
    Mar 27 21:42:50.013: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034333105s
    STEP: Saw pod success 03/27/23 21:42:50.013
    Mar 27 21:42:50.013: INFO: Pod "downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9" satisfied condition "Succeeded or Failed"
    Mar 27 21:42:50.030: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:42:50.059
    Mar 27 21:42:50.110: INFO: Waiting for pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 to disappear
    Mar 27 21:42:50.123: INFO: Pod downwardapi-volume-ab2ac57f-5a47-4f05-b754-d9c0ea3ac9c9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9810" for this suite. 03/27/23 21:42:50.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:50.177
Mar 27 21:42:50.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-runtime 03/27/23 21:42:50.178
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:50.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:50.232
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 03/27/23 21:42:50.24
STEP: wait for the container to reach Succeeded 03/27/23 21:42:50.271
STEP: get the container status 03/27/23 21:42:54.368
STEP: the container should be terminated 03/27/23 21:42:54.382
STEP: the termination message should be set 03/27/23 21:42:54.382
Mar 27 21:42:54.382: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/27/23 21:42:54.382
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9620" for this suite. 03/27/23 21:42:54.452
------------------------------
• [4.297 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:50.177
    Mar 27 21:42:50.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-runtime 03/27/23 21:42:50.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:50.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:50.232
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 03/27/23 21:42:50.24
    STEP: wait for the container to reach Succeeded 03/27/23 21:42:50.271
    STEP: get the container status 03/27/23 21:42:54.368
    STEP: the container should be terminated 03/27/23 21:42:54.382
    STEP: the termination message should be set 03/27/23 21:42:54.382
    Mar 27 21:42:54.382: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/27/23 21:42:54.382
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9620" for this suite. 03/27/23 21:42:54.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:54.482
Mar 27 21:42:54.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename proxy 03/27/23 21:42:54.484
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:54.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:54.541
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 27 21:42:54.550: INFO: Creating pod...
Mar 27 21:42:54.577: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7956" to be "running"
Mar 27 21:42:54.590: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.905548ms
Mar 27 21:42:56.605: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.027515606s
Mar 27 21:42:56.605: INFO: Pod "agnhost" satisfied condition "running"
Mar 27 21:42:56.605: INFO: Creating service...
Mar 27 21:42:56.640: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=DELETE
Mar 27 21:42:56.696: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 21:42:56.696: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=OPTIONS
Mar 27 21:42:56.716: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 21:42:56.716: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=PATCH
Mar 27 21:42:56.736: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 21:42:56.737: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=POST
Mar 27 21:42:56.765: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 21:42:56.765: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=PUT
Mar 27 21:42:56.784: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 21:42:56.784: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 27 21:42:56.807: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 27 21:42:56.807: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 27 21:42:56.833: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 27 21:42:56.833: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 27 21:42:56.858: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 27 21:42:56.858: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=POST
Mar 27 21:42:56.883: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 27 21:42:56.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=PUT
Mar 27 21:42:56.911: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 27 21:42:56.911: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=GET
Mar 27 21:42:56.923: INFO: http.Client request:GET StatusCode:301
Mar 27 21:42:56.923: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=GET
Mar 27 21:42:56.942: INFO: http.Client request:GET StatusCode:301
Mar 27 21:42:56.942: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=HEAD
Mar 27 21:42:56.955: INFO: http.Client request:HEAD StatusCode:301
Mar 27 21:42:56.955: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 27 21:42:56.973: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Mar 27 21:42:56.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7956" for this suite. 03/27/23 21:42:56.989
------------------------------
• [2.532 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:54.482
    Mar 27 21:42:54.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename proxy 03/27/23 21:42:54.484
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:54.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:54.541
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 27 21:42:54.550: INFO: Creating pod...
    Mar 27 21:42:54.577: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7956" to be "running"
    Mar 27 21:42:54.590: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.905548ms
    Mar 27 21:42:56.605: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.027515606s
    Mar 27 21:42:56.605: INFO: Pod "agnhost" satisfied condition "running"
    Mar 27 21:42:56.605: INFO: Creating service...
    Mar 27 21:42:56.640: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=DELETE
    Mar 27 21:42:56.696: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 21:42:56.696: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=OPTIONS
    Mar 27 21:42:56.716: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 21:42:56.716: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=PATCH
    Mar 27 21:42:56.736: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 21:42:56.737: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=POST
    Mar 27 21:42:56.765: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 21:42:56.765: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=PUT
    Mar 27 21:42:56.784: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 21:42:56.784: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 27 21:42:56.807: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 27 21:42:56.807: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 27 21:42:56.833: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 27 21:42:56.833: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 27 21:42:56.858: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 27 21:42:56.858: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=POST
    Mar 27 21:42:56.883: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 27 21:42:56.883: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 27 21:42:56.911: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 27 21:42:56.911: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=GET
    Mar 27 21:42:56.923: INFO: http.Client request:GET StatusCode:301
    Mar 27 21:42:56.923: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=GET
    Mar 27 21:42:56.942: INFO: http.Client request:GET StatusCode:301
    Mar 27 21:42:56.942: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy?method=HEAD
    Mar 27 21:42:56.955: INFO: http.Client request:HEAD StatusCode:301
    Mar 27 21:42:56.955: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-7956/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 27 21:42:56.973: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:42:56.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7956" for this suite. 03/27/23 21:42:56.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:42:57.017
Mar 27 21:42:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption 03/27/23 21:42:57.018
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:57.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:57.071
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 27 21:42:57.128: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 21:43:57.228: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:43:57.24
Mar 27 21:43:57.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 21:43:57.242
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:43:57.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:43:57.298
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 03/27/23 21:43:57.307
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:43:57.307
Mar 27 21:43:57.338: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5121" to be "running"
Mar 27 21:43:57.352: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 13.541792ms
Mar 27 21:43:59.368: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029839624s
Mar 27 21:43:59.368: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:43:59.382
Mar 27 21:43:59.431: INFO: found a healthy node: 10.176.99.177
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Mar 27 21:44:05.692: INFO: pods created so far: [1 1 1]
Mar 27 21:44:05.692: INFO: length of pods created so far: 3
Mar 27 21:44:07.726: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Mar 27 21:44:14.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:44:14.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5121" for this suite. 03/27/23 21:44:15.072
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3699" for this suite. 03/27/23 21:44:15.096
------------------------------
• [SLOW TEST] [78.101 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:42:57.017
    Mar 27 21:42:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 21:42:57.018
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:42:57.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:42:57.071
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 27 21:42:57.128: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 21:43:57.228: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:43:57.24
    Mar 27 21:43:57.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 21:43:57.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:43:57.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:43:57.298
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 03/27/23 21:43:57.307
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:43:57.307
    Mar 27 21:43:57.338: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5121" to be "running"
    Mar 27 21:43:57.352: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 13.541792ms
    Mar 27 21:43:59.368: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.029839624s
    Mar 27 21:43:59.368: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:43:59.382
    Mar 27 21:43:59.431: INFO: found a healthy node: 10.176.99.177
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Mar 27 21:44:05.692: INFO: pods created so far: [1 1 1]
    Mar 27 21:44:05.692: INFO: length of pods created so far: 3
    Mar 27 21:44:07.726: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:44:14.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:44:14.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5121" for this suite. 03/27/23 21:44:15.072
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3699" for this suite. 03/27/23 21:44:15.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:44:15.123
Mar 27 21:44:15.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename watch 03/27/23 21:44:15.125
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:15.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:15.178
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/27/23 21:44:15.186
STEP: creating a watch on configmaps with label B 03/27/23 21:44:15.191
STEP: creating a watch on configmaps with label A or B 03/27/23 21:44:15.194
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.207
Mar 27 21:44:15.220: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36196 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:15.220: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36196 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.221
Mar 27 21:44:15.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36197 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:15.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36197 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/27/23 21:44:15.25
Mar 27 21:44:15.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36198 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:15.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36198 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.276
Mar 27 21:44:15.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36199 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:15.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36199 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/27/23 21:44:15.3
Mar 27 21:44:15.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36200 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:15.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36200 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/27/23 21:44:25.314
Mar 27 21:44:25.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36281 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 21:44:25.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36281 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 27 21:44:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2912" for this suite. 03/27/23 21:44:35.358
------------------------------
• [SLOW TEST] [20.257 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:44:15.123
    Mar 27 21:44:15.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename watch 03/27/23 21:44:15.125
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:15.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:15.178
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/27/23 21:44:15.186
    STEP: creating a watch on configmaps with label B 03/27/23 21:44:15.191
    STEP: creating a watch on configmaps with label A or B 03/27/23 21:44:15.194
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.207
    Mar 27 21:44:15.220: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36196 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:15.220: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36196 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.221
    Mar 27 21:44:15.248: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36197 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:15.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36197 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/27/23 21:44:15.25
    Mar 27 21:44:15.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36198 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:15.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36198 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/27/23 21:44:15.276
    Mar 27 21:44:15.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36199 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:15.300: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2912  aa6a8043-9dd9-4add-8d7c-f922dd9970e9 36199 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/27/23 21:44:15.3
    Mar 27 21:44:15.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36200 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:15.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36200 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/27/23 21:44:25.314
    Mar 27 21:44:25.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36281 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 21:44:25.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2912  bab9cf2e-7200-4719-bbfa-82bdadd72853 36281 0 2023-03-27 21:44:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-27 21:44:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:44:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2912" for this suite. 03/27/23 21:44:35.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:44:35.384
Mar 27 21:44:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:44:35.386
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:35.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:35.457
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-639ef0d1-8072-49dd-a11e-0315e0d981e9 03/27/23 21:44:35.465
STEP: Creating a pod to test consume secrets 03/27/23 21:44:35.479
Mar 27 21:44:35.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4" in namespace "projected-3710" to be "Succeeded or Failed"
Mar 27 21:44:35.523: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.890279ms
Mar 27 21:44:37.540: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033056017s
Mar 27 21:44:39.537: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030104925s
STEP: Saw pod success 03/27/23 21:44:39.537
Mar 27 21:44:39.538: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4" satisfied condition "Succeeded or Failed"
Mar 27 21:44:39.552: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:44:39.636
Mar 27 21:44:39.673: INFO: Waiting for pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 to disappear
Mar 27 21:44:39.688: INFO: Pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:44:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3710" for this suite. 03/27/23 21:44:39.705
------------------------------
• [4.342 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:44:35.384
    Mar 27 21:44:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:44:35.386
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:35.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:35.457
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-639ef0d1-8072-49dd-a11e-0315e0d981e9 03/27/23 21:44:35.465
    STEP: Creating a pod to test consume secrets 03/27/23 21:44:35.479
    Mar 27 21:44:35.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4" in namespace "projected-3710" to be "Succeeded or Failed"
    Mar 27 21:44:35.523: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.890279ms
    Mar 27 21:44:37.540: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033056017s
    Mar 27 21:44:39.537: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030104925s
    STEP: Saw pod success 03/27/23 21:44:39.537
    Mar 27 21:44:39.538: INFO: Pod "pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4" satisfied condition "Succeeded or Failed"
    Mar 27 21:44:39.552: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:44:39.636
    Mar 27 21:44:39.673: INFO: Waiting for pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 to disappear
    Mar 27 21:44:39.688: INFO: Pod pod-projected-secrets-cde87388-269c-4169-8358-2b90dc202ed4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:44:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3710" for this suite. 03/27/23 21:44:39.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:44:39.728
Mar 27 21:44:39.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:44:39.73
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:39.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:39.79
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:44:39.797
Mar 27 21:44:39.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c" in namespace "projected-8977" to be "Succeeded or Failed"
Mar 27 21:44:39.841: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.406283ms
Mar 27 21:44:41.860: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0328699s
Mar 27 21:44:43.857: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029493615s
STEP: Saw pod success 03/27/23 21:44:43.857
Mar 27 21:44:43.857: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c" satisfied condition "Succeeded or Failed"
Mar 27 21:44:43.871: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c container client-container: <nil>
STEP: delete the pod 03/27/23 21:44:43.901
Mar 27 21:44:43.938: INFO: Waiting for pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c to disappear
Mar 27 21:44:43.950: INFO: Pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:44:43.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8977" for this suite. 03/27/23 21:44:43.965
------------------------------
• [4.273 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:44:39.728
    Mar 27 21:44:39.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:44:39.73
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:39.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:39.79
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:44:39.797
    Mar 27 21:44:39.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c" in namespace "projected-8977" to be "Succeeded or Failed"
    Mar 27 21:44:39.841: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.406283ms
    Mar 27 21:44:41.860: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0328699s
    Mar 27 21:44:43.857: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029493615s
    STEP: Saw pod success 03/27/23 21:44:43.857
    Mar 27 21:44:43.857: INFO: Pod "downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c" satisfied condition "Succeeded or Failed"
    Mar 27 21:44:43.871: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c container client-container: <nil>
    STEP: delete the pod 03/27/23 21:44:43.901
    Mar 27 21:44:43.938: INFO: Waiting for pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c to disappear
    Mar 27 21:44:43.950: INFO: Pod downwardapi-volume-08ac62d3-7c8f-430d-86d2-b5f78f97460c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:44:43.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8977" for this suite. 03/27/23 21:44:43.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:44:44.008
Mar 27 21:44:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 21:44:44.01
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:44.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:44.064
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 in namespace container-probe-1694 03/27/23 21:44:44.072
Mar 27 21:44:44.103: INFO: Waiting up to 5m0s for pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03" in namespace "container-probe-1694" to be "not pending"
Mar 27 21:44:44.116: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03": Phase="Pending", Reason="", readiness=false. Elapsed: 13.649462ms
Mar 27 21:44:46.135: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03": Phase="Running", Reason="", readiness=true. Elapsed: 2.032568685s
Mar 27 21:44:46.135: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03" satisfied condition "not pending"
Mar 27 21:44:46.135: INFO: Started pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 in namespace container-probe-1694
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:44:46.135
Mar 27 21:44:46.172: INFO: Initial restart count of pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 is 0
Mar 27 21:45:06.380: INFO: Restart count of pod container-probe-1694/liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 is now 1 (20.207944783s elapsed)
STEP: deleting the pod 03/27/23 21:45:06.38
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 21:45:06.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1694" for this suite. 03/27/23 21:45:06.434
------------------------------
• [SLOW TEST] [22.461 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:44:44.008
    Mar 27 21:44:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 21:44:44.01
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:44:44.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:44:44.064
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 in namespace container-probe-1694 03/27/23 21:44:44.072
    Mar 27 21:44:44.103: INFO: Waiting up to 5m0s for pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03" in namespace "container-probe-1694" to be "not pending"
    Mar 27 21:44:44.116: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03": Phase="Pending", Reason="", readiness=false. Elapsed: 13.649462ms
    Mar 27 21:44:46.135: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03": Phase="Running", Reason="", readiness=true. Elapsed: 2.032568685s
    Mar 27 21:44:46.135: INFO: Pod "liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03" satisfied condition "not pending"
    Mar 27 21:44:46.135: INFO: Started pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 in namespace container-probe-1694
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:44:46.135
    Mar 27 21:44:46.172: INFO: Initial restart count of pod liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 is 0
    Mar 27 21:45:06.380: INFO: Restart count of pod container-probe-1694/liveness-4ef717d2-f848-4ddb-814c-9a8f9b830c03 is now 1 (20.207944783s elapsed)
    STEP: deleting the pod 03/27/23 21:45:06.38
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:45:06.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1694" for this suite. 03/27/23 21:45:06.434
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:45:06.47
Mar 27 21:45:06.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename subpath 03/27/23 21:45:06.472
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:06.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:06.53
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 21:45:06.538
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-g42c 03/27/23 21:45:06.567
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:45:06.568
Mar 27 21:45:06.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g42c" in namespace "subpath-5487" to be "Succeeded or Failed"
Mar 27 21:45:06.615: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026952ms
Mar 27 21:45:08.636: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 2.035302232s
Mar 27 21:45:10.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 4.031169386s
Mar 27 21:45:12.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 6.030671817s
Mar 27 21:45:14.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 8.028689195s
Mar 27 21:45:16.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 10.028396631s
Mar 27 21:45:18.631: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 12.030009917s
Mar 27 21:45:20.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 14.029120258s
Mar 27 21:45:22.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 16.030830518s
Mar 27 21:45:24.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 18.028998457s
Mar 27 21:45:26.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 20.030360509s
Mar 27 21:45:28.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=false. Elapsed: 22.029214583s
Mar 27 21:45:30.631: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.030023286s
STEP: Saw pod success 03/27/23 21:45:30.631
Mar 27 21:45:30.632: INFO: Pod "pod-subpath-test-configmap-g42c" satisfied condition "Succeeded or Failed"
Mar 27 21:45:30.645: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-configmap-g42c container test-container-subpath-configmap-g42c: <nil>
STEP: delete the pod 03/27/23 21:45:30.677
Mar 27 21:45:30.719: INFO: Waiting for pod pod-subpath-test-configmap-g42c to disappear
Mar 27 21:45:30.744: INFO: Pod pod-subpath-test-configmap-g42c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g42c 03/27/23 21:45:30.744
Mar 27 21:45:30.744: INFO: Deleting pod "pod-subpath-test-configmap-g42c" in namespace "subpath-5487"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 27 21:45:30.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5487" for this suite. 03/27/23 21:45:30.774
------------------------------
• [SLOW TEST] [24.330 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:45:06.47
    Mar 27 21:45:06.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename subpath 03/27/23 21:45:06.472
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:06.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:06.53
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 21:45:06.538
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-g42c 03/27/23 21:45:06.567
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 21:45:06.568
    Mar 27 21:45:06.601: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g42c" in namespace "subpath-5487" to be "Succeeded or Failed"
    Mar 27 21:45:06.615: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026952ms
    Mar 27 21:45:08.636: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 2.035302232s
    Mar 27 21:45:10.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 4.031169386s
    Mar 27 21:45:12.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 6.030671817s
    Mar 27 21:45:14.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 8.028689195s
    Mar 27 21:45:16.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 10.028396631s
    Mar 27 21:45:18.631: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 12.030009917s
    Mar 27 21:45:20.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 14.029120258s
    Mar 27 21:45:22.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 16.030830518s
    Mar 27 21:45:24.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 18.028998457s
    Mar 27 21:45:26.632: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=true. Elapsed: 20.030360509s
    Mar 27 21:45:28.630: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Running", Reason="", readiness=false. Elapsed: 22.029214583s
    Mar 27 21:45:30.631: INFO: Pod "pod-subpath-test-configmap-g42c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.030023286s
    STEP: Saw pod success 03/27/23 21:45:30.631
    Mar 27 21:45:30.632: INFO: Pod "pod-subpath-test-configmap-g42c" satisfied condition "Succeeded or Failed"
    Mar 27 21:45:30.645: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-configmap-g42c container test-container-subpath-configmap-g42c: <nil>
    STEP: delete the pod 03/27/23 21:45:30.677
    Mar 27 21:45:30.719: INFO: Waiting for pod pod-subpath-test-configmap-g42c to disappear
    Mar 27 21:45:30.744: INFO: Pod pod-subpath-test-configmap-g42c no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-g42c 03/27/23 21:45:30.744
    Mar 27 21:45:30.744: INFO: Deleting pod "pod-subpath-test-configmap-g42c" in namespace "subpath-5487"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:45:30.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5487" for this suite. 03/27/23 21:45:30.774
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:45:30.801
Mar 27 21:45:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:45:30.802
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:30.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:30.861
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Mar 27 21:45:30.928: INFO: created pod pod-service-account-defaultsa
Mar 27 21:45:30.928: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 27 21:45:30.946: INFO: created pod pod-service-account-mountsa
Mar 27 21:45:30.946: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 27 21:45:30.972: INFO: created pod pod-service-account-nomountsa
Mar 27 21:45:30.972: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 27 21:45:30.990: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 27 21:45:30.990: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 27 21:45:31.008: INFO: created pod pod-service-account-mountsa-mountspec
Mar 27 21:45:31.008: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 27 21:45:31.029: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 27 21:45:31.029: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 27 21:45:31.048: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 27 21:45:31.048: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 27 21:45:31.070: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 27 21:45:31.070: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 27 21:45:31.102: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 27 21:45:31.102: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 21:45:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4787" for this suite. 03/27/23 21:45:31.12
------------------------------
• [0.345 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:45:30.801
    Mar 27 21:45:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:45:30.802
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:30.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:30.861
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Mar 27 21:45:30.928: INFO: created pod pod-service-account-defaultsa
    Mar 27 21:45:30.928: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 27 21:45:30.946: INFO: created pod pod-service-account-mountsa
    Mar 27 21:45:30.946: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 27 21:45:30.972: INFO: created pod pod-service-account-nomountsa
    Mar 27 21:45:30.972: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 27 21:45:30.990: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 27 21:45:30.990: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 27 21:45:31.008: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 27 21:45:31.008: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 27 21:45:31.029: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 27 21:45:31.029: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 27 21:45:31.048: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 27 21:45:31.048: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 27 21:45:31.070: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 27 21:45:31.070: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 27 21:45:31.102: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 27 21:45:31.102: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:45:31.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4787" for this suite. 03/27/23 21:45:31.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:45:31.151
Mar 27 21:45:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:45:31.153
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:31.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:31.227
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4542 03/27/23 21:45:31.236
STEP: creating a selector 03/27/23 21:45:31.236
STEP: Creating the service pods in kubernetes 03/27/23 21:45:31.236
Mar 27 21:45:31.236: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 21:45:31.323: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4542" to be "running and ready"
Mar 27 21:45:31.341: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.350627ms
Mar 27 21:45:31.341: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:45:33.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.034091398s
Mar 27 21:45:33.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:35.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032660686s
Mar 27 21:45:35.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:37.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.034212374s
Mar 27 21:45:37.358: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:39.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.033369739s
Mar 27 21:45:39.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:41.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033499637s
Mar 27 21:45:41.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:43.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032710666s
Mar 27 21:45:43.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:45.354: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030960134s
Mar 27 21:45:45.354: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:47.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.0377713s
Mar 27 21:45:47.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:49.359: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03546945s
Mar 27 21:45:49.359: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:51.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.041755455s
Mar 27 21:45:51.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 21:45:53.354: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.03034482s
Mar 27 21:45:53.354: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 21:45:53.354: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 21:45:53.366: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4542" to be "running and ready"
Mar 27 21:45:53.380: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.188665ms
Mar 27 21:45:53.380: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 21:45:53.380: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 21:45:53.404: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4542" to be "running and ready"
Mar 27 21:45:53.418: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.172552ms
Mar 27 21:45:53.418: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 21:45:53.418: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 21:45:53.431
Mar 27 21:45:53.450: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4542" to be "running"
Mar 27 21:45:53.463: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.82221ms
Mar 27 21:45:55.478: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028047301s
Mar 27 21:45:57.482: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032446503s
Mar 27 21:45:57.482: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 21:45:57.497: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 21:45:57.497: INFO: Breadth first check of 172.30.56.113 on host 10.176.99.175...
Mar 27 21:45:57.512: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.56.113&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:45:57.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:45:57.513: INFO: ExecWithOptions: Clientset creation
Mar 27 21:45:57.513: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.56.113%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:45:57.662: INFO: Waiting for responses: map[]
Mar 27 21:45:57.662: INFO: reached 172.30.56.113 after 0/1 tries
Mar 27 21:45:57.663: INFO: Breadth first check of 172.30.85.152 on host 10.176.99.177...
Mar 27 21:45:57.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.85.152&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:45:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:45:57.678: INFO: ExecWithOptions: Clientset creation
Mar 27 21:45:57.678: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.85.152%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:45:57.829: INFO: Waiting for responses: map[]
Mar 27 21:45:57.830: INFO: reached 172.30.85.152 after 0/1 tries
Mar 27 21:45:57.830: INFO: Breadth first check of 172.30.4.96 on host 10.176.99.178...
Mar 27 21:45:57.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.4.96&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:45:57.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:45:57.846: INFO: ExecWithOptions: Clientset creation
Mar 27 21:45:57.846: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.4.96%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 27 21:45:58.004: INFO: Waiting for responses: map[]
Mar 27 21:45:58.005: INFO: reached 172.30.4.96 after 0/1 tries
Mar 27 21:45:58.005: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 27 21:45:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4542" for this suite. 03/27/23 21:45:58.04
------------------------------
• [SLOW TEST] [26.916 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:45:31.151
    Mar 27 21:45:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 21:45:31.153
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:31.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:31.227
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4542 03/27/23 21:45:31.236
    STEP: creating a selector 03/27/23 21:45:31.236
    STEP: Creating the service pods in kubernetes 03/27/23 21:45:31.236
    Mar 27 21:45:31.236: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 21:45:31.323: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4542" to be "running and ready"
    Mar 27 21:45:31.341: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.350627ms
    Mar 27 21:45:31.341: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:45:33.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.034091398s
    Mar 27 21:45:33.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:35.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032660686s
    Mar 27 21:45:35.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:37.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.034212374s
    Mar 27 21:45:37.358: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:39.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.033369739s
    Mar 27 21:45:39.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:41.357: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033499637s
    Mar 27 21:45:41.357: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:43.356: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.032710666s
    Mar 27 21:45:43.356: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:45.354: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030960134s
    Mar 27 21:45:45.354: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:47.361: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.0377713s
    Mar 27 21:45:47.361: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:49.359: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03546945s
    Mar 27 21:45:49.359: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:51.365: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.041755455s
    Mar 27 21:45:51.365: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 21:45:53.354: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.03034482s
    Mar 27 21:45:53.354: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 21:45:53.354: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 21:45:53.366: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4542" to be "running and ready"
    Mar 27 21:45:53.380: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 13.188665ms
    Mar 27 21:45:53.380: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 21:45:53.380: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 21:45:53.404: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4542" to be "running and ready"
    Mar 27 21:45:53.418: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.172552ms
    Mar 27 21:45:53.418: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 21:45:53.418: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 21:45:53.431
    Mar 27 21:45:53.450: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4542" to be "running"
    Mar 27 21:45:53.463: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.82221ms
    Mar 27 21:45:55.478: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028047301s
    Mar 27 21:45:57.482: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032446503s
    Mar 27 21:45:57.482: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 21:45:57.497: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 21:45:57.497: INFO: Breadth first check of 172.30.56.113 on host 10.176.99.175...
    Mar 27 21:45:57.512: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.56.113&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:45:57.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:45:57.513: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:45:57.513: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.56.113%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:45:57.662: INFO: Waiting for responses: map[]
    Mar 27 21:45:57.662: INFO: reached 172.30.56.113 after 0/1 tries
    Mar 27 21:45:57.663: INFO: Breadth first check of 172.30.85.152 on host 10.176.99.177...
    Mar 27 21:45:57.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.85.152&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:45:57.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:45:57.678: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:45:57.678: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.85.152%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:45:57.829: INFO: Waiting for responses: map[]
    Mar 27 21:45:57.830: INFO: reached 172.30.85.152 after 0/1 tries
    Mar 27 21:45:57.830: INFO: Breadth first check of 172.30.4.96 on host 10.176.99.178...
    Mar 27 21:45:57.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.85.155:9080/dial?request=hostname&protocol=udp&host=172.30.4.96&port=8081&tries=1'] Namespace:pod-network-test-4542 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:45:57.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:45:57.846: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:45:57.846: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4542/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.85.155%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.4.96%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 27 21:45:58.004: INFO: Waiting for responses: map[]
    Mar 27 21:45:58.005: INFO: reached 172.30.4.96 after 0/1 tries
    Mar 27 21:45:58.005: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:45:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4542" for this suite. 03/27/23 21:45:58.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:45:58.07
Mar 27 21:45:58.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:45:58.072
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:58.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:58.127
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6169 03/27/23 21:45:58.168
STEP: changing the ExternalName service to type=NodePort 03/27/23 21:45:58.183
STEP: creating replication controller externalname-service in namespace services-6169 03/27/23 21:45:58.252
I0327 21:45:58.268815      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6169, replica count: 2
I0327 21:46:01.320302      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 21:46:01.320: INFO: Creating new exec pod
Mar 27 21:46:01.353: INFO: Waiting up to 5m0s for pod "execpod9x6gp" in namespace "services-6169" to be "running"
Mar 27 21:46:01.375: INFO: Pod "execpod9x6gp": Phase="Pending", Reason="", readiness=false. Elapsed: 21.934173ms
Mar 27 21:46:03.394: INFO: Pod "execpod9x6gp": Phase="Running", Reason="", readiness=true. Elapsed: 2.040722111s
Mar 27 21:46:03.394: INFO: Pod "execpod9x6gp" satisfied condition "running"
Mar 27 21:46:04.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Mar 27 21:46:04.693: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 27 21:46:04.693: INFO: stdout: ""
Mar 27 21:46:04.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 172.21.79.59 80'
Mar 27 21:46:05.021: INFO: stderr: "+ nc -v -z -w 2 172.21.79.59 80\nConnection to 172.21.79.59 80 port [tcp/http] succeeded!\n"
Mar 27 21:46:05.021: INFO: stdout: ""
Mar 27 21:46:05.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 10.176.99.175 31977'
Mar 27 21:46:05.331: INFO: stderr: "+ nc -v -z -w 2 10.176.99.175 31977\nConnection to 10.176.99.175 31977 port [tcp/*] succeeded!\n"
Mar 27 21:46:05.331: INFO: stdout: ""
Mar 27 21:46:05.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 31977'
Mar 27 21:46:05.622: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 31977\nConnection to 10.176.99.177 31977 port [tcp/*] succeeded!\n"
Mar 27 21:46:05.622: INFO: stdout: ""
Mar 27 21:46:05.622: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:05.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6169" for this suite. 03/27/23 21:46:05.726
------------------------------
• [SLOW TEST] [7.678 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:45:58.07
    Mar 27 21:45:58.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:45:58.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:45:58.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:45:58.127
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6169 03/27/23 21:45:58.168
    STEP: changing the ExternalName service to type=NodePort 03/27/23 21:45:58.183
    STEP: creating replication controller externalname-service in namespace services-6169 03/27/23 21:45:58.252
    I0327 21:45:58.268815      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6169, replica count: 2
    I0327 21:46:01.320302      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 21:46:01.320: INFO: Creating new exec pod
    Mar 27 21:46:01.353: INFO: Waiting up to 5m0s for pod "execpod9x6gp" in namespace "services-6169" to be "running"
    Mar 27 21:46:01.375: INFO: Pod "execpod9x6gp": Phase="Pending", Reason="", readiness=false. Elapsed: 21.934173ms
    Mar 27 21:46:03.394: INFO: Pod "execpod9x6gp": Phase="Running", Reason="", readiness=true. Elapsed: 2.040722111s
    Mar 27 21:46:03.394: INFO: Pod "execpod9x6gp" satisfied condition "running"
    Mar 27 21:46:04.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Mar 27 21:46:04.693: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 27 21:46:04.693: INFO: stdout: ""
    Mar 27 21:46:04.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 172.21.79.59 80'
    Mar 27 21:46:05.021: INFO: stderr: "+ nc -v -z -w 2 172.21.79.59 80\nConnection to 172.21.79.59 80 port [tcp/http] succeeded!\n"
    Mar 27 21:46:05.021: INFO: stdout: ""
    Mar 27 21:46:05.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 10.176.99.175 31977'
    Mar 27 21:46:05.331: INFO: stderr: "+ nc -v -z -w 2 10.176.99.175 31977\nConnection to 10.176.99.175 31977 port [tcp/*] succeeded!\n"
    Mar 27 21:46:05.331: INFO: stdout: ""
    Mar 27 21:46:05.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6169 exec execpod9x6gp -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 31977'
    Mar 27 21:46:05.622: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 31977\nConnection to 10.176.99.177 31977 port [tcp/*] succeeded!\n"
    Mar 27 21:46:05.622: INFO: stdout: ""
    Mar 27 21:46:05.622: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:05.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6169" for this suite. 03/27/23 21:46:05.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:05.751
Mar 27 21:46:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename cronjob 03/27/23 21:46:05.753
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:05.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:05.821
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/27/23 21:46:05.83
STEP: creating 03/27/23 21:46:05.83
STEP: getting 03/27/23 21:46:05.842
STEP: listing 03/27/23 21:46:05.852
STEP: watching 03/27/23 21:46:05.864
Mar 27 21:46:05.864: INFO: starting watch
STEP: cluster-wide listing 03/27/23 21:46:05.867
STEP: cluster-wide watching 03/27/23 21:46:05.884
Mar 27 21:46:05.884: INFO: starting watch
STEP: patching 03/27/23 21:46:05.887
STEP: updating 03/27/23 21:46:05.907
Mar 27 21:46:05.937: INFO: waiting for watch events with expected annotations
Mar 27 21:46:05.937: INFO: saw patched and updated annotations
STEP: patching /status 03/27/23 21:46:05.938
STEP: updating /status 03/27/23 21:46:05.955
STEP: get /status 03/27/23 21:46:06.016
STEP: deleting 03/27/23 21:46:06.027
STEP: deleting a collection 03/27/23 21:46:06.076
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:06.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-178" for this suite. 03/27/23 21:46:06.147
------------------------------
• [0.418 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:05.751
    Mar 27 21:46:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename cronjob 03/27/23 21:46:05.753
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:05.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:05.821
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/27/23 21:46:05.83
    STEP: creating 03/27/23 21:46:05.83
    STEP: getting 03/27/23 21:46:05.842
    STEP: listing 03/27/23 21:46:05.852
    STEP: watching 03/27/23 21:46:05.864
    Mar 27 21:46:05.864: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 21:46:05.867
    STEP: cluster-wide watching 03/27/23 21:46:05.884
    Mar 27 21:46:05.884: INFO: starting watch
    STEP: patching 03/27/23 21:46:05.887
    STEP: updating 03/27/23 21:46:05.907
    Mar 27 21:46:05.937: INFO: waiting for watch events with expected annotations
    Mar 27 21:46:05.937: INFO: saw patched and updated annotations
    STEP: patching /status 03/27/23 21:46:05.938
    STEP: updating /status 03/27/23 21:46:05.955
    STEP: get /status 03/27/23 21:46:06.016
    STEP: deleting 03/27/23 21:46:06.027
    STEP: deleting a collection 03/27/23 21:46:06.076
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:06.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-178" for this suite. 03/27/23 21:46:06.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:06.173
Mar 27 21:46:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 21:46:06.175
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:06.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:06.23
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 03/27/23 21:46:06.237
STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:46:06.251
STEP: Creating a ResourceQuota with not best effort scope 03/27/23 21:46:08.263
STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:46:08.277
STEP: Creating a best-effort pod 03/27/23 21:46:10.289
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/27/23 21:46:10.33
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/27/23 21:46:12.343
STEP: Deleting the pod 03/27/23 21:46:14.353
STEP: Ensuring resource quota status released the pod usage 03/27/23 21:46:14.446
STEP: Creating a not best-effort pod 03/27/23 21:46:16.456
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/27/23 21:46:16.486
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/27/23 21:46:18.498
STEP: Deleting the pod 03/27/23 21:46:20.51
STEP: Ensuring resource quota status released the pod usage 03/27/23 21:46:20.55
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:22.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4254" for this suite. 03/27/23 21:46:22.58
------------------------------
• [SLOW TEST] [16.434 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:06.173
    Mar 27 21:46:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 21:46:06.175
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:06.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:06.23
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 03/27/23 21:46:06.237
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:46:06.251
    STEP: Creating a ResourceQuota with not best effort scope 03/27/23 21:46:08.263
    STEP: Ensuring ResourceQuota status is calculated 03/27/23 21:46:08.277
    STEP: Creating a best-effort pod 03/27/23 21:46:10.289
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/27/23 21:46:10.33
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/27/23 21:46:12.343
    STEP: Deleting the pod 03/27/23 21:46:14.353
    STEP: Ensuring resource quota status released the pod usage 03/27/23 21:46:14.446
    STEP: Creating a not best-effort pod 03/27/23 21:46:16.456
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/27/23 21:46:16.486
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/27/23 21:46:18.498
    STEP: Deleting the pod 03/27/23 21:46:20.51
    STEP: Ensuring resource quota status released the pod usage 03/27/23 21:46:20.55
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:22.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4254" for this suite. 03/27/23 21:46:22.58
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:22.608
Mar 27 21:46:22.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:46:22.61
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:22.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:22.692
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-cec533d5-1dd4-4699-87be-590c062d8619 03/27/23 21:46:22.722
STEP: Creating configMap with name cm-test-opt-upd-b575f62c-4f0c-47f3-a100-543f5e388115 03/27/23 21:46:22.736
STEP: Creating the pod 03/27/23 21:46:22.751
Mar 27 21:46:22.783: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06" in namespace "projected-6666" to be "running and ready"
Mar 27 21:46:22.802: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06": Phase="Pending", Reason="", readiness=false. Elapsed: 18.460114ms
Mar 27 21:46:22.802: INFO: The phase of Pod pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:46:24.817: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06": Phase="Running", Reason="", readiness=true. Elapsed: 2.03365177s
Mar 27 21:46:24.817: INFO: The phase of Pod pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06 is Running (Ready = true)
Mar 27 21:46:24.817: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-cec533d5-1dd4-4699-87be-590c062d8619 03/27/23 21:46:24.929
STEP: Updating configmap cm-test-opt-upd-b575f62c-4f0c-47f3-a100-543f5e388115 03/27/23 21:46:24.95
STEP: Creating configMap with name cm-test-opt-create-6c58e556-efe2-4d11-a466-eb4b2b029c7d 03/27/23 21:46:24.964
STEP: waiting to observe update in volume 03/27/23 21:46:24.977
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6666" for this suite. 03/27/23 21:46:27.106
------------------------------
• [4.521 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:22.608
    Mar 27 21:46:22.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:46:22.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:22.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:22.692
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-cec533d5-1dd4-4699-87be-590c062d8619 03/27/23 21:46:22.722
    STEP: Creating configMap with name cm-test-opt-upd-b575f62c-4f0c-47f3-a100-543f5e388115 03/27/23 21:46:22.736
    STEP: Creating the pod 03/27/23 21:46:22.751
    Mar 27 21:46:22.783: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06" in namespace "projected-6666" to be "running and ready"
    Mar 27 21:46:22.802: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06": Phase="Pending", Reason="", readiness=false. Elapsed: 18.460114ms
    Mar 27 21:46:22.802: INFO: The phase of Pod pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:46:24.817: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06": Phase="Running", Reason="", readiness=true. Elapsed: 2.03365177s
    Mar 27 21:46:24.817: INFO: The phase of Pod pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06 is Running (Ready = true)
    Mar 27 21:46:24.817: INFO: Pod "pod-projected-configmaps-a50ba9d3-704b-4f09-80cb-91f058433a06" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-cec533d5-1dd4-4699-87be-590c062d8619 03/27/23 21:46:24.929
    STEP: Updating configmap cm-test-opt-upd-b575f62c-4f0c-47f3-a100-543f5e388115 03/27/23 21:46:24.95
    STEP: Creating configMap with name cm-test-opt-create-6c58e556-efe2-4d11-a466-eb4b2b029c7d 03/27/23 21:46:24.964
    STEP: waiting to observe update in volume 03/27/23 21:46:24.977
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6666" for this suite. 03/27/23 21:46:27.106
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:27.131
Mar 27 21:46:27.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:46:27.132
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:27.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:27.19
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-a03be842-142e-45fa-9ffe-a09b40ff9e8a 03/27/23 21:46:27.201
STEP: Creating a pod to test consume configMaps 03/27/23 21:46:27.216
Mar 27 21:46:27.246: INFO: Waiting up to 5m0s for pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb" in namespace "configmap-2835" to be "Succeeded or Failed"
Mar 27 21:46:27.259: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.53247ms
Mar 27 21:46:29.274: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028505039s
Mar 27 21:46:31.276: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029969639s
STEP: Saw pod success 03/27/23 21:46:31.276
Mar 27 21:46:31.276: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb" satisfied condition "Succeeded or Failed"
Mar 27 21:46:31.289: INFO: Trying to get logs from node 10.176.99.175 pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:46:31.385
Mar 27 21:46:31.424: INFO: Waiting for pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb to disappear
Mar 27 21:46:31.437: INFO: Pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2835" for this suite. 03/27/23 21:46:31.455
------------------------------
• [4.353 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:27.131
    Mar 27 21:46:27.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:46:27.132
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:27.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:27.19
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-a03be842-142e-45fa-9ffe-a09b40ff9e8a 03/27/23 21:46:27.201
    STEP: Creating a pod to test consume configMaps 03/27/23 21:46:27.216
    Mar 27 21:46:27.246: INFO: Waiting up to 5m0s for pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb" in namespace "configmap-2835" to be "Succeeded or Failed"
    Mar 27 21:46:27.259: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.53247ms
    Mar 27 21:46:29.274: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028505039s
    Mar 27 21:46:31.276: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029969639s
    STEP: Saw pod success 03/27/23 21:46:31.276
    Mar 27 21:46:31.276: INFO: Pod "pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb" satisfied condition "Succeeded or Failed"
    Mar 27 21:46:31.289: INFO: Trying to get logs from node 10.176.99.175 pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:46:31.385
    Mar 27 21:46:31.424: INFO: Waiting for pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb to disappear
    Mar 27 21:46:31.437: INFO: Pod pod-configmaps-d15276a7-d778-4d5e-88d1-eaeadc35ddeb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2835" for this suite. 03/27/23 21:46:31.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:31.487
Mar 27 21:46:31.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:46:31.488
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:31.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:31.551
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/27/23 21:46:31.587
Mar 27 21:46:31.587: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4" in namespace "kubelet-test-4062" to be "completed"
Mar 27 21:46:31.604: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.320835ms
Mar 27 21:46:33.619: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031239477s
Mar 27 21:46:35.622: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034082377s
Mar 27 21:46:35.622: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:35.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4062" for this suite. 03/27/23 21:46:35.674
------------------------------
• [4.206 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:31.487
    Mar 27 21:46:31.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 21:46:31.488
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:31.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:31.551
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/27/23 21:46:31.587
    Mar 27 21:46:31.587: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4" in namespace "kubelet-test-4062" to be "completed"
    Mar 27 21:46:31.604: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.320835ms
    Mar 27 21:46:33.619: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031239477s
    Mar 27 21:46:35.622: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034082377s
    Mar 27 21:46:35.622: INFO: Pod "agnhost-host-aliases6efea793-fde9-4e97-b191-7c55e0828fe4" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:35.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4062" for this suite. 03/27/23 21:46:35.674
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:35.694
Mar 27 21:46:35.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:46:35.696
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:35.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:35.758
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-63b07ab9-8ad5-46e1-9c53-a773a3cc4cd8 03/27/23 21:46:35.765
STEP: Creating a pod to test consume configMaps 03/27/23 21:46:35.779
Mar 27 21:46:35.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652" in namespace "projected-6088" to be "Succeeded or Failed"
Mar 27 21:46:35.831: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Pending", Reason="", readiness=false. Elapsed: 13.237411ms
Mar 27 21:46:37.847: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028827847s
Mar 27 21:46:39.848: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030273549s
STEP: Saw pod success 03/27/23 21:46:39.848
Mar 27 21:46:39.848: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652" satisfied condition "Succeeded or Failed"
Mar 27 21:46:39.863: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/27/23 21:46:39.89
Mar 27 21:46:39.928: INFO: Waiting for pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 to disappear
Mar 27 21:46:39.943: INFO: Pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:39.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6088" for this suite. 03/27/23 21:46:39.966
------------------------------
• [4.294 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:35.694
    Mar 27 21:46:35.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:46:35.696
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:35.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:35.758
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-63b07ab9-8ad5-46e1-9c53-a773a3cc4cd8 03/27/23 21:46:35.765
    STEP: Creating a pod to test consume configMaps 03/27/23 21:46:35.779
    Mar 27 21:46:35.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652" in namespace "projected-6088" to be "Succeeded or Failed"
    Mar 27 21:46:35.831: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Pending", Reason="", readiness=false. Elapsed: 13.237411ms
    Mar 27 21:46:37.847: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028827847s
    Mar 27 21:46:39.848: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030273549s
    STEP: Saw pod success 03/27/23 21:46:39.848
    Mar 27 21:46:39.848: INFO: Pod "pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652" satisfied condition "Succeeded or Failed"
    Mar 27 21:46:39.863: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:46:39.89
    Mar 27 21:46:39.928: INFO: Waiting for pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 to disappear
    Mar 27 21:46:39.943: INFO: Pod pod-projected-configmaps-e00beee3-0068-4929-a0ce-190e644f5652 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:39.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6088" for this suite. 03/27/23 21:46:39.966
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:39.989
Mar 27 21:46:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:46:39.991
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:40.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:40.047
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-f66ac43b-a670-4305-8913-7100135664c4 03/27/23 21:46:40.056
STEP: Creating a pod to test consume secrets 03/27/23 21:46:40.071
Mar 27 21:46:40.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd" in namespace "projected-7091" to be "Succeeded or Failed"
Mar 27 21:46:40.119: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.624758ms
Mar 27 21:46:42.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033822095s
Mar 27 21:46:44.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033548143s
STEP: Saw pod success 03/27/23 21:46:44.136
Mar 27 21:46:44.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd" satisfied condition "Succeeded or Failed"
Mar 27 21:46:44.150: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:46:44.18
Mar 27 21:46:44.228: INFO: Waiting for pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd to disappear
Mar 27 21:46:44.242: INFO: Pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:44.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7091" for this suite. 03/27/23 21:46:44.259
------------------------------
• [4.297 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:39.989
    Mar 27 21:46:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:46:39.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:40.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:40.047
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-f66ac43b-a670-4305-8913-7100135664c4 03/27/23 21:46:40.056
    STEP: Creating a pod to test consume secrets 03/27/23 21:46:40.071
    Mar 27 21:46:40.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd" in namespace "projected-7091" to be "Succeeded or Failed"
    Mar 27 21:46:40.119: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.624758ms
    Mar 27 21:46:42.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033822095s
    Mar 27 21:46:44.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033548143s
    STEP: Saw pod success 03/27/23 21:46:44.136
    Mar 27 21:46:44.136: INFO: Pod "pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd" satisfied condition "Succeeded or Failed"
    Mar 27 21:46:44.150: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:46:44.18
    Mar 27 21:46:44.228: INFO: Waiting for pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd to disappear
    Mar 27 21:46:44.242: INFO: Pod pod-projected-secrets-1bd36584-3f6e-47a9-96d0-415f6b63e9fd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:44.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7091" for this suite. 03/27/23 21:46:44.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:44.287
Mar 27 21:46:44.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:46:44.288
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:44.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:44.339
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Mar 27 21:46:44.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:46:46.659
Mar 27 21:46:46.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 create -f -'
Mar 27 21:46:47.418: INFO: stderr: ""
Mar 27 21:46:47.418: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 27 21:46:47.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 delete e2e-test-crd-publish-openapi-1761-crds test-cr'
Mar 27 21:46:47.609: INFO: stderr: ""
Mar 27 21:46:47.609: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 27 21:46:47.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 apply -f -'
Mar 27 21:46:48.259: INFO: stderr: ""
Mar 27 21:46:48.259: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 27 21:46:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 delete e2e-test-crd-publish-openapi-1761-crds test-cr'
Mar 27 21:46:48.440: INFO: stderr: ""
Mar 27 21:46:48.440: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/27/23 21:46:48.44
Mar 27 21:46:48.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 explain e2e-test-crd-publish-openapi-1761-crds'
Mar 27 21:46:48.730: INFO: stderr: ""
Mar 27 21:46:48.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1761-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1073" for this suite. 03/27/23 21:46:50.906
------------------------------
• [SLOW TEST] [6.642 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:44.287
    Mar 27 21:46:44.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 21:46:44.288
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:44.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:44.339
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Mar 27 21:46:44.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/27/23 21:46:46.659
    Mar 27 21:46:46.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 create -f -'
    Mar 27 21:46:47.418: INFO: stderr: ""
    Mar 27 21:46:47.418: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 27 21:46:47.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 delete e2e-test-crd-publish-openapi-1761-crds test-cr'
    Mar 27 21:46:47.609: INFO: stderr: ""
    Mar 27 21:46:47.609: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 27 21:46:47.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 apply -f -'
    Mar 27 21:46:48.259: INFO: stderr: ""
    Mar 27 21:46:48.259: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 27 21:46:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 --namespace=crd-publish-openapi-1073 delete e2e-test-crd-publish-openapi-1761-crds test-cr'
    Mar 27 21:46:48.440: INFO: stderr: ""
    Mar 27 21:46:48.440: INFO: stdout: "e2e-test-crd-publish-openapi-1761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/27/23 21:46:48.44
    Mar 27 21:46:48.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-1073 explain e2e-test-crd-publish-openapi-1761-crds'
    Mar 27 21:46:48.730: INFO: stderr: ""
    Mar 27 21:46:48.730: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1761-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1073" for this suite. 03/27/23 21:46:50.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:50.933
Mar 27 21:46:50.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename discovery 03/27/23 21:46:50.935
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:50.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:51
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/27/23 21:46:51.017
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 27 21:46:51.280: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 27 21:46:51.287: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 27 21:46:51.287: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 27 21:46:51.287: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 27 21:46:51.288: INFO: Checking APIGroup: apps
Mar 27 21:46:51.293: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 27 21:46:51.293: INFO: Versions found [{apps/v1 v1}]
Mar 27 21:46:51.293: INFO: apps/v1 matches apps/v1
Mar 27 21:46:51.293: INFO: Checking APIGroup: events.k8s.io
Mar 27 21:46:51.298: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 27 21:46:51.298: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 27 21:46:51.298: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 27 21:46:51.298: INFO: Checking APIGroup: authentication.k8s.io
Mar 27 21:46:51.303: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 27 21:46:51.303: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 27 21:46:51.303: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 27 21:46:51.303: INFO: Checking APIGroup: authorization.k8s.io
Mar 27 21:46:51.308: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 27 21:46:51.308: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 27 21:46:51.308: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 27 21:46:51.308: INFO: Checking APIGroup: autoscaling
Mar 27 21:46:51.314: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 27 21:46:51.314: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Mar 27 21:46:51.314: INFO: autoscaling/v2 matches autoscaling/v2
Mar 27 21:46:51.314: INFO: Checking APIGroup: batch
Mar 27 21:46:51.319: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 27 21:46:51.319: INFO: Versions found [{batch/v1 v1}]
Mar 27 21:46:51.319: INFO: batch/v1 matches batch/v1
Mar 27 21:46:51.319: INFO: Checking APIGroup: certificates.k8s.io
Mar 27 21:46:51.324: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 27 21:46:51.324: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 27 21:46:51.324: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 27 21:46:51.324: INFO: Checking APIGroup: networking.k8s.io
Mar 27 21:46:51.329: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 27 21:46:51.329: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 27 21:46:51.329: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 27 21:46:51.329: INFO: Checking APIGroup: policy
Mar 27 21:46:51.334: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 27 21:46:51.334: INFO: Versions found [{policy/v1 v1}]
Mar 27 21:46:51.334: INFO: policy/v1 matches policy/v1
Mar 27 21:46:51.335: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 27 21:46:51.339: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 27 21:46:51.340: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 27 21:46:51.340: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 27 21:46:51.340: INFO: Checking APIGroup: storage.k8s.io
Mar 27 21:46:51.344: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 27 21:46:51.344: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 27 21:46:51.344: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 27 21:46:51.344: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 27 21:46:51.350: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 27 21:46:51.350: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 27 21:46:51.350: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 27 21:46:51.350: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 27 21:46:51.355: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 27 21:46:51.355: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 27 21:46:51.355: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 27 21:46:51.355: INFO: Checking APIGroup: scheduling.k8s.io
Mar 27 21:46:51.359: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 27 21:46:51.359: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 27 21:46:51.359: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 27 21:46:51.359: INFO: Checking APIGroup: coordination.k8s.io
Mar 27 21:46:51.364: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 27 21:46:51.364: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 27 21:46:51.364: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 27 21:46:51.364: INFO: Checking APIGroup: node.k8s.io
Mar 27 21:46:51.368: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 27 21:46:51.368: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 27 21:46:51.368: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 27 21:46:51.369: INFO: Checking APIGroup: discovery.k8s.io
Mar 27 21:46:51.374: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 27 21:46:51.374: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 27 21:46:51.374: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 27 21:46:51.374: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 27 21:46:51.392: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Mar 27 21:46:51.392: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Mar 27 21:46:51.392: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Mar 27 21:46:51.393: INFO: Checking APIGroup: crd.projectcalico.org
Mar 27 21:46:51.397: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar 27 21:46:51.397: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar 27 21:46:51.397: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar 27 21:46:51.397: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 27 21:46:51.402: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar 27 21:46:51.402: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Mar 27 21:46:51.402: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar 27 21:46:51.402: INFO: Checking APIGroup: ibm.com
Mar 27 21:46:51.406: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Mar 27 21:46:51.407: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Mar 27 21:46:51.407: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Mar 27 21:46:51.407: INFO: Checking APIGroup: metrics.k8s.io
Mar 27 21:46:51.412: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 27 21:46:51.412: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 27 21:46:51.412: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:51.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3239" for this suite. 03/27/23 21:46:51.442
------------------------------
• [0.534 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:50.933
    Mar 27 21:46:50.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename discovery 03/27/23 21:46:50.935
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:50.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:51
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/27/23 21:46:51.017
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 27 21:46:51.280: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 27 21:46:51.287: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 27 21:46:51.287: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 27 21:46:51.287: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 27 21:46:51.288: INFO: Checking APIGroup: apps
    Mar 27 21:46:51.293: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 27 21:46:51.293: INFO: Versions found [{apps/v1 v1}]
    Mar 27 21:46:51.293: INFO: apps/v1 matches apps/v1
    Mar 27 21:46:51.293: INFO: Checking APIGroup: events.k8s.io
    Mar 27 21:46:51.298: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 27 21:46:51.298: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 27 21:46:51.298: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 27 21:46:51.298: INFO: Checking APIGroup: authentication.k8s.io
    Mar 27 21:46:51.303: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 27 21:46:51.303: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 27 21:46:51.303: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 27 21:46:51.303: INFO: Checking APIGroup: authorization.k8s.io
    Mar 27 21:46:51.308: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 27 21:46:51.308: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 27 21:46:51.308: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 27 21:46:51.308: INFO: Checking APIGroup: autoscaling
    Mar 27 21:46:51.314: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 27 21:46:51.314: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Mar 27 21:46:51.314: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 27 21:46:51.314: INFO: Checking APIGroup: batch
    Mar 27 21:46:51.319: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 27 21:46:51.319: INFO: Versions found [{batch/v1 v1}]
    Mar 27 21:46:51.319: INFO: batch/v1 matches batch/v1
    Mar 27 21:46:51.319: INFO: Checking APIGroup: certificates.k8s.io
    Mar 27 21:46:51.324: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 27 21:46:51.324: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 27 21:46:51.324: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 27 21:46:51.324: INFO: Checking APIGroup: networking.k8s.io
    Mar 27 21:46:51.329: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 27 21:46:51.329: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 27 21:46:51.329: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 27 21:46:51.329: INFO: Checking APIGroup: policy
    Mar 27 21:46:51.334: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 27 21:46:51.334: INFO: Versions found [{policy/v1 v1}]
    Mar 27 21:46:51.334: INFO: policy/v1 matches policy/v1
    Mar 27 21:46:51.335: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 27 21:46:51.339: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 27 21:46:51.340: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 27 21:46:51.340: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 27 21:46:51.340: INFO: Checking APIGroup: storage.k8s.io
    Mar 27 21:46:51.344: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 27 21:46:51.344: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 27 21:46:51.344: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 27 21:46:51.344: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 27 21:46:51.350: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 27 21:46:51.350: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 27 21:46:51.350: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 27 21:46:51.350: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 27 21:46:51.355: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 27 21:46:51.355: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 27 21:46:51.355: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 27 21:46:51.355: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 27 21:46:51.359: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 27 21:46:51.359: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 27 21:46:51.359: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 27 21:46:51.359: INFO: Checking APIGroup: coordination.k8s.io
    Mar 27 21:46:51.364: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 27 21:46:51.364: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 27 21:46:51.364: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 27 21:46:51.364: INFO: Checking APIGroup: node.k8s.io
    Mar 27 21:46:51.368: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 27 21:46:51.368: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 27 21:46:51.368: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 27 21:46:51.369: INFO: Checking APIGroup: discovery.k8s.io
    Mar 27 21:46:51.374: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 27 21:46:51.374: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 27 21:46:51.374: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 27 21:46:51.374: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 27 21:46:51.392: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Mar 27 21:46:51.392: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Mar 27 21:46:51.392: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Mar 27 21:46:51.393: INFO: Checking APIGroup: crd.projectcalico.org
    Mar 27 21:46:51.397: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Mar 27 21:46:51.397: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Mar 27 21:46:51.397: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Mar 27 21:46:51.397: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar 27 21:46:51.402: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar 27 21:46:51.402: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Mar 27 21:46:51.402: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar 27 21:46:51.402: INFO: Checking APIGroup: ibm.com
    Mar 27 21:46:51.406: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Mar 27 21:46:51.407: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Mar 27 21:46:51.407: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Mar 27 21:46:51.407: INFO: Checking APIGroup: metrics.k8s.io
    Mar 27 21:46:51.412: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 27 21:46:51.412: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 27 21:46:51.412: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:51.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3239" for this suite. 03/27/23 21:46:51.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:51.482
Mar 27 21:46:51.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:46:51.485
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:51.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:51.549
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:46:51.603
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:46:52.462
STEP: Deploying the webhook pod 03/27/23 21:46:52.487
STEP: Wait for the deployment to be ready 03/27/23 21:46:52.524
Mar 27 21:46:52.559: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:46:54.627
STEP: Verifying the service has paired with the endpoint 03/27/23 21:46:54.661
Mar 27 21:46:55.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Mar 27 21:46:55.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7142-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 21:46:56.215
STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 21:46:56.288
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:46:59.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-709" for this suite. 03/27/23 21:46:59.242
STEP: Destroying namespace "webhook-709-markers" for this suite. 03/27/23 21:46:59.267
------------------------------
• [SLOW TEST] [7.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:51.482
    Mar 27 21:46:51.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:46:51.485
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:51.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:51.549
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:46:51.603
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:46:52.462
    STEP: Deploying the webhook pod 03/27/23 21:46:52.487
    STEP: Wait for the deployment to be ready 03/27/23 21:46:52.524
    Mar 27 21:46:52.559: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:46:54.627
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:46:54.661
    Mar 27 21:46:55.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Mar 27 21:46:55.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7142-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 21:46:56.215
    STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 21:46:56.288
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:46:59.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-709" for this suite. 03/27/23 21:46:59.242
    STEP: Destroying namespace "webhook-709-markers" for this suite. 03/27/23 21:46:59.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:46:59.293
Mar 27 21:46:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 21:46:59.294
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:59.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:59.358
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 03/27/23 21:46:59.376
Mar 27 21:46:59.398: INFO: Waiting up to 5m0s for pod "pod-lf4z9" in namespace "pods-478" to be "running"
Mar 27 21:46:59.409: INFO: Pod "pod-lf4z9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.261502ms
Mar 27 21:47:01.440: INFO: Pod "pod-lf4z9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042351508s
Mar 27 21:47:01.440: INFO: Pod "pod-lf4z9" satisfied condition "running"
STEP: patching /status 03/27/23 21:47:01.44
Mar 27 21:47:01.463: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:01.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-478" for this suite. 03/27/23 21:47:01.482
------------------------------
• [2.217 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:46:59.293
    Mar 27 21:46:59.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 21:46:59.294
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:46:59.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:46:59.358
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 03/27/23 21:46:59.376
    Mar 27 21:46:59.398: INFO: Waiting up to 5m0s for pod "pod-lf4z9" in namespace "pods-478" to be "running"
    Mar 27 21:46:59.409: INFO: Pod "pod-lf4z9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.261502ms
    Mar 27 21:47:01.440: INFO: Pod "pod-lf4z9": Phase="Running", Reason="", readiness=true. Elapsed: 2.042351508s
    Mar 27 21:47:01.440: INFO: Pod "pod-lf4z9" satisfied condition "running"
    STEP: patching /status 03/27/23 21:47:01.44
    Mar 27 21:47:01.463: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:01.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-478" for this suite. 03/27/23 21:47:01.482
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:01.511
Mar 27 21:47:01.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 21:47:01.513
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:01.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:01.586
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 03/27/23 21:47:01.597
STEP: Ensuring job reaches completions 03/27/23 21:47:01.614
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9037" for this suite. 03/27/23 21:47:13.648
------------------------------
• [SLOW TEST] [12.162 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:01.511
    Mar 27 21:47:01.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 21:47:01.513
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:01.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:01.586
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 03/27/23 21:47:01.597
    STEP: Ensuring job reaches completions 03/27/23 21:47:01.614
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9037" for this suite. 03/27/23 21:47:13.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:13.684
Mar 27 21:47:13.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename certificates 03/27/23 21:47:13.685
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:13.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:13.751
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/27/23 21:47:14.518
STEP: getting /apis/certificates.k8s.io 03/27/23 21:47:14.529
STEP: getting /apis/certificates.k8s.io/v1 03/27/23 21:47:14.536
STEP: creating 03/27/23 21:47:14.541
STEP: getting 03/27/23 21:47:14.604
STEP: listing 03/27/23 21:47:14.618
STEP: watching 03/27/23 21:47:14.648
Mar 27 21:47:14.648: INFO: starting watch
STEP: patching 03/27/23 21:47:14.653
STEP: updating 03/27/23 21:47:14.674
Mar 27 21:47:14.695: INFO: waiting for watch events with expected annotations
Mar 27 21:47:14.695: INFO: saw patched and updated annotations
STEP: getting /approval 03/27/23 21:47:14.696
STEP: patching /approval 03/27/23 21:47:14.713
STEP: updating /approval 03/27/23 21:47:14.733
STEP: getting /status 03/27/23 21:47:14.754
STEP: patching /status 03/27/23 21:47:14.77
STEP: updating /status 03/27/23 21:47:14.796
STEP: deleting 03/27/23 21:47:14.84
STEP: deleting a collection 03/27/23 21:47:14.898
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:14.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-8551" for this suite. 03/27/23 21:47:14.992
------------------------------
• [1.331 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:13.684
    Mar 27 21:47:13.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename certificates 03/27/23 21:47:13.685
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:13.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:13.751
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/27/23 21:47:14.518
    STEP: getting /apis/certificates.k8s.io 03/27/23 21:47:14.529
    STEP: getting /apis/certificates.k8s.io/v1 03/27/23 21:47:14.536
    STEP: creating 03/27/23 21:47:14.541
    STEP: getting 03/27/23 21:47:14.604
    STEP: listing 03/27/23 21:47:14.618
    STEP: watching 03/27/23 21:47:14.648
    Mar 27 21:47:14.648: INFO: starting watch
    STEP: patching 03/27/23 21:47:14.653
    STEP: updating 03/27/23 21:47:14.674
    Mar 27 21:47:14.695: INFO: waiting for watch events with expected annotations
    Mar 27 21:47:14.695: INFO: saw patched and updated annotations
    STEP: getting /approval 03/27/23 21:47:14.696
    STEP: patching /approval 03/27/23 21:47:14.713
    STEP: updating /approval 03/27/23 21:47:14.733
    STEP: getting /status 03/27/23 21:47:14.754
    STEP: patching /status 03/27/23 21:47:14.77
    STEP: updating /status 03/27/23 21:47:14.796
    STEP: deleting 03/27/23 21:47:14.84
    STEP: deleting a collection 03/27/23 21:47:14.898
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:14.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-8551" for this suite. 03/27/23 21:47:14.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:15.036
Mar 27 21:47:15.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:47:15.038
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:15.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:15.101
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:15.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1094" for this suite. 03/27/23 21:47:15.291
------------------------------
• [0.274 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:15.036
    Mar 27 21:47:15.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:47:15.038
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:15.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:15.101
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:15.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1094" for this suite. 03/27/23 21:47:15.291
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:15.311
Mar 27 21:47:15.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:47:15.313
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:15.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:15.379
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Mar 27 21:47:15.432: INFO: Waiting up to 5m0s for pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d" in namespace "svcaccounts-3803" to be "running"
Mar 27 21:47:15.444: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614175ms
Mar 27 21:47:17.457: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.02426184s
Mar 27 21:47:17.457: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d" satisfied condition "running"
STEP: reading a file in the container 03/27/23 21:47:17.457
Mar 27 21:47:17.457: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/27/23 21:47:17.749
Mar 27 21:47:17.749: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/27/23 21:47:18.014
Mar 27 21:47:18.014: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 27 21:47:18.361: INFO: Got root ca configmap in namespace "svcaccounts-3803"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3803" for this suite. 03/27/23 21:47:18.387
------------------------------
• [3.122 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:15.311
    Mar 27 21:47:15.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 21:47:15.313
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:15.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:15.379
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Mar 27 21:47:15.432: INFO: Waiting up to 5m0s for pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d" in namespace "svcaccounts-3803" to be "running"
    Mar 27 21:47:15.444: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614175ms
    Mar 27 21:47:17.457: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.02426184s
    Mar 27 21:47:17.457: INFO: Pod "pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d" satisfied condition "running"
    STEP: reading a file in the container 03/27/23 21:47:17.457
    Mar 27 21:47:17.457: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/27/23 21:47:17.749
    Mar 27 21:47:17.749: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/27/23 21:47:18.014
    Mar 27 21:47:18.014: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3803 pod-service-account-7ee5eadd-e7c0-4546-827a-84715fad6a2d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 27 21:47:18.361: INFO: Got root ca configmap in namespace "svcaccounts-3803"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3803" for this suite. 03/27/23 21:47:18.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:18.436
Mar 27 21:47:18.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:47:18.438
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:18.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:18.506
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 21:47:18.517
Mar 27 21:47:18.541: INFO: Waiting up to 5m0s for pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef" in namespace "emptydir-4491" to be "Succeeded or Failed"
Mar 27 21:47:18.552: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.555018ms
Mar 27 21:47:20.565: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024554917s
Mar 27 21:47:22.567: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025839709s
STEP: Saw pod success 03/27/23 21:47:22.567
Mar 27 21:47:22.567: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef" satisfied condition "Succeeded or Failed"
Mar 27 21:47:22.579: INFO: Trying to get logs from node 10.176.99.175 pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef container test-container: <nil>
STEP: delete the pod 03/27/23 21:47:22.654
Mar 27 21:47:22.696: INFO: Waiting for pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef to disappear
Mar 27 21:47:22.718: INFO: Pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4491" for this suite. 03/27/23 21:47:22.736
------------------------------
• [4.324 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:18.436
    Mar 27 21:47:18.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:47:18.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:18.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:18.506
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/27/23 21:47:18.517
    Mar 27 21:47:18.541: INFO: Waiting up to 5m0s for pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef" in namespace "emptydir-4491" to be "Succeeded or Failed"
    Mar 27 21:47:18.552: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Pending", Reason="", readiness=false. Elapsed: 11.555018ms
    Mar 27 21:47:20.565: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024554917s
    Mar 27 21:47:22.567: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025839709s
    STEP: Saw pod success 03/27/23 21:47:22.567
    Mar 27 21:47:22.567: INFO: Pod "pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef" satisfied condition "Succeeded or Failed"
    Mar 27 21:47:22.579: INFO: Trying to get logs from node 10.176.99.175 pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef container test-container: <nil>
    STEP: delete the pod 03/27/23 21:47:22.654
    Mar 27 21:47:22.696: INFO: Waiting for pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef to disappear
    Mar 27 21:47:22.718: INFO: Pod pod-9fe3e1be-e3d6-406e-87f2-7db3f79b48ef no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4491" for this suite. 03/27/23 21:47:22.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:22.764
Mar 27 21:47:22.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:47:22.766
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:22.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:22.829
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-7100eb8f-d7d2-49c1-ae0e-c88a4391709a 03/27/23 21:47:22.841
STEP: Creating a pod to test consume secrets 03/27/23 21:47:22.855
Mar 27 21:47:22.878: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6" in namespace "projected-372" to be "Succeeded or Failed"
Mar 27 21:47:22.890: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.351641ms
Mar 27 21:47:24.903: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02466079s
Mar 27 21:47:26.905: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026539315s
STEP: Saw pod success 03/27/23 21:47:26.905
Mar 27 21:47:26.905: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6" satisfied condition "Succeeded or Failed"
Mar 27 21:47:26.917: INFO: Trying to get logs from node 10.176.99.175 pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:47:26.945
Mar 27 21:47:26.989: INFO: Waiting for pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 to disappear
Mar 27 21:47:26.999: INFO: Pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:26.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-372" for this suite. 03/27/23 21:47:27.023
------------------------------
• [4.283 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:22.764
    Mar 27 21:47:22.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:47:22.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:22.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:22.829
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-7100eb8f-d7d2-49c1-ae0e-c88a4391709a 03/27/23 21:47:22.841
    STEP: Creating a pod to test consume secrets 03/27/23 21:47:22.855
    Mar 27 21:47:22.878: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6" in namespace "projected-372" to be "Succeeded or Failed"
    Mar 27 21:47:22.890: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.351641ms
    Mar 27 21:47:24.903: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02466079s
    Mar 27 21:47:26.905: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026539315s
    STEP: Saw pod success 03/27/23 21:47:26.905
    Mar 27 21:47:26.905: INFO: Pod "pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6" satisfied condition "Succeeded or Failed"
    Mar 27 21:47:26.917: INFO: Trying to get logs from node 10.176.99.175 pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:47:26.945
    Mar 27 21:47:26.989: INFO: Waiting for pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 to disappear
    Mar 27 21:47:26.999: INFO: Pod pod-projected-secrets-1e36582c-c3db-4dcf-9b7b-1681767f3ba6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:26.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-372" for this suite. 03/27/23 21:47:27.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:27.048
Mar 27 21:47:27.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 21:47:27.05
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:27.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:27.116
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Mar 27 21:47:27.160: INFO: Waiting up to 5m0s for pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9" in namespace "container-probe-2130" to be "running and ready"
Mar 27 21:47:27.174: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.548341ms
Mar 27 21:47:27.174: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:47:29.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 2.028008605s
Mar 27 21:47:29.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:31.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 4.02775901s
Mar 27 21:47:31.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:33.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 6.026509876s
Mar 27 21:47:33.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:35.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 8.026447913s
Mar 27 21:47:35.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:37.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 10.027921396s
Mar 27 21:47:37.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:39.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 12.027135621s
Mar 27 21:47:39.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:41.189: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 14.028845363s
Mar 27 21:47:41.189: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:43.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 16.027580318s
Mar 27 21:47:43.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:45.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 18.02567154s
Mar 27 21:47:45.186: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:47.191: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 20.031113867s
Mar 27 21:47:47.191: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
Mar 27 21:47:49.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=true. Elapsed: 22.025844751s
Mar 27 21:47:49.186: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = true)
Mar 27 21:47:49.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9" satisfied condition "running and ready"
Mar 27 21:47:49.198: INFO: Container started at 2023-03-27 21:47:28 +0000 UTC, pod became ready at 2023-03-27 21:47:47 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2130" for this suite. 03/27/23 21:47:49.217
------------------------------
• [SLOW TEST] [22.191 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:27.048
    Mar 27 21:47:27.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 21:47:27.05
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:27.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:27.116
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Mar 27 21:47:27.160: INFO: Waiting up to 5m0s for pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9" in namespace "container-probe-2130" to be "running and ready"
    Mar 27 21:47:27.174: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.548341ms
    Mar 27 21:47:27.174: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:47:29.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 2.028008605s
    Mar 27 21:47:29.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:31.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 4.02775901s
    Mar 27 21:47:31.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:33.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 6.026509876s
    Mar 27 21:47:33.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:35.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 8.026447913s
    Mar 27 21:47:35.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:37.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 10.027921396s
    Mar 27 21:47:37.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:39.187: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 12.027135621s
    Mar 27 21:47:39.187: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:41.189: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 14.028845363s
    Mar 27 21:47:41.189: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:43.188: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 16.027580318s
    Mar 27 21:47:43.188: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:45.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 18.02567154s
    Mar 27 21:47:45.186: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:47.191: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=false. Elapsed: 20.031113867s
    Mar 27 21:47:47.191: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = false)
    Mar 27 21:47:49.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9": Phase="Running", Reason="", readiness=true. Elapsed: 22.025844751s
    Mar 27 21:47:49.186: INFO: The phase of Pod test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 is Running (Ready = true)
    Mar 27 21:47:49.186: INFO: Pod "test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9" satisfied condition "running and ready"
    Mar 27 21:47:49.198: INFO: Container started at 2023-03-27 21:47:28 +0000 UTC, pod became ready at 2023-03-27 21:47:47 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2130" for this suite. 03/27/23 21:47:49.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:49.241
Mar 27 21:47:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:47:49.242
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:49.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:49.31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:47:49.331
Mar 27 21:47:49.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff" in namespace "downward-api-900" to be "Succeeded or Failed"
Mar 27 21:47:49.366: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.418867ms
Mar 27 21:47:51.382: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027549443s
Mar 27 21:47:53.381: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026040196s
STEP: Saw pod success 03/27/23 21:47:53.381
Mar 27 21:47:53.381: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff" satisfied condition "Succeeded or Failed"
Mar 27 21:47:53.400: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff container client-container: <nil>
STEP: delete the pod 03/27/23 21:47:53.493
Mar 27 21:47:53.529: INFO: Waiting for pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff to disappear
Mar 27 21:47:53.541: INFO: Pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:47:53.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-900" for this suite. 03/27/23 21:47:53.592
------------------------------
• [4.379 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:49.241
    Mar 27 21:47:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:47:49.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:49.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:49.31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:47:49.331
    Mar 27 21:47:49.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff" in namespace "downward-api-900" to be "Succeeded or Failed"
    Mar 27 21:47:49.366: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Pending", Reason="", readiness=false. Elapsed: 11.418867ms
    Mar 27 21:47:51.382: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027549443s
    Mar 27 21:47:53.381: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026040196s
    STEP: Saw pod success 03/27/23 21:47:53.381
    Mar 27 21:47:53.381: INFO: Pod "downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff" satisfied condition "Succeeded or Failed"
    Mar 27 21:47:53.400: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff container client-container: <nil>
    STEP: delete the pod 03/27/23 21:47:53.493
    Mar 27 21:47:53.529: INFO: Waiting for pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff to disappear
    Mar 27 21:47:53.541: INFO: Pod downwardapi-volume-becf068d-75c9-4376-9746-370da3a3feff no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:47:53.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-900" for this suite. 03/27/23 21:47:53.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:47:53.621
Mar 27 21:47:53.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-pred 03/27/23 21:47:53.623
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:53.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:53.689
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 27 21:47:53.701: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 21:47:53.734: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 21:47:53.747: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.175 before test
Mar 27 21:47:53.779: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:47:53.779: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:47:53.779: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:47:53.779: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:47:53.779: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:47:53.779: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:47:53.779: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:47:53.779: INFO: ingress-cluster-healthcheck-b895f86ff-nwcp2 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Mar 27 21:47:53.779: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:47:53.779: INFO: metrics-server-6c65f45547-ptg94 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (3 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:47:53.779: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:47:53.779: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container e2e ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:47:53.779: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:47:53.779: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.779: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar 27 21:47:53.779: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.177 before test
Mar 27 21:47:53.804: INFO: test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 from container-probe-2130 started at 2023-03-27 21:47:27 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container test-webserver ready: true, restart count 0
Mar 27 21:47:53.804: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:47:53.804: INFO: calico-typha-7f67cb7cc9-7v492 from kube-system started at 2023-03-27 21:41:56 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:47:53.804: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:47:53.804: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:47:53.804: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:47:53.804: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:47:53.804: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:47:53.804: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 21:47:53.804: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:47:53.804: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:47:53.804: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.178 before test
Mar 27 21:47:53.835: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr from ibm-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:47:53.835: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 21:47:53.835: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:47:53.835: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:47:53.835: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:47:53.835: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:47:53.835: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 21:47:53.835: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:47:53.835: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:47:53.835: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 27 21:47:53.835: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:47:53.835: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 27 21:47:53.835: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:47:53.835: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:47:53.835: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:47:53.835: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:47:53.835: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:47:53.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:47:53.835: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:47:53.835
Mar 27 21:47:53.859: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1784" to be "running"
Mar 27 21:47:53.870: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.539045ms
Mar 27 21:47:55.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024367963s
Mar 27 21:47:57.885: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.026242477s
Mar 27 21:47:57.885: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:47:57.898
STEP: Trying to apply a random label on the found node. 03/27/23 21:47:57.928
STEP: verifying the node has the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 42 03/27/23 21:47:57.954
STEP: Trying to relaunch the pod, now with labels. 03/27/23 21:47:57.966
Mar 27 21:47:57.983: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1784" to be "not pending"
Mar 27 21:47:57.996: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 12.726447ms
Mar 27 21:48:00.008: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.025396404s
Mar 27 21:48:00.008: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 off the node 10.176.99.177 03/27/23 21:48:00.02
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 03/27/23 21:48:00.071
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:48:00.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1784" for this suite. 03/27/23 21:48:00.102
------------------------------
• [SLOW TEST] [6.508 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:47:53.621
    Mar 27 21:47:53.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-pred 03/27/23 21:47:53.623
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:47:53.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:47:53.689
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 27 21:47:53.701: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 21:47:53.734: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 21:47:53.747: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.175 before test
    Mar 27 21:47:53.779: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: ingress-cluster-healthcheck-b895f86ff-nwcp2 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: metrics-server-6c65f45547-ptg94 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (3 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.779: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Mar 27 21:47:53.779: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.177 before test
    Mar 27 21:47:53.804: INFO: test-webserver-dd1bfdb0-ce9d-4ee9-ac27-9a02a3ae4aa9 from container-probe-2130 started at 2023-03-27 21:47:27 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container test-webserver ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: calico-typha-7f67cb7cc9-7v492 from kube-system started at 2023-03-27 21:41:56 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.804: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:47:53.804: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.178 before test
    Mar 27 21:47:53.835: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr from ibm-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:47:53.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:47:53.835: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/27/23 21:47:53.835
    Mar 27 21:47:53.859: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1784" to be "running"
    Mar 27 21:47:53.870: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.539045ms
    Mar 27 21:47:55.883: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024367963s
    Mar 27 21:47:57.885: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.026242477s
    Mar 27 21:47:57.885: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/27/23 21:47:57.898
    STEP: Trying to apply a random label on the found node. 03/27/23 21:47:57.928
    STEP: verifying the node has the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 42 03/27/23 21:47:57.954
    STEP: Trying to relaunch the pod, now with labels. 03/27/23 21:47:57.966
    Mar 27 21:47:57.983: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1784" to be "not pending"
    Mar 27 21:47:57.996: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 12.726447ms
    Mar 27 21:48:00.008: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.025396404s
    Mar 27 21:48:00.008: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 off the node 10.176.99.177 03/27/23 21:48:00.02
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-fb2888d1-df08-4d1f-aacb-4c97bcb11146 03/27/23 21:48:00.071
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:48:00.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1784" for this suite. 03/27/23 21:48:00.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:48:00.134
Mar 27 21:48:00.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 21:48:00.135
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:48:00.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:48:00.199
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5972 03/27/23 21:48:00.213
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 03/27/23 21:48:00.237
Mar 27 21:48:00.269: INFO: Found 0 stateful pods, waiting for 3
Mar 27 21:48:10.284: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:48:10.284: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:48:10.284: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 21:48:10.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:48:10.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:48:10.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:48:10.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/27/23 21:48:20.732
Mar 27 21:48:20.769: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/27/23 21:48:20.769
STEP: Updating Pods in reverse ordinal order 03/27/23 21:48:30.82
Mar 27 21:48:30.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:48:31.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:48:31.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:48:31.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:48:51.209: INFO: Waiting for StatefulSet statefulset-5972/ss2 to complete update
STEP: Rolling back to a previous revision 03/27/23 21:49:01.236
Mar 27 21:49:01.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 27 21:49:01.508: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 27 21:49:01.508: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 27 21:49:01.508: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 27 21:49:11.605: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/27/23 21:49:21.655
Mar 27 21:49:21.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 27 21:49:21.924: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 27 21:49:21.924: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 27 21:49:21.924: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 27 21:49:31.998: INFO: Waiting for StatefulSet statefulset-5972/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 21:49:42.027: INFO: Deleting all statefulset in ns statefulset-5972
Mar 27 21:49:42.040: INFO: Scaling statefulset ss2 to 0
Mar 27 21:49:52.093: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 21:49:52.105: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 21:49:52.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5972" for this suite. 03/27/23 21:49:52.17
------------------------------
• [SLOW TEST] [112.060 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:48:00.134
    Mar 27 21:48:00.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 21:48:00.135
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:48:00.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:48:00.199
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5972 03/27/23 21:48:00.213
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 03/27/23 21:48:00.237
    Mar 27 21:48:00.269: INFO: Found 0 stateful pods, waiting for 3
    Mar 27 21:48:10.284: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:48:10.284: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:48:10.284: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 21:48:10.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:48:10.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:48:10.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:48:10.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/27/23 21:48:20.732
    Mar 27 21:48:20.769: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/27/23 21:48:20.769
    STEP: Updating Pods in reverse ordinal order 03/27/23 21:48:30.82
    Mar 27 21:48:30.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:48:31.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:48:31.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:48:31.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:48:51.209: INFO: Waiting for StatefulSet statefulset-5972/ss2 to complete update
    STEP: Rolling back to a previous revision 03/27/23 21:49:01.236
    Mar 27 21:49:01.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 27 21:49:01.508: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 27 21:49:01.508: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 27 21:49:01.508: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 27 21:49:11.605: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/27/23 21:49:21.655
    Mar 27 21:49:21.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=statefulset-5972 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 27 21:49:21.924: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 27 21:49:21.924: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 27 21:49:21.924: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 27 21:49:31.998: INFO: Waiting for StatefulSet statefulset-5972/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 21:49:42.027: INFO: Deleting all statefulset in ns statefulset-5972
    Mar 27 21:49:42.040: INFO: Scaling statefulset ss2 to 0
    Mar 27 21:49:52.093: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 21:49:52.105: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:49:52.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5972" for this suite. 03/27/23 21:49:52.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:49:52.195
Mar 27 21:49:52.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-webhook 03/27/23 21:49:52.197
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:49:52.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:49:52.281
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/27/23 21:49:52.299
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 21:49:53.908
STEP: Deploying the custom resource conversion webhook pod 03/27/23 21:49:53.936
STEP: Wait for the deployment to be ready 03/27/23 21:49:53.972
Mar 27 21:49:54.008: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:49:56.062
STEP: Verifying the service has paired with the endpoint 03/27/23 21:49:56.112
Mar 27 21:49:57.114: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 27 21:49:57.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Creating a v1 custom resource 03/27/23 21:49:59.901
STEP: v2 custom resource should be converted 03/27/23 21:49:59.918
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:00.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4235" for this suite. 03/27/23 21:50:00.638
------------------------------
• [SLOW TEST] [8.466 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:49:52.195
    Mar 27 21:49:52.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-webhook 03/27/23 21:49:52.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:49:52.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:49:52.281
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/27/23 21:49:52.299
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/27/23 21:49:53.908
    STEP: Deploying the custom resource conversion webhook pod 03/27/23 21:49:53.936
    STEP: Wait for the deployment to be ready 03/27/23 21:49:53.972
    Mar 27 21:49:54.008: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:49:56.062
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:49:56.112
    Mar 27 21:49:57.114: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 27 21:49:57.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Creating a v1 custom resource 03/27/23 21:49:59.901
    STEP: v2 custom resource should be converted 03/27/23 21:49:59.918
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:00.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4235" for this suite. 03/27/23 21:50:00.638
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:00.661
Mar 27 21:50:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 21:50:00.663
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:00.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:00.725
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 03/27/23 21:50:00.831
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:50:00.847
Mar 27 21:50:00.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:50:00.874: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:50:01.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:50:01.910: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 21:50:02.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 27 21:50:02.908: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 21:50:03.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 27 21:50:03.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/27/23 21:50:03.916
Mar 27 21:50:03.927: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/27/23 21:50:03.927
Mar 27 21:50:03.950: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/27/23 21:50:03.95
Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: ADDED
Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.957: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.957: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.957: INFO: Found daemon set daemon-set in namespace daemonsets-5469 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 21:50:03.957: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/27/23 21:50:03.957
STEP: watching for the daemon set status to be patched 03/27/23 21:50:03.975
Mar 27 21:50:03.981: INFO: Observed &DaemonSet event: ADDED
Mar 27 21:50:03.981: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.982: INFO: Observed daemon set daemon-set in namespace daemonsets-5469 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
Mar 27 21:50:03.982: INFO: Found daemon set daemon-set in namespace daemonsets-5469 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 27 21:50:03.982: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:50:03.993
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5469, will wait for the garbage collector to delete the pods 03/27/23 21:50:03.993
Mar 27 21:50:04.075: INFO: Deleting DaemonSet.extensions daemon-set took: 18.714055ms
Mar 27 21:50:04.176: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.872276ms
Mar 27 21:50:06.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 21:50:06.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 21:50:06.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38542"},"items":null}

Mar 27 21:50:06.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38542"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5469" for this suite. 03/27/23 21:50:06.382
------------------------------
• [SLOW TEST] [5.743 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:00.661
    Mar 27 21:50:00.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 21:50:00.663
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:00.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:00.725
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 03/27/23 21:50:00.831
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 21:50:00.847
    Mar 27 21:50:00.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:50:00.874: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:50:01.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:50:01.910: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 21:50:02.908: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 27 21:50:02.908: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 21:50:03.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 27 21:50:03.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/27/23 21:50:03.916
    Mar 27 21:50:03.927: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/27/23 21:50:03.927
    Mar 27 21:50:03.950: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/27/23 21:50:03.95
    Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: ADDED
    Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.956: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.957: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.957: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.957: INFO: Found daemon set daemon-set in namespace daemonsets-5469 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 21:50:03.957: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/27/23 21:50:03.957
    STEP: watching for the daemon set status to be patched 03/27/23 21:50:03.975
    Mar 27 21:50:03.981: INFO: Observed &DaemonSet event: ADDED
    Mar 27 21:50:03.981: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.982: INFO: Observed daemon set daemon-set in namespace daemonsets-5469 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 27 21:50:03.982: INFO: Observed &DaemonSet event: MODIFIED
    Mar 27 21:50:03.982: INFO: Found daemon set daemon-set in namespace daemonsets-5469 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 27 21:50:03.982: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 21:50:03.993
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5469, will wait for the garbage collector to delete the pods 03/27/23 21:50:03.993
    Mar 27 21:50:04.075: INFO: Deleting DaemonSet.extensions daemon-set took: 18.714055ms
    Mar 27 21:50:04.176: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.872276ms
    Mar 27 21:50:06.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 21:50:06.287: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 21:50:06.296: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38542"},"items":null}

    Mar 27 21:50:06.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38542"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5469" for this suite. 03/27/23 21:50:06.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:06.406
Mar 27 21:50:06.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:50:06.407
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:06.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:06.486
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 21:50:06.497
Mar 27 21:50:06.520: INFO: Waiting up to 5m0s for pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6" in namespace "emptydir-5468" to be "Succeeded or Failed"
Mar 27 21:50:06.535: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382882ms
Mar 27 21:50:08.554: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033493452s
Mar 27 21:50:10.548: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027347636s
STEP: Saw pod success 03/27/23 21:50:10.548
Mar 27 21:50:10.548: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6" satisfied condition "Succeeded or Failed"
Mar 27 21:50:10.562: INFO: Trying to get logs from node 10.176.99.177 pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 container test-container: <nil>
STEP: delete the pod 03/27/23 21:50:10.641
Mar 27 21:50:10.677: INFO: Waiting for pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 to disappear
Mar 27 21:50:10.689: INFO: Pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5468" for this suite. 03/27/23 21:50:10.706
------------------------------
• [4.322 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:06.406
    Mar 27 21:50:06.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:50:06.407
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:06.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:06.486
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 21:50:06.497
    Mar 27 21:50:06.520: INFO: Waiting up to 5m0s for pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6" in namespace "emptydir-5468" to be "Succeeded or Failed"
    Mar 27 21:50:06.535: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.382882ms
    Mar 27 21:50:08.554: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033493452s
    Mar 27 21:50:10.548: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027347636s
    STEP: Saw pod success 03/27/23 21:50:10.548
    Mar 27 21:50:10.548: INFO: Pod "pod-2b2e3f1a-3071-4b72-94f7-3991853258d6" satisfied condition "Succeeded or Failed"
    Mar 27 21:50:10.562: INFO: Trying to get logs from node 10.176.99.177 pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 container test-container: <nil>
    STEP: delete the pod 03/27/23 21:50:10.641
    Mar 27 21:50:10.677: INFO: Waiting for pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 to disappear
    Mar 27 21:50:10.689: INFO: Pod pod-2b2e3f1a-3071-4b72-94f7-3991853258d6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:10.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5468" for this suite. 03/27/23 21:50:10.706
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:10.73
Mar 27 21:50:10.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 21:50:10.731
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:10.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:10.79
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 03/27/23 21:50:10.802
STEP: waiting for pod running 03/27/23 21:50:10.827
Mar 27 21:50:10.827: INFO: Waiting up to 2m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086" to be "running"
Mar 27 21:50:10.838: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.937507ms
Mar 27 21:50:12.850: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.022953627s
Mar 27 21:50:12.850: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" satisfied condition "running"
STEP: creating a file in subpath 03/27/23 21:50:12.85
Mar 27 21:50:12.863: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9086 PodName:var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:50:12.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:50:12.864: INFO: ExecWithOptions: Clientset creation
Mar 27 21:50:12.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9086/pods/var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/27/23 21:50:13.019
Mar 27 21:50:13.031: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9086 PodName:var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:50:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:50:13.032: INFO: ExecWithOptions: Clientset creation
Mar 27 21:50:13.033: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9086/pods/var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/27/23 21:50:13.188
Mar 27 21:50:13.725: INFO: Successfully updated pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe"
STEP: waiting for annotated pod running 03/27/23 21:50:13.726
Mar 27 21:50:13.726: INFO: Waiting up to 2m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086" to be "running"
Mar 27 21:50:13.738: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Running", Reason="", readiness=true. Elapsed: 11.767342ms
Mar 27 21:50:13.738: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 21:50:13.738
Mar 27 21:50:13.738: INFO: Deleting pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086"
Mar 27 21:50:13.759: INFO: Wait up to 5m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9086" for this suite. 03/27/23 21:50:47.802
------------------------------
• [SLOW TEST] [37.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:10.73
    Mar 27 21:50:10.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 21:50:10.731
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:10.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:10.79
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 03/27/23 21:50:10.802
    STEP: waiting for pod running 03/27/23 21:50:10.827
    Mar 27 21:50:10.827: INFO: Waiting up to 2m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086" to be "running"
    Mar 27 21:50:10.838: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.937507ms
    Mar 27 21:50:12.850: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.022953627s
    Mar 27 21:50:12.850: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" satisfied condition "running"
    STEP: creating a file in subpath 03/27/23 21:50:12.85
    Mar 27 21:50:12.863: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9086 PodName:var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:50:12.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:50:12.864: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:50:12.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9086/pods/var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/27/23 21:50:13.019
    Mar 27 21:50:13.031: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9086 PodName:var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:50:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:50:13.032: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:50:13.033: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-9086/pods/var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/27/23 21:50:13.188
    Mar 27 21:50:13.725: INFO: Successfully updated pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe"
    STEP: waiting for annotated pod running 03/27/23 21:50:13.726
    Mar 27 21:50:13.726: INFO: Waiting up to 2m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086" to be "running"
    Mar 27 21:50:13.738: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe": Phase="Running", Reason="", readiness=true. Elapsed: 11.767342ms
    Mar 27 21:50:13.738: INFO: Pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 21:50:13.738
    Mar 27 21:50:13.738: INFO: Deleting pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" in namespace "var-expansion-9086"
    Mar 27 21:50:13.759: INFO: Wait up to 5m0s for pod "var-expansion-848e28d2-4a68-4fb3-8ef4-6406772c3ffe" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9086" for this suite. 03/27/23 21:50:47.802
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:47.83
Mar 27 21:50:47.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption 03/27/23 21:50:47.831
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:47.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:47.927
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 03/27/23 21:50:47.936
STEP: Waiting for the pdb to be processed 03/27/23 21:50:47.951
STEP: updating the pdb 03/27/23 21:50:47.966
STEP: Waiting for the pdb to be processed 03/27/23 21:50:48.004
STEP: patching the pdb 03/27/23 21:50:50.056
STEP: Waiting for the pdb to be processed 03/27/23 21:50:50.09
STEP: Waiting for the pdb to be deleted 03/27/23 21:50:50.127
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9257" for this suite. 03/27/23 21:50:50.159
------------------------------
• [2.352 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:47.83
    Mar 27 21:50:47.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption 03/27/23 21:50:47.831
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:47.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:47.927
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 03/27/23 21:50:47.936
    STEP: Waiting for the pdb to be processed 03/27/23 21:50:47.951
    STEP: updating the pdb 03/27/23 21:50:47.966
    STEP: Waiting for the pdb to be processed 03/27/23 21:50:48.004
    STEP: patching the pdb 03/27/23 21:50:50.056
    STEP: Waiting for the pdb to be processed 03/27/23 21:50:50.09
    STEP: Waiting for the pdb to be deleted 03/27/23 21:50:50.127
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9257" for this suite. 03/27/23 21:50:50.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:50.185
Mar 27 21:50:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:50:50.186
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:50.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:50.25
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:50:50.262
Mar 27 21:50:50.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30" in namespace "projected-3431" to be "Succeeded or Failed"
Mar 27 21:50:50.301: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 14.689421ms
Mar 27 21:50:52.316: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02947762s
Mar 27 21:50:54.315: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02810171s
Mar 27 21:50:56.315: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028830426s
STEP: Saw pod success 03/27/23 21:50:56.316
Mar 27 21:50:56.316: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30" satisfied condition "Succeeded or Failed"
Mar 27 21:50:56.328: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 container client-container: <nil>
STEP: delete the pod 03/27/23 21:50:56.355
Mar 27 21:50:56.385: INFO: Waiting for pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 to disappear
Mar 27 21:50:56.397: INFO: Pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:50:56.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3431" for this suite. 03/27/23 21:50:56.414
------------------------------
• [SLOW TEST] [6.252 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:50.185
    Mar 27 21:50:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:50:50.186
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:50.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:50.25
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:50:50.262
    Mar 27 21:50:50.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30" in namespace "projected-3431" to be "Succeeded or Failed"
    Mar 27 21:50:50.301: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 14.689421ms
    Mar 27 21:50:52.316: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02947762s
    Mar 27 21:50:54.315: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02810171s
    Mar 27 21:50:56.315: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028830426s
    STEP: Saw pod success 03/27/23 21:50:56.316
    Mar 27 21:50:56.316: INFO: Pod "downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30" satisfied condition "Succeeded or Failed"
    Mar 27 21:50:56.328: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:50:56.355
    Mar 27 21:50:56.385: INFO: Waiting for pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 to disappear
    Mar 27 21:50:56.397: INFO: Pod downwardapi-volume-6a8115cc-bc6a-44d8-b7d9-bed76b81ba30 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:50:56.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3431" for this suite. 03/27/23 21:50:56.414
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:50:56.437
Mar 27 21:50:56.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 21:50:56.439
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:56.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:56.514
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-52caa0b3-83f1-4ef5-a326-c941a45956ec 03/27/23 21:50:56.525
STEP: Creating a pod to test consume configMaps 03/27/23 21:50:56.542
Mar 27 21:50:56.568: INFO: Waiting up to 5m0s for pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4" in namespace "configmap-4440" to be "Succeeded or Failed"
Mar 27 21:50:56.579: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.277279ms
Mar 27 21:50:58.592: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024060263s
Mar 27 21:51:00.593: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02540108s
STEP: Saw pod success 03/27/23 21:51:00.593
Mar 27 21:51:00.594: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4" satisfied condition "Succeeded or Failed"
Mar 27 21:51:00.634: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:51:00.663
Mar 27 21:51:00.692: INFO: Waiting for pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 to disappear
Mar 27 21:51:00.703: INFO: Pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:51:00.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4440" for this suite. 03/27/23 21:51:00.722
------------------------------
• [4.308 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:50:56.437
    Mar 27 21:50:56.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 21:50:56.439
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:50:56.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:50:56.514
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-52caa0b3-83f1-4ef5-a326-c941a45956ec 03/27/23 21:50:56.525
    STEP: Creating a pod to test consume configMaps 03/27/23 21:50:56.542
    Mar 27 21:50:56.568: INFO: Waiting up to 5m0s for pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4" in namespace "configmap-4440" to be "Succeeded or Failed"
    Mar 27 21:50:56.579: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.277279ms
    Mar 27 21:50:58.592: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024060263s
    Mar 27 21:51:00.593: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02540108s
    STEP: Saw pod success 03/27/23 21:51:00.593
    Mar 27 21:51:00.594: INFO: Pod "pod-configmaps-22706756-aa27-4339-afba-85fe552443c4" satisfied condition "Succeeded or Failed"
    Mar 27 21:51:00.634: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:51:00.663
    Mar 27 21:51:00.692: INFO: Waiting for pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 to disappear
    Mar 27 21:51:00.703: INFO: Pod pod-configmaps-22706756-aa27-4339-afba-85fe552443c4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:51:00.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4440" for this suite. 03/27/23 21:51:00.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:51:00.754
Mar 27 21:51:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename namespaces 03/27/23 21:51:00.755
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:00.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:00.825
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6340" 03/27/23 21:51:00.836
Mar 27 21:51:00.865: INFO: Namespace "namespaces-6340" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"d74a9d93-502c-4e4a-9eef-4f4e0d8780d7", "kubernetes.io/metadata.name":"namespaces-6340", "namespaces-6340":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:51:00.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6340" for this suite. 03/27/23 21:51:00.884
------------------------------
• [0.153 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:51:00.754
    Mar 27 21:51:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename namespaces 03/27/23 21:51:00.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:00.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:00.825
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6340" 03/27/23 21:51:00.836
    Mar 27 21:51:00.865: INFO: Namespace "namespaces-6340" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"d74a9d93-502c-4e4a-9eef-4f4e0d8780d7", "kubernetes.io/metadata.name":"namespaces-6340", "namespaces-6340":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:51:00.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6340" for this suite. 03/27/23 21:51:00.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:51:00.91
Mar 27 21:51:00.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:51:00.912
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:00.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:00.982
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 03/27/23 21:51:00.994
Mar 27 21:51:01.021: INFO: Waiting up to 5m0s for pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e" in namespace "downward-api-3345" to be "running and ready"
Mar 27 21:51:01.033: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163539ms
Mar 27 21:51:01.033: INFO: The phase of Pod annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e is Pending, waiting for it to be Running (with Ready = true)
Mar 27 21:51:03.054: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e": Phase="Running", Reason="", readiness=true. Elapsed: 2.032187415s
Mar 27 21:51:03.054: INFO: The phase of Pod annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e is Running (Ready = true)
Mar 27 21:51:03.054: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e" satisfied condition "running and ready"
Mar 27 21:51:03.626: INFO: Successfully updated pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:51:07.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3345" for this suite. 03/27/23 21:51:07.734
------------------------------
• [SLOW TEST] [6.857 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:51:00.91
    Mar 27 21:51:00.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:51:00.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:00.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:00.982
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 03/27/23 21:51:00.994
    Mar 27 21:51:01.021: INFO: Waiting up to 5m0s for pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e" in namespace "downward-api-3345" to be "running and ready"
    Mar 27 21:51:01.033: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.163539ms
    Mar 27 21:51:01.033: INFO: The phase of Pod annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 21:51:03.054: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e": Phase="Running", Reason="", readiness=true. Elapsed: 2.032187415s
    Mar 27 21:51:03.054: INFO: The phase of Pod annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e is Running (Ready = true)
    Mar 27 21:51:03.054: INFO: Pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e" satisfied condition "running and ready"
    Mar 27 21:51:03.626: INFO: Successfully updated pod "annotationupdate70b86c46-3457-445e-a3c7-dbe25baba47e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:51:07.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3345" for this suite. 03/27/23 21:51:07.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:51:07.772
Mar 27 21:51:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename taint-single-pod 03/27/23 21:51:07.773
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:07.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:07.855
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Mar 27 21:51:07.866: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 21:52:07.967: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Mar 27 21:52:07.978: INFO: Starting informer...
STEP: Starting pod... 03/27/23 21:52:07.979
Mar 27 21:52:08.237: INFO: Pod is running on 10.176.99.177. Tainting Node
STEP: Trying to apply a taint on the Node 03/27/23 21:52:08.237
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:52:08.282
STEP: Waiting short time to make sure Pod is queued for deletion 03/27/23 21:52:08.297
Mar 27 21:52:08.297: INFO: Pod wasn't evicted. Proceeding
Mar 27 21:52:08.297: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:52:08.34
STEP: Waiting some time to make sure that toleration time passed. 03/27/23 21:52:08.36
Mar 27 21:53:23.363: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:23.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-4049" for this suite. 03/27/23 21:53:23.385
------------------------------
• [SLOW TEST] [135.636 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:51:07.772
    Mar 27 21:51:07.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename taint-single-pod 03/27/23 21:51:07.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:51:07.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:51:07.855
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Mar 27 21:51:07.866: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 21:52:07.967: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Mar 27 21:52:07.978: INFO: Starting informer...
    STEP: Starting pod... 03/27/23 21:52:07.979
    Mar 27 21:52:08.237: INFO: Pod is running on 10.176.99.177. Tainting Node
    STEP: Trying to apply a taint on the Node 03/27/23 21:52:08.237
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:52:08.282
    STEP: Waiting short time to make sure Pod is queued for deletion 03/27/23 21:52:08.297
    Mar 27 21:52:08.297: INFO: Pod wasn't evicted. Proceeding
    Mar 27 21:52:08.297: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/27/23 21:52:08.34
    STEP: Waiting some time to make sure that toleration time passed. 03/27/23 21:52:08.36
    Mar 27 21:53:23.363: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:23.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-4049" for this suite. 03/27/23 21:53:23.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:23.411
Mar 27 21:53:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:53:23.412
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:23.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:23.47
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:53:23.481
Mar 27 21:53:23.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 27 21:53:23.590: INFO: stderr: ""
Mar 27 21:53:23.590: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/27/23 21:53:23.591
STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 21:53:28.645
Mar 27 21:53:28.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 get pod e2e-test-httpd-pod -o json'
Mar 27 21:53:28.760: INFO: stderr: ""
Mar 27 21:53:28.760: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5901b68372b5951b85c4e271bd05c7d1590929f3c408bea9e9189e06af99549e\",\n            \"cni.projectcalico.org/podIP\": \"172.30.85.170/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.85.170/32\"\n        },\n        \"creationTimestamp\": \"2023-03-27T21:53:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3968\",\n        \"resourceVersion\": \"39072\",\n        \"uid\": \"dfbe9f34-d12c-4f2a-83aa-c149fa57dcba\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-n9gqb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.176.99.177\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-n9gqb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://67627fcc083c922a3675ff825840b16a81a656dc4585a498d51b6bae3ac4d611\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-27T21:53:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.176.99.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.85.170\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.85.170\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-27T21:53:23Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/27/23 21:53:28.76
Mar 27 21:53:28.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 replace -f -'
Mar 27 21:53:29.104: INFO: stderr: ""
Mar 27 21:53:29.104: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/27/23 21:53:29.104
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Mar 27 21:53:29.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 delete pods e2e-test-httpd-pod'
Mar 27 21:53:30.860: INFO: stderr: ""
Mar 27 21:53:30.860: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:30.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3968" for this suite. 03/27/23 21:53:30.878
------------------------------
• [SLOW TEST] [7.496 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:23.411
    Mar 27 21:53:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:53:23.412
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:23.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:23.47
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 03/27/23 21:53:23.481
    Mar 27 21:53:23.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 27 21:53:23.590: INFO: stderr: ""
    Mar 27 21:53:23.590: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/27/23 21:53:23.591
    STEP: verifying the pod e2e-test-httpd-pod was created 03/27/23 21:53:28.645
    Mar 27 21:53:28.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 get pod e2e-test-httpd-pod -o json'
    Mar 27 21:53:28.760: INFO: stderr: ""
    Mar 27 21:53:28.760: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5901b68372b5951b85c4e271bd05c7d1590929f3c408bea9e9189e06af99549e\",\n            \"cni.projectcalico.org/podIP\": \"172.30.85.170/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.85.170/32\"\n        },\n        \"creationTimestamp\": \"2023-03-27T21:53:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3968\",\n        \"resourceVersion\": \"39072\",\n        \"uid\": \"dfbe9f34-d12c-4f2a-83aa-c149fa57dcba\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-n9gqb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.176.99.177\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-n9gqb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:24Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-27T21:53:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://67627fcc083c922a3675ff825840b16a81a656dc4585a498d51b6bae3ac4d611\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-27T21:53:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.176.99.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.85.170\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.85.170\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-27T21:53:23Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/27/23 21:53:28.76
    Mar 27 21:53:28.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 replace -f -'
    Mar 27 21:53:29.104: INFO: stderr: ""
    Mar 27 21:53:29.104: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 03/27/23 21:53:29.104
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Mar 27 21:53:29.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-3968 delete pods e2e-test-httpd-pod'
    Mar 27 21:53:30.860: INFO: stderr: ""
    Mar 27 21:53:30.860: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:30.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3968" for this suite. 03/27/23 21:53:30.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:30.914
Mar 27 21:53:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 21:53:30.916
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:30.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:30.974
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 03/27/23 21:53:30.985
Mar 27 21:53:31.011: INFO: Waiting up to 5m0s for pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4" in namespace "var-expansion-159" to be "Succeeded or Failed"
Mar 27 21:53:31.022: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.469488ms
Mar 27 21:53:33.034: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023801895s
Mar 27 21:53:35.036: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024982698s
STEP: Saw pod success 03/27/23 21:53:35.036
Mar 27 21:53:35.036: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4" satisfied condition "Succeeded or Failed"
Mar 27 21:53:35.048: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:53:35.133
Mar 27 21:53:35.172: INFO: Waiting for pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 to disappear
Mar 27 21:53:35.182: INFO: Pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-159" for this suite. 03/27/23 21:53:35.202
------------------------------
• [4.314 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:30.914
    Mar 27 21:53:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 21:53:30.916
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:30.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:30.974
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 03/27/23 21:53:30.985
    Mar 27 21:53:31.011: INFO: Waiting up to 5m0s for pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4" in namespace "var-expansion-159" to be "Succeeded or Failed"
    Mar 27 21:53:31.022: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.469488ms
    Mar 27 21:53:33.034: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023801895s
    Mar 27 21:53:35.036: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024982698s
    STEP: Saw pod success 03/27/23 21:53:35.036
    Mar 27 21:53:35.036: INFO: Pod "var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4" satisfied condition "Succeeded or Failed"
    Mar 27 21:53:35.048: INFO: Trying to get logs from node 10.176.99.177 pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:53:35.133
    Mar 27 21:53:35.172: INFO: Waiting for pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 to disappear
    Mar 27 21:53:35.182: INFO: Pod var-expansion-533f2fe1-28f1-4ca7-81d1-35b7b42ecbd4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:35.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-159" for this suite. 03/27/23 21:53:35.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:35.235
Mar 27 21:53:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 21:53:35.236
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:35.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:35.297
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 03/27/23 21:53:35.309
Mar 27 21:53:35.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 create -f -'
Mar 27 21:53:36.243: INFO: stderr: ""
Mar 27 21:53:36.243: INFO: stdout: "pod/pause created\n"
Mar 27 21:53:36.243: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 27 21:53:36.243: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2375" to be "running and ready"
Mar 27 21:53:36.258: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.384352ms
Mar 27 21:53:36.258: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.176.99.177' to be 'Running' but was 'Pending'
Mar 27 21:53:38.273: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.029749208s
Mar 27 21:53:38.273: INFO: Pod "pause" satisfied condition "running and ready"
Mar 27 21:53:38.273: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 03/27/23 21:53:38.273
Mar 27 21:53:38.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 label pods pause testing-label=testing-label-value'
Mar 27 21:53:38.399: INFO: stderr: ""
Mar 27 21:53:38.399: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/27/23 21:53:38.399
Mar 27 21:53:38.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pod pause -L testing-label'
Mar 27 21:53:38.505: INFO: stderr: ""
Mar 27 21:53:38.505: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/27/23 21:53:38.505
Mar 27 21:53:38.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 label pods pause testing-label-'
Mar 27 21:53:38.638: INFO: stderr: ""
Mar 27 21:53:38.638: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/27/23 21:53:38.638
Mar 27 21:53:38.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pod pause -L testing-label'
Mar 27 21:53:38.749: INFO: stderr: ""
Mar 27 21:53:38.749: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 03/27/23 21:53:38.749
Mar 27 21:53:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 delete --grace-period=0 --force -f -'
Mar 27 21:53:38.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 27 21:53:38.892: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 27 21:53:38.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get rc,svc -l name=pause --no-headers'
Mar 27 21:53:39.015: INFO: stderr: "No resources found in kubectl-2375 namespace.\n"
Mar 27 21:53:39.015: INFO: stdout: ""
Mar 27 21:53:39.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 27 21:53:39.114: INFO: stderr: ""
Mar 27 21:53:39.114: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:39.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2375" for this suite. 03/27/23 21:53:39.133
------------------------------
• [3.921 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:35.235
    Mar 27 21:53:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 21:53:35.236
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:35.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:35.297
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 03/27/23 21:53:35.309
    Mar 27 21:53:35.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 create -f -'
    Mar 27 21:53:36.243: INFO: stderr: ""
    Mar 27 21:53:36.243: INFO: stdout: "pod/pause created\n"
    Mar 27 21:53:36.243: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 27 21:53:36.243: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2375" to be "running and ready"
    Mar 27 21:53:36.258: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.384352ms
    Mar 27 21:53:36.258: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.176.99.177' to be 'Running' but was 'Pending'
    Mar 27 21:53:38.273: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.029749208s
    Mar 27 21:53:38.273: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 27 21:53:38.273: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 03/27/23 21:53:38.273
    Mar 27 21:53:38.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 label pods pause testing-label=testing-label-value'
    Mar 27 21:53:38.399: INFO: stderr: ""
    Mar 27 21:53:38.399: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/27/23 21:53:38.399
    Mar 27 21:53:38.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pod pause -L testing-label'
    Mar 27 21:53:38.505: INFO: stderr: ""
    Mar 27 21:53:38.505: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/27/23 21:53:38.505
    Mar 27 21:53:38.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 label pods pause testing-label-'
    Mar 27 21:53:38.638: INFO: stderr: ""
    Mar 27 21:53:38.638: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/27/23 21:53:38.638
    Mar 27 21:53:38.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pod pause -L testing-label'
    Mar 27 21:53:38.749: INFO: stderr: ""
    Mar 27 21:53:38.749: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 03/27/23 21:53:38.749
    Mar 27 21:53:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 delete --grace-period=0 --force -f -'
    Mar 27 21:53:38.892: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 27 21:53:38.892: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 27 21:53:38.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get rc,svc -l name=pause --no-headers'
    Mar 27 21:53:39.015: INFO: stderr: "No resources found in kubectl-2375 namespace.\n"
    Mar 27 21:53:39.015: INFO: stdout: ""
    Mar 27 21:53:39.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-2375 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 27 21:53:39.114: INFO: stderr: ""
    Mar 27 21:53:39.114: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:39.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2375" for this suite. 03/27/23 21:53:39.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:39.157
Mar 27 21:53:39.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption 03/27/23 21:53:39.159
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:39.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:39.229
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:39.241
Mar 27 21:53:39.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption-2 03/27/23 21:53:39.242
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:39.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:39.313
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 03/27/23 21:53:39.34
STEP: Waiting for the pdb to be processed 03/27/23 21:53:41.384
STEP: Waiting for the pdb to be processed 03/27/23 21:53:41.413
STEP: listing a collection of PDBs across all namespaces 03/27/23 21:53:41.428
STEP: listing a collection of PDBs in namespace disruption-8929 03/27/23 21:53:41.442
STEP: deleting a collection of PDBs 03/27/23 21:53:41.455
STEP: Waiting for the PDB collection to be deleted 03/27/23 21:53:41.502
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-7886" for this suite. 03/27/23 21:53:41.551
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8929" for this suite. 03/27/23 21:53:41.576
------------------------------
• [2.443 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:39.157
    Mar 27 21:53:39.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption 03/27/23 21:53:39.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:39.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:39.229
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:39.241
    Mar 27 21:53:39.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption-2 03/27/23 21:53:39.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:39.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:39.313
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 03/27/23 21:53:39.34
    STEP: Waiting for the pdb to be processed 03/27/23 21:53:41.384
    STEP: Waiting for the pdb to be processed 03/27/23 21:53:41.413
    STEP: listing a collection of PDBs across all namespaces 03/27/23 21:53:41.428
    STEP: listing a collection of PDBs in namespace disruption-8929 03/27/23 21:53:41.442
    STEP: deleting a collection of PDBs 03/27/23 21:53:41.455
    STEP: Waiting for the PDB collection to be deleted 03/27/23 21:53:41.502
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:41.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:41.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-7886" for this suite. 03/27/23 21:53:41.551
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8929" for this suite. 03/27/23 21:53:41.576
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:41.602
Mar 27 21:53:41.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 21:53:41.604
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:41.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:41.663
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/27/23 21:53:41.674
STEP: delete the rc 03/27/23 21:53:46.706
STEP: wait for all pods to be garbage collected 03/27/23 21:53:46.729
STEP: Gathering metrics 03/27/23 21:53:51.755
W0327 21:53:51.789664      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 21:53:51.789: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 21:53:51.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9534" for this suite. 03/27/23 21:53:51.807
------------------------------
• [SLOW TEST] [10.229 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:41.602
    Mar 27 21:53:41.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 21:53:41.604
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:41.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:41.663
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/27/23 21:53:41.674
    STEP: delete the rc 03/27/23 21:53:46.706
    STEP: wait for all pods to be garbage collected 03/27/23 21:53:46.729
    STEP: Gathering metrics 03/27/23 21:53:51.755
    W0327 21:53:51.789664      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 21:53:51.789: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:53:51.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9534" for this suite. 03/27/23 21:53:51.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:53:51.836
Mar 27 21:53:51.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 21:53:51.837
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:51.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:51.9
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1304 03/27/23 21:53:51.912
STEP: creating service affinity-clusterip in namespace services-1304 03/27/23 21:53:51.912
STEP: creating replication controller affinity-clusterip in namespace services-1304 03/27/23 21:53:51.944
I0327 21:53:51.963942      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1304, replica count: 3
I0327 21:53:55.016509      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 21:53:55.042: INFO: Creating new exec pod
Mar 27 21:53:55.066: INFO: Waiting up to 5m0s for pod "execpod-affinityppqnr" in namespace "services-1304" to be "running"
Mar 27 21:53:55.077: INFO: Pod "execpod-affinityppqnr": Phase="Pending", Reason="", readiness=false. Elapsed: 11.717142ms
Mar 27 21:53:57.092: INFO: Pod "execpod-affinityppqnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.026363636s
Mar 27 21:53:57.092: INFO: Pod "execpod-affinityppqnr" satisfied condition "running"
Mar 27 21:53:58.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Mar 27 21:53:58.398: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 27 21:53:58.398: INFO: stdout: ""
Mar 27 21:53:58.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c nc -v -z -w 2 172.21.34.205 80'
Mar 27 21:53:58.641: INFO: stderr: "+ nc -v -z -w 2 172.21.34.205 80\nConnection to 172.21.34.205 80 port [tcp/http] succeeded!\n"
Mar 27 21:53:58.642: INFO: stdout: ""
Mar 27 21:53:58.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.34.205:80/ ; done'
Mar 27 21:53:58.953: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n"
Mar 27 21:53:58.953: INFO: stdout: "\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9"
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
Mar 27 21:53:58.953: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1304, will wait for the garbage collector to delete the pods 03/27/23 21:53:58.987
Mar 27 21:53:59.080: INFO: Deleting ReplicationController affinity-clusterip took: 25.041404ms
Mar 27 21:53:59.181: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.6049ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1304" for this suite. 03/27/23 21:54:01.665
------------------------------
• [SLOW TEST] [9.853 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:53:51.836
    Mar 27 21:53:51.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 21:53:51.837
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:53:51.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:53:51.9
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1304 03/27/23 21:53:51.912
    STEP: creating service affinity-clusterip in namespace services-1304 03/27/23 21:53:51.912
    STEP: creating replication controller affinity-clusterip in namespace services-1304 03/27/23 21:53:51.944
    I0327 21:53:51.963942      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1304, replica count: 3
    I0327 21:53:55.016509      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 21:53:55.042: INFO: Creating new exec pod
    Mar 27 21:53:55.066: INFO: Waiting up to 5m0s for pod "execpod-affinityppqnr" in namespace "services-1304" to be "running"
    Mar 27 21:53:55.077: INFO: Pod "execpod-affinityppqnr": Phase="Pending", Reason="", readiness=false. Elapsed: 11.717142ms
    Mar 27 21:53:57.092: INFO: Pod "execpod-affinityppqnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.026363636s
    Mar 27 21:53:57.092: INFO: Pod "execpod-affinityppqnr" satisfied condition "running"
    Mar 27 21:53:58.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Mar 27 21:53:58.398: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 27 21:53:58.398: INFO: stdout: ""
    Mar 27 21:53:58.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c nc -v -z -w 2 172.21.34.205 80'
    Mar 27 21:53:58.641: INFO: stderr: "+ nc -v -z -w 2 172.21.34.205 80\nConnection to 172.21.34.205 80 port [tcp/http] succeeded!\n"
    Mar 27 21:53:58.642: INFO: stdout: ""
    Mar 27 21:53:58.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1304 exec execpod-affinityppqnr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.34.205:80/ ; done'
    Mar 27 21:53:58.953: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.34.205:80/\n"
    Mar 27 21:53:58.953: INFO: stdout: "\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9\naffinity-clusterip-xdfg9"
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Received response from host: affinity-clusterip-xdfg9
    Mar 27 21:53:58.953: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1304, will wait for the garbage collector to delete the pods 03/27/23 21:53:58.987
    Mar 27 21:53:59.080: INFO: Deleting ReplicationController affinity-clusterip took: 25.041404ms
    Mar 27 21:53:59.181: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.6049ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:01.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1304" for this suite. 03/27/23 21:54:01.665
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:01.689
Mar 27 21:54:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:54:01.691
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:01.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:01.764
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:54:01.776
Mar 27 21:54:01.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806" in namespace "projected-3285" to be "Succeeded or Failed"
Mar 27 21:54:01.810: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00013ms
Mar 27 21:54:03.823: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Running", Reason="", readiness=true. Elapsed: 2.025053181s
Mar 27 21:54:05.822: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Running", Reason="", readiness=false. Elapsed: 4.02428895s
Mar 27 21:54:07.827: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029352197s
STEP: Saw pod success 03/27/23 21:54:07.828
Mar 27 21:54:07.828: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806" satisfied condition "Succeeded or Failed"
Mar 27 21:54:07.839: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 container client-container: <nil>
STEP: delete the pod 03/27/23 21:54:07.869
Mar 27 21:54:07.906: INFO: Waiting for pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 to disappear
Mar 27 21:54:07.918: INFO: Pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:07.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3285" for this suite. 03/27/23 21:54:07.936
------------------------------
• [SLOW TEST] [6.270 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:01.689
    Mar 27 21:54:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:54:01.691
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:01.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:01.764
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:54:01.776
    Mar 27 21:54:01.798: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806" in namespace "projected-3285" to be "Succeeded or Failed"
    Mar 27 21:54:01.810: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00013ms
    Mar 27 21:54:03.823: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Running", Reason="", readiness=true. Elapsed: 2.025053181s
    Mar 27 21:54:05.822: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Running", Reason="", readiness=false. Elapsed: 4.02428895s
    Mar 27 21:54:07.827: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029352197s
    STEP: Saw pod success 03/27/23 21:54:07.828
    Mar 27 21:54:07.828: INFO: Pod "downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806" satisfied condition "Succeeded or Failed"
    Mar 27 21:54:07.839: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 container client-container: <nil>
    STEP: delete the pod 03/27/23 21:54:07.869
    Mar 27 21:54:07.906: INFO: Waiting for pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 to disappear
    Mar 27 21:54:07.918: INFO: Pod downwardapi-volume-9294147b-7b4e-45af-9cb0-c6144f85b806 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:07.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3285" for this suite. 03/27/23 21:54:07.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:07.961
Mar 27 21:54:07.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename csiinlinevolumes 03/27/23 21:54:07.964
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:08.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:08.054
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 03/27/23 21:54:08.064
STEP: getting 03/27/23 21:54:08.129
STEP: listing 03/27/23 21:54:08.158
STEP: deleting 03/27/23 21:54:08.174
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:08.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9311" for this suite. 03/27/23 21:54:08.298
------------------------------
• [0.362 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:07.961
    Mar 27 21:54:07.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename csiinlinevolumes 03/27/23 21:54:07.964
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:08.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:08.054
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 03/27/23 21:54:08.064
    STEP: getting 03/27/23 21:54:08.129
    STEP: listing 03/27/23 21:54:08.158
    STEP: deleting 03/27/23 21:54:08.174
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:08.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9311" for this suite. 03/27/23 21:54:08.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:08.329
Mar 27 21:54:08.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 21:54:08.33
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:08.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:08.393
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-56f6483b-987f-4223-8822-d9930e07a5a1 03/27/23 21:54:08.404
STEP: Creating a pod to test consume configMaps 03/27/23 21:54:08.419
Mar 27 21:54:08.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b" in namespace "projected-8005" to be "Succeeded or Failed"
Mar 27 21:54:08.454: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.131154ms
Mar 27 21:54:10.467: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024732378s
Mar 27 21:54:12.467: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025257111s
STEP: Saw pod success 03/27/23 21:54:12.467
Mar 27 21:54:12.468: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b" satisfied condition "Succeeded or Failed"
Mar 27 21:54:12.480: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b container agnhost-container: <nil>
STEP: delete the pod 03/27/23 21:54:12.506
Mar 27 21:54:12.536: INFO: Waiting for pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b to disappear
Mar 27 21:54:12.547: INFO: Pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:12.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8005" for this suite. 03/27/23 21:54:12.567
------------------------------
• [4.266 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:08.329
    Mar 27 21:54:08.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 21:54:08.33
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:08.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:08.393
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-56f6483b-987f-4223-8822-d9930e07a5a1 03/27/23 21:54:08.404
    STEP: Creating a pod to test consume configMaps 03/27/23 21:54:08.419
    Mar 27 21:54:08.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b" in namespace "projected-8005" to be "Succeeded or Failed"
    Mar 27 21:54:08.454: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.131154ms
    Mar 27 21:54:10.467: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024732378s
    Mar 27 21:54:12.467: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025257111s
    STEP: Saw pod success 03/27/23 21:54:12.467
    Mar 27 21:54:12.468: INFO: Pod "pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b" satisfied condition "Succeeded or Failed"
    Mar 27 21:54:12.480: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 21:54:12.506
    Mar 27 21:54:12.536: INFO: Waiting for pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b to disappear
    Mar 27 21:54:12.547: INFO: Pod pod-projected-configmaps-9ebd4c44-f6b1-4997-a139-6b3b5e46a16b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:12.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8005" for this suite. 03/27/23 21:54:12.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:12.598
Mar 27 21:54:12.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename server-version 03/27/23 21:54:12.599
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:12.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:12.661
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/27/23 21:54:12.671
STEP: Confirm major version 03/27/23 21:54:12.676
Mar 27 21:54:12.676: INFO: Major version: 1
STEP: Confirm minor version 03/27/23 21:54:12.676
Mar 27 21:54:12.676: INFO: cleanMinorVersion: 26
Mar 27 21:54:12.676: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-268" for this suite. 03/27/23 21:54:12.69
------------------------------
• [0.116 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:12.598
    Mar 27 21:54:12.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename server-version 03/27/23 21:54:12.599
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:12.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:12.661
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/27/23 21:54:12.671
    STEP: Confirm major version 03/27/23 21:54:12.676
    Mar 27 21:54:12.676: INFO: Major version: 1
    STEP: Confirm minor version 03/27/23 21:54:12.676
    Mar 27 21:54:12.676: INFO: cleanMinorVersion: 26
    Mar 27 21:54:12.676: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-268" for this suite. 03/27/23 21:54:12.69
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:12.714
Mar 27 21:54:12.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:54:12.716
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:12.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:12.778
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 03/27/23 21:54:12.793
Mar 27 21:54:12.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d" in namespace "downward-api-9815" to be "Succeeded or Failed"
Mar 27 21:54:12.831: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.239225ms
Mar 27 21:54:14.853: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.035227049s
Mar 27 21:54:16.846: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.027544383s
Mar 27 21:54:18.845: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027033968s
STEP: Saw pod success 03/27/23 21:54:18.845
Mar 27 21:54:18.845: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d" satisfied condition "Succeeded or Failed"
Mar 27 21:54:18.858: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d container client-container: <nil>
STEP: delete the pod 03/27/23 21:54:18.896
Mar 27 21:54:18.932: INFO: Waiting for pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d to disappear
Mar 27 21:54:18.943: INFO: Pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9815" for this suite. 03/27/23 21:54:18.961
------------------------------
• [SLOW TEST] [6.269 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:12.714
    Mar 27 21:54:12.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:54:12.716
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:12.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:12.778
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 03/27/23 21:54:12.793
    Mar 27 21:54:12.818: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d" in namespace "downward-api-9815" to be "Succeeded or Failed"
    Mar 27 21:54:12.831: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.239225ms
    Mar 27 21:54:14.853: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.035227049s
    Mar 27 21:54:16.846: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.027544383s
    Mar 27 21:54:18.845: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027033968s
    STEP: Saw pod success 03/27/23 21:54:18.845
    Mar 27 21:54:18.845: INFO: Pod "downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d" satisfied condition "Succeeded or Failed"
    Mar 27 21:54:18.858: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d container client-container: <nil>
    STEP: delete the pod 03/27/23 21:54:18.896
    Mar 27 21:54:18.932: INFO: Waiting for pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d to disappear
    Mar 27 21:54:18.943: INFO: Pod downwardapi-volume-4dd70a3a-e39a-4180-adcf-66b5aade6b9d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9815" for this suite. 03/27/23 21:54:18.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:18.99
Mar 27 21:54:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename ingressclass 03/27/23 21:54:18.991
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:19.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:19.057
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/27/23 21:54:19.069
STEP: getting /apis/networking.k8s.io 03/27/23 21:54:19.079
STEP: getting /apis/networking.k8s.iov1 03/27/23 21:54:19.084
STEP: creating 03/27/23 21:54:19.089
STEP: getting 03/27/23 21:54:19.143
STEP: listing 03/27/23 21:54:19.164
STEP: watching 03/27/23 21:54:19.18
Mar 27 21:54:19.180: INFO: starting watch
STEP: patching 03/27/23 21:54:19.185
STEP: updating 03/27/23 21:54:19.204
Mar 27 21:54:19.240: INFO: waiting for watch events with expected annotations
Mar 27 21:54:19.241: INFO: saw patched and updated annotations
STEP: deleting 03/27/23 21:54:19.241
STEP: deleting a collection 03/27/23 21:54:19.304
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:54:19.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2687" for this suite. 03/27/23 21:54:19.395
------------------------------
• [0.428 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:18.99
    Mar 27 21:54:18.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename ingressclass 03/27/23 21:54:18.991
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:19.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:19.057
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/27/23 21:54:19.069
    STEP: getting /apis/networking.k8s.io 03/27/23 21:54:19.079
    STEP: getting /apis/networking.k8s.iov1 03/27/23 21:54:19.084
    STEP: creating 03/27/23 21:54:19.089
    STEP: getting 03/27/23 21:54:19.143
    STEP: listing 03/27/23 21:54:19.164
    STEP: watching 03/27/23 21:54:19.18
    Mar 27 21:54:19.180: INFO: starting watch
    STEP: patching 03/27/23 21:54:19.185
    STEP: updating 03/27/23 21:54:19.204
    Mar 27 21:54:19.240: INFO: waiting for watch events with expected annotations
    Mar 27 21:54:19.241: INFO: saw patched and updated annotations
    STEP: deleting 03/27/23 21:54:19.241
    STEP: deleting a collection 03/27/23 21:54:19.304
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:54:19.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2687" for this suite. 03/27/23 21:54:19.395
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:54:19.422
Mar 27 21:54:19.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 21:54:19.423
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:19.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:19.494
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 in namespace container-probe-8921 03/27/23 21:54:19.504
Mar 27 21:54:19.529: INFO: Waiting up to 5m0s for pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885" in namespace "container-probe-8921" to be "not pending"
Mar 27 21:54:19.539: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885": Phase="Pending", Reason="", readiness=false. Elapsed: 9.975112ms
Mar 27 21:54:21.551: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885": Phase="Running", Reason="", readiness=true. Elapsed: 2.022664138s
Mar 27 21:54:21.551: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885" satisfied condition "not pending"
Mar 27 21:54:21.551: INFO: Started pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 in namespace container-probe-8921
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:54:21.551
Mar 27 21:54:21.562: INFO: Initial restart count of pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 is 0
STEP: deleting the pod 03/27/23 21:58:23.209
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 21:58:23.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8921" for this suite. 03/27/23 21:58:23.265
------------------------------
• [SLOW TEST] [243.866 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:54:19.422
    Mar 27 21:54:19.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 21:54:19.423
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:54:19.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:54:19.494
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 in namespace container-probe-8921 03/27/23 21:54:19.504
    Mar 27 21:54:19.529: INFO: Waiting up to 5m0s for pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885" in namespace "container-probe-8921" to be "not pending"
    Mar 27 21:54:19.539: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885": Phase="Pending", Reason="", readiness=false. Elapsed: 9.975112ms
    Mar 27 21:54:21.551: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885": Phase="Running", Reason="", readiness=true. Elapsed: 2.022664138s
    Mar 27 21:54:21.551: INFO: Pod "test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885" satisfied condition "not pending"
    Mar 27 21:54:21.551: INFO: Started pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 in namespace container-probe-8921
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 21:54:21.551
    Mar 27 21:54:21.562: INFO: Initial restart count of pod test-webserver-44b68994-4028-4e2c-84ba-6b8c9df50885 is 0
    STEP: deleting the pod 03/27/23 21:58:23.209
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:58:23.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8921" for this suite. 03/27/23 21:58:23.265
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:58:23.289
Mar 27 21:58:23.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename endpointslice 03/27/23 21:58:23.291
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:23.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:23.358
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 03/27/23 21:58:28.525
STEP: referencing matching pods with named port 03/27/23 21:58:33.557
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/27/23 21:58:38.585
STEP: recreating EndpointSlices after they've been deleted 03/27/23 21:58:43.615
Mar 27 21:58:43.691: INFO: EndpointSlice for Service endpointslice-1885/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 27 21:58:53.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1885" for this suite. 03/27/23 21:58:53.746
------------------------------
• [SLOW TEST] [30.480 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:58:23.289
    Mar 27 21:58:23.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename endpointslice 03/27/23 21:58:23.291
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:23.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:23.358
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 03/27/23 21:58:28.525
    STEP: referencing matching pods with named port 03/27/23 21:58:33.557
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/27/23 21:58:38.585
    STEP: recreating EndpointSlices after they've been deleted 03/27/23 21:58:43.615
    Mar 27 21:58:43.691: INFO: EndpointSlice for Service endpointslice-1885/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:58:53.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1885" for this suite. 03/27/23 21:58:53.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:58:53.771
Mar 27 21:58:53.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 21:58:53.773
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:53.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:53.847
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 03/27/23 21:58:53.871
Mar 27 21:58:53.895: INFO: Waiting up to 5m0s for pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373" in namespace "downward-api-1385" to be "Succeeded or Failed"
Mar 27 21:58:53.907: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511354ms
Mar 27 21:58:55.920: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024889727s
Mar 27 21:58:57.921: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026052318s
STEP: Saw pod success 03/27/23 21:58:57.921
Mar 27 21:58:57.922: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373" satisfied condition "Succeeded or Failed"
Mar 27 21:58:57.934: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 container dapi-container: <nil>
STEP: delete the pod 03/27/23 21:58:58.026
Mar 27 21:58:58.064: INFO: Waiting for pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 to disappear
Mar 27 21:58:58.079: INFO: Pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:58:58.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1385" for this suite. 03/27/23 21:58:58.098
------------------------------
• [4.351 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:58:53.771
    Mar 27 21:58:53.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 21:58:53.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:53.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:53.847
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 03/27/23 21:58:53.871
    Mar 27 21:58:53.895: INFO: Waiting up to 5m0s for pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373" in namespace "downward-api-1385" to be "Succeeded or Failed"
    Mar 27 21:58:53.907: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511354ms
    Mar 27 21:58:55.920: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024889727s
    Mar 27 21:58:57.921: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026052318s
    STEP: Saw pod success 03/27/23 21:58:57.921
    Mar 27 21:58:57.922: INFO: Pod "downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373" satisfied condition "Succeeded or Failed"
    Mar 27 21:58:57.934: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 21:58:58.026
    Mar 27 21:58:58.064: INFO: Waiting for pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 to disappear
    Mar 27 21:58:58.079: INFO: Pod downward-api-3e427bc9-a6f2-48cf-901f-8f336bea8373 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:58:58.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1385" for this suite. 03/27/23 21:58:58.098
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:58:58.123
Mar 27 21:58:58.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename limitrange 03/27/23 21:58:58.125
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:58.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:58.197
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 03/27/23 21:58:58.21
STEP: Setting up watch 03/27/23 21:58:58.21
STEP: Submitting a LimitRange 03/27/23 21:58:58.322
STEP: Verifying LimitRange creation was observed 03/27/23 21:58:58.338
STEP: Fetching the LimitRange to ensure it has proper values 03/27/23 21:58:58.338
Mar 27 21:58:58.351: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 27 21:58:58.351: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/27/23 21:58:58.351
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/27/23 21:58:58.367
Mar 27 21:58:58.378: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 27 21:58:58.378: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/27/23 21:58:58.378
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/27/23 21:58:58.393
Mar 27 21:58:58.405: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 27 21:58:58.405: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/27/23 21:58:58.405
STEP: Failing to create a Pod with more than max resources 03/27/23 21:58:58.412
STEP: Updating a LimitRange 03/27/23 21:58:58.421
STEP: Verifying LimitRange updating is effective 03/27/23 21:58:58.436
STEP: Creating a Pod with less than former min resources 03/27/23 21:59:00.449
STEP: Failing to create a Pod with more than max resources 03/27/23 21:59:00.467
STEP: Deleting a LimitRange 03/27/23 21:59:00.473
STEP: Verifying the LimitRange was deleted 03/27/23 21:59:00.494
Mar 27 21:59:05.508: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/27/23 21:59:05.508
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:05.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6198" for this suite. 03/27/23 21:59:05.55
------------------------------
• [SLOW TEST] [7.449 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:58:58.123
    Mar 27 21:58:58.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename limitrange 03/27/23 21:58:58.125
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:58:58.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:58:58.197
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 03/27/23 21:58:58.21
    STEP: Setting up watch 03/27/23 21:58:58.21
    STEP: Submitting a LimitRange 03/27/23 21:58:58.322
    STEP: Verifying LimitRange creation was observed 03/27/23 21:58:58.338
    STEP: Fetching the LimitRange to ensure it has proper values 03/27/23 21:58:58.338
    Mar 27 21:58:58.351: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 27 21:58:58.351: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/27/23 21:58:58.351
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/27/23 21:58:58.367
    Mar 27 21:58:58.378: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 27 21:58:58.378: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/27/23 21:58:58.378
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/27/23 21:58:58.393
    Mar 27 21:58:58.405: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 27 21:58:58.405: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/27/23 21:58:58.405
    STEP: Failing to create a Pod with more than max resources 03/27/23 21:58:58.412
    STEP: Updating a LimitRange 03/27/23 21:58:58.421
    STEP: Verifying LimitRange updating is effective 03/27/23 21:58:58.436
    STEP: Creating a Pod with less than former min resources 03/27/23 21:59:00.449
    STEP: Failing to create a Pod with more than max resources 03/27/23 21:59:00.467
    STEP: Deleting a LimitRange 03/27/23 21:59:00.473
    STEP: Verifying the LimitRange was deleted 03/27/23 21:59:00.494
    Mar 27 21:59:05.508: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/27/23 21:59:05.508
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:05.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6198" for this suite. 03/27/23 21:59:05.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:05.576
Mar 27 21:59:05.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:59:05.577
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:05.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:05.642
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/27/23 21:59:05.656
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/27/23 21:59:05.662
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 21:59:05.663
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/27/23 21:59:05.663
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/27/23 21:59:05.672
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 21:59:05.672
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 21:59:05.677
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:05.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5894" for this suite. 03/27/23 21:59:05.696
------------------------------
• [0.148 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:05.576
    Mar 27 21:59:05.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:59:05.577
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:05.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:05.642
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/27/23 21:59:05.656
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/27/23 21:59:05.662
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/27/23 21:59:05.663
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/27/23 21:59:05.663
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/27/23 21:59:05.672
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 21:59:05.672
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/27/23 21:59:05.677
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:05.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5894" for this suite. 03/27/23 21:59:05.696
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:05.725
Mar 27 21:59:05.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:59:05.727
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:05.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:05.795
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 27 21:59:05.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4855" for this suite. 03/27/23 21:59:06.477
------------------------------
• [0.778 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:05.725
    Mar 27 21:59:05.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename custom-resource-definition 03/27/23 21:59:05.727
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:05.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:05.795
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 27 21:59:05.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4855" for this suite. 03/27/23 21:59:06.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:06.505
Mar 27 21:59:06.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 21:59:06.506
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:06.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:06.583
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-d7a5147f-a4e8-4687-b560-235de61a48ac 03/27/23 21:59:06.594
STEP: Creating a pod to test consume secrets 03/27/23 21:59:06.607
Mar 27 21:59:06.630: INFO: Waiting up to 5m0s for pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c" in namespace "secrets-645" to be "Succeeded or Failed"
Mar 27 21:59:06.642: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.837576ms
Mar 27 21:59:08.654: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024085702s
Mar 27 21:59:10.657: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026264804s
STEP: Saw pod success 03/27/23 21:59:10.657
Mar 27 21:59:10.657: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c" satisfied condition "Succeeded or Failed"
Mar 27 21:59:10.673: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 21:59:10.733
Mar 27 21:59:10.772: INFO: Waiting for pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c to disappear
Mar 27 21:59:10.784: INFO: Pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:10.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-645" for this suite. 03/27/23 21:59:10.807
------------------------------
• [4.328 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:06.505
    Mar 27 21:59:06.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 21:59:06.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:06.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:06.583
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-d7a5147f-a4e8-4687-b560-235de61a48ac 03/27/23 21:59:06.594
    STEP: Creating a pod to test consume secrets 03/27/23 21:59:06.607
    Mar 27 21:59:06.630: INFO: Waiting up to 5m0s for pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c" in namespace "secrets-645" to be "Succeeded or Failed"
    Mar 27 21:59:06.642: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.837576ms
    Mar 27 21:59:08.654: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024085702s
    Mar 27 21:59:10.657: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026264804s
    STEP: Saw pod success 03/27/23 21:59:10.657
    Mar 27 21:59:10.657: INFO: Pod "pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c" satisfied condition "Succeeded or Failed"
    Mar 27 21:59:10.673: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 21:59:10.733
    Mar 27 21:59:10.772: INFO: Waiting for pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c to disappear
    Mar 27 21:59:10.784: INFO: Pod pod-secrets-a396e249-45de-48af-88b6-894a181b5c9c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:10.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-645" for this suite. 03/27/23 21:59:10.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:10.838
Mar 27 21:59:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename events 03/27/23 21:59:10.839
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:10.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:10.919
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/27/23 21:59:10.93
STEP: get a list of Events with a label in the current namespace 03/27/23 21:59:10.983
STEP: delete a list of events 03/27/23 21:59:10.995
Mar 27 21:59:10.995: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/27/23 21:59:11.065
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2242" for this suite. 03/27/23 21:59:11.096
------------------------------
• [0.284 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:10.838
    Mar 27 21:59:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename events 03/27/23 21:59:10.839
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:10.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:10.919
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/27/23 21:59:10.93
    STEP: get a list of Events with a label in the current namespace 03/27/23 21:59:10.983
    STEP: delete a list of events 03/27/23 21:59:10.995
    Mar 27 21:59:10.995: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/27/23 21:59:11.065
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2242" for this suite. 03/27/23 21:59:11.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:11.128
Mar 27 21:59:11.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:11.128
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:11.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:11.194
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 03/27/23 21:59:11.205
Mar 27 21:59:11.230: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd" in namespace "emptydir-4439" to be "running"
Mar 27 21:59:11.242: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.086234ms
Mar 27 21:59:13.256: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd": Phase="Running", Reason="", readiness=false. Elapsed: 2.025742398s
Mar 27 21:59:13.256: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/27/23 21:59:13.256
Mar 27 21:59:13.257: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4439 PodName:pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 21:59:13.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 21:59:13.257: INFO: ExecWithOptions: Clientset creation
Mar 27 21:59:13.258: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-4439/pods/pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 27 21:59:13.429: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:13.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4439" for this suite. 03/27/23 21:59:13.449
------------------------------
• [2.347 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:11.128
    Mar 27 21:59:11.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:11.128
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:11.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:11.194
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 03/27/23 21:59:11.205
    Mar 27 21:59:11.230: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd" in namespace "emptydir-4439" to be "running"
    Mar 27 21:59:11.242: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.086234ms
    Mar 27 21:59:13.256: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd": Phase="Running", Reason="", readiness=false. Elapsed: 2.025742398s
    Mar 27 21:59:13.256: INFO: Pod "pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/27/23 21:59:13.256
    Mar 27 21:59:13.257: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4439 PodName:pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 21:59:13.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 21:59:13.257: INFO: ExecWithOptions: Clientset creation
    Mar 27 21:59:13.258: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-4439/pods/pod-sharedvolume-c1f4ee66-5bcb-43e7-a94f-49e04c7c78fd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 27 21:59:13.429: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:13.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4439" for this suite. 03/27/23 21:59:13.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:13.478
Mar 27 21:59:13.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:13.479
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:13.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:13.542
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 21:59:13.557
Mar 27 21:59:13.581: INFO: Waiting up to 5m0s for pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9" in namespace "emptydir-9574" to be "Succeeded or Failed"
Mar 27 21:59:13.593: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.380249ms
Mar 27 21:59:15.609: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027460863s
Mar 27 21:59:17.605: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02400258s
STEP: Saw pod success 03/27/23 21:59:17.605
Mar 27 21:59:17.606: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9" satisfied condition "Succeeded or Failed"
Mar 27 21:59:17.618: INFO: Trying to get logs from node 10.176.99.177 pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 container test-container: <nil>
STEP: delete the pod 03/27/23 21:59:17.645
Mar 27 21:59:17.674: INFO: Waiting for pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 to disappear
Mar 27 21:59:17.686: INFO: Pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:17.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9574" for this suite. 03/27/23 21:59:17.705
------------------------------
• [4.251 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:13.478
    Mar 27 21:59:13.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:13.479
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:13.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:13.542
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/27/23 21:59:13.557
    Mar 27 21:59:13.581: INFO: Waiting up to 5m0s for pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9" in namespace "emptydir-9574" to be "Succeeded or Failed"
    Mar 27 21:59:13.593: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.380249ms
    Mar 27 21:59:15.609: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027460863s
    Mar 27 21:59:17.605: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02400258s
    STEP: Saw pod success 03/27/23 21:59:17.605
    Mar 27 21:59:17.606: INFO: Pod "pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9" satisfied condition "Succeeded or Failed"
    Mar 27 21:59:17.618: INFO: Trying to get logs from node 10.176.99.177 pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 container test-container: <nil>
    STEP: delete the pod 03/27/23 21:59:17.645
    Mar 27 21:59:17.674: INFO: Waiting for pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 to disappear
    Mar 27 21:59:17.686: INFO: Pod pod-a615e7be-6ec3-4b2b-ad06-3cc061b95fb9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:17.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9574" for this suite. 03/27/23 21:59:17.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:17.729
Mar 27 21:59:17.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:17.731
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:17.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:17.799
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 21:59:17.81
Mar 27 21:59:17.834: INFO: Waiting up to 5m0s for pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717" in namespace "emptydir-9730" to be "Succeeded or Failed"
Mar 27 21:59:17.845: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Pending", Reason="", readiness=false. Elapsed: 11.406122ms
Mar 27 21:59:19.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024714926s
Mar 27 21:59:21.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024077753s
STEP: Saw pod success 03/27/23 21:59:21.858
Mar 27 21:59:21.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717" satisfied condition "Succeeded or Failed"
Mar 27 21:59:21.869: INFO: Trying to get logs from node 10.176.99.177 pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 container test-container: <nil>
STEP: delete the pod 03/27/23 21:59:21.895
Mar 27 21:59:21.925: INFO: Waiting for pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 to disappear
Mar 27 21:59:21.938: INFO: Pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:21.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9730" for this suite. 03/27/23 21:59:21.956
------------------------------
• [4.247 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:17.729
    Mar 27 21:59:17.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 21:59:17.731
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:17.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:17.799
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/27/23 21:59:17.81
    Mar 27 21:59:17.834: INFO: Waiting up to 5m0s for pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717" in namespace "emptydir-9730" to be "Succeeded or Failed"
    Mar 27 21:59:17.845: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Pending", Reason="", readiness=false. Elapsed: 11.406122ms
    Mar 27 21:59:19.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024714926s
    Mar 27 21:59:21.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024077753s
    STEP: Saw pod success 03/27/23 21:59:21.858
    Mar 27 21:59:21.858: INFO: Pod "pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717" satisfied condition "Succeeded or Failed"
    Mar 27 21:59:21.869: INFO: Trying to get logs from node 10.176.99.177 pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 container test-container: <nil>
    STEP: delete the pod 03/27/23 21:59:21.895
    Mar 27 21:59:21.925: INFO: Waiting for pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 to disappear
    Mar 27 21:59:21.938: INFO: Pod pod-d21ee8fc-ac1e-41fa-9ebc-047628de2717 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:21.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9730" for this suite. 03/27/23 21:59:21.956
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:21.977
Mar 27 21:59:21.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-pred 03/27/23 21:59:21.979
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:22.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:22.041
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Mar 27 21:59:22.052: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 27 21:59:22.079: INFO: Waiting for terminating namespaces to be deleted...
Mar 27 21:59:22.095: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.175 before test
Mar 27 21:59:22.125: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:59:22.125: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:59:22.125: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:59:22.125: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:59:22.125: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:59:22.125: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:59:22.125: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:59:22.125: INFO: ingress-cluster-healthcheck-b895f86ff-nwcp2 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Mar 27 21:59:22.125: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:59:22.125: INFO: metrics-server-6c65f45547-ptg94 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (3 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:59:22.125: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:59:22.125: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container e2e ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:59:22.125: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:59:22.125: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.125: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar 27 21:59:22.125: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.177 before test
Mar 27 21:59:22.150: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:59:22.150: INFO: calico-typha-7f67cb7cc9-6vdsx from kube-system started at 2023-03-27 21:52:10 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:59:22.150: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:59:22.150: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:59:22.150: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:59:22.150: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:59:22.150: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:59:22.150: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 27 21:59:22.150: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:59:22.150: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 27 21:59:22.150: INFO: 
Logging pods the apiserver thinks is on node 10.176.99.178 before test
Mar 27 21:59:22.177: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr from ibm-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
Mar 27 21:59:22.177: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 27 21:59:22.177: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container calico-node ready: true, restart count 0
Mar 27 21:59:22.177: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container calico-typha ready: true, restart count 0
Mar 27 21:59:22.177: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:59:22.177: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container coredns ready: true, restart count 0
Mar 27 21:59:22.177: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container autoscaler ready: true, restart count 0
Mar 27 21:59:22.177: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 27 21:59:22.177: INFO: 	Container pause ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 27 21:59:22.177: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 27 21:59:22.177: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar 27 21:59:22.177: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 27 21:59:22.177: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container config-watcher ready: true, restart count 0
Mar 27 21:59:22.177: INFO: 	Container metrics-server ready: true, restart count 0
Mar 27 21:59:22.177: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 27 21:59:22.177: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar 27 21:59:22.177: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
Mar 27 21:59:22.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 27 21:59:22.177: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.176.99.175 03/27/23 21:59:22.246
STEP: verifying the node has the label node 10.176.99.177 03/27/23 21:59:22.289
STEP: verifying the node has the label node 10.176.99.178 03/27/23 21:59:22.341
Mar 27 21:59:22.391: INFO: Pod ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r requesting resource cpu=5m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr requesting resource cpu=5m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod calico-kube-controllers-58f7b65f54-wsdqd requesting resource cpu=10m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod calico-node-2fpb2 requesting resource cpu=250m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod calico-node-cmh2z requesting resource cpu=250m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod calico-node-w5dkb requesting resource cpu=250m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-6vdsx requesting resource cpu=250m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-lsk8p requesting resource cpu=250m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-tkqql requesting resource cpu=250m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-2wt9x requesting resource cpu=100m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-8xcfl requesting resource cpu=100m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-rvn24 requesting resource cpu=100m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod coredns-autoscaler-57c58584b6-rgd9s requesting resource cpu=1m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod dashboard-metrics-scraper-65455cf995-72578 requesting resource cpu=1m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibm-file-plugin-59487845b7-kfvgf requesting resource cpu=50m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-7b4mc requesting resource cpu=5m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-d9666 requesting resource cpu=5m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-t6qjc requesting resource cpu=5m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.175 requesting resource cpu=25m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.177 requesting resource cpu=25m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.178 requesting resource cpu=25m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibm-storage-watcher-556f8995dd-8dh96 requesting resource cpu=50m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-m9m46 requesting resource cpu=50m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-tljbq requesting resource cpu=50m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-vk6lt requesting resource cpu=50m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs requesting resource cpu=50m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod ingress-cluster-healthcheck-b895f86ff-nwcp2 requesting resource cpu=10m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-77h7z requesting resource cpu=10m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-9qsfk requesting resource cpu=10m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-xqxrf requesting resource cpu=10m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod kubernetes-dashboard-5ccdc9cbb8-d8pn2 requesting resource cpu=50m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod metrics-server-6c65f45547-bvng4 requesting resource cpu=126m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod metrics-server-6c65f45547-ptg94 requesting resource cpu=126m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl requesting resource cpu=20m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql requesting resource cpu=20m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-gp9lv requesting resource cpu=10m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-q4qjb requesting resource cpu=10m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-wnwxt requesting resource cpu=10m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod sonobuoy-e2e-job-5e955573fdc94d86 requesting resource cpu=0m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp requesting resource cpu=0m on Node 10.176.99.178
Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 requesting resource cpu=0m on Node 10.176.99.177
Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x requesting resource cpu=0m on Node 10.176.99.175
Mar 27 21:59:22.391: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.176.99.175
STEP: Starting Pods to consume most of the cluster CPU. 03/27/23 21:59:22.391
Mar 27 21:59:22.391: INFO: Creating a pod which consumes cpu=2141m on Node 10.176.99.175
Mar 27 21:59:22.421: INFO: Creating a pod which consumes cpu=2324m on Node 10.176.99.177
Mar 27 21:59:22.436: INFO: Creating a pod which consumes cpu=1908m on Node 10.176.99.178
Mar 27 21:59:22.452: INFO: Waiting up to 5m0s for pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba" in namespace "sched-pred-5607" to be "running"
Mar 27 21:59:22.464: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba": Phase="Pending", Reason="", readiness=false. Elapsed: 11.692116ms
Mar 27 21:59:24.476: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba": Phase="Running", Reason="", readiness=true. Elapsed: 2.024454602s
Mar 27 21:59:24.476: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba" satisfied condition "running"
Mar 27 21:59:24.476: INFO: Waiting up to 5m0s for pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f" in namespace "sched-pred-5607" to be "running"
Mar 27 21:59:24.490: INFO: Pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f": Phase="Running", Reason="", readiness=true. Elapsed: 13.106915ms
Mar 27 21:59:24.490: INFO: Pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f" satisfied condition "running"
Mar 27 21:59:24.490: INFO: Waiting up to 5m0s for pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4" in namespace "sched-pred-5607" to be "running"
Mar 27 21:59:24.503: INFO: Pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4": Phase="Running", Reason="", readiness=true. Elapsed: 13.188913ms
Mar 27 21:59:24.503: INFO: Pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/27/23 21:59:24.503
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.1750659784126ea7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba to 10.176.99.175] 03/27/23 21:59:24.519
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597bf9c5359], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.519
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597c10f2c7e], Reason = [Created], Message = [Created container filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba] 03/27/23 21:59:24.52
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597c90af799], Reason = [Started], Message = [Started container filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba] 03/27/23 21:59:24.52
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.1750659785a70c7f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-2b870157-e404-4c21-a393-739fcf5307f4 to 10.176.99.178] 03/27/23 21:59:24.521
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c06828e6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.521
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c1aaad40], Reason = [Created], Message = [Created container filler-pod-2b870157-e404-4c21-a393-739fcf5307f4] 03/27/23 21:59:24.521
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c9eb64f4], Reason = [Started], Message = [Started container filler-pod-2b870157-e404-4c21-a393-739fcf5307f4] 03/27/23 21:59:24.522
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.175065978505fa5d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f to 10.176.99.177] 03/27/23 21:59:24.522
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597b8d74ee6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.522
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597b972b038], Reason = [Created], Message = [Created container filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f] 03/27/23 21:59:24.523
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597bfee55e5], Reason = [Started], Message = [Started container filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f] 03/27/23 21:59:24.523
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175065980156c6d1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 03/27/23 21:59:24.571
STEP: removing the label node off the node 10.176.99.175 03/27/23 21:59:25.558
STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.591
STEP: removing the label node off the node 10.176.99.177 03/27/23 21:59:25.603
STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.641
STEP: removing the label node off the node 10.176.99.178 03/27/23 21:59:25.659
STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.713
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:25.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5607" for this suite. 03/27/23 21:59:25.742
------------------------------
• [3.788 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:21.977
    Mar 27 21:59:21.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-pred 03/27/23 21:59:21.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:22.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:22.041
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Mar 27 21:59:22.052: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 27 21:59:22.079: INFO: Waiting for terminating namespaces to be deleted...
    Mar 27 21:59:22.095: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.175 before test
    Mar 27 21:59:22.125: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r from ibm-system started at 2023-03-27 19:08:08 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: calico-node-cmh2z from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: calico-typha-7f67cb7cc9-lsk8p from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: coredns-5845f98d4-rvn24 from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: ibm-keepalived-watcher-d9666 from kube-system started at 2023-03-27 19:00:18 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: ibm-master-proxy-static-10.176.99.175 from kube-system started at 2023-03-27 19:00:17 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: ibmcloud-block-storage-driver-vk6lt from kube-system started at 2023-03-27 19:00:28 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: ingress-cluster-healthcheck-b895f86ff-nwcp2 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: konnectivity-agent-xqxrf from kube-system started at 2023-03-27 19:12:56 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: metrics-server-6c65f45547-ptg94 from kube-system started at 2023-03-27 21:41:19 +0000 UTC (3 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl from kube-system started at 2023-03-27 19:12:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: sonobuoy-e2e-job-5e955573fdc94d86 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container e2e ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-27 19:08:25 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.125: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Mar 27 21:59:22.125: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.177 before test
    Mar 27 21:59:22.150: INFO: calico-node-2fpb2 from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: calico-typha-7f67cb7cc9-6vdsx from kube-system started at 2023-03-27 21:52:10 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: ibm-keepalived-watcher-7b4mc from kube-system started at 2023-03-27 18:59:54 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: ibm-master-proxy-static-10.176.99.177 from kube-system started at 2023-03-27 18:59:51 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: ibmcloud-block-storage-driver-m9m46 from kube-system started at 2023-03-27 19:00:00 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: konnectivity-agent-77h7z from kube-system started at 2023-03-27 19:12:59 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: sonobuoy from sonobuoy started at 2023-03-27 20:49:32 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 27 21:59:22.150: INFO: 
    Logging pods the apiserver thinks is on node 10.176.99.178 before test
    Mar 27 21:59:22.177: INFO: ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr from ibm-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibm-cloud-provider-ip-169-46-25-66 ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: calico-kube-controllers-58f7b65f54-wsdqd from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: calico-node-w5dkb from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container calico-node ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: calico-typha-7f67cb7cc9-tkqql from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container calico-typha ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: coredns-5845f98d4-2wt9x from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: coredns-5845f98d4-8xcfl from kube-system started at 2023-03-27 19:13:24 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container coredns ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: coredns-autoscaler-57c58584b6-rgd9s from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container autoscaler ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: dashboard-metrics-scraper-65455cf995-72578 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibm-file-plugin-59487845b7-kfvgf from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibm-keepalived-watcher-t6qjc from kube-system started at 2023-03-27 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibm-master-proxy-static-10.176.99.178 from kube-system started at 2023-03-27 18:57:05 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: 	Container pause ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibm-storage-watcher-556f8995dd-8dh96 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibmcloud-block-storage-driver-tljbq from kube-system started at 2023-03-27 18:57:16 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: konnectivity-agent-9qsfk from kube-system started at 2023-03-27 19:12:53 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: kubernetes-dashboard-5ccdc9cbb8-d8pn2 from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: metrics-server-6c65f45547-bvng4 from kube-system started at 2023-03-27 19:38:32 +0000 UTC (3 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container config-watcher ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql from kube-system started at 2023-03-27 21:41:19 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container nginx-ingress ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-gp9lv from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-q4qjb from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: snapshot-controller-6db47fc545-wnwxt from kube-system started at 2023-03-27 18:57:17 +0000 UTC (1 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container snapshot-controller ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp from sonobuoy started at 2023-03-27 20:49:35 +0000 UTC (2 container statuses recorded)
    Mar 27 21:59:22.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 27 21:59:22.177: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.176.99.175 03/27/23 21:59:22.246
    STEP: verifying the node has the label node 10.176.99.177 03/27/23 21:59:22.289
    STEP: verifying the node has the label node 10.176.99.178 03/27/23 21:59:22.341
    Mar 27 21:59:22.391: INFO: Pod ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-fds5r requesting resource cpu=5m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod ibm-cloud-provider-ip-169-46-25-66-7fc99d7477-pqppr requesting resource cpu=5m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod calico-kube-controllers-58f7b65f54-wsdqd requesting resource cpu=10m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod calico-node-2fpb2 requesting resource cpu=250m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod calico-node-cmh2z requesting resource cpu=250m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod calico-node-w5dkb requesting resource cpu=250m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-6vdsx requesting resource cpu=250m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-lsk8p requesting resource cpu=250m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod calico-typha-7f67cb7cc9-tkqql requesting resource cpu=250m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-2wt9x requesting resource cpu=100m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-8xcfl requesting resource cpu=100m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod coredns-5845f98d4-rvn24 requesting resource cpu=100m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod coredns-autoscaler-57c58584b6-rgd9s requesting resource cpu=1m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod dashboard-metrics-scraper-65455cf995-72578 requesting resource cpu=1m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibm-file-plugin-59487845b7-kfvgf requesting resource cpu=50m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-7b4mc requesting resource cpu=5m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-d9666 requesting resource cpu=5m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod ibm-keepalived-watcher-t6qjc requesting resource cpu=5m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.175 requesting resource cpu=25m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.177 requesting resource cpu=25m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod ibm-master-proxy-static-10.176.99.178 requesting resource cpu=25m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibm-storage-watcher-556f8995dd-8dh96 requesting resource cpu=50m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-m9m46 requesting resource cpu=50m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-tljbq requesting resource cpu=50m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-driver-vk6lt requesting resource cpu=50m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod ibmcloud-block-storage-plugin-64f84d9f7d-4sbfs requesting resource cpu=50m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod ingress-cluster-healthcheck-b895f86ff-nwcp2 requesting resource cpu=10m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-77h7z requesting resource cpu=10m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-9qsfk requesting resource cpu=10m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod konnectivity-agent-xqxrf requesting resource cpu=10m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod kubernetes-dashboard-5ccdc9cbb8-d8pn2 requesting resource cpu=50m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod metrics-server-6c65f45547-bvng4 requesting resource cpu=126m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod metrics-server-6c65f45547-ptg94 requesting resource cpu=126m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-2r8kl requesting resource cpu=20m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod public-crcggu674d0f9r07ur77kg-alb1-6fccd4c5fd-cr6ql requesting resource cpu=20m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-gp9lv requesting resource cpu=10m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-q4qjb requesting resource cpu=10m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod snapshot-controller-6db47fc545-wnwxt requesting resource cpu=10m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod sonobuoy-e2e-job-5e955573fdc94d86 requesting resource cpu=0m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-876dp requesting resource cpu=0m on Node 10.176.99.178
    Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-kd4m2 requesting resource cpu=0m on Node 10.176.99.177
    Mar 27 21:59:22.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-1dd6c020e90b43ce-rbx9x requesting resource cpu=0m on Node 10.176.99.175
    Mar 27 21:59:22.391: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.176.99.175
    STEP: Starting Pods to consume most of the cluster CPU. 03/27/23 21:59:22.391
    Mar 27 21:59:22.391: INFO: Creating a pod which consumes cpu=2141m on Node 10.176.99.175
    Mar 27 21:59:22.421: INFO: Creating a pod which consumes cpu=2324m on Node 10.176.99.177
    Mar 27 21:59:22.436: INFO: Creating a pod which consumes cpu=1908m on Node 10.176.99.178
    Mar 27 21:59:22.452: INFO: Waiting up to 5m0s for pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba" in namespace "sched-pred-5607" to be "running"
    Mar 27 21:59:22.464: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba": Phase="Pending", Reason="", readiness=false. Elapsed: 11.692116ms
    Mar 27 21:59:24.476: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba": Phase="Running", Reason="", readiness=true. Elapsed: 2.024454602s
    Mar 27 21:59:24.476: INFO: Pod "filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba" satisfied condition "running"
    Mar 27 21:59:24.476: INFO: Waiting up to 5m0s for pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f" in namespace "sched-pred-5607" to be "running"
    Mar 27 21:59:24.490: INFO: Pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f": Phase="Running", Reason="", readiness=true. Elapsed: 13.106915ms
    Mar 27 21:59:24.490: INFO: Pod "filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f" satisfied condition "running"
    Mar 27 21:59:24.490: INFO: Waiting up to 5m0s for pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4" in namespace "sched-pred-5607" to be "running"
    Mar 27 21:59:24.503: INFO: Pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4": Phase="Running", Reason="", readiness=true. Elapsed: 13.188913ms
    Mar 27 21:59:24.503: INFO: Pod "filler-pod-2b870157-e404-4c21-a393-739fcf5307f4" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/27/23 21:59:24.503
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.1750659784126ea7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba to 10.176.99.175] 03/27/23 21:59:24.519
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597bf9c5359], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.519
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597c10f2c7e], Reason = [Created], Message = [Created container filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba] 03/27/23 21:59:24.52
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba.17506597c90af799], Reason = [Started], Message = [Started container filler-pod-0bc3ce8f-9ebd-474d-8d19-8d075f001aba] 03/27/23 21:59:24.52
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.1750659785a70c7f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-2b870157-e404-4c21-a393-739fcf5307f4 to 10.176.99.178] 03/27/23 21:59:24.521
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c06828e6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.521
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c1aaad40], Reason = [Created], Message = [Created container filler-pod-2b870157-e404-4c21-a393-739fcf5307f4] 03/27/23 21:59:24.521
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2b870157-e404-4c21-a393-739fcf5307f4.17506597c9eb64f4], Reason = [Started], Message = [Started container filler-pod-2b870157-e404-4c21-a393-739fcf5307f4] 03/27/23 21:59:24.522
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.175065978505fa5d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5607/filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f to 10.176.99.177] 03/27/23 21:59:24.522
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597b8d74ee6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 03/27/23 21:59:24.522
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597b972b038], Reason = [Created], Message = [Created container filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f] 03/27/23 21:59:24.523
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f.17506597bfee55e5], Reason = [Started], Message = [Started container filler-pod-5c156a51-3521-4272-b1e0-4688f0ee253f] 03/27/23 21:59:24.523
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175065980156c6d1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 03/27/23 21:59:24.571
    STEP: removing the label node off the node 10.176.99.175 03/27/23 21:59:25.558
    STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.591
    STEP: removing the label node off the node 10.176.99.177 03/27/23 21:59:25.603
    STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.641
    STEP: removing the label node off the node 10.176.99.178 03/27/23 21:59:25.659
    STEP: verifying the node doesn't have the label node 03/27/23 21:59:25.713
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:25.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5607" for this suite. 03/27/23 21:59:25.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:25.771
Mar 27 21:59:25.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 21:59:25.773
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:25.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:25.842
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 21:59:25.901
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:59:26.292
STEP: Deploying the webhook pod 03/27/23 21:59:26.319
STEP: Wait for the deployment to be ready 03/27/23 21:59:26.357
Mar 27 21:59:26.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 21:59:28.453
STEP: Verifying the service has paired with the endpoint 03/27/23 21:59:28.485
Mar 27 21:59:29.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/27/23 21:59:29.5
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:29.5
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/27/23 21:59:29.586
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/27/23 21:59:30.623
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:30.624
STEP: Having no error when timeout is longer than webhook latency 03/27/23 21:59:31.728
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:31.729
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/27/23 21:59:36.868
STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:36.868
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6175" for this suite. 03/27/23 21:59:42.117
STEP: Destroying namespace "webhook-6175-markers" for this suite. 03/27/23 21:59:42.138
------------------------------
• [SLOW TEST] [16.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:25.771
    Mar 27 21:59:25.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 21:59:25.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:25.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:25.842
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 21:59:25.901
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 21:59:26.292
    STEP: Deploying the webhook pod 03/27/23 21:59:26.319
    STEP: Wait for the deployment to be ready 03/27/23 21:59:26.357
    Mar 27 21:59:26.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 21:59:28.453
    STEP: Verifying the service has paired with the endpoint 03/27/23 21:59:28.485
    Mar 27 21:59:29.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/27/23 21:59:29.5
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:29.5
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/27/23 21:59:29.586
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/27/23 21:59:30.623
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:30.624
    STEP: Having no error when timeout is longer than webhook latency 03/27/23 21:59:31.728
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:31.729
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/27/23 21:59:36.868
    STEP: Registering slow webhook via the AdmissionRegistration API 03/27/23 21:59:36.868
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6175" for this suite. 03/27/23 21:59:42.117
    STEP: Destroying namespace "webhook-6175-markers" for this suite. 03/27/23 21:59:42.138
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:42.159
Mar 27 21:59:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context-test 03/27/23 21:59:42.16
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:42.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:42.22
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Mar 27 21:59:42.255: INFO: Waiting up to 5m0s for pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658" in namespace "security-context-test-9673" to be "Succeeded or Failed"
Mar 27 21:59:42.275: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Pending", Reason="", readiness=false. Elapsed: 20.090809ms
Mar 27 21:59:44.287: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032673728s
Mar 27 21:59:46.288: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033301778s
Mar 27 21:59:46.288: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:46.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9673" for this suite. 03/27/23 21:59:46.309
------------------------------
• [4.172 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:42.159
    Mar 27 21:59:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context-test 03/27/23 21:59:42.16
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:42.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:42.22
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Mar 27 21:59:42.255: INFO: Waiting up to 5m0s for pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658" in namespace "security-context-test-9673" to be "Succeeded or Failed"
    Mar 27 21:59:42.275: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Pending", Reason="", readiness=false. Elapsed: 20.090809ms
    Mar 27 21:59:44.287: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032673728s
    Mar 27 21:59:46.288: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033301778s
    Mar 27 21:59:46.288: INFO: Pod "busybox-user-65534-bfddb268-2282-48e1-98f6-1c7f7a2b8658" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:46.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9673" for this suite. 03/27/23 21:59:46.309
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:46.332
Mar 27 21:59:46.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption 03/27/23 21:59:46.333
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:46.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:46.4
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 03/27/23 21:59:46.411
STEP: Waiting for the pdb to be processed 03/27/23 21:59:46.429
STEP: First trying to evict a pod which shouldn't be evictable 03/27/23 21:59:46.461
STEP: Waiting for all pods to be running 03/27/23 21:59:46.461
Mar 27 21:59:46.472: INFO: pods: 0 < 3
STEP: locating a running pod 03/27/23 21:59:48.486
STEP: Updating the pdb to allow a pod to be evicted 03/27/23 21:59:48.52
STEP: Waiting for the pdb to be processed 03/27/23 21:59:48.549
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 21:59:48.563
STEP: Waiting for all pods to be running 03/27/23 21:59:48.563
STEP: Waiting for the pdb to observed all healthy pods 03/27/23 21:59:48.576
STEP: Patching the pdb to disallow a pod to be evicted 03/27/23 21:59:48.652
STEP: Waiting for the pdb to be processed 03/27/23 21:59:48.698
STEP: Waiting for all pods to be running 03/27/23 21:59:48.714
Mar 27 21:59:48.728: INFO: running pods: 2 < 3
STEP: locating a running pod 03/27/23 21:59:50.745
STEP: Deleting the pdb to allow a pod to be evicted 03/27/23 21:59:50.781
STEP: Waiting for the pdb to be deleted 03/27/23 21:59:50.805
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 21:59:50.818
STEP: Waiting for all pods to be running 03/27/23 21:59:50.818
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:50.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5864" for this suite. 03/27/23 21:59:50.91
------------------------------
• [4.603 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:46.332
    Mar 27 21:59:46.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption 03/27/23 21:59:46.333
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:46.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:46.4
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 03/27/23 21:59:46.411
    STEP: Waiting for the pdb to be processed 03/27/23 21:59:46.429
    STEP: First trying to evict a pod which shouldn't be evictable 03/27/23 21:59:46.461
    STEP: Waiting for all pods to be running 03/27/23 21:59:46.461
    Mar 27 21:59:46.472: INFO: pods: 0 < 3
    STEP: locating a running pod 03/27/23 21:59:48.486
    STEP: Updating the pdb to allow a pod to be evicted 03/27/23 21:59:48.52
    STEP: Waiting for the pdb to be processed 03/27/23 21:59:48.549
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 21:59:48.563
    STEP: Waiting for all pods to be running 03/27/23 21:59:48.563
    STEP: Waiting for the pdb to observed all healthy pods 03/27/23 21:59:48.576
    STEP: Patching the pdb to disallow a pod to be evicted 03/27/23 21:59:48.652
    STEP: Waiting for the pdb to be processed 03/27/23 21:59:48.698
    STEP: Waiting for all pods to be running 03/27/23 21:59:48.714
    Mar 27 21:59:48.728: INFO: running pods: 2 < 3
    STEP: locating a running pod 03/27/23 21:59:50.745
    STEP: Deleting the pdb to allow a pod to be evicted 03/27/23 21:59:50.781
    STEP: Waiting for the pdb to be deleted 03/27/23 21:59:50.805
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/27/23 21:59:50.818
    STEP: Waiting for all pods to be running 03/27/23 21:59:50.818
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:50.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5864" for this suite. 03/27/23 21:59:50.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:50.938
Mar 27 21:59:50.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 21:59:50.939
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:50.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:51.007
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-xpmzz" 03/27/23 21:59:51.018
Mar 27 21:59:51.035: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
Mar 27 21:59:52.048: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
Mar 27 21:59:52.062: INFO: Found 1 replicas for "e2e-rc-xpmzz" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-xpmzz" 03/27/23 21:59:52.062
STEP: Updating a scale subresource 03/27/23 21:59:52.074
STEP: Verifying replicas where modified for replication controller "e2e-rc-xpmzz" 03/27/23 21:59:52.09
Mar 27 21:59:52.090: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
Mar 27 21:59:53.104: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
Mar 27 21:59:53.118: INFO: Found 2 replicas for "e2e-rc-xpmzz" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 21:59:53.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1485" for this suite. 03/27/23 21:59:53.142
------------------------------
• [2.227 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:50.938
    Mar 27 21:59:50.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 21:59:50.939
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:50.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:51.007
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-xpmzz" 03/27/23 21:59:51.018
    Mar 27 21:59:51.035: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
    Mar 27 21:59:52.048: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
    Mar 27 21:59:52.062: INFO: Found 1 replicas for "e2e-rc-xpmzz" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-xpmzz" 03/27/23 21:59:52.062
    STEP: Updating a scale subresource 03/27/23 21:59:52.074
    STEP: Verifying replicas where modified for replication controller "e2e-rc-xpmzz" 03/27/23 21:59:52.09
    Mar 27 21:59:52.090: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
    Mar 27 21:59:53.104: INFO: Get Replication Controller "e2e-rc-xpmzz" to confirm replicas
    Mar 27 21:59:53.118: INFO: Found 2 replicas for "e2e-rc-xpmzz" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 21:59:53.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1485" for this suite. 03/27/23 21:59:53.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 21:59:53.193
Mar 27 21:59:53.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 21:59:53.195
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:53.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:53.287
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 03/27/23 21:59:53.302
STEP: Ensuring active pods == parallelism 03/27/23 21:59:53.326
STEP: Orphaning one of the Job's Pods 03/27/23 21:59:55.34
Mar 27 21:59:55.897: INFO: Successfully updated pod "adopt-release-ngxf7"
STEP: Checking that the Job readopts the Pod 03/27/23 21:59:55.897
Mar 27 21:59:55.898: INFO: Waiting up to 15m0s for pod "adopt-release-ngxf7" in namespace "job-9998" to be "adopted"
Mar 27 21:59:55.909: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 11.109726ms
Mar 27 21:59:57.922: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02388155s
Mar 27 21:59:57.922: INFO: Pod "adopt-release-ngxf7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/27/23 21:59:57.922
Mar 27 21:59:58.464: INFO: Successfully updated pod "adopt-release-ngxf7"
STEP: Checking that the Job releases the Pod 03/27/23 21:59:58.464
Mar 27 21:59:58.464: INFO: Waiting up to 15m0s for pod "adopt-release-ngxf7" in namespace "job-9998" to be "released"
Mar 27 21:59:58.478: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 13.611677ms
Mar 27 22:00:00.494: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.029734567s
Mar 27 22:00:00.494: INFO: Pod "adopt-release-ngxf7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9998" for this suite. 03/27/23 22:00:00.518
------------------------------
• [SLOW TEST] [7.349 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 21:59:53.193
    Mar 27 21:59:53.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 21:59:53.195
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 21:59:53.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 21:59:53.287
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 03/27/23 21:59:53.302
    STEP: Ensuring active pods == parallelism 03/27/23 21:59:53.326
    STEP: Orphaning one of the Job's Pods 03/27/23 21:59:55.34
    Mar 27 21:59:55.897: INFO: Successfully updated pod "adopt-release-ngxf7"
    STEP: Checking that the Job readopts the Pod 03/27/23 21:59:55.897
    Mar 27 21:59:55.898: INFO: Waiting up to 15m0s for pod "adopt-release-ngxf7" in namespace "job-9998" to be "adopted"
    Mar 27 21:59:55.909: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 11.109726ms
    Mar 27 21:59:57.922: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.02388155s
    Mar 27 21:59:57.922: INFO: Pod "adopt-release-ngxf7" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/27/23 21:59:57.922
    Mar 27 21:59:58.464: INFO: Successfully updated pod "adopt-release-ngxf7"
    STEP: Checking that the Job releases the Pod 03/27/23 21:59:58.464
    Mar 27 21:59:58.464: INFO: Waiting up to 15m0s for pod "adopt-release-ngxf7" in namespace "job-9998" to be "released"
    Mar 27 21:59:58.478: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 13.611677ms
    Mar 27 22:00:00.494: INFO: Pod "adopt-release-ngxf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.029734567s
    Mar 27 22:00:00.494: INFO: Pod "adopt-release-ngxf7" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9998" for this suite. 03/27/23 22:00:00.518
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:00.543
Mar 27 22:00:00.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:00:00.545
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:00.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:00.613
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Mar 27 22:00:00.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/27/23 22:00:03.253
Mar 27 22:00:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
Mar 27 22:00:04.205: INFO: stderr: ""
Mar 27 22:00:04.205: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 27 22:00:04.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 delete e2e-test-crd-publish-openapi-2172-crds test-foo'
Mar 27 22:00:04.405: INFO: stderr: ""
Mar 27 22:00:04.405: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 27 22:00:04.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
Mar 27 22:00:05.232: INFO: stderr: ""
Mar 27 22:00:05.232: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 27 22:00:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 delete e2e-test-crd-publish-openapi-2172-crds test-foo'
Mar 27 22:00:05.362: INFO: stderr: ""
Mar 27 22:00:05.362: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/27/23 22:00:05.362
Mar 27 22:00:05.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
Mar 27 22:00:06.048: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/27/23 22:00:06.048
Mar 27 22:00:06.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
Mar 27 22:00:06.336: INFO: rc: 1
Mar 27 22:00:06.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
Mar 27 22:00:06.638: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/27/23 22:00:06.638
Mar 27 22:00:06.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
Mar 27 22:00:06.920: INFO: rc: 1
Mar 27 22:00:06.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
Mar 27 22:00:07.257: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/27/23 22:00:07.257
Mar 27 22:00:07.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds'
Mar 27 22:00:07.539: INFO: stderr: ""
Mar 27 22:00:07.539: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/27/23 22:00:07.539
Mar 27 22:00:07.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.metadata'
Mar 27 22:00:07.813: INFO: stderr: ""
Mar 27 22:00:07.813: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 27 22:00:07.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec'
Mar 27 22:00:08.118: INFO: stderr: ""
Mar 27 22:00:08.118: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 27 22:00:08.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec.bars'
Mar 27 22:00:08.396: INFO: stderr: ""
Mar 27 22:00:08.396: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/27/23 22:00:08.396
Mar 27 22:00:08.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec.bars2'
Mar 27 22:00:08.673: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:10.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7605" for this suite. 03/27/23 22:00:10.983
------------------------------
• [SLOW TEST] [10.466 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:00.543
    Mar 27 22:00:00.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:00:00.545
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:00.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:00.613
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Mar 27 22:00:00.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/27/23 22:00:03.253
    Mar 27 22:00:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
    Mar 27 22:00:04.205: INFO: stderr: ""
    Mar 27 22:00:04.205: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 27 22:00:04.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 delete e2e-test-crd-publish-openapi-2172-crds test-foo'
    Mar 27 22:00:04.405: INFO: stderr: ""
    Mar 27 22:00:04.405: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 27 22:00:04.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
    Mar 27 22:00:05.232: INFO: stderr: ""
    Mar 27 22:00:05.232: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 27 22:00:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 delete e2e-test-crd-publish-openapi-2172-crds test-foo'
    Mar 27 22:00:05.362: INFO: stderr: ""
    Mar 27 22:00:05.362: INFO: stdout: "e2e-test-crd-publish-openapi-2172-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/27/23 22:00:05.362
    Mar 27 22:00:05.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
    Mar 27 22:00:06.048: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/27/23 22:00:06.048
    Mar 27 22:00:06.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
    Mar 27 22:00:06.336: INFO: rc: 1
    Mar 27 22:00:06.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
    Mar 27 22:00:06.638: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/27/23 22:00:06.638
    Mar 27 22:00:06.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 create -f -'
    Mar 27 22:00:06.920: INFO: rc: 1
    Mar 27 22:00:06.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 --namespace=crd-publish-openapi-7605 apply -f -'
    Mar 27 22:00:07.257: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/27/23 22:00:07.257
    Mar 27 22:00:07.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds'
    Mar 27 22:00:07.539: INFO: stderr: ""
    Mar 27 22:00:07.539: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/27/23 22:00:07.539
    Mar 27 22:00:07.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.metadata'
    Mar 27 22:00:07.813: INFO: stderr: ""
    Mar 27 22:00:07.813: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 27 22:00:07.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec'
    Mar 27 22:00:08.118: INFO: stderr: ""
    Mar 27 22:00:08.118: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 27 22:00:08.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec.bars'
    Mar 27 22:00:08.396: INFO: stderr: ""
    Mar 27 22:00:08.396: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2172-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/27/23 22:00:08.396
    Mar 27 22:00:08.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=crd-publish-openapi-7605 explain e2e-test-crd-publish-openapi-2172-crds.spec.bars2'
    Mar 27 22:00:08.673: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:10.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7605" for this suite. 03/27/23 22:00:10.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:11.02
Mar 27 22:00:11.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename podtemplate 03/27/23 22:00:11.022
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:11.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:11.068
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6837" for this suite. 03/27/23 22:00:11.233
------------------------------
• [0.243 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:11.02
    Mar 27 22:00:11.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename podtemplate 03/27/23 22:00:11.022
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:11.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:11.068
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6837" for this suite. 03/27/23 22:00:11.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:11.27
Mar 27 22:00:11.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 22:00:11.271
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:11.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:11.329
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 03/27/23 22:00:11.338
Mar 27 22:00:11.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7" in namespace "downward-api-1775" to be "Succeeded or Failed"
Mar 27 22:00:11.387: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.94594ms
Mar 27 22:00:13.402: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028046298s
Mar 27 22:00:15.401: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02704506s
STEP: Saw pod success 03/27/23 22:00:15.402
Mar 27 22:00:15.402: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7" satisfied condition "Succeeded or Failed"
Mar 27 22:00:15.414: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 container client-container: <nil>
STEP: delete the pod 03/27/23 22:00:15.469
Mar 27 22:00:15.516: INFO: Waiting for pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 to disappear
Mar 27 22:00:15.529: INFO: Pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:15.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1775" for this suite. 03/27/23 22:00:15.558
------------------------------
• [4.313 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:11.27
    Mar 27 22:00:11.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 22:00:11.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:11.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:11.329
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 03/27/23 22:00:11.338
    Mar 27 22:00:11.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7" in namespace "downward-api-1775" to be "Succeeded or Failed"
    Mar 27 22:00:11.387: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.94594ms
    Mar 27 22:00:13.402: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028046298s
    Mar 27 22:00:15.401: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02704506s
    STEP: Saw pod success 03/27/23 22:00:15.402
    Mar 27 22:00:15.402: INFO: Pod "downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7" satisfied condition "Succeeded or Failed"
    Mar 27 22:00:15.414: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 container client-container: <nil>
    STEP: delete the pod 03/27/23 22:00:15.469
    Mar 27 22:00:15.516: INFO: Waiting for pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 to disappear
    Mar 27 22:00:15.529: INFO: Pod downwardapi-volume-c3d65dc0-5e72-422c-82f8-57e8a02e0cf7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:15.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1775" for this suite. 03/27/23 22:00:15.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:15.589
Mar 27 22:00:15.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename subpath 03/27/23 22:00:15.59
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:15.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:15.635
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/27/23 22:00:15.645
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-gkl8 03/27/23 22:00:15.676
STEP: Creating a pod to test atomic-volume-subpath 03/27/23 22:00:15.677
Mar 27 22:00:15.701: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gkl8" in namespace "subpath-636" to be "Succeeded or Failed"
Mar 27 22:00:15.715: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.965407ms
Mar 27 22:00:17.733: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031953222s
Mar 27 22:00:19.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.028354982s
Mar 27 22:00:21.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.027894461s
Mar 27 22:00:23.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.029413894s
Mar 27 22:00:25.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.028935424s
Mar 27 22:00:27.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.027742196s
Mar 27 22:00:29.732: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.030942105s
Mar 27 22:00:31.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.027964579s
Mar 27 22:00:33.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.028587269s
Mar 27 22:00:35.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.028699789s
Mar 27 22:00:37.732: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 22.031619745s
Mar 27 22:00:39.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=false. Elapsed: 24.027693626s
Mar 27 22:00:41.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029185842s
STEP: Saw pod success 03/27/23 22:00:41.73
Mar 27 22:00:41.730: INFO: Pod "pod-subpath-test-secret-gkl8" satisfied condition "Succeeded or Failed"
Mar 27 22:00:41.744: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-secret-gkl8 container test-container-subpath-secret-gkl8: <nil>
STEP: delete the pod 03/27/23 22:00:41.776
Mar 27 22:00:41.822: INFO: Waiting for pod pod-subpath-test-secret-gkl8 to disappear
Mar 27 22:00:41.835: INFO: Pod pod-subpath-test-secret-gkl8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-gkl8 03/27/23 22:00:41.835
Mar 27 22:00:41.836: INFO: Deleting pod "pod-subpath-test-secret-gkl8" in namespace "subpath-636"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-636" for this suite. 03/27/23 22:00:41.87
------------------------------
• [SLOW TEST] [26.307 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:15.589
    Mar 27 22:00:15.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename subpath 03/27/23 22:00:15.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:15.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:15.635
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/27/23 22:00:15.645
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-gkl8 03/27/23 22:00:15.676
    STEP: Creating a pod to test atomic-volume-subpath 03/27/23 22:00:15.677
    Mar 27 22:00:15.701: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gkl8" in namespace "subpath-636" to be "Succeeded or Failed"
    Mar 27 22:00:15.715: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.965407ms
    Mar 27 22:00:17.733: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031953222s
    Mar 27 22:00:19.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 4.028354982s
    Mar 27 22:00:21.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 6.027894461s
    Mar 27 22:00:23.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 8.029413894s
    Mar 27 22:00:25.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 10.028935424s
    Mar 27 22:00:27.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 12.027742196s
    Mar 27 22:00:29.732: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 14.030942105s
    Mar 27 22:00:31.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 16.027964579s
    Mar 27 22:00:33.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 18.028587269s
    Mar 27 22:00:35.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 20.028699789s
    Mar 27 22:00:37.732: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=true. Elapsed: 22.031619745s
    Mar 27 22:00:39.729: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Running", Reason="", readiness=false. Elapsed: 24.027693626s
    Mar 27 22:00:41.730: INFO: Pod "pod-subpath-test-secret-gkl8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029185842s
    STEP: Saw pod success 03/27/23 22:00:41.73
    Mar 27 22:00:41.730: INFO: Pod "pod-subpath-test-secret-gkl8" satisfied condition "Succeeded or Failed"
    Mar 27 22:00:41.744: INFO: Trying to get logs from node 10.176.99.177 pod pod-subpath-test-secret-gkl8 container test-container-subpath-secret-gkl8: <nil>
    STEP: delete the pod 03/27/23 22:00:41.776
    Mar 27 22:00:41.822: INFO: Waiting for pod pod-subpath-test-secret-gkl8 to disappear
    Mar 27 22:00:41.835: INFO: Pod pod-subpath-test-secret-gkl8 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-gkl8 03/27/23 22:00:41.835
    Mar 27 22:00:41.836: INFO: Deleting pod "pod-subpath-test-secret-gkl8" in namespace "subpath-636"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-636" for this suite. 03/27/23 22:00:41.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:41.897
Mar 27 22:00:41.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename runtimeclass 03/27/23 22:00:41.898
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:41.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:41.945
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 27 22:00:41.981: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5080 to be scheduled
Mar 27 22:00:41.994: INFO: 1 pods are not scheduled: [runtimeclass-5080/test-runtimeclass-runtimeclass-5080-preconfigured-handler-bgtsv(42a3b38a-af16-43e3-85f0-bf19cc9bbe51)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:44.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5080" for this suite. 03/27/23 22:00:44.04
------------------------------
• [2.167 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:41.897
    Mar 27 22:00:41.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 22:00:41.898
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:41.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:41.945
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 27 22:00:41.981: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5080 to be scheduled
    Mar 27 22:00:41.994: INFO: 1 pods are not scheduled: [runtimeclass-5080/test-runtimeclass-runtimeclass-5080-preconfigured-handler-bgtsv(42a3b38a-af16-43e3-85f0-bf19cc9bbe51)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:44.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5080" for this suite. 03/27/23 22:00:44.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:44.065
Mar 27 22:00:44.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename endpointslice 03/27/23 22:00:44.066
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:44.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:44.112
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 03/27/23 22:00:44.119
STEP: getting /apis/discovery.k8s.io 03/27/23 22:00:44.126
STEP: getting /apis/discovery.k8s.iov1 03/27/23 22:00:44.13
STEP: creating 03/27/23 22:00:44.133
STEP: getting 03/27/23 22:00:44.18
STEP: listing 03/27/23 22:00:44.193
STEP: watching 03/27/23 22:00:44.206
Mar 27 22:00:44.206: INFO: starting watch
STEP: cluster-wide listing 03/27/23 22:00:44.21
STEP: cluster-wide watching 03/27/23 22:00:44.225
Mar 27 22:00:44.225: INFO: starting watch
STEP: patching 03/27/23 22:00:44.231
STEP: updating 03/27/23 22:00:44.247
Mar 27 22:00:44.277: INFO: waiting for watch events with expected annotations
Mar 27 22:00:44.277: INFO: saw patched and updated annotations
STEP: deleting 03/27/23 22:00:44.277
STEP: deleting a collection 03/27/23 22:00:44.33
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Mar 27 22:00:44.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5087" for this suite. 03/27/23 22:00:44.429
------------------------------
• [0.389 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:44.065
    Mar 27 22:00:44.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename endpointslice 03/27/23 22:00:44.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:44.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:44.112
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 03/27/23 22:00:44.119
    STEP: getting /apis/discovery.k8s.io 03/27/23 22:00:44.126
    STEP: getting /apis/discovery.k8s.iov1 03/27/23 22:00:44.13
    STEP: creating 03/27/23 22:00:44.133
    STEP: getting 03/27/23 22:00:44.18
    STEP: listing 03/27/23 22:00:44.193
    STEP: watching 03/27/23 22:00:44.206
    Mar 27 22:00:44.206: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 22:00:44.21
    STEP: cluster-wide watching 03/27/23 22:00:44.225
    Mar 27 22:00:44.225: INFO: starting watch
    STEP: patching 03/27/23 22:00:44.231
    STEP: updating 03/27/23 22:00:44.247
    Mar 27 22:00:44.277: INFO: waiting for watch events with expected annotations
    Mar 27 22:00:44.277: INFO: saw patched and updated annotations
    STEP: deleting 03/27/23 22:00:44.277
    STEP: deleting a collection 03/27/23 22:00:44.33
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:00:44.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5087" for this suite. 03/27/23 22:00:44.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:00:44.455
Mar 27 22:00:44.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename aggregator 03/27/23 22:00:44.457
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:44.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:44.502
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 27 22:00:44.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/27/23 22:00:44.512
Mar 27 22:00:45.403: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 27 22:00:47.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:49.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:51.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:53.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:55.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:57.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:00:59.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:01:01.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:01:03.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:01:05.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:01:07.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:01:09.730: INFO: Waited 167.818844ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/27/23 22:01:09.864
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/27/23 22:01:09.877
STEP: List APIServices 03/27/23 22:01:09.893
Mar 27 22:01:09.915: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:10.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9995" for this suite. 03/27/23 22:01:10.314
------------------------------
• [SLOW TEST] [25.882 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:00:44.455
    Mar 27 22:00:44.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename aggregator 03/27/23 22:00:44.457
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:00:44.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:00:44.502
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 27 22:00:44.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/27/23 22:00:44.512
    Mar 27 22:00:45.403: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 27 22:00:47.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:49.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:51.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:53.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:55.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:57.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:00:59.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:01:01.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:01:03.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:01:05.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:01:07.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 0, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:01:09.730: INFO: Waited 167.818844ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/27/23 22:01:09.864
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/27/23 22:01:09.877
    STEP: List APIServices 03/27/23 22:01:09.893
    Mar 27 22:01:09.915: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:10.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9995" for this suite. 03/27/23 22:01:10.314
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:10.339
Mar 27 22:01:10.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename disruption 03/27/23 22:01:10.34
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:10.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:10.383
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 03/27/23 22:01:10.399
STEP: Waiting for all pods to be running 03/27/23 22:01:12.466
Mar 27 22:01:12.479: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:14.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9302" for this suite. 03/27/23 22:01:14.522
------------------------------
• [4.207 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:10.339
    Mar 27 22:01:10.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename disruption 03/27/23 22:01:10.34
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:10.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:10.383
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 03/27/23 22:01:10.399
    STEP: Waiting for all pods to be running 03/27/23 22:01:12.466
    Mar 27 22:01:12.479: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:14.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9302" for this suite. 03/27/23 22:01:14.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:14.55
Mar 27 22:01:14.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 22:01:14.551
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:14.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:14.597
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-2753/secret-test-50e5f781-ff84-483f-b0c6-a4cba08d100b 03/27/23 22:01:14.604
STEP: Creating a pod to test consume secrets 03/27/23 22:01:14.62
Mar 27 22:01:14.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970" in namespace "secrets-2753" to be "Succeeded or Failed"
Mar 27 22:01:14.654: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Pending", Reason="", readiness=false. Elapsed: 13.828774ms
Mar 27 22:01:16.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02848341s
Mar 27 22:01:18.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028598547s
STEP: Saw pod success 03/27/23 22:01:18.669
Mar 27 22:01:18.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970" satisfied condition "Succeeded or Failed"
Mar 27 22:01:18.688: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 container env-test: <nil>
STEP: delete the pod 03/27/23 22:01:18.722
Mar 27 22:01:18.761: INFO: Waiting for pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 to disappear
Mar 27 22:01:18.784: INFO: Pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:18.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2753" for this suite. 03/27/23 22:01:18.803
------------------------------
• [4.276 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:14.55
    Mar 27 22:01:14.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 22:01:14.551
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:14.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:14.597
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-2753/secret-test-50e5f781-ff84-483f-b0c6-a4cba08d100b 03/27/23 22:01:14.604
    STEP: Creating a pod to test consume secrets 03/27/23 22:01:14.62
    Mar 27 22:01:14.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970" in namespace "secrets-2753" to be "Succeeded or Failed"
    Mar 27 22:01:14.654: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Pending", Reason="", readiness=false. Elapsed: 13.828774ms
    Mar 27 22:01:16.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02848341s
    Mar 27 22:01:18.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028598547s
    STEP: Saw pod success 03/27/23 22:01:18.669
    Mar 27 22:01:18.669: INFO: Pod "pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970" satisfied condition "Succeeded or Failed"
    Mar 27 22:01:18.688: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 container env-test: <nil>
    STEP: delete the pod 03/27/23 22:01:18.722
    Mar 27 22:01:18.761: INFO: Waiting for pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 to disappear
    Mar 27 22:01:18.784: INFO: Pod pod-configmaps-3fd4c90e-f8f1-4b0c-8870-fe7702bd6970 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:18.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2753" for this suite. 03/27/23 22:01:18.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:18.833
Mar 27 22:01:18.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context-test 03/27/23 22:01:18.834
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:18.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:18.882
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Mar 27 22:01:18.940: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3" in namespace "security-context-test-700" to be "Succeeded or Failed"
Mar 27 22:01:18.954: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.830817ms
Mar 27 22:01:20.969: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028730635s
Mar 27 22:01:22.969: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028951783s
Mar 27 22:01:24.970: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030050333s
Mar 27 22:01:24.970: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3" satisfied condition "Succeeded or Failed"
Mar 27 22:01:25.001: INFO: Got logs for pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-700" for this suite. 03/27/23 22:01:25.021
------------------------------
• [SLOW TEST] [6.213 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:18.833
    Mar 27 22:01:18.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context-test 03/27/23 22:01:18.834
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:18.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:18.882
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Mar 27 22:01:18.940: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3" in namespace "security-context-test-700" to be "Succeeded or Failed"
    Mar 27 22:01:18.954: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.830817ms
    Mar 27 22:01:20.969: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028730635s
    Mar 27 22:01:22.969: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028951783s
    Mar 27 22:01:24.970: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030050333s
    Mar 27 22:01:24.970: INFO: Pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3" satisfied condition "Succeeded or Failed"
    Mar 27 22:01:25.001: INFO: Got logs for pod "busybox-privileged-false-f711c127-3c15-4254-ac38-c9b9a9984fc3": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-700" for this suite. 03/27/23 22:01:25.021
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:25.047
Mar 27 22:01:25.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 22:01:25.048
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:25.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:25.093
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/27/23 22:01:25.121
Mar 27 22:01:25.122: INFO: Creating simple deployment test-deployment-xznrj
Mar 27 22:01:25.172: INFO: deployment "test-deployment-xznrj" doesn't have the required revision set
STEP: Getting /status 03/27/23 22:01:27.248
Mar 27 22:01:27.263: INFO: Deployment test-deployment-xznrj has Conditions: [{Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 03/27/23 22:01:27.263
Mar 27 22:01:27.294: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 1, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xznrj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/27/23 22:01:27.294
Mar 27 22:01:27.298: INFO: Observed &Deployment event: ADDED
Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
Mar 27 22:01:27.299: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 22:01:27.300: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.300: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 22:01:27.300: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xznrj-54bc444df" is progressing.}
Mar 27 22:01:27.300: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
Mar 27 22:01:27.301: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
Mar 27 22:01:27.302: INFO: Found Deployment test-deployment-xznrj in namespace deployment-7624 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 22:01:27.302: INFO: Deployment test-deployment-xznrj has an updated status
STEP: patching the Statefulset Status 03/27/23 22:01:27.302
Mar 27 22:01:27.302: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 22:01:27.321: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/27/23 22:01:27.321
Mar 27 22:01:27.327: INFO: Observed &Deployment event: ADDED
Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
Mar 27 22:01:27.327: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 22:01:27.327: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xznrj-54bc444df" is progressing.}
Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
Mar 27 22:01:27.328: INFO: Found deployment test-deployment-xznrj in namespace deployment-7624 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 27 22:01:27.328: INFO: Deployment test-deployment-xznrj has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 22:01:27.346: INFO: Deployment "test-deployment-xznrj":
&Deployment{ObjectMeta:{test-deployment-xznrj  deployment-7624  4ee3d36e-e2b3-47b9-8a69-5d247b746ca2 41432 1 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054247f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-xznrj-54bc444df",LastUpdateTime:2023-03-27 22:01:27 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 22:01:27.363: INFO: New ReplicaSet "test-deployment-xznrj-54bc444df" of Deployment "test-deployment-xznrj":
&ReplicaSet{ObjectMeta:{test-deployment-xznrj-54bc444df  deployment-7624  d1785558-adc6-4809-9b5a-bb2fb780c9cc 41428 1 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xznrj 4ee3d36e-e2b3-47b9-8a69-5d247b746ca2 0xc00497c650 0xc00497c651}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ee3d36e-e2b3-47b9-8a69-5d247b746ca2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00497c6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:01:27.376: INFO: Pod "test-deployment-xznrj-54bc444df-dvvqf" is available:
&Pod{ObjectMeta:{test-deployment-xznrj-54bc444df-dvvqf test-deployment-xznrj-54bc444df- deployment-7624  9d949207-fce8-4c69-85b5-f2f02645eb2e 41427 0 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:c21b809c3e14e4cf54fe0fd7a77b71e7b2aba92b6297a40bf4010538bb87e2eb cni.projectcalico.org/podIP:172.30.85.158/32 cni.projectcalico.org/podIPs:172.30.85.158/32] [{apps/v1 ReplicaSet test-deployment-xznrj-54bc444df d1785558-adc6-4809-9b5a-bb2fb780c9cc 0xc00497cac0 0xc00497cac1}] [] [{kube-controller-manager Update v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1785558-adc6-4809-9b5a-bb2fb780c9cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7t5b9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7t5b9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.158,StartTime:2023-03-27 22:01:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:01:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d2d26162df4bb8c76bf2c7bcaf02110245edc3611de6f3ffb3b63ba8dd6ee50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:27.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7624" for this suite. 03/27/23 22:01:27.412
------------------------------
• [2.396 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:25.047
    Mar 27 22:01:25.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 22:01:25.048
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:25.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:25.093
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/27/23 22:01:25.121
    Mar 27 22:01:25.122: INFO: Creating simple deployment test-deployment-xznrj
    Mar 27 22:01:25.172: INFO: deployment "test-deployment-xznrj" doesn't have the required revision set
    STEP: Getting /status 03/27/23 22:01:27.248
    Mar 27 22:01:27.263: INFO: Deployment test-deployment-xznrj has Conditions: [{Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 03/27/23 22:01:27.263
    Mar 27 22:01:27.294: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 1, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 1, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xznrj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/27/23 22:01:27.294
    Mar 27 22:01:27.298: INFO: Observed &Deployment event: ADDED
    Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
    Mar 27 22:01:27.299: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
    Mar 27 22:01:27.299: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 22:01:27.300: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.300: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 22:01:27.300: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xznrj-54bc444df" is progressing.}
    Mar 27 22:01:27.300: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
    Mar 27 22:01:27.301: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 22:01:27.301: INFO: Observed Deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
    Mar 27 22:01:27.302: INFO: Found Deployment test-deployment-xznrj in namespace deployment-7624 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 22:01:27.302: INFO: Deployment test-deployment-xznrj has an updated status
    STEP: patching the Statefulset Status 03/27/23 22:01:27.302
    Mar 27 22:01:27.302: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 22:01:27.321: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/27/23 22:01:27.321
    Mar 27 22:01:27.327: INFO: Observed &Deployment event: ADDED
    Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
    Mar 27 22:01:27.327: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xznrj-54bc444df"}
    Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 22:01:27.327: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.327: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:25 +0000 UTC 2023-03-27 22:01:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xznrj-54bc444df" is progressing.}
    Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
    Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-27 22:01:27 +0000 UTC 2023-03-27 22:01:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xznrj-54bc444df" has successfully progressed.}
    Mar 27 22:01:27.328: INFO: Observed deployment test-deployment-xznrj in namespace deployment-7624 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 22:01:27.328: INFO: Observed &Deployment event: MODIFIED
    Mar 27 22:01:27.328: INFO: Found deployment test-deployment-xznrj in namespace deployment-7624 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 27 22:01:27.328: INFO: Deployment test-deployment-xznrj has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 22:01:27.346: INFO: Deployment "test-deployment-xznrj":
    &Deployment{ObjectMeta:{test-deployment-xznrj  deployment-7624  4ee3d36e-e2b3-47b9-8a69-5d247b746ca2 41432 1 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054247f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-xznrj-54bc444df",LastUpdateTime:2023-03-27 22:01:27 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 22:01:27.363: INFO: New ReplicaSet "test-deployment-xznrj-54bc444df" of Deployment "test-deployment-xznrj":
    &ReplicaSet{ObjectMeta:{test-deployment-xznrj-54bc444df  deployment-7624  d1785558-adc6-4809-9b5a-bb2fb780c9cc 41428 1 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xznrj 4ee3d36e-e2b3-47b9-8a69-5d247b746ca2 0xc00497c650 0xc00497c651}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ee3d36e-e2b3-47b9-8a69-5d247b746ca2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00497c6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:01:27.376: INFO: Pod "test-deployment-xznrj-54bc444df-dvvqf" is available:
    &Pod{ObjectMeta:{test-deployment-xznrj-54bc444df-dvvqf test-deployment-xznrj-54bc444df- deployment-7624  9d949207-fce8-4c69-85b5-f2f02645eb2e 41427 0 2023-03-27 22:01:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:c21b809c3e14e4cf54fe0fd7a77b71e7b2aba92b6297a40bf4010538bb87e2eb cni.projectcalico.org/podIP:172.30.85.158/32 cni.projectcalico.org/podIPs:172.30.85.158/32] [{apps/v1 ReplicaSet test-deployment-xznrj-54bc444df d1785558-adc6-4809-9b5a-bb2fb780c9cc 0xc00497cac0 0xc00497cac1}] [] [{kube-controller-manager Update v1 2023-03-27 22:01:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1785558-adc6-4809-9b5a-bb2fb780c9cc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:01:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7t5b9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7t5b9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:01:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.158,StartTime:2023-03-27 22:01:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:01:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d2d26162df4bb8c76bf2c7bcaf02110245edc3611de6f3ffb3b63ba8dd6ee50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:27.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7624" for this suite. 03/27/23 22:01:27.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:27.447
Mar 27 22:01:27.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 22:01:27.449
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:27.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:27.496
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 03/27/23 22:01:27.504
Mar 27 22:01:27.524: INFO: created test-pod-1
Mar 27 22:01:27.540: INFO: created test-pod-2
Mar 27 22:01:27.555: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/27/23 22:01:27.555
Mar 27 22:01:27.556: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9252' to be running and ready
Mar 27 22:01:27.594: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 22:01:27.594: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 22:01:27.594: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 27 22:01:27.594: INFO: 0 / 3 pods in namespace 'pods-9252' are running and ready (0 seconds elapsed)
Mar 27 22:01:27.594: INFO: expected 0 pod replicas in namespace 'pods-9252', 0 are Running and Ready.
Mar 27 22:01:27.594: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Mar 27 22:01:27.594: INFO: test-pod-1  10.176.99.177  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
Mar 27 22:01:27.594: INFO: test-pod-2  10.176.99.177  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
Mar 27 22:01:27.594: INFO: test-pod-3  10.176.99.177  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
Mar 27 22:01:27.594: INFO: 
Mar 27 22:01:29.629: INFO: 3 / 3 pods in namespace 'pods-9252' are running and ready (2 seconds elapsed)
Mar 27 22:01:29.629: INFO: expected 0 pod replicas in namespace 'pods-9252', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/27/23 22:01:29.714
Mar 27 22:01:29.727: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 22:01:30.742: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 22:01:31.742: INFO: Pod quantity 3 is different from expected quantity 0
Mar 27 22:01:32.741: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9252" for this suite. 03/27/23 22:01:33.762
------------------------------
• [SLOW TEST] [6.341 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:27.447
    Mar 27 22:01:27.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 22:01:27.449
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:27.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:27.496
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 03/27/23 22:01:27.504
    Mar 27 22:01:27.524: INFO: created test-pod-1
    Mar 27 22:01:27.540: INFO: created test-pod-2
    Mar 27 22:01:27.555: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/27/23 22:01:27.555
    Mar 27 22:01:27.556: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9252' to be running and ready
    Mar 27 22:01:27.594: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 22:01:27.594: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 22:01:27.594: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 27 22:01:27.594: INFO: 0 / 3 pods in namespace 'pods-9252' are running and ready (0 seconds elapsed)
    Mar 27 22:01:27.594: INFO: expected 0 pod replicas in namespace 'pods-9252', 0 are Running and Ready.
    Mar 27 22:01:27.594: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Mar 27 22:01:27.594: INFO: test-pod-1  10.176.99.177  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
    Mar 27 22:01:27.594: INFO: test-pod-2  10.176.99.177  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
    Mar 27 22:01:27.594: INFO: test-pod-3  10.176.99.177  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:01:27 +0000 UTC  }]
    Mar 27 22:01:27.594: INFO: 
    Mar 27 22:01:29.629: INFO: 3 / 3 pods in namespace 'pods-9252' are running and ready (2 seconds elapsed)
    Mar 27 22:01:29.629: INFO: expected 0 pod replicas in namespace 'pods-9252', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/27/23 22:01:29.714
    Mar 27 22:01:29.727: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 22:01:30.742: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 22:01:31.742: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 27 22:01:32.741: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9252" for this suite. 03/27/23 22:01:33.762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:33.789
Mar 27 22:01:33.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 22:01:33.791
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:33.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:33.837
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 03/27/23 22:01:33.847
STEP: Creating a ResourceQuota 03/27/23 22:01:38.863
STEP: Ensuring resource quota status is calculated 03/27/23 22:01:38.878
STEP: Creating a Service 03/27/23 22:01:40.894
STEP: Creating a NodePort Service 03/27/23 22:01:40.928
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/27/23 22:01:40.982
STEP: Ensuring resource quota status captures service creation 03/27/23 22:01:41.047
STEP: Deleting Services 03/27/23 22:01:43.068
STEP: Ensuring resource quota status released usage 03/27/23 22:01:43.142
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:45.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7119" for this suite. 03/27/23 22:01:45.175
------------------------------
• [SLOW TEST] [11.411 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:33.789
    Mar 27 22:01:33.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 22:01:33.791
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:33.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:33.837
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 03/27/23 22:01:33.847
    STEP: Creating a ResourceQuota 03/27/23 22:01:38.863
    STEP: Ensuring resource quota status is calculated 03/27/23 22:01:38.878
    STEP: Creating a Service 03/27/23 22:01:40.894
    STEP: Creating a NodePort Service 03/27/23 22:01:40.928
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/27/23 22:01:40.982
    STEP: Ensuring resource quota status captures service creation 03/27/23 22:01:41.047
    STEP: Deleting Services 03/27/23 22:01:43.068
    STEP: Ensuring resource quota status released usage 03/27/23 22:01:43.142
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:45.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7119" for this suite. 03/27/23 22:01:45.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:45.202
Mar 27 22:01:45.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename watch 03/27/23 22:01:45.203
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:45.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:45.25
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/27/23 22:01:45.261
STEP: creating a new configmap 03/27/23 22:01:45.264
STEP: modifying the configmap once 03/27/23 22:01:45.279
STEP: changing the label value of the configmap 03/27/23 22:01:45.306
STEP: Expecting to observe a delete notification for the watched object 03/27/23 22:01:45.334
Mar 27 22:01:45.334: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41594 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:01:45.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41595 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:01:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41596 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/27/23 22:01:45.335
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/27/23 22:01:45.362
STEP: changing the label value of the configmap back 03/27/23 22:01:55.362
STEP: modifying the configmap a third time 03/27/23 22:01:55.394
STEP: deleting the configmap 03/27/23 22:01:55.453
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/27/23 22:01:55.488
Mar 27 22:01:55.488: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41621 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:01:55.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41622 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:01:55.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41623 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:55.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9671" for this suite. 03/27/23 22:01:55.533
------------------------------
• [SLOW TEST] [10.359 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:45.202
    Mar 27 22:01:45.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename watch 03/27/23 22:01:45.203
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:45.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:45.25
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/27/23 22:01:45.261
    STEP: creating a new configmap 03/27/23 22:01:45.264
    STEP: modifying the configmap once 03/27/23 22:01:45.279
    STEP: changing the label value of the configmap 03/27/23 22:01:45.306
    STEP: Expecting to observe a delete notification for the watched object 03/27/23 22:01:45.334
    Mar 27 22:01:45.334: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41594 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:01:45.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41595 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:01:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41596 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/27/23 22:01:45.335
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/27/23 22:01:45.362
    STEP: changing the label value of the configmap back 03/27/23 22:01:55.362
    STEP: modifying the configmap a third time 03/27/23 22:01:55.394
    STEP: deleting the configmap 03/27/23 22:01:55.453
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/27/23 22:01:55.488
    Mar 27 22:01:55.488: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41621 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:01:55.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41622 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:01:55.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9671  853820af-ddb2-4ead-8bd0-ce5657970ebf 41623 0 2023-03-27 22:01:45 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-27 22:01:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:55.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9671" for this suite. 03/27/23 22:01:55.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:55.564
Mar 27 22:01:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 22:01:55.565
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:55.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:55.613
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Mar 27 22:01:55.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: creating the pod 03/27/23 22:01:55.623
STEP: submitting the pod to kubernetes 03/27/23 22:01:55.623
Mar 27 22:01:55.646: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f" in namespace "pods-463" to be "running and ready"
Mar 27 22:01:55.660: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043757ms
Mar 27 22:01:55.660: INFO: The phase of Pod pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:01:57.682: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036308479s
Mar 27 22:01:57.683: INFO: The phase of Pod pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f is Running (Ready = true)
Mar 27 22:01:57.683: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 22:01:57.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-463" for this suite. 03/27/23 22:01:57.901
------------------------------
• [2.363 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:55.564
    Mar 27 22:01:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 22:01:55.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:55.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:55.613
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Mar 27 22:01:55.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: creating the pod 03/27/23 22:01:55.623
    STEP: submitting the pod to kubernetes 03/27/23 22:01:55.623
    Mar 27 22:01:55.646: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f" in namespace "pods-463" to be "running and ready"
    Mar 27 22:01:55.660: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043757ms
    Mar 27 22:01:55.660: INFO: The phase of Pod pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:01:57.682: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f": Phase="Running", Reason="", readiness=true. Elapsed: 2.036308479s
    Mar 27 22:01:57.683: INFO: The phase of Pod pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f is Running (Ready = true)
    Mar 27 22:01:57.683: INFO: Pod "pod-exec-websocket-50e1b1bb-4b62-45ff-82f8-e02e4809427f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:01:57.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-463" for this suite. 03/27/23 22:01:57.901
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:01:57.928
Mar 27 22:01:57.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 22:01:57.929
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:57.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:57.983
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 22:01:58.035
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:01:58.651
STEP: Deploying the webhook pod 03/27/23 22:01:58.686
STEP: Wait for the deployment to be ready 03/27/23 22:01:58.722
Mar 27 22:01:58.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 22:02:00.791
STEP: Verifying the service has paired with the endpoint 03/27/23 22:02:00.812
Mar 27 22:02:01.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Mar 27 22:02:01.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1445-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 22:02:02.348
STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 22:02:02.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:02:05.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6889" for this suite. 03/27/23 22:02:05.207
STEP: Destroying namespace "webhook-6889-markers" for this suite. 03/27/23 22:02:05.232
------------------------------
• [SLOW TEST] [7.329 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:01:57.928
    Mar 27 22:01:57.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 22:01:57.929
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:01:57.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:01:57.983
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 22:01:58.035
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:01:58.651
    STEP: Deploying the webhook pod 03/27/23 22:01:58.686
    STEP: Wait for the deployment to be ready 03/27/23 22:01:58.722
    Mar 27 22:01:58.750: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 22:02:00.791
    STEP: Verifying the service has paired with the endpoint 03/27/23 22:02:00.812
    Mar 27 22:02:01.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Mar 27 22:02:01.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1445-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 22:02:02.348
    STEP: Creating a custom resource that should be mutated by the webhook 03/27/23 22:02:02.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:02:05.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6889" for this suite. 03/27/23 22:02:05.207
    STEP: Destroying namespace "webhook-6889-markers" for this suite. 03/27/23 22:02:05.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:02:05.272
Mar 27 22:02:05.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption 03/27/23 22:02:05.273
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:02:05.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:02:05.319
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 27 22:02:05.353: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 22:03:05.431: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:03:05.444
Mar 27 22:03:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 22:03:05.446
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:05.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:05.492
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Mar 27 22:03:05.526: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 27 22:03:05.534: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Mar 27 22:03:05.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:03:05.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5221" for this suite. 03/27/23 22:03:05.735
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5310" for this suite. 03/27/23 22:03:05.777
------------------------------
• [SLOW TEST] [60.530 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:02:05.272
    Mar 27 22:02:05.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 22:02:05.273
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:02:05.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:02:05.319
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 27 22:02:05.353: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 22:03:05.431: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:03:05.444
    Mar 27 22:03:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption-path 03/27/23 22:03:05.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:05.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:05.492
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Mar 27 22:03:05.526: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 27 22:03:05.534: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:03:05.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:03:05.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5221" for this suite. 03/27/23 22:03:05.735
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5310" for this suite. 03/27/23 22:03:05.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:03:05.803
Mar 27 22:03:05.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:03:05.804
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:05.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:05.851
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-caff4598-f932-41a3-81d8-9441bef21f6d 03/27/23 22:03:05.859
STEP: Creating a pod to test consume configMaps 03/27/23 22:03:05.874
Mar 27 22:03:05.895: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f" in namespace "projected-2199" to be "Succeeded or Failed"
Mar 27 22:03:05.909: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.703905ms
Mar 27 22:03:07.924: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028928948s
Mar 27 22:03:09.922: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026863313s
STEP: Saw pod success 03/27/23 22:03:09.922
Mar 27 22:03:09.922: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f" satisfied condition "Succeeded or Failed"
Mar 27 22:03:09.935: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f container agnhost-container: <nil>
STEP: delete the pod 03/27/23 22:03:09.996
Mar 27 22:03:10.035: INFO: Waiting for pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f to disappear
Mar 27 22:03:10.048: INFO: Pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:03:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2199" for this suite. 03/27/23 22:03:10.069
------------------------------
• [4.291 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:03:05.803
    Mar 27 22:03:05.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:03:05.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:05.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:05.851
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-caff4598-f932-41a3-81d8-9441bef21f6d 03/27/23 22:03:05.859
    STEP: Creating a pod to test consume configMaps 03/27/23 22:03:05.874
    Mar 27 22:03:05.895: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f" in namespace "projected-2199" to be "Succeeded or Failed"
    Mar 27 22:03:05.909: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.703905ms
    Mar 27 22:03:07.924: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028928948s
    Mar 27 22:03:09.922: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026863313s
    STEP: Saw pod success 03/27/23 22:03:09.922
    Mar 27 22:03:09.922: INFO: Pod "pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f" satisfied condition "Succeeded or Failed"
    Mar 27 22:03:09.935: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 22:03:09.996
    Mar 27 22:03:10.035: INFO: Waiting for pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f to disappear
    Mar 27 22:03:10.048: INFO: Pod pod-projected-configmaps-61feeced-c71f-49c3-be52-cac6c137ed5f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:03:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2199" for this suite. 03/27/23 22:03:10.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:03:10.095
Mar 27 22:03:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:03:10.097
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:10.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:10.141
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-9bnbt"  03/27/23 22:03:10.149
Mar 27 22:03:10.163: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-9bnbt"  03/27/23 22:03:10.163
Mar 27 22:03:10.192: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 22:03:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1849" for this suite. 03/27/23 22:03:10.21
------------------------------
• [0.145 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:03:10.095
    Mar 27 22:03:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:03:10.097
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:10.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:10.141
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-9bnbt"  03/27/23 22:03:10.149
    Mar 27 22:03:10.163: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-9bnbt"  03/27/23 22:03:10.163
    Mar 27 22:03:10.192: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:03:10.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1849" for this suite. 03/27/23 22:03:10.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:03:10.243
Mar 27 22:03:10.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:03:10.245
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:10.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:10.304
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-e1996d10-afa0-4898-86b0-8860351b499d 03/27/23 22:03:10.328
STEP: Creating secret with name s-test-opt-upd-2acfc44b-1243-4a08-8118-2dce0ce3921b 03/27/23 22:03:10.343
STEP: Creating the pod 03/27/23 22:03:10.357
Mar 27 22:03:10.379: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643" in namespace "projected-3468" to be "running and ready"
Mar 27 22:03:10.392: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Pending", Reason="", readiness=false. Elapsed: 12.696268ms
Mar 27 22:03:10.392: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:03:12.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027713597s
Mar 27 22:03:12.407: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:03:14.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Running", Reason="", readiness=true. Elapsed: 4.027483438s
Mar 27 22:03:14.407: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Running (Ready = true)
Mar 27 22:03:14.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e1996d10-afa0-4898-86b0-8860351b499d 03/27/23 22:03:14.513
STEP: Updating secret s-test-opt-upd-2acfc44b-1243-4a08-8118-2dce0ce3921b 03/27/23 22:03:14.537
STEP: Creating secret with name s-test-opt-create-37ac04d6-7e5a-4e33-9cde-f6b17b3578fc 03/27/23 22:03:14.553
STEP: waiting to observe update in volume 03/27/23 22:03:14.568
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Mar 27 22:04:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3468" for this suite. 03/27/23 22:04:31.969
------------------------------
• [SLOW TEST] [81.803 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:03:10.243
    Mar 27 22:03:10.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:03:10.245
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:03:10.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:03:10.304
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-e1996d10-afa0-4898-86b0-8860351b499d 03/27/23 22:03:10.328
    STEP: Creating secret with name s-test-opt-upd-2acfc44b-1243-4a08-8118-2dce0ce3921b 03/27/23 22:03:10.343
    STEP: Creating the pod 03/27/23 22:03:10.357
    Mar 27 22:03:10.379: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643" in namespace "projected-3468" to be "running and ready"
    Mar 27 22:03:10.392: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Pending", Reason="", readiness=false. Elapsed: 12.696268ms
    Mar 27 22:03:10.392: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:03:12.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027713597s
    Mar 27 22:03:12.407: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:03:14.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643": Phase="Running", Reason="", readiness=true. Elapsed: 4.027483438s
    Mar 27 22:03:14.407: INFO: The phase of Pod pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643 is Running (Ready = true)
    Mar 27 22:03:14.407: INFO: Pod "pod-projected-secrets-fe1a2959-3a53-4cd1-9b94-219b4f328643" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e1996d10-afa0-4898-86b0-8860351b499d 03/27/23 22:03:14.513
    STEP: Updating secret s-test-opt-upd-2acfc44b-1243-4a08-8118-2dce0ce3921b 03/27/23 22:03:14.537
    STEP: Creating secret with name s-test-opt-create-37ac04d6-7e5a-4e33-9cde-f6b17b3578fc 03/27/23 22:03:14.553
    STEP: waiting to observe update in volume 03/27/23 22:03:14.568
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:04:31.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3468" for this suite. 03/27/23 22:04:31.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:04:32.072
Mar 27 22:04:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 22:04:32.074
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:32.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:32.153
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1421.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1421.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/27/23 22:04:32.177
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1421.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1421.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/27/23 22:04:32.178
STEP: creating a pod to probe /etc/hosts 03/27/23 22:04:32.178
STEP: submitting the pod to kubernetes 03/27/23 22:04:32.178
Mar 27 22:04:32.201: INFO: Waiting up to 15m0s for pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d" in namespace "dns-1421" to be "running"
Mar 27 22:04:32.219: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.389067ms
Mar 27 22:04:34.237: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.036084459s
Mar 27 22:04:34.237: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d" satisfied condition "running"
STEP: retrieving the pod 03/27/23 22:04:34.238
STEP: looking for the results for each expected name from probers 03/27/23 22:04:34.253
Mar 27 22:04:34.352: INFO: DNS probes using dns-1421/dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d succeeded

STEP: deleting the pod 03/27/23 22:04:34.352
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 22:04:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1421" for this suite. 03/27/23 22:04:34.426
------------------------------
• [2.394 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:04:32.072
    Mar 27 22:04:32.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 22:04:32.074
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:32.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:32.153
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1421.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1421.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/27/23 22:04:32.177
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1421.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1421.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/27/23 22:04:32.178
    STEP: creating a pod to probe /etc/hosts 03/27/23 22:04:32.178
    STEP: submitting the pod to kubernetes 03/27/23 22:04:32.178
    Mar 27 22:04:32.201: INFO: Waiting up to 15m0s for pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d" in namespace "dns-1421" to be "running"
    Mar 27 22:04:32.219: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.389067ms
    Mar 27 22:04:34.237: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.036084459s
    Mar 27 22:04:34.237: INFO: Pod "dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 22:04:34.238
    STEP: looking for the results for each expected name from probers 03/27/23 22:04:34.253
    Mar 27 22:04:34.352: INFO: DNS probes using dns-1421/dns-test-56eac14c-2ca7-4021-8e0e-afc33d0a3a4d succeeded

    STEP: deleting the pod 03/27/23 22:04:34.352
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:04:34.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1421" for this suite. 03/27/23 22:04:34.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:04:34.474
Mar 27 22:04:34.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:04:34.475
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:34.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:34.539
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 03/27/23 22:04:34.547
Mar 27 22:04:34.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4" in namespace "projected-548" to be "Succeeded or Failed"
Mar 27 22:04:34.608: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 35.487807ms
Mar 27 22:04:36.623: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050597645s
Mar 27 22:04:38.626: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052918784s
Mar 27 22:04:40.622: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049353959s
STEP: Saw pod success 03/27/23 22:04:40.622
Mar 27 22:04:40.623: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4" satisfied condition "Succeeded or Failed"
Mar 27 22:04:40.639: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 container client-container: <nil>
STEP: delete the pod 03/27/23 22:04:40.695
Mar 27 22:04:40.731: INFO: Waiting for pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 to disappear
Mar 27 22:04:40.745: INFO: Pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 22:04:40.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-548" for this suite. 03/27/23 22:04:40.764
------------------------------
• [SLOW TEST] [6.317 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:04:34.474
    Mar 27 22:04:34.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:04:34.475
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:34.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:34.539
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 03/27/23 22:04:34.547
    Mar 27 22:04:34.573: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4" in namespace "projected-548" to be "Succeeded or Failed"
    Mar 27 22:04:34.608: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 35.487807ms
    Mar 27 22:04:36.623: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050597645s
    Mar 27 22:04:38.626: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052918784s
    Mar 27 22:04:40.622: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049353959s
    STEP: Saw pod success 03/27/23 22:04:40.622
    Mar 27 22:04:40.623: INFO: Pod "downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4" satisfied condition "Succeeded or Failed"
    Mar 27 22:04:40.639: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 container client-container: <nil>
    STEP: delete the pod 03/27/23 22:04:40.695
    Mar 27 22:04:40.731: INFO: Waiting for pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 to disappear
    Mar 27 22:04:40.745: INFO: Pod downwardapi-volume-5fec78a5-ac9d-4f77-9ec4-e294cf53e5c4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:04:40.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-548" for this suite. 03/27/23 22:04:40.764
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:04:40.791
Mar 27 22:04:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 22:04:40.792
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:40.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:40.838
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 in namespace container-probe-4860 03/27/23 22:04:40.845
Mar 27 22:04:40.868: INFO: Waiting up to 5m0s for pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825" in namespace "container-probe-4860" to be "not pending"
Mar 27 22:04:40.882: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825": Phase="Pending", Reason="", readiness=false. Elapsed: 13.897255ms
Mar 27 22:04:42.916: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825": Phase="Running", Reason="", readiness=true. Elapsed: 2.047505046s
Mar 27 22:04:42.916: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825" satisfied condition "not pending"
Mar 27 22:04:42.916: INFO: Started pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 in namespace container-probe-4860
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 22:04:42.916
Mar 27 22:04:42.948: INFO: Initial restart count of pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is 0
Mar 27 22:05:03.146: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 1 (20.197950052s elapsed)
Mar 27 22:05:23.298: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 2 (40.350536526s elapsed)
Mar 27 22:05:43.450: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 3 (1m0.502524078s elapsed)
Mar 27 22:06:03.608: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 4 (1m20.660265947s elapsed)
Mar 27 22:07:16.171: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 5 (2m33.223504508s elapsed)
STEP: deleting the pod 03/27/23 22:07:16.171
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:16.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4860" for this suite. 03/27/23 22:07:16.243
------------------------------
• [SLOW TEST] [155.476 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:04:40.791
    Mar 27 22:04:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 22:04:40.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:04:40.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:04:40.838
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 in namespace container-probe-4860 03/27/23 22:04:40.845
    Mar 27 22:04:40.868: INFO: Waiting up to 5m0s for pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825" in namespace "container-probe-4860" to be "not pending"
    Mar 27 22:04:40.882: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825": Phase="Pending", Reason="", readiness=false. Elapsed: 13.897255ms
    Mar 27 22:04:42.916: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825": Phase="Running", Reason="", readiness=true. Elapsed: 2.047505046s
    Mar 27 22:04:42.916: INFO: Pod "liveness-914b7e00-a005-4790-a717-f4d36e7b7825" satisfied condition "not pending"
    Mar 27 22:04:42.916: INFO: Started pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 in namespace container-probe-4860
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 22:04:42.916
    Mar 27 22:04:42.948: INFO: Initial restart count of pod liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is 0
    Mar 27 22:05:03.146: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 1 (20.197950052s elapsed)
    Mar 27 22:05:23.298: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 2 (40.350536526s elapsed)
    Mar 27 22:05:43.450: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 3 (1m0.502524078s elapsed)
    Mar 27 22:06:03.608: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 4 (1m20.660265947s elapsed)
    Mar 27 22:07:16.171: INFO: Restart count of pod container-probe-4860/liveness-914b7e00-a005-4790-a717-f4d36e7b7825 is now 5 (2m33.223504508s elapsed)
    STEP: deleting the pod 03/27/23 22:07:16.171
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:16.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4860" for this suite. 03/27/23 22:07:16.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:16.276
Mar 27 22:07:16.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 22:07:16.277
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:16.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:16.326
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 22:07:16.334
Mar 27 22:07:16.359: INFO: Waiting up to 5m0s for pod "pod-a00af637-a6aa-44cf-864f-216635188f72" in namespace "emptydir-1158" to be "Succeeded or Failed"
Mar 27 22:07:16.372: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Pending", Reason="", readiness=false. Elapsed: 12.299662ms
Mar 27 22:07:18.389: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029666479s
Mar 27 22:07:20.386: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027051199s
STEP: Saw pod success 03/27/23 22:07:20.387
Mar 27 22:07:20.387: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72" satisfied condition "Succeeded or Failed"
Mar 27 22:07:20.401: INFO: Trying to get logs from node 10.176.99.177 pod pod-a00af637-a6aa-44cf-864f-216635188f72 container test-container: <nil>
STEP: delete the pod 03/27/23 22:07:20.459
Mar 27 22:07:20.510: INFO: Waiting for pod pod-a00af637-a6aa-44cf-864f-216635188f72 to disappear
Mar 27 22:07:20.524: INFO: Pod pod-a00af637-a6aa-44cf-864f-216635188f72 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:20.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1158" for this suite. 03/27/23 22:07:20.546
------------------------------
• [4.300 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:16.276
    Mar 27 22:07:16.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 22:07:16.277
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:16.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:16.326
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/27/23 22:07:16.334
    Mar 27 22:07:16.359: INFO: Waiting up to 5m0s for pod "pod-a00af637-a6aa-44cf-864f-216635188f72" in namespace "emptydir-1158" to be "Succeeded or Failed"
    Mar 27 22:07:16.372: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Pending", Reason="", readiness=false. Elapsed: 12.299662ms
    Mar 27 22:07:18.389: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029666479s
    Mar 27 22:07:20.386: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027051199s
    STEP: Saw pod success 03/27/23 22:07:20.387
    Mar 27 22:07:20.387: INFO: Pod "pod-a00af637-a6aa-44cf-864f-216635188f72" satisfied condition "Succeeded or Failed"
    Mar 27 22:07:20.401: INFO: Trying to get logs from node 10.176.99.177 pod pod-a00af637-a6aa-44cf-864f-216635188f72 container test-container: <nil>
    STEP: delete the pod 03/27/23 22:07:20.459
    Mar 27 22:07:20.510: INFO: Waiting for pod pod-a00af637-a6aa-44cf-864f-216635188f72 to disappear
    Mar 27 22:07:20.524: INFO: Pod pod-a00af637-a6aa-44cf-864f-216635188f72 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:20.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1158" for this suite. 03/27/23 22:07:20.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:20.596
Mar 27 22:07:20.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 22:07:20.598
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:20.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:20.648
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 03/27/23 22:07:20.687
STEP: watching for Pod to be ready 03/27/23 22:07:20.709
Mar 27 22:07:20.713: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 27 22:07:20.731: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
Mar 27 22:07:20.765: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
Mar 27 22:07:21.481: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
Mar 27 22:07:22.053: INFO: Found Pod pod-test in namespace pods-386 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/27/23 22:07:22.067
STEP: getting the Pod and ensuring that it's patched 03/27/23 22:07:22.089
STEP: replacing the Pod's status Ready condition to False 03/27/23 22:07:22.102
STEP: check the Pod again to ensure its Ready conditions are False 03/27/23 22:07:22.156
STEP: deleting the Pod via a Collection with a LabelSelector 03/27/23 22:07:22.156
STEP: watching for the Pod to be deleted 03/27/23 22:07:22.192
Mar 27 22:07:22.197: INFO: observed event type MODIFIED
Mar 27 22:07:24.058: INFO: observed event type MODIFIED
Mar 27 22:07:24.253: INFO: observed event type MODIFIED
Mar 27 22:07:25.067: INFO: observed event type MODIFIED
Mar 27 22:07:25.097: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:25.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-386" for this suite. 03/27/23 22:07:25.147
------------------------------
• [4.580 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:20.596
    Mar 27 22:07:20.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 22:07:20.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:20.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:20.648
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 03/27/23 22:07:20.687
    STEP: watching for Pod to be ready 03/27/23 22:07:20.709
    Mar 27 22:07:20.713: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 27 22:07:20.731: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
    Mar 27 22:07:20.765: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
    Mar 27 22:07:21.481: INFO: observed Pod pod-test in namespace pods-386 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
    Mar 27 22:07:22.053: INFO: Found Pod pod-test in namespace pods-386 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-27 22:07:20 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/27/23 22:07:22.067
    STEP: getting the Pod and ensuring that it's patched 03/27/23 22:07:22.089
    STEP: replacing the Pod's status Ready condition to False 03/27/23 22:07:22.102
    STEP: check the Pod again to ensure its Ready conditions are False 03/27/23 22:07:22.156
    STEP: deleting the Pod via a Collection with a LabelSelector 03/27/23 22:07:22.156
    STEP: watching for the Pod to be deleted 03/27/23 22:07:22.192
    Mar 27 22:07:22.197: INFO: observed event type MODIFIED
    Mar 27 22:07:24.058: INFO: observed event type MODIFIED
    Mar 27 22:07:24.253: INFO: observed event type MODIFIED
    Mar 27 22:07:25.067: INFO: observed event type MODIFIED
    Mar 27 22:07:25.097: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:25.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-386" for this suite. 03/27/23 22:07:25.147
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:25.177
Mar 27 22:07:25.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 22:07:25.178
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:25.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:25.227
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 03/27/23 22:07:25.239
STEP: Creating a ResourceQuota 03/27/23 22:07:30.255
STEP: Ensuring resource quota status is calculated 03/27/23 22:07:30.271
STEP: Creating a ReplicationController 03/27/23 22:07:32.286
STEP: Ensuring resource quota status captures replication controller creation 03/27/23 22:07:32.312
STEP: Deleting a ReplicationController 03/27/23 22:07:34.327
STEP: Ensuring resource quota status released usage 03/27/23 22:07:34.34
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9438" for this suite. 03/27/23 22:07:36.392
------------------------------
• [SLOW TEST] [11.241 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:25.177
    Mar 27 22:07:25.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 22:07:25.178
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:25.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:25.227
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 03/27/23 22:07:25.239
    STEP: Creating a ResourceQuota 03/27/23 22:07:30.255
    STEP: Ensuring resource quota status is calculated 03/27/23 22:07:30.271
    STEP: Creating a ReplicationController 03/27/23 22:07:32.286
    STEP: Ensuring resource quota status captures replication controller creation 03/27/23 22:07:32.312
    STEP: Deleting a ReplicationController 03/27/23 22:07:34.327
    STEP: Ensuring resource quota status released usage 03/27/23 22:07:34.34
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9438" for this suite. 03/27/23 22:07:36.392
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:36.419
Mar 27 22:07:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename dns 03/27/23 22:07:36.42
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:36.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:36.465
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/27/23 22:07:36.473
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/27/23 22:07:36.483
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/27/23 22:07:36.484
STEP: creating a pod to probe DNS 03/27/23 22:07:36.484
STEP: submitting the pod to kubernetes 03/27/23 22:07:36.484
Mar 27 22:07:36.508: INFO: Waiting up to 15m0s for pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf" in namespace "dns-4590" to be "running"
Mar 27 22:07:36.522: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.409973ms
Mar 27 22:07:38.537: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf": Phase="Running", Reason="", readiness=true. Elapsed: 2.028372025s
Mar 27 22:07:38.537: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf" satisfied condition "running"
STEP: retrieving the pod 03/27/23 22:07:38.537
STEP: looking for the results for each expected name from probers 03/27/23 22:07:38.552
Mar 27 22:07:38.644: INFO: DNS probes using dns-4590/dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf succeeded

STEP: deleting the pod 03/27/23 22:07:38.645
STEP: deleting the test headless service 03/27/23 22:07:38.695
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:38.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4590" for this suite. 03/27/23 22:07:38.738
------------------------------
• [2.341 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:36.419
    Mar 27 22:07:36.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename dns 03/27/23 22:07:36.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:36.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:36.465
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/27/23 22:07:36.473
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/27/23 22:07:36.483
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/27/23 22:07:36.484
    STEP: creating a pod to probe DNS 03/27/23 22:07:36.484
    STEP: submitting the pod to kubernetes 03/27/23 22:07:36.484
    Mar 27 22:07:36.508: INFO: Waiting up to 15m0s for pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf" in namespace "dns-4590" to be "running"
    Mar 27 22:07:36.522: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.409973ms
    Mar 27 22:07:38.537: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf": Phase="Running", Reason="", readiness=true. Elapsed: 2.028372025s
    Mar 27 22:07:38.537: INFO: Pod "dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf" satisfied condition "running"
    STEP: retrieving the pod 03/27/23 22:07:38.537
    STEP: looking for the results for each expected name from probers 03/27/23 22:07:38.552
    Mar 27 22:07:38.644: INFO: DNS probes using dns-4590/dns-test-69b1add6-a7b7-434f-87b6-a22622c96baf succeeded

    STEP: deleting the pod 03/27/23 22:07:38.645
    STEP: deleting the test headless service 03/27/23 22:07:38.695
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:38.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4590" for this suite. 03/27/23 22:07:38.738
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:38.761
Mar 27 22:07:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename podtemplate 03/27/23 22:07:38.762
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:38.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:38.813
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/27/23 22:07:38.821
Mar 27 22:07:38.837: INFO: created test-podtemplate-1
Mar 27 22:07:38.852: INFO: created test-podtemplate-2
Mar 27 22:07:38.867: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/27/23 22:07:38.867
STEP: delete collection of pod templates 03/27/23 22:07:38.881
Mar 27 22:07:38.881: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/27/23 22:07:38.956
Mar 27 22:07:38.957: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:38.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9269" for this suite. 03/27/23 22:07:38.99
------------------------------
• [0.255 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:38.761
    Mar 27 22:07:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename podtemplate 03/27/23 22:07:38.762
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:38.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:38.813
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/27/23 22:07:38.821
    Mar 27 22:07:38.837: INFO: created test-podtemplate-1
    Mar 27 22:07:38.852: INFO: created test-podtemplate-2
    Mar 27 22:07:38.867: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/27/23 22:07:38.867
    STEP: delete collection of pod templates 03/27/23 22:07:38.881
    Mar 27 22:07:38.881: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/27/23 22:07:38.956
    Mar 27 22:07:38.957: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:38.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9269" for this suite. 03/27/23 22:07:38.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:39.024
Mar 27 22:07:39.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 22:07:39.025
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:39.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:39.069
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 03/27/23 22:07:39.076
STEP: setting up watch 03/27/23 22:07:39.076
STEP: submitting the pod to kubernetes 03/27/23 22:07:39.191
STEP: verifying the pod is in kubernetes 03/27/23 22:07:39.212
STEP: verifying pod creation was observed 03/27/23 22:07:39.227
Mar 27 22:07:39.227: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1" in namespace "pods-5805" to be "running"
Mar 27 22:07:39.240: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.485709ms
Mar 27 22:07:41.253: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026410777s
Mar 27 22:07:41.253: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 22:07:41.267
STEP: verifying pod deletion was observed 03/27/23 22:07:41.293
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5805" for this suite. 03/27/23 22:07:44.215
------------------------------
• [SLOW TEST] [5.228 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:39.024
    Mar 27 22:07:39.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 22:07:39.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:39.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:39.069
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 03/27/23 22:07:39.076
    STEP: setting up watch 03/27/23 22:07:39.076
    STEP: submitting the pod to kubernetes 03/27/23 22:07:39.191
    STEP: verifying the pod is in kubernetes 03/27/23 22:07:39.212
    STEP: verifying pod creation was observed 03/27/23 22:07:39.227
    Mar 27 22:07:39.227: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1" in namespace "pods-5805" to be "running"
    Mar 27 22:07:39.240: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.485709ms
    Mar 27 22:07:41.253: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.026410777s
    Mar 27 22:07:41.253: INFO: Pod "pod-submit-remove-6ef1e66a-d5ed-4cd1-86f1-13a8da0b33e1" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 22:07:41.267
    STEP: verifying pod deletion was observed 03/27/23 22:07:41.293
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:44.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5805" for this suite. 03/27/23 22:07:44.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:44.256
Mar 27 22:07:44.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 22:07:44.257
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:44.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:44.309
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Mar 27 22:07:44.344: INFO: Waiting up to 2m0s for pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" in namespace "var-expansion-5008" to be "container 0 failed with reason CreateContainerConfigError"
Mar 27 22:07:44.363: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.93701ms
Mar 27 22:07:46.378: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034100991s
Mar 27 22:07:46.378: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 27 22:07:46.378: INFO: Deleting pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" in namespace "var-expansion-5008"
Mar 27 22:07:46.403: INFO: Wait up to 5m0s for pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 22:07:50.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5008" for this suite. 03/27/23 22:07:50.456
------------------------------
• [SLOW TEST] [6.223 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:44.256
    Mar 27 22:07:44.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 22:07:44.257
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:44.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:44.309
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Mar 27 22:07:44.344: INFO: Waiting up to 2m0s for pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" in namespace "var-expansion-5008" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 27 22:07:44.363: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78": Phase="Pending", Reason="", readiness=false. Elapsed: 18.93701ms
    Mar 27 22:07:46.378: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034100991s
    Mar 27 22:07:46.378: INFO: Pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 27 22:07:46.378: INFO: Deleting pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" in namespace "var-expansion-5008"
    Mar 27 22:07:46.403: INFO: Wait up to 5m0s for pod "var-expansion-f1768fbb-1ff1-41e7-9642-f6a8a211af78" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:07:50.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5008" for this suite. 03/27/23 22:07:50.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:07:50.482
Mar 27 22:07:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:07:50.483
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:50.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:50.542
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Mar 27 22:07:50.582: INFO: created pod
Mar 27 22:07:50.582: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2686" to be "Succeeded or Failed"
Mar 27 22:07:50.599: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.235442ms
Mar 27 22:07:52.613: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031057427s
Mar 27 22:07:54.614: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031599103s
STEP: Saw pod success 03/27/23 22:07:54.614
Mar 27 22:07:54.614: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 27 22:08:24.615: INFO: polling logs
Mar 27 22:08:24.647: INFO: Pod logs: 
I0327 22:07:51.606741       1 log.go:198] OK: Got token
I0327 22:07:51.606779       1 log.go:198] validating with in-cluster discovery
I0327 22:07:51.607234       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0327 22:07:51.607275       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2686:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679955470, NotBefore:1679954870, IssuedAt:1679954870, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2686", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f23bfc03-1c1f-4439-be17-d9616b863b9e"}}}
I0327 22:07:51.629511       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0327 22:07:51.647023       1 log.go:198] OK: Validated signature on JWT
I0327 22:07:51.647092       1 log.go:198] OK: Got valid claims from token!
I0327 22:07:51.647117       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2686:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679955470, NotBefore:1679954870, IssuedAt:1679954870, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2686", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f23bfc03-1c1f-4439-be17-d9616b863b9e"}}}

Mar 27 22:08:24.647: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 22:08:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2686" for this suite. 03/27/23 22:08:24.68
------------------------------
• [SLOW TEST] [34.222 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:07:50.482
    Mar 27 22:07:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:07:50.483
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:07:50.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:07:50.542
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Mar 27 22:07:50.582: INFO: created pod
    Mar 27 22:07:50.582: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2686" to be "Succeeded or Failed"
    Mar 27 22:07:50.599: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.235442ms
    Mar 27 22:07:52.613: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031057427s
    Mar 27 22:07:54.614: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031599103s
    STEP: Saw pod success 03/27/23 22:07:54.614
    Mar 27 22:07:54.614: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 27 22:08:24.615: INFO: polling logs
    Mar 27 22:08:24.647: INFO: Pod logs: 
    I0327 22:07:51.606741       1 log.go:198] OK: Got token
    I0327 22:07:51.606779       1 log.go:198] validating with in-cluster discovery
    I0327 22:07:51.607234       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0327 22:07:51.607275       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2686:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679955470, NotBefore:1679954870, IssuedAt:1679954870, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2686", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f23bfc03-1c1f-4439-be17-d9616b863b9e"}}}
    I0327 22:07:51.629511       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0327 22:07:51.647023       1 log.go:198] OK: Validated signature on JWT
    I0327 22:07:51.647092       1 log.go:198] OK: Got valid claims from token!
    I0327 22:07:51.647117       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2686:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679955470, NotBefore:1679954870, IssuedAt:1679954870, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2686", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f23bfc03-1c1f-4439-be17-d9616b863b9e"}}}

    Mar 27 22:08:24.647: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:08:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2686" for this suite. 03/27/23 22:08:24.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:08:24.706
Mar 27 22:08:24.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 22:08:24.707
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:24.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:24.752
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/27/23 22:08:24.776
STEP: create the rc2 03/27/23 22:08:24.786
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/27/23 22:08:29.803
STEP: delete the rc simpletest-rc-to-be-deleted 03/27/23 22:08:31.106
STEP: wait for the rc to be deleted 03/27/23 22:08:31.121
STEP: Gathering metrics 03/27/23 22:08:36.174
W0327 22:08:36.203043      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar 27 22:08:36.203: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 27 22:08:36.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-25m6q" in namespace "gc-3585"
Mar 27 22:08:36.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-27brj" in namespace "gc-3585"
Mar 27 22:08:36.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fcm6" in namespace "gc-3585"
Mar 27 22:08:36.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-2njrr" in namespace "gc-3585"
Mar 27 22:08:36.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lh9p" in namespace "gc-3585"
Mar 27 22:08:36.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-54v55" in namespace "gc-3585"
Mar 27 22:08:36.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kssr" in namespace "gc-3585"
Mar 27 22:08:36.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q2b4" in namespace "gc-3585"
Mar 27 22:08:36.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s9w7" in namespace "gc-3585"
Mar 27 22:08:36.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-6crmr" in namespace "gc-3585"
Mar 27 22:08:36.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ntqm" in namespace "gc-3585"
Mar 27 22:08:36.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-79m7j" in namespace "gc-3585"
Mar 27 22:08:36.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pt9p" in namespace "gc-3585"
Mar 27 22:08:36.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wpzl" in namespace "gc-3585"
Mar 27 22:08:36.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-84zg4" in namespace "gc-3585"
Mar 27 22:08:37.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fb7v" in namespace "gc-3585"
Mar 27 22:08:37.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ksmh" in namespace "gc-3585"
Mar 27 22:08:37.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qsxz" in namespace "gc-3585"
Mar 27 22:08:37.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-8spcq" in namespace "gc-3585"
Mar 27 22:08:37.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z88c" in namespace "gc-3585"
Mar 27 22:08:37.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k666" in namespace "gc-3585"
Mar 27 22:08:37.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfgz" in namespace "gc-3585"
Mar 27 22:08:37.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp58z" in namespace "gc-3585"
Mar 27 22:08:37.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvrjk" in namespace "gc-3585"
Mar 27 22:08:37.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6dhv" in namespace "gc-3585"
Mar 27 22:08:37.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-c76f4" in namespace "gc-3585"
Mar 27 22:08:37.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl4m2" in namespace "gc-3585"
Mar 27 22:08:37.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmv6q" in namespace "gc-3585"
Mar 27 22:08:37.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-crp7f" in namespace "gc-3585"
Mar 27 22:08:37.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctdls" in namespace "gc-3585"
Mar 27 22:08:37.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-czx2n" in namespace "gc-3585"
Mar 27 22:08:37.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmt6s" in namespace "gc-3585"
Mar 27 22:08:37.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-f577x" in namespace "gc-3585"
Mar 27 22:08:37.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-fml7d" in namespace "gc-3585"
Mar 27 22:08:37.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftc2d" in namespace "gc-3585"
Mar 27 22:08:38.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-g47b4" in namespace "gc-3585"
Mar 27 22:08:38.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-gckqf" in namespace "gc-3585"
Mar 27 22:08:38.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqknb" in namespace "gc-3585"
Mar 27 22:08:38.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzkn6" in namespace "gc-3585"
Mar 27 22:08:38.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45bh" in namespace "gc-3585"
Mar 27 22:08:38.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4pp9" in namespace "gc-3585"
Mar 27 22:08:38.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc9t8" in namespace "gc-3585"
Mar 27 22:08:38.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwknt" in namespace "gc-3585"
Mar 27 22:08:38.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2shf" in namespace "gc-3585"
Mar 27 22:08:38.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5x4r" in namespace "gc-3585"
Mar 27 22:08:38.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjw4n" in namespace "gc-3585"
Mar 27 22:08:38.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfcjn" in namespace "gc-3585"
Mar 27 22:08:38.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-khs4h" in namespace "gc-3585"
Mar 27 22:08:38.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-kktfn" in namespace "gc-3585"
Mar 27 22:08:38.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwd5k" in namespace "gc-3585"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 22:08:38.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3585" for this suite. 03/27/23 22:08:38.791
------------------------------
• [SLOW TEST] [14.113 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:08:24.706
    Mar 27 22:08:24.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 22:08:24.707
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:24.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:24.752
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/27/23 22:08:24.776
    STEP: create the rc2 03/27/23 22:08:24.786
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/27/23 22:08:29.803
    STEP: delete the rc simpletest-rc-to-be-deleted 03/27/23 22:08:31.106
    STEP: wait for the rc to be deleted 03/27/23 22:08:31.121
    STEP: Gathering metrics 03/27/23 22:08:36.174
    W0327 22:08:36.203043      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Mar 27 22:08:36.203: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Mar 27 22:08:36.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-25m6q" in namespace "gc-3585"
    Mar 27 22:08:36.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-27brj" in namespace "gc-3585"
    Mar 27 22:08:36.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fcm6" in namespace "gc-3585"
    Mar 27 22:08:36.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-2njrr" in namespace "gc-3585"
    Mar 27 22:08:36.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lh9p" in namespace "gc-3585"
    Mar 27 22:08:36.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-54v55" in namespace "gc-3585"
    Mar 27 22:08:36.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kssr" in namespace "gc-3585"
    Mar 27 22:08:36.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q2b4" in namespace "gc-3585"
    Mar 27 22:08:36.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s9w7" in namespace "gc-3585"
    Mar 27 22:08:36.691: INFO: Deleting pod "simpletest-rc-to-be-deleted-6crmr" in namespace "gc-3585"
    Mar 27 22:08:36.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ntqm" in namespace "gc-3585"
    Mar 27 22:08:36.784: INFO: Deleting pod "simpletest-rc-to-be-deleted-79m7j" in namespace "gc-3585"
    Mar 27 22:08:36.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pt9p" in namespace "gc-3585"
    Mar 27 22:08:36.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wpzl" in namespace "gc-3585"
    Mar 27 22:08:36.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-84zg4" in namespace "gc-3585"
    Mar 27 22:08:37.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fb7v" in namespace "gc-3585"
    Mar 27 22:08:37.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ksmh" in namespace "gc-3585"
    Mar 27 22:08:37.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qsxz" in namespace "gc-3585"
    Mar 27 22:08:37.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-8spcq" in namespace "gc-3585"
    Mar 27 22:08:37.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z88c" in namespace "gc-3585"
    Mar 27 22:08:37.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k666" in namespace "gc-3585"
    Mar 27 22:08:37.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmfgz" in namespace "gc-3585"
    Mar 27 22:08:37.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp58z" in namespace "gc-3585"
    Mar 27 22:08:37.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvrjk" in namespace "gc-3585"
    Mar 27 22:08:37.516: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6dhv" in namespace "gc-3585"
    Mar 27 22:08:37.554: INFO: Deleting pod "simpletest-rc-to-be-deleted-c76f4" in namespace "gc-3585"
    Mar 27 22:08:37.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl4m2" in namespace "gc-3585"
    Mar 27 22:08:37.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmv6q" in namespace "gc-3585"
    Mar 27 22:08:37.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-crp7f" in namespace "gc-3585"
    Mar 27 22:08:37.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctdls" in namespace "gc-3585"
    Mar 27 22:08:37.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-czx2n" in namespace "gc-3585"
    Mar 27 22:08:37.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmt6s" in namespace "gc-3585"
    Mar 27 22:08:37.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-f577x" in namespace "gc-3585"
    Mar 27 22:08:37.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-fml7d" in namespace "gc-3585"
    Mar 27 22:08:37.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftc2d" in namespace "gc-3585"
    Mar 27 22:08:38.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-g47b4" in namespace "gc-3585"
    Mar 27 22:08:38.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-gckqf" in namespace "gc-3585"
    Mar 27 22:08:38.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqknb" in namespace "gc-3585"
    Mar 27 22:08:38.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzkn6" in namespace "gc-3585"
    Mar 27 22:08:38.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45bh" in namespace "gc-3585"
    Mar 27 22:08:38.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4pp9" in namespace "gc-3585"
    Mar 27 22:08:38.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc9t8" in namespace "gc-3585"
    Mar 27 22:08:38.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwknt" in namespace "gc-3585"
    Mar 27 22:08:38.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2shf" in namespace "gc-3585"
    Mar 27 22:08:38.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5x4r" in namespace "gc-3585"
    Mar 27 22:08:38.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjw4n" in namespace "gc-3585"
    Mar 27 22:08:38.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfcjn" in namespace "gc-3585"
    Mar 27 22:08:38.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-khs4h" in namespace "gc-3585"
    Mar 27 22:08:38.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-kktfn" in namespace "gc-3585"
    Mar 27 22:08:38.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwd5k" in namespace "gc-3585"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:08:38.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3585" for this suite. 03/27/23 22:08:38.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:08:38.821
Mar 27 22:08:38.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:08:38.823
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:38.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:38.886
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/27/23 22:08:38.904
Mar 27 22:08:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:08:41.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:08:50.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8488" for this suite. 03/27/23 22:08:50.402
------------------------------
• [SLOW TEST] [11.606 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:08:38.821
    Mar 27 22:08:38.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:08:38.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:38.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:38.886
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/27/23 22:08:38.904
    Mar 27 22:08:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:08:41.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:08:50.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8488" for this suite. 03/27/23 22:08:50.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:08:50.432
Mar 27 22:08:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 22:08:50.433
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:50.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:50.493
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-343 03/27/23 22:08:50.508
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 03/27/23 22:08:50.525
STEP: Creating pod with conflicting port in namespace statefulset-343 03/27/23 22:08:50.548
STEP: Waiting until pod test-pod will start running in namespace statefulset-343 03/27/23 22:08:50.579
Mar 27 22:08:50.579: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-343" to be "running"
Mar 27 22:08:50.596: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.445527ms
Mar 27 22:08:52.615: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03608548s
Mar 27 22:08:52.615: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-343 03/27/23 22:08:52.615
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-343 03/27/23 22:08:52.632
Mar 27 22:08:52.704: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Pending. Waiting for statefulset controller to delete.
Mar 27 22:08:52.742: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Failed. Waiting for statefulset controller to delete.
Mar 27 22:08:52.765: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Failed. Waiting for statefulset controller to delete.
Mar 27 22:08:52.775: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-343
STEP: Removing pod with conflicting port in namespace statefulset-343 03/27/23 22:08:52.776
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-343 and will be in running state 03/27/23 22:08:52.821
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 22:08:54.856: INFO: Deleting all statefulset in ns statefulset-343
Mar 27 22:08:54.869: INFO: Scaling statefulset ss to 0
Mar 27 22:09:04.938: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 22:09:04.952: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:05.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-343" for this suite. 03/27/23 22:09:05.036
------------------------------
• [SLOW TEST] [14.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:08:50.432
    Mar 27 22:08:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 22:08:50.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:08:50.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:08:50.493
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-343 03/27/23 22:08:50.508
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 03/27/23 22:08:50.525
    STEP: Creating pod with conflicting port in namespace statefulset-343 03/27/23 22:08:50.548
    STEP: Waiting until pod test-pod will start running in namespace statefulset-343 03/27/23 22:08:50.579
    Mar 27 22:08:50.579: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-343" to be "running"
    Mar 27 22:08:50.596: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.445527ms
    Mar 27 22:08:52.615: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03608548s
    Mar 27 22:08:52.615: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-343 03/27/23 22:08:52.615
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-343 03/27/23 22:08:52.632
    Mar 27 22:08:52.704: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 27 22:08:52.742: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 27 22:08:52.765: INFO: Observed stateful pod in namespace: statefulset-343, name: ss-0, uid: 6f3e0e9f-9c4b-43e9-abc0-71ee667bdaae, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 27 22:08:52.775: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-343
    STEP: Removing pod with conflicting port in namespace statefulset-343 03/27/23 22:08:52.776
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-343 and will be in running state 03/27/23 22:08:52.821
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 22:08:54.856: INFO: Deleting all statefulset in ns statefulset-343
    Mar 27 22:08:54.869: INFO: Scaling statefulset ss to 0
    Mar 27 22:09:04.938: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 22:09:04.952: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:05.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-343" for this suite. 03/27/23 22:09:05.036
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:05.065
Mar 27 22:09:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 22:09:05.067
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:05.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:05.155
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 03/27/23 22:09:05.171
Mar 27 22:09:05.205: INFO: Waiting up to 5m0s for pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7" in namespace "emptydir-9322" to be "Succeeded or Failed"
Mar 27 22:09:05.223: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.928703ms
Mar 27 22:09:07.241: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035991869s
Mar 27 22:09:09.240: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034694161s
STEP: Saw pod success 03/27/23 22:09:09.24
Mar 27 22:09:09.240: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7" satisfied condition "Succeeded or Failed"
Mar 27 22:09:09.256: INFO: Trying to get logs from node 10.176.99.177 pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 container test-container: <nil>
STEP: delete the pod 03/27/23 22:09:09.342
Mar 27 22:09:09.394: INFO: Waiting for pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 to disappear
Mar 27 22:09:09.410: INFO: Pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:09.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9322" for this suite. 03/27/23 22:09:09.432
------------------------------
• [4.393 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:05.065
    Mar 27 22:09:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 22:09:05.067
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:05.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:05.155
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/27/23 22:09:05.171
    Mar 27 22:09:05.205: INFO: Waiting up to 5m0s for pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7" in namespace "emptydir-9322" to be "Succeeded or Failed"
    Mar 27 22:09:05.223: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.928703ms
    Mar 27 22:09:07.241: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035991869s
    Mar 27 22:09:09.240: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034694161s
    STEP: Saw pod success 03/27/23 22:09:09.24
    Mar 27 22:09:09.240: INFO: Pod "pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7" satisfied condition "Succeeded or Failed"
    Mar 27 22:09:09.256: INFO: Trying to get logs from node 10.176.99.177 pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 container test-container: <nil>
    STEP: delete the pod 03/27/23 22:09:09.342
    Mar 27 22:09:09.394: INFO: Waiting for pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 to disappear
    Mar 27 22:09:09.410: INFO: Pod pod-08cc65b9-7070-4d5c-958d-03fffcfd33d7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:09.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9322" for this suite. 03/27/23 22:09:09.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:09.461
Mar 27 22:09:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:09:09.462
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:09.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:09.53
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 03/27/23 22:09:09.545
Mar 27 22:09:09.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595" in namespace "projected-290" to be "Succeeded or Failed"
Mar 27 22:09:09.592: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Pending", Reason="", readiness=false. Elapsed: 16.269477ms
Mar 27 22:09:11.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033776257s
Mar 27 22:09:13.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034172153s
STEP: Saw pod success 03/27/23 22:09:13.61
Mar 27 22:09:13.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595" satisfied condition "Succeeded or Failed"
Mar 27 22:09:13.627: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 container client-container: <nil>
STEP: delete the pod 03/27/23 22:09:13.657
Mar 27 22:09:13.698: INFO: Waiting for pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 to disappear
Mar 27 22:09:13.716: INFO: Pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:13.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-290" for this suite. 03/27/23 22:09:13.739
------------------------------
• [4.305 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:09.461
    Mar 27 22:09:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:09:09.462
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:09.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:09.53
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 03/27/23 22:09:09.545
    Mar 27 22:09:09.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595" in namespace "projected-290" to be "Succeeded or Failed"
    Mar 27 22:09:09.592: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Pending", Reason="", readiness=false. Elapsed: 16.269477ms
    Mar 27 22:09:11.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033776257s
    Mar 27 22:09:13.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034172153s
    STEP: Saw pod success 03/27/23 22:09:13.61
    Mar 27 22:09:13.610: INFO: Pod "downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595" satisfied condition "Succeeded or Failed"
    Mar 27 22:09:13.627: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 container client-container: <nil>
    STEP: delete the pod 03/27/23 22:09:13.657
    Mar 27 22:09:13.698: INFO: Waiting for pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 to disappear
    Mar 27 22:09:13.716: INFO: Pod downwardapi-volume-d8cd98d8-7495-4479-b0e0-ebc944886595 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:13.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-290" for this suite. 03/27/23 22:09:13.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:13.769
Mar 27 22:09:13.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 22:09:13.77
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:13.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:13.832
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 22:09:13.891
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:09:14.644
STEP: Deploying the webhook pod 03/27/23 22:09:14.672
STEP: Wait for the deployment to be ready 03/27/23 22:09:14.718
Mar 27 22:09:14.751: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 27 22:09:16.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/27/23 22:09:18.821
STEP: Verifying the service has paired with the endpoint 03/27/23 22:09:18.862
Mar 27 22:09:19.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 03/27/23 22:09:20.046
STEP: Creating a configMap that should be mutated 03/27/23 22:09:20.136
STEP: Deleting the collection of validation webhooks 03/27/23 22:09:20.297
STEP: Creating a configMap that should not be mutated 03/27/23 22:09:20.455
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:20.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5743" for this suite. 03/27/23 22:09:20.679
STEP: Destroying namespace "webhook-5743-markers" for this suite. 03/27/23 22:09:20.704
------------------------------
• [SLOW TEST] [6.958 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:13.769
    Mar 27 22:09:13.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 22:09:13.77
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:13.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:13.832
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 22:09:13.891
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:09:14.644
    STEP: Deploying the webhook pod 03/27/23 22:09:14.672
    STEP: Wait for the deployment to be ready 03/27/23 22:09:14.718
    Mar 27 22:09:14.751: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar 27 22:09:16.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 9, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/27/23 22:09:18.821
    STEP: Verifying the service has paired with the endpoint 03/27/23 22:09:18.862
    Mar 27 22:09:19.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 03/27/23 22:09:20.046
    STEP: Creating a configMap that should be mutated 03/27/23 22:09:20.136
    STEP: Deleting the collection of validation webhooks 03/27/23 22:09:20.297
    STEP: Creating a configMap that should not be mutated 03/27/23 22:09:20.455
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:20.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5743" for this suite. 03/27/23 22:09:20.679
    STEP: Destroying namespace "webhook-5743-markers" for this suite. 03/27/23 22:09:20.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:20.728
Mar 27 22:09:20.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename ingress 03/27/23 22:09:20.73
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:20.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:20.791
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/27/23 22:09:20.806
STEP: getting /apis/networking.k8s.io 03/27/23 22:09:20.821
STEP: getting /apis/networking.k8s.iov1 03/27/23 22:09:20.828
STEP: creating 03/27/23 22:09:20.835
STEP: getting 03/27/23 22:09:20.884
STEP: listing 03/27/23 22:09:20.897
STEP: watching 03/27/23 22:09:20.91
Mar 27 22:09:20.910: INFO: starting watch
STEP: cluster-wide listing 03/27/23 22:09:20.917
STEP: cluster-wide watching 03/27/23 22:09:20.931
Mar 27 22:09:20.931: INFO: starting watch
STEP: patching 03/27/23 22:09:20.937
STEP: updating 03/27/23 22:09:20.954
Mar 27 22:09:21.012: INFO: waiting for watch events with expected annotations
Mar 27 22:09:21.013: INFO: saw patched and updated annotations
STEP: patching /status 03/27/23 22:09:21.013
STEP: updating /status 03/27/23 22:09:21.06
STEP: get /status 03/27/23 22:09:21.094
STEP: deleting 03/27/23 22:09:21.107
STEP: deleting a collection 03/27/23 22:09:21.154
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-3465" for this suite. 03/27/23 22:09:21.229
------------------------------
• [0.524 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:20.728
    Mar 27 22:09:20.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename ingress 03/27/23 22:09:20.73
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:20.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:20.791
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/27/23 22:09:20.806
    STEP: getting /apis/networking.k8s.io 03/27/23 22:09:20.821
    STEP: getting /apis/networking.k8s.iov1 03/27/23 22:09:20.828
    STEP: creating 03/27/23 22:09:20.835
    STEP: getting 03/27/23 22:09:20.884
    STEP: listing 03/27/23 22:09:20.897
    STEP: watching 03/27/23 22:09:20.91
    Mar 27 22:09:20.910: INFO: starting watch
    STEP: cluster-wide listing 03/27/23 22:09:20.917
    STEP: cluster-wide watching 03/27/23 22:09:20.931
    Mar 27 22:09:20.931: INFO: starting watch
    STEP: patching 03/27/23 22:09:20.937
    STEP: updating 03/27/23 22:09:20.954
    Mar 27 22:09:21.012: INFO: waiting for watch events with expected annotations
    Mar 27 22:09:21.013: INFO: saw patched and updated annotations
    STEP: patching /status 03/27/23 22:09:21.013
    STEP: updating /status 03/27/23 22:09:21.06
    STEP: get /status 03/27/23 22:09:21.094
    STEP: deleting 03/27/23 22:09:21.107
    STEP: deleting a collection 03/27/23 22:09:21.154
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-3465" for this suite. 03/27/23 22:09:21.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:21.263
Mar 27 22:09:21.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename watch 03/27/23 22:09:21.265
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:21.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:21.329
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/27/23 22:09:21.341
STEP: modifying the configmap once 03/27/23 22:09:21.359
STEP: modifying the configmap a second time 03/27/23 22:09:21.391
STEP: deleting the configmap 03/27/23 22:09:21.424
STEP: creating a watch on configmaps from the resource version returned by the first update 03/27/23 22:09:21.449
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/27/23 22:09:21.456
Mar 27 22:09:21.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7747  c54aa692-61d9-456a-adf9-9a205c828159 45019 0 2023-03-27 22:09:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 22:09:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:09:21.456: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7747  c54aa692-61d9-456a-adf9-9a205c828159 45020 0 2023-03-27 22:09:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 22:09:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:21.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7747" for this suite. 03/27/23 22:09:21.483
------------------------------
• [0.246 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:21.263
    Mar 27 22:09:21.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename watch 03/27/23 22:09:21.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:21.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:21.329
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/27/23 22:09:21.341
    STEP: modifying the configmap once 03/27/23 22:09:21.359
    STEP: modifying the configmap a second time 03/27/23 22:09:21.391
    STEP: deleting the configmap 03/27/23 22:09:21.424
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/27/23 22:09:21.449
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/27/23 22:09:21.456
    Mar 27 22:09:21.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7747  c54aa692-61d9-456a-adf9-9a205c828159 45019 0 2023-03-27 22:09:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 22:09:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:09:21.456: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7747  c54aa692-61d9-456a-adf9-9a205c828159 45020 0 2023-03-27 22:09:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-27 22:09:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:21.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7747" for this suite. 03/27/23 22:09:21.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:21.513
Mar 27 22:09:21.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replicaset 03/27/23 22:09:21.514
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:21.589
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/27/23 22:09:21.604
Mar 27 22:09:21.638: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6726" to be "running and ready"
Mar 27 22:09:21.657: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 18.324939ms
Mar 27 22:09:21.657: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:09:23.675: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.037102649s
Mar 27 22:09:23.676: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 27 22:09:23.676: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/27/23 22:09:23.693
STEP: Then the orphan pod is adopted 03/27/23 22:09:23.714
STEP: When the matched label of one of its pods change 03/27/23 22:09:24.75
Mar 27 22:09:24.768: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/27/23 22:09:24.808
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:25.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6726" for this suite. 03/27/23 22:09:25.867
------------------------------
• [4.381 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:21.513
    Mar 27 22:09:21.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replicaset 03/27/23 22:09:21.514
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:21.589
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/27/23 22:09:21.604
    Mar 27 22:09:21.638: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6726" to be "running and ready"
    Mar 27 22:09:21.657: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 18.324939ms
    Mar 27 22:09:21.657: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:09:23.675: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.037102649s
    Mar 27 22:09:23.676: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 27 22:09:23.676: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/27/23 22:09:23.693
    STEP: Then the orphan pod is adopted 03/27/23 22:09:23.714
    STEP: When the matched label of one of its pods change 03/27/23 22:09:24.75
    Mar 27 22:09:24.768: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/27/23 22:09:24.808
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:25.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6726" for this suite. 03/27/23 22:09:25.867
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:25.896
Mar 27 22:09:25.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 22:09:25.899
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:25.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:25.963
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/27/23 22:09:26
Mar 27 22:09:26.037: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2064" to be "running and ready"
Mar 27 22:09:26.061: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 24.287357ms
Mar 27 22:09:26.061: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:09:28.081: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043707673s
Mar 27 22:09:28.081: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:09:30.083: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.04593381s
Mar 27 22:09:30.083: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 22:09:30.083: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 03/27/23 22:09:30.105
Mar 27 22:09:30.125: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2064" to be "running and ready"
Mar 27 22:09:30.142: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.244198ms
Mar 27 22:09:30.143: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:09:32.158: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.032831302s
Mar 27 22:09:32.158: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 27 22:09:32.158: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/27/23 22:09:32.174
Mar 27 22:09:32.202: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 22:09:32.218: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 27 22:09:34.219: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 22:09:34.236: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 27 22:09:36.219: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 27 22:09:36.236: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/27/23 22:09:36.236
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:36.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2064" for this suite. 03/27/23 22:09:36.349
------------------------------
• [SLOW TEST] [10.481 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:25.896
    Mar 27 22:09:25.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 22:09:25.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:25.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:25.963
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 22:09:26
    Mar 27 22:09:26.037: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2064" to be "running and ready"
    Mar 27 22:09:26.061: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 24.287357ms
    Mar 27 22:09:26.061: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:09:28.081: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043707673s
    Mar 27 22:09:28.081: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:09:30.083: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.04593381s
    Mar 27 22:09:30.083: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 22:09:30.083: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 03/27/23 22:09:30.105
    Mar 27 22:09:30.125: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2064" to be "running and ready"
    Mar 27 22:09:30.142: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.244198ms
    Mar 27 22:09:30.143: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:09:32.158: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.032831302s
    Mar 27 22:09:32.158: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 27 22:09:32.158: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/27/23 22:09:32.174
    Mar 27 22:09:32.202: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 22:09:32.218: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 27 22:09:34.219: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 22:09:34.236: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 27 22:09:36.219: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 27 22:09:36.236: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/27/23 22:09:36.236
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:36.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2064" for this suite. 03/27/23 22:09:36.349
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:36.377
Mar 27 22:09:36.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:09:36.379
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.443
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 03/27/23 22:09:36.475
STEP: waiting for available Endpoint 03/27/23 22:09:36.492
STEP: listing all Endpoints 03/27/23 22:09:36.5
STEP: updating the Endpoint 03/27/23 22:09:36.514
STEP: fetching the Endpoint 03/27/23 22:09:36.536
STEP: patching the Endpoint 03/27/23 22:09:36.551
STEP: fetching the Endpoint 03/27/23 22:09:36.58
STEP: deleting the Endpoint by Collection 03/27/23 22:09:36.594
STEP: waiting for Endpoint deletion 03/27/23 22:09:36.623
STEP: fetching the Endpoint 03/27/23 22:09:36.63
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:36.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1165" for this suite. 03/27/23 22:09:36.668
------------------------------
• [0.313 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:36.377
    Mar 27 22:09:36.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:09:36.379
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.443
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 03/27/23 22:09:36.475
    STEP: waiting for available Endpoint 03/27/23 22:09:36.492
    STEP: listing all Endpoints 03/27/23 22:09:36.5
    STEP: updating the Endpoint 03/27/23 22:09:36.514
    STEP: fetching the Endpoint 03/27/23 22:09:36.536
    STEP: patching the Endpoint 03/27/23 22:09:36.551
    STEP: fetching the Endpoint 03/27/23 22:09:36.58
    STEP: deleting the Endpoint by Collection 03/27/23 22:09:36.594
    STEP: waiting for Endpoint deletion 03/27/23 22:09:36.623
    STEP: fetching the Endpoint 03/27/23 22:09:36.63
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:36.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1165" for this suite. 03/27/23 22:09:36.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:36.691
Mar 27 22:09:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:09:36.693
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.754
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:36.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4869" for this suite. 03/27/23 22:09:36.804
------------------------------
• [0.139 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:36.691
    Mar 27 22:09:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:09:36.693
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.754
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:36.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4869" for this suite. 03/27/23 22:09:36.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:36.832
Mar 27 22:09:36.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pods 03/27/23 22:09:36.834
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.902
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/27/23 22:09:36.917
STEP: submitting the pod to kubernetes 03/27/23 22:09:36.918
STEP: verifying QOS class is set on the pod 03/27/23 22:09:36.95
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:36.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7738" for this suite. 03/27/23 22:09:37.004
------------------------------
• [0.198 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:36.832
    Mar 27 22:09:36.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pods 03/27/23 22:09:36.834
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:36.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:36.902
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/27/23 22:09:36.917
    STEP: submitting the pod to kubernetes 03/27/23 22:09:36.918
    STEP: verifying QOS class is set on the pod 03/27/23 22:09:36.95
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:36.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7738" for this suite. 03/27/23 22:09:37.004
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:37.03
Mar 27 22:09:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename csiinlinevolumes 03/27/23 22:09:37.032
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:37.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:37.097
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 03/27/23 22:09:37.111
STEP: getting 03/27/23 22:09:37.181
STEP: listing in namespace 03/27/23 22:09:37.197
STEP: patching 03/27/23 22:09:37.214
STEP: deleting 03/27/23 22:09:37.239
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:37.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8377" for this suite. 03/27/23 22:09:37.313
------------------------------
• [0.324 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:37.03
    Mar 27 22:09:37.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename csiinlinevolumes 03/27/23 22:09:37.032
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:37.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:37.097
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 03/27/23 22:09:37.111
    STEP: getting 03/27/23 22:09:37.181
    STEP: listing in namespace 03/27/23 22:09:37.197
    STEP: patching 03/27/23 22:09:37.214
    STEP: deleting 03/27/23 22:09:37.239
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:37.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8377" for this suite. 03/27/23 22:09:37.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:37.364
Mar 27 22:09:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 22:09:37.366
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:37.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:37.429
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-f09ad5a9-80f7-46f3-a9fb-9a42524a8dfd 03/27/23 22:09:37.464
STEP: Creating configMap with name cm-test-opt-upd-3ea8429e-de1f-497d-a63e-a23921b340f2 03/27/23 22:09:37.485
STEP: Creating the pod 03/27/23 22:09:37.502
Mar 27 22:09:37.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4" in namespace "configmap-5522" to be "running and ready"
Mar 27 22:09:37.557: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.591498ms
Mar 27 22:09:37.557: INFO: The phase of Pod pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:09:39.575: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.037793772s
Mar 27 22:09:39.575: INFO: The phase of Pod pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4 is Running (Ready = true)
Mar 27 22:09:39.575: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-f09ad5a9-80f7-46f3-a9fb-9a42524a8dfd 03/27/23 22:09:39.692
STEP: Updating configmap cm-test-opt-upd-3ea8429e-de1f-497d-a63e-a23921b340f2 03/27/23 22:09:39.718
STEP: Creating configMap with name cm-test-opt-create-62883cb6-cd8a-44f8-b180-56471f887e9b 03/27/23 22:09:39.736
STEP: waiting to observe update in volume 03/27/23 22:09:39.758
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:09:41.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5522" for this suite. 03/27/23 22:09:41.934
------------------------------
• [4.600 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:37.364
    Mar 27 22:09:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 22:09:37.366
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:37.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:37.429
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-f09ad5a9-80f7-46f3-a9fb-9a42524a8dfd 03/27/23 22:09:37.464
    STEP: Creating configMap with name cm-test-opt-upd-3ea8429e-de1f-497d-a63e-a23921b340f2 03/27/23 22:09:37.485
    STEP: Creating the pod 03/27/23 22:09:37.502
    Mar 27 22:09:37.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4" in namespace "configmap-5522" to be "running and ready"
    Mar 27 22:09:37.557: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.591498ms
    Mar 27 22:09:37.557: INFO: The phase of Pod pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:09:39.575: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.037793772s
    Mar 27 22:09:39.575: INFO: The phase of Pod pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4 is Running (Ready = true)
    Mar 27 22:09:39.575: INFO: Pod "pod-configmaps-a41dfd38-378e-484f-a534-eb04b395d4b4" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-f09ad5a9-80f7-46f3-a9fb-9a42524a8dfd 03/27/23 22:09:39.692
    STEP: Updating configmap cm-test-opt-upd-3ea8429e-de1f-497d-a63e-a23921b340f2 03/27/23 22:09:39.718
    STEP: Creating configMap with name cm-test-opt-create-62883cb6-cd8a-44f8-b180-56471f887e9b 03/27/23 22:09:39.736
    STEP: waiting to observe update in volume 03/27/23 22:09:39.758
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:09:41.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5522" for this suite. 03/27/23 22:09:41.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:09:41.966
Mar 27 22:09:41.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename var-expansion 03/27/23 22:09:41.967
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:42.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:42.032
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 03/27/23 22:09:42.048
Mar 27 22:09:42.082: INFO: Waiting up to 2m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632" to be "running"
Mar 27 22:09:42.102: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.380307ms
Mar 27 22:09:44.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03869459s
Mar 27 22:09:46.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038693356s
Mar 27 22:09:48.125: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043506124s
Mar 27 22:09:50.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0406125s
Mar 27 22:09:52.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040218806s
Mar 27 22:09:54.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039740178s
Mar 27 22:09:56.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.038538464s
Mar 27 22:09:58.123: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041394393s
Mar 27 22:10:00.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039718247s
Mar 27 22:10:02.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038485533s
Mar 27 22:10:04.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.038872768s
Mar 27 22:10:06.146: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.064308276s
Mar 27 22:10:08.149: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.067583512s
Mar 27 22:10:10.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.037728148s
Mar 27 22:10:12.145: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.063061731s
Mar 27 22:10:14.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.03835751s
Mar 27 22:10:16.150: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.067920951s
Mar 27 22:10:18.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.039418225s
Mar 27 22:10:20.146: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.064569723s
Mar 27 22:10:22.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.037164826s
Mar 27 22:10:24.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.040258854s
Mar 27 22:10:26.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.039575078s
Mar 27 22:10:28.350: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.268700027s
Mar 27 22:10:30.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.040846659s
Mar 27 22:10:32.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03793888s
Mar 27 22:10:34.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.039889788s
Mar 27 22:10:36.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.040762779s
Mar 27 22:10:38.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.038515365s
Mar 27 22:10:40.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.038491916s
Mar 27 22:10:42.139: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.057572706s
Mar 27 22:10:44.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.038434964s
Mar 27 22:10:46.127: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.045394612s
Mar 27 22:10:48.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.038987626s
Mar 27 22:10:50.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.03862729s
Mar 27 22:10:52.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.039221086s
Mar 27 22:10:54.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.038979304s
Mar 27 22:10:56.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.03709785s
Mar 27 22:10:58.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.040590563s
Mar 27 22:11:00.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.03916698s
Mar 27 22:11:02.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.03977555s
Mar 27 22:11:04.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.03953062s
Mar 27 22:11:06.131: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.049814077s
Mar 27 22:11:08.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038317089s
Mar 27 22:11:10.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.038959464s
Mar 27 22:11:12.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.038788495s
Mar 27 22:11:14.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.040052811s
Mar 27 22:11:16.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.037440892s
Mar 27 22:11:18.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.039616201s
Mar 27 22:11:20.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.037941812s
Mar 27 22:11:22.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.03872291s
Mar 27 22:11:24.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.040616058s
Mar 27 22:11:26.136: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.054702275s
Mar 27 22:11:28.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.03886977s
Mar 27 22:11:30.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038117492s
Mar 27 22:11:32.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.040334872s
Mar 27 22:11:34.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.038903362s
Mar 27 22:11:36.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.038663896s
Mar 27 22:11:38.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.03941564s
Mar 27 22:11:40.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.03929639s
Mar 27 22:11:42.127: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.045667225s
Mar 27 22:11:42.144: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.062792783s
STEP: updating the pod 03/27/23 22:11:42.145
Mar 27 22:11:42.691: INFO: Successfully updated pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5"
STEP: waiting for pod running 03/27/23 22:11:42.692
Mar 27 22:11:42.692: INFO: Waiting up to 2m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632" to be "running"
Mar 27 22:11:42.709: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.121729ms
Mar 27 22:11:44.726: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.034303616s
Mar 27 22:11:44.726: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" satisfied condition "running"
STEP: deleting the pod gracefully 03/27/23 22:11:44.727
Mar 27 22:11:44.727: INFO: Deleting pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632"
Mar 27 22:11:44.776: INFO: Wait up to 5m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Mar 27 22:12:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1632" for this suite. 03/27/23 22:12:16.833
------------------------------
• [SLOW TEST] [154.893 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:09:41.966
    Mar 27 22:09:41.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename var-expansion 03/27/23 22:09:41.967
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:09:42.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:09:42.032
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 03/27/23 22:09:42.048
    Mar 27 22:09:42.082: INFO: Waiting up to 2m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632" to be "running"
    Mar 27 22:09:42.102: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.380307ms
    Mar 27 22:09:44.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03869459s
    Mar 27 22:09:46.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038693356s
    Mar 27 22:09:48.125: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043506124s
    Mar 27 22:09:50.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0406125s
    Mar 27 22:09:52.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040218806s
    Mar 27 22:09:54.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039740178s
    Mar 27 22:09:56.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.038538464s
    Mar 27 22:09:58.123: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.041394393s
    Mar 27 22:10:00.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039718247s
    Mar 27 22:10:02.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038485533s
    Mar 27 22:10:04.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.038872768s
    Mar 27 22:10:06.146: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.064308276s
    Mar 27 22:10:08.149: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.067583512s
    Mar 27 22:10:10.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.037728148s
    Mar 27 22:10:12.145: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.063061731s
    Mar 27 22:10:14.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.03835751s
    Mar 27 22:10:16.150: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.067920951s
    Mar 27 22:10:18.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.039418225s
    Mar 27 22:10:20.146: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.064569723s
    Mar 27 22:10:22.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.037164826s
    Mar 27 22:10:24.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.040258854s
    Mar 27 22:10:26.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.039575078s
    Mar 27 22:10:28.350: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.268700027s
    Mar 27 22:10:30.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.040846659s
    Mar 27 22:10:32.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03793888s
    Mar 27 22:10:34.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.039889788s
    Mar 27 22:10:36.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.040762779s
    Mar 27 22:10:38.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.038515365s
    Mar 27 22:10:40.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.038491916s
    Mar 27 22:10:42.139: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.057572706s
    Mar 27 22:10:44.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.038434964s
    Mar 27 22:10:46.127: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.045394612s
    Mar 27 22:10:48.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.038987626s
    Mar 27 22:10:50.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.03862729s
    Mar 27 22:10:52.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.039221086s
    Mar 27 22:10:54.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.038979304s
    Mar 27 22:10:56.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.03709785s
    Mar 27 22:10:58.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.040590563s
    Mar 27 22:11:00.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.03916698s
    Mar 27 22:11:02.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.03977555s
    Mar 27 22:11:04.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.03953062s
    Mar 27 22:11:06.131: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.049814077s
    Mar 27 22:11:08.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038317089s
    Mar 27 22:11:10.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.038959464s
    Mar 27 22:11:12.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.038788495s
    Mar 27 22:11:14.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.040052811s
    Mar 27 22:11:16.119: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.037440892s
    Mar 27 22:11:18.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.039616201s
    Mar 27 22:11:20.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.037941812s
    Mar 27 22:11:22.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.03872291s
    Mar 27 22:11:24.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.040616058s
    Mar 27 22:11:26.136: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.054702275s
    Mar 27 22:11:28.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.03886977s
    Mar 27 22:11:30.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038117492s
    Mar 27 22:11:32.122: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.040334872s
    Mar 27 22:11:34.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.038903362s
    Mar 27 22:11:36.120: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.038663896s
    Mar 27 22:11:38.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.03941564s
    Mar 27 22:11:40.121: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.03929639s
    Mar 27 22:11:42.127: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.045667225s
    Mar 27 22:11:42.144: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.062792783s
    STEP: updating the pod 03/27/23 22:11:42.145
    Mar 27 22:11:42.691: INFO: Successfully updated pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5"
    STEP: waiting for pod running 03/27/23 22:11:42.692
    Mar 27 22:11:42.692: INFO: Waiting up to 2m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632" to be "running"
    Mar 27 22:11:42.709: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.121729ms
    Mar 27 22:11:44.726: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.034303616s
    Mar 27 22:11:44.726: INFO: Pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" satisfied condition "running"
    STEP: deleting the pod gracefully 03/27/23 22:11:44.727
    Mar 27 22:11:44.727: INFO: Deleting pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" in namespace "var-expansion-1632"
    Mar 27 22:11:44.776: INFO: Wait up to 5m0s for pod "var-expansion-d7259b27-46dc-4414-ab17-c021d90eedb5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:12:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1632" for this suite. 03/27/23 22:12:16.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:12:16.863
Mar 27 22:12:16.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svc-latency 03/27/23 22:12:16.865
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:16.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:16.927
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 27 22:12:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: creating replication controller svc-latency-rc in namespace svc-latency-574 03/27/23 22:12:16.941
I0327 22:12:16.959092      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-574, replica count: 1
I0327 22:12:18.009866      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0327 22:12:19.010248      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 22:12:19.151: INFO: Created: latency-svc-wp74v
Mar 27 22:12:19.158: INFO: Got endpoints: latency-svc-wp74v [47.442012ms]
Mar 27 22:12:19.201: INFO: Created: latency-svc-6f64f
Mar 27 22:12:19.207: INFO: Got endpoints: latency-svc-6f64f [48.817786ms]
Mar 27 22:12:19.227: INFO: Created: latency-svc-bpvl9
Mar 27 22:12:19.235: INFO: Got endpoints: latency-svc-bpvl9 [76.350437ms]
Mar 27 22:12:19.267: INFO: Created: latency-svc-bq4cm
Mar 27 22:12:19.272: INFO: Got endpoints: latency-svc-bq4cm [113.286835ms]
Mar 27 22:12:19.294: INFO: Created: latency-svc-fdfcm
Mar 27 22:12:19.300: INFO: Got endpoints: latency-svc-fdfcm [140.875869ms]
Mar 27 22:12:19.314: INFO: Created: latency-svc-58cp5
Mar 27 22:12:19.320: INFO: Got endpoints: latency-svc-58cp5 [160.98778ms]
Mar 27 22:12:19.338: INFO: Created: latency-svc-wjvm5
Mar 27 22:12:19.344: INFO: Got endpoints: latency-svc-wjvm5 [184.298977ms]
Mar 27 22:12:19.370: INFO: Created: latency-svc-sxtgl
Mar 27 22:12:19.377: INFO: Got endpoints: latency-svc-sxtgl [217.133299ms]
Mar 27 22:12:19.396: INFO: Created: latency-svc-vd92s
Mar 27 22:12:19.404: INFO: Got endpoints: latency-svc-vd92s [243.69425ms]
Mar 27 22:12:19.771: INFO: Created: latency-svc-wvmx4
Mar 27 22:12:19.771: INFO: Got endpoints: latency-svc-wvmx4 [471.053201ms]
Mar 27 22:12:19.771: INFO: Created: latency-svc-pczdn
Mar 27 22:12:19.771: INFO: Got endpoints: latency-svc-pczdn [394.680096ms]
Mar 27 22:12:19.772: INFO: Created: latency-svc-5gcpm
Mar 27 22:12:19.772: INFO: Got endpoints: latency-svc-5gcpm [612.290805ms]
Mar 27 22:12:19.772: INFO: Created: latency-svc-qv72v
Mar 27 22:12:19.772: INFO: Got endpoints: latency-svc-qv72v [612.418284ms]
Mar 27 22:12:19.774: INFO: Created: latency-svc-5k8v6
Mar 27 22:12:19.774: INFO: Created: latency-svc-9r9kz
Mar 27 22:12:19.775: INFO: Created: latency-svc-wf2pl
Mar 27 22:12:19.775: INFO: Created: latency-svc-wwphz
Mar 27 22:12:19.775: INFO: Created: latency-svc-x6rsw
Mar 27 22:12:19.775: INFO: Created: latency-svc-qhxrd
Mar 27 22:12:19.775: INFO: Created: latency-svc-2fp7l
Mar 27 22:12:19.776: INFO: Created: latency-svc-kqvcb
Mar 27 22:12:19.776: INFO: Created: latency-svc-zt9jm
Mar 27 22:12:19.776: INFO: Created: latency-svc-z5nch
Mar 27 22:12:19.776: INFO: Created: latency-svc-pqcgl
Mar 27 22:12:19.780: INFO: Got endpoints: latency-svc-z5nch [435.63456ms]
Mar 27 22:12:19.781: INFO: Got endpoints: latency-svc-x6rsw [622.370372ms]
Mar 27 22:12:19.781: INFO: Got endpoints: latency-svc-zt9jm [574.211697ms]
Mar 27 22:12:19.784: INFO: Got endpoints: latency-svc-kqvcb [624.224864ms]
Mar 27 22:12:19.784: INFO: Got endpoints: latency-svc-5k8v6 [379.95732ms]
Mar 27 22:12:19.786: INFO: Got endpoints: latency-svc-9r9kz [626.275718ms]
Mar 27 22:12:19.788: INFO: Got endpoints: latency-svc-wwphz [515.608842ms]
Mar 27 22:12:19.787: INFO: Got endpoints: latency-svc-pqcgl [466.225092ms]
Mar 27 22:12:19.787: INFO: Got endpoints: latency-svc-qhxrd [551.653876ms]
Mar 27 22:12:19.789: INFO: Got endpoints: latency-svc-wf2pl [629.57974ms]
Mar 27 22:12:19.794: INFO: Got endpoints: latency-svc-2fp7l [634.575113ms]
Mar 27 22:12:19.816: INFO: Created: latency-svc-hxf5x
Mar 27 22:12:19.823: INFO: Got endpoints: latency-svc-hxf5x [52.130574ms]
Mar 27 22:12:19.841: INFO: Created: latency-svc-f9sgj
Mar 27 22:12:19.847: INFO: Got endpoints: latency-svc-f9sgj [75.959429ms]
Mar 27 22:12:19.866: INFO: Created: latency-svc-59qlq
Mar 27 22:12:19.872: INFO: Got endpoints: latency-svc-59qlq [100.06448ms]
Mar 27 22:12:19.888: INFO: Created: latency-svc-9lr8k
Mar 27 22:12:19.893: INFO: Got endpoints: latency-svc-9lr8k [120.908076ms]
Mar 27 22:12:19.913: INFO: Created: latency-svc-sfqvv
Mar 27 22:12:19.919: INFO: Got endpoints: latency-svc-sfqvv [138.981021ms]
Mar 27 22:12:19.936: INFO: Created: latency-svc-hjfnv
Mar 27 22:12:19.943: INFO: Got endpoints: latency-svc-hjfnv [158.500659ms]
Mar 27 22:12:19.971: INFO: Created: latency-svc-jf2dz
Mar 27 22:12:19.971: INFO: Got endpoints: latency-svc-jf2dz [186.211213ms]
Mar 27 22:12:19.981: INFO: Created: latency-svc-szkpc
Mar 27 22:12:19.986: INFO: Got endpoints: latency-svc-szkpc [200.916732ms]
Mar 27 22:12:20.007: INFO: Created: latency-svc-d7fk9
Mar 27 22:12:20.012: INFO: Got endpoints: latency-svc-d7fk9 [225.152236ms]
Mar 27 22:12:20.037: INFO: Created: latency-svc-sqb4b
Mar 27 22:12:20.045: INFO: Got endpoints: latency-svc-sqb4b [257.161856ms]
Mar 27 22:12:20.077: INFO: Created: latency-svc-69h2s
Mar 27 22:12:20.082: INFO: Got endpoints: latency-svc-69h2s [293.48329ms]
Mar 27 22:12:20.098: INFO: Created: latency-svc-vt25d
Mar 27 22:12:20.103: INFO: Got endpoints: latency-svc-vt25d [315.940163ms]
Mar 27 22:12:20.121: INFO: Created: latency-svc-z4fmr
Mar 27 22:12:20.126: INFO: Got endpoints: latency-svc-z4fmr [337.112026ms]
Mar 27 22:12:20.152: INFO: Created: latency-svc-csshg
Mar 27 22:12:20.157: INFO: Got endpoints: latency-svc-csshg [367.601879ms]
Mar 27 22:12:20.173: INFO: Created: latency-svc-sfmws
Mar 27 22:12:20.184: INFO: Got endpoints: latency-svc-sfmws [389.830199ms]
Mar 27 22:12:20.206: INFO: Created: latency-svc-b49h7
Mar 27 22:12:20.212: INFO: Got endpoints: latency-svc-b49h7 [388.677198ms]
Mar 27 22:12:20.232: INFO: Created: latency-svc-6fxjz
Mar 27 22:12:20.237: INFO: Got endpoints: latency-svc-6fxjz [389.288081ms]
Mar 27 22:12:20.254: INFO: Created: latency-svc-k55ln
Mar 27 22:12:20.260: INFO: Got endpoints: latency-svc-k55ln [387.801575ms]
Mar 27 22:12:20.280: INFO: Created: latency-svc-zctw8
Mar 27 22:12:20.285: INFO: Got endpoints: latency-svc-zctw8 [391.334836ms]
Mar 27 22:12:20.301: INFO: Created: latency-svc-9hqnh
Mar 27 22:12:20.309: INFO: Got endpoints: latency-svc-9hqnh [390.379785ms]
Mar 27 22:12:20.323: INFO: Created: latency-svc-xxvvr
Mar 27 22:12:20.333: INFO: Got endpoints: latency-svc-xxvvr [389.841832ms]
Mar 27 22:12:20.344: INFO: Created: latency-svc-jwbqd
Mar 27 22:12:20.350: INFO: Got endpoints: latency-svc-jwbqd [378.594587ms]
Mar 27 22:12:20.367: INFO: Created: latency-svc-88nst
Mar 27 22:12:20.374: INFO: Got endpoints: latency-svc-88nst [388.115018ms]
Mar 27 22:12:20.397: INFO: Created: latency-svc-fljwg
Mar 27 22:12:20.397: INFO: Got endpoints: latency-svc-fljwg [384.976042ms]
Mar 27 22:12:20.742: INFO: Created: latency-svc-59mxf
Mar 27 22:12:20.749: INFO: Created: latency-svc-rtwd8
Mar 27 22:12:20.750: INFO: Created: latency-svc-x5fzj
Mar 27 22:12:20.750: INFO: Created: latency-svc-ns7k6
Mar 27 22:12:20.750: INFO: Created: latency-svc-z7szc
Mar 27 22:12:20.750: INFO: Created: latency-svc-pxzpv
Mar 27 22:12:20.751: INFO: Created: latency-svc-fx9gf
Mar 27 22:12:20.751: INFO: Created: latency-svc-2t6mj
Mar 27 22:12:20.751: INFO: Got endpoints: latency-svc-59mxf [376.744333ms]
Mar 27 22:12:20.752: INFO: Created: latency-svc-d6hwf
Mar 27 22:12:20.752: INFO: Created: latency-svc-g5kn8
Mar 27 22:12:20.752: INFO: Created: latency-svc-7gxf5
Mar 27 22:12:20.754: INFO: Created: latency-svc-g9h28
Mar 27 22:12:20.754: INFO: Created: latency-svc-sg7hd
Mar 27 22:12:20.754: INFO: Created: latency-svc-fkhtp
Mar 27 22:12:20.754: INFO: Got endpoints: latency-svc-rtwd8 [469.472729ms]
Mar 27 22:12:20.754: INFO: Got endpoints: latency-svc-2t6mj [709.425495ms]
Mar 27 22:12:20.755: INFO: Got endpoints: latency-svc-pxzpv [597.227682ms]
Mar 27 22:12:20.755: INFO: Got endpoints: latency-svc-fx9gf [495.221907ms]
Mar 27 22:12:20.755: INFO: Created: latency-svc-hh475
Mar 27 22:12:20.756: INFO: Got endpoints: latency-svc-ns7k6 [422.979896ms]
Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-d6hwf [460.492967ms]
Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-x5fzj [557.988649ms]
Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-fkhtp [586.840873ms]
Mar 27 22:12:20.771: INFO: Got endpoints: latency-svc-g9h28 [533.736189ms]
Mar 27 22:12:20.776: INFO: Got endpoints: latency-svc-sg7hd [673.276605ms]
Mar 27 22:12:20.777: INFO: Got endpoints: latency-svc-z7szc [427.731747ms]
Mar 27 22:12:20.778: INFO: Got endpoints: latency-svc-7gxf5 [380.808524ms]
Mar 27 22:12:20.780: INFO: Got endpoints: latency-svc-g5kn8 [697.830164ms]
Mar 27 22:12:20.780: INFO: Got endpoints: latency-svc-hh475 [654.369589ms]
Mar 27 22:12:20.805: INFO: Created: latency-svc-nv2jk
Mar 27 22:12:20.808: INFO: Got endpoints: latency-svc-nv2jk [53.620068ms]
Mar 27 22:12:20.827: INFO: Created: latency-svc-ws69v
Mar 27 22:12:20.832: INFO: Got endpoints: latency-svc-ws69v [77.793035ms]
Mar 27 22:12:20.853: INFO: Created: latency-svc-88g7l
Mar 27 22:12:20.856: INFO: Got endpoints: latency-svc-88g7l [101.647799ms]
Mar 27 22:12:20.875: INFO: Created: latency-svc-qpcqz
Mar 27 22:12:20.880: INFO: Got endpoints: latency-svc-qpcqz [124.877532ms]
Mar 27 22:12:20.898: INFO: Created: latency-svc-7qc7t
Mar 27 22:12:20.904: INFO: Got endpoints: latency-svc-7qc7t [148.376079ms]
Mar 27 22:12:20.922: INFO: Created: latency-svc-b2bs9
Mar 27 22:12:20.928: INFO: Got endpoints: latency-svc-b2bs9 [172.769006ms]
Mar 27 22:12:20.944: INFO: Created: latency-svc-mtmpb
Mar 27 22:12:20.950: INFO: Got endpoints: latency-svc-mtmpb [179.782244ms]
Mar 27 22:12:20.966: INFO: Created: latency-svc-47k57
Mar 27 22:12:20.972: INFO: Got endpoints: latency-svc-47k57 [202.216617ms]
Mar 27 22:12:20.990: INFO: Created: latency-svc-9s55j
Mar 27 22:12:20.994: INFO: Got endpoints: latency-svc-9s55j [223.506734ms]
Mar 27 22:12:21.035: INFO: Created: latency-svc-rmzq8
Mar 27 22:12:21.042: INFO: Got endpoints: latency-svc-rmzq8 [271.848706ms]
Mar 27 22:12:21.057: INFO: Created: latency-svc-b8lkg
Mar 27 22:12:21.063: INFO: Got endpoints: latency-svc-b8lkg [285.431263ms]
Mar 27 22:12:21.084: INFO: Created: latency-svc-nvl6s
Mar 27 22:12:21.089: INFO: Got endpoints: latency-svc-nvl6s [311.219917ms]
Mar 27 22:12:21.107: INFO: Created: latency-svc-h79vr
Mar 27 22:12:21.111: INFO: Got endpoints: latency-svc-h79vr [334.893834ms]
Mar 27 22:12:21.129: INFO: Created: latency-svc-fph82
Mar 27 22:12:21.140: INFO: Got endpoints: latency-svc-fph82 [360.520564ms]
Mar 27 22:12:21.157: INFO: Created: latency-svc-d7qhc
Mar 27 22:12:21.163: INFO: Got endpoints: latency-svc-d7qhc [382.831236ms]
Mar 27 22:12:21.181: INFO: Created: latency-svc-9drfw
Mar 27 22:12:21.187: INFO: Got endpoints: latency-svc-9drfw [378.853265ms]
Mar 27 22:12:21.204: INFO: Created: latency-svc-fjjxh
Mar 27 22:12:21.209: INFO: Got endpoints: latency-svc-fjjxh [376.305501ms]
Mar 27 22:12:21.563: INFO: Created: latency-svc-zt9r8
Mar 27 22:12:21.571: INFO: Created: latency-svc-rqmlr
Mar 27 22:12:21.571: INFO: Created: latency-svc-l9j5m
Mar 27 22:12:21.571: INFO: Created: latency-svc-h62xd
Mar 27 22:12:21.572: INFO: Created: latency-svc-5hq28
Mar 27 22:12:21.572: INFO: Created: latency-svc-strmg
Mar 27 22:12:21.573: INFO: Created: latency-svc-h8czn
Mar 27 22:12:21.573: INFO: Created: latency-svc-7jp4m
Mar 27 22:12:21.576: INFO: Created: latency-svc-svmxd
Mar 27 22:12:21.577: INFO: Created: latency-svc-h7bnv
Mar 27 22:12:21.577: INFO: Created: latency-svc-5fxbv
Mar 27 22:12:21.578: INFO: Created: latency-svc-cvpm2
Mar 27 22:12:21.577: INFO: Created: latency-svc-z6d7b
Mar 27 22:12:21.577: INFO: Created: latency-svc-spwb2
Mar 27 22:12:21.578: INFO: Created: latency-svc-qt7nm
Mar 27 22:12:21.580: INFO: Got endpoints: latency-svc-rqmlr [491.235905ms]
Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-zt9r8 [608.639095ms]
Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-h62xd [586.452699ms]
Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-5hq28 [652.75907ms]
Mar 27 22:12:21.582: INFO: Got endpoints: latency-svc-h8czn [518.608351ms]
Mar 27 22:12:21.588: INFO: Got endpoints: latency-svc-strmg [684.296167ms]
Mar 27 22:12:21.588: INFO: Got endpoints: latency-svc-svmxd [708.59894ms]
Mar 27 22:12:21.589: INFO: Got endpoints: latency-svc-l9j5m [732.424536ms]
Mar 27 22:12:21.593: INFO: Got endpoints: latency-svc-z6d7b [481.758441ms]
Mar 27 22:12:21.594: INFO: Got endpoints: latency-svc-spwb2 [453.493865ms]
Mar 27 22:12:21.595: INFO: Got endpoints: latency-svc-7jp4m [431.443068ms]
Mar 27 22:12:21.595: INFO: Got endpoints: latency-svc-cvpm2 [552.692598ms]
Mar 27 22:12:21.597: INFO: Got endpoints: latency-svc-h7bnv [647.669248ms]
Mar 27 22:12:21.599: INFO: Got endpoints: latency-svc-5fxbv [412.629974ms]
Mar 27 22:12:21.601: INFO: Got endpoints: latency-svc-qt7nm [391.733848ms]
Mar 27 22:12:21.645: INFO: Created: latency-svc-xf49f
Mar 27 22:12:21.650: INFO: Got endpoints: latency-svc-xf49f [69.808096ms]
Mar 27 22:12:21.665: INFO: Created: latency-svc-hfqfn
Mar 27 22:12:21.671: INFO: Got endpoints: latency-svc-hfqfn [90.14967ms]
Mar 27 22:12:21.688: INFO: Created: latency-svc-rchh4
Mar 27 22:12:21.694: INFO: Got endpoints: latency-svc-rchh4 [113.557499ms]
Mar 27 22:12:21.710: INFO: Created: latency-svc-twchs
Mar 27 22:12:21.716: INFO: Got endpoints: latency-svc-twchs [134.89405ms]
Mar 27 22:12:21.733: INFO: Created: latency-svc-bklzr
Mar 27 22:12:21.738: INFO: Got endpoints: latency-svc-bklzr [156.825258ms]
Mar 27 22:12:21.761: INFO: Created: latency-svc-6bvcp
Mar 27 22:12:21.766: INFO: Got endpoints: latency-svc-6bvcp [177.664049ms]
Mar 27 22:12:21.784: INFO: Created: latency-svc-8m5l6
Mar 27 22:12:21.789: INFO: Got endpoints: latency-svc-8m5l6 [200.835009ms]
Mar 27 22:12:21.808: INFO: Created: latency-svc-lgrqz
Mar 27 22:12:21.815: INFO: Got endpoints: latency-svc-lgrqz [226.143184ms]
Mar 27 22:12:21.844: INFO: Created: latency-svc-dbl9m
Mar 27 22:12:21.846: INFO: Got endpoints: latency-svc-dbl9m [252.092388ms]
Mar 27 22:12:21.867: INFO: Created: latency-svc-pqrl5
Mar 27 22:12:21.873: INFO: Got endpoints: latency-svc-pqrl5 [277.770714ms]
Mar 27 22:12:21.891: INFO: Created: latency-svc-kdcj9
Mar 27 22:12:21.896: INFO: Got endpoints: latency-svc-kdcj9 [301.516291ms]
Mar 27 22:12:21.928: INFO: Created: latency-svc-frs6b
Mar 27 22:12:21.933: INFO: Got endpoints: latency-svc-frs6b [337.368305ms]
Mar 27 22:12:21.947: INFO: Created: latency-svc-mlxgd
Mar 27 22:12:21.953: INFO: Got endpoints: latency-svc-mlxgd [355.361022ms]
Mar 27 22:12:21.969: INFO: Created: latency-svc-k6btv
Mar 27 22:12:21.976: INFO: Got endpoints: latency-svc-k6btv [376.581299ms]
Mar 27 22:12:21.993: INFO: Created: latency-svc-jjvls
Mar 27 22:12:21.998: INFO: Got endpoints: latency-svc-jjvls [397.515987ms]
Mar 27 22:12:22.021: INFO: Created: latency-svc-pfxzl
Mar 27 22:12:22.026: INFO: Got endpoints: latency-svc-pfxzl [375.593303ms]
Mar 27 22:12:22.049: INFO: Created: latency-svc-xcpf4
Mar 27 22:12:22.059: INFO: Got endpoints: latency-svc-xcpf4 [387.425704ms]
Mar 27 22:12:22.078: INFO: Created: latency-svc-vwzcz
Mar 27 22:12:22.082: INFO: Got endpoints: latency-svc-vwzcz [388.04675ms]
Mar 27 22:12:22.122: INFO: Created: latency-svc-4vxwl
Mar 27 22:12:22.122: INFO: Got endpoints: latency-svc-4vxwl [406.089304ms]
Mar 27 22:12:22.142: INFO: Created: latency-svc-bjklm
Mar 27 22:12:22.147: INFO: Got endpoints: latency-svc-bjklm [408.278639ms]
Mar 27 22:12:22.167: INFO: Created: latency-svc-9n6nx
Mar 27 22:12:22.173: INFO: Got endpoints: latency-svc-9n6nx [407.247091ms]
Mar 27 22:12:22.196: INFO: Created: latency-svc-tpllr
Mar 27 22:12:22.204: INFO: Got endpoints: latency-svc-tpllr [414.622037ms]
Mar 27 22:12:22.221: INFO: Created: latency-svc-xv278
Mar 27 22:12:22.227: INFO: Got endpoints: latency-svc-xv278 [410.955048ms]
Mar 27 22:12:22.643: INFO: Created: latency-svc-sn79r
Mar 27 22:12:22.644: INFO: Created: latency-svc-9vzwd
Mar 27 22:12:22.644: INFO: Created: latency-svc-jqqxl
Mar 27 22:12:22.645: INFO: Created: latency-svc-hzkv5
Mar 27 22:12:22.645: INFO: Created: latency-svc-mxxwj
Mar 27 22:12:22.645: INFO: Created: latency-svc-xtd68
Mar 27 22:12:22.647: INFO: Created: latency-svc-29lqh
Mar 27 22:12:22.647: INFO: Created: latency-svc-wcfr4
Mar 27 22:12:22.648: INFO: Created: latency-svc-dwcm8
Mar 27 22:12:22.648: INFO: Created: latency-svc-8mfg5
Mar 27 22:12:22.649: INFO: Created: latency-svc-4snzk
Mar 27 22:12:22.649: INFO: Created: latency-svc-l96qr
Mar 27 22:12:22.650: INFO: Created: latency-svc-rztbh
Mar 27 22:12:22.651: INFO: Created: latency-svc-jnq8h
Mar 27 22:12:22.652: INFO: Created: latency-svc-hv9rr
Mar 27 22:12:22.656: INFO: Got endpoints: latency-svc-hv9rr [533.420407ms]
Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-hzkv5 [576.549154ms]
Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-9vzwd [706.087752ms]
Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-mxxwj [683.184572ms]
Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-sn79r [813.891632ms]
Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-jqqxl [762.992503ms]
Mar 27 22:12:22.664: INFO: Got endpoints: latency-svc-29lqh [637.912014ms]
Mar 27 22:12:22.668: INFO: Got endpoints: latency-svc-xtd68 [795.711871ms]
Mar 27 22:12:22.669: INFO: Got endpoints: latency-svc-dwcm8 [442.246368ms]
Mar 27 22:12:22.670: INFO: Got endpoints: latency-svc-l96qr [736.696376ms]
Mar 27 22:12:22.670: INFO: Got endpoints: latency-svc-wcfr4 [611.060898ms]
Mar 27 22:12:22.676: INFO: Got endpoints: latency-svc-8mfg5 [472.022226ms]
Mar 27 22:12:22.676: INFO: Got endpoints: latency-svc-4snzk [677.978384ms]
Mar 27 22:12:22.677: INFO: Got endpoints: latency-svc-jnq8h [530.035581ms]
Mar 27 22:12:22.678: INFO: Got endpoints: latency-svc-rztbh [504.076785ms]
Mar 27 22:12:22.711: INFO: Created: latency-svc-njhl4
Mar 27 22:12:22.719: INFO: Got endpoints: latency-svc-njhl4 [62.33737ms]
Mar 27 22:12:22.733: INFO: Created: latency-svc-92sxk
Mar 27 22:12:22.739: INFO: Got endpoints: latency-svc-92sxk [79.119683ms]
Mar 27 22:12:22.760: INFO: Created: latency-svc-bp8q7
Mar 27 22:12:22.765: INFO: Got endpoints: latency-svc-bp8q7 [105.039669ms]
Mar 27 22:12:22.786: INFO: Created: latency-svc-6bvxk
Mar 27 22:12:22.791: INFO: Got endpoints: latency-svc-6bvxk [132.230667ms]
Mar 27 22:12:22.813: INFO: Created: latency-svc-pg5dn
Mar 27 22:12:22.821: INFO: Got endpoints: latency-svc-pg5dn [157.188128ms]
Mar 27 22:12:22.836: INFO: Created: latency-svc-p4mzc
Mar 27 22:12:22.842: INFO: Got endpoints: latency-svc-p4mzc [182.584332ms]
Mar 27 22:12:22.858: INFO: Created: latency-svc-4xgj6
Mar 27 22:12:22.864: INFO: Got endpoints: latency-svc-4xgj6 [204.399706ms]
Mar 27 22:12:22.883: INFO: Created: latency-svc-5cjzd
Mar 27 22:12:22.888: INFO: Got endpoints: latency-svc-5cjzd [219.557626ms]
Mar 27 22:12:22.904: INFO: Created: latency-svc-h47f9
Mar 27 22:12:22.912: INFO: Got endpoints: latency-svc-h47f9 [242.698522ms]
Mar 27 22:12:22.934: INFO: Created: latency-svc-kjngh
Mar 27 22:12:22.938: INFO: Got endpoints: latency-svc-kjngh [268.733778ms]
Mar 27 22:12:22.954: INFO: Created: latency-svc-vrvjh
Mar 27 22:12:22.963: INFO: Got endpoints: latency-svc-vrvjh [286.823566ms]
Mar 27 22:12:22.986: INFO: Created: latency-svc-h8d9n
Mar 27 22:12:22.990: INFO: Got endpoints: latency-svc-h8d9n [320.034148ms]
Mar 27 22:12:23.009: INFO: Created: latency-svc-lwcmk
Mar 27 22:12:23.014: INFO: Got endpoints: latency-svc-lwcmk [336.511552ms]
Mar 27 22:12:23.034: INFO: Created: latency-svc-f87d9
Mar 27 22:12:23.039: INFO: Got endpoints: latency-svc-f87d9 [362.190585ms]
Mar 27 22:12:23.077: INFO: Created: latency-svc-5hjnd
Mar 27 22:12:23.083: INFO: Got endpoints: latency-svc-5hjnd [406.375245ms]
Mar 27 22:12:23.108: INFO: Created: latency-svc-k7pbw
Mar 27 22:12:23.112: INFO: Got endpoints: latency-svc-k7pbw [392.428498ms]
Mar 27 22:12:23.127: INFO: Created: latency-svc-xj7r2
Mar 27 22:12:23.132: INFO: Got endpoints: latency-svc-xj7r2 [393.105161ms]
Mar 27 22:12:23.183: INFO: Created: latency-svc-klvw2
Mar 27 22:12:23.189: INFO: Got endpoints: latency-svc-klvw2 [424.306203ms]
Mar 27 22:12:23.209: INFO: Created: latency-svc-lzwmn
Mar 27 22:12:23.215: INFO: Got endpoints: latency-svc-lzwmn [423.781227ms]
Mar 27 22:12:23.232: INFO: Created: latency-svc-g8ktg
Mar 27 22:12:23.238: INFO: Got endpoints: latency-svc-g8ktg [416.187707ms]
Mar 27 22:12:23.254: INFO: Created: latency-svc-k8d4n
Mar 27 22:12:23.261: INFO: Got endpoints: latency-svc-k8d4n [419.024729ms]
Mar 27 22:12:23.275: INFO: Created: latency-svc-jqq6h
Mar 27 22:12:23.282: INFO: Got endpoints: latency-svc-jqq6h [417.593356ms]
Mar 27 22:12:23.303: INFO: Created: latency-svc-qvwkp
Mar 27 22:12:23.309: INFO: Got endpoints: latency-svc-qvwkp [420.44616ms]
Mar 27 22:12:23.321: INFO: Created: latency-svc-gt569
Mar 27 22:12:23.326: INFO: Got endpoints: latency-svc-gt569 [414.677833ms]
Mar 27 22:12:23.345: INFO: Created: latency-svc-6bn5z
Mar 27 22:12:23.350: INFO: Got endpoints: latency-svc-6bn5z [411.003573ms]
Mar 27 22:12:23.368: INFO: Created: latency-svc-zgnfx
Mar 27 22:12:23.375: INFO: Got endpoints: latency-svc-zgnfx [412.548392ms]
Mar 27 22:12:23.390: INFO: Created: latency-svc-wsxvk
Mar 27 22:12:23.397: INFO: Got endpoints: latency-svc-wsxvk [406.780132ms]
Mar 27 22:12:23.413: INFO: Created: latency-svc-6zzds
Mar 27 22:12:23.426: INFO: Got endpoints: latency-svc-6zzds [411.651522ms]
Mar 27 22:12:23.434: INFO: Created: latency-svc-rrx7w
Mar 27 22:12:23.440: INFO: Got endpoints: latency-svc-rrx7w [399.414069ms]
Mar 27 22:12:23.458: INFO: Created: latency-svc-7zccv
Mar 27 22:12:23.464: INFO: Got endpoints: latency-svc-7zccv [381.444062ms]
Mar 27 22:12:23.482: INFO: Created: latency-svc-9n9r2
Mar 27 22:12:23.488: INFO: Got endpoints: latency-svc-9n9r2 [375.916768ms]
Mar 27 22:12:23.506: INFO: Created: latency-svc-qjg47
Mar 27 22:12:23.511: INFO: Got endpoints: latency-svc-qjg47 [379.649189ms]
Mar 27 22:12:23.528: INFO: Created: latency-svc-55kzw
Mar 27 22:12:23.534: INFO: Got endpoints: latency-svc-55kzw [344.258442ms]
Mar 27 22:12:23.551: INFO: Created: latency-svc-5m7vm
Mar 27 22:12:23.559: INFO: Got endpoints: latency-svc-5m7vm [343.411066ms]
Mar 27 22:12:23.575: INFO: Created: latency-svc-b47s5
Mar 27 22:12:23.581: INFO: Got endpoints: latency-svc-b47s5 [343.646818ms]
Mar 27 22:12:23.599: INFO: Created: latency-svc-zhnbp
Mar 27 22:12:23.604: INFO: Got endpoints: latency-svc-zhnbp [342.991735ms]
Mar 27 22:12:23.942: INFO: Created: latency-svc-wmqxz
Mar 27 22:12:23.946: INFO: Created: latency-svc-25gql
Mar 27 22:12:23.950: INFO: Created: latency-svc-czgpv
Mar 27 22:12:23.952: INFO: Created: latency-svc-s9kkn
Mar 27 22:12:23.952: INFO: Created: latency-svc-fvvp8
Mar 27 22:12:23.952: INFO: Created: latency-svc-9mtb8
Mar 27 22:12:23.953: INFO: Created: latency-svc-sfnzg
Mar 27 22:12:23.953: INFO: Created: latency-svc-2pvbs
Mar 27 22:12:23.953: INFO: Created: latency-svc-sndxn
Mar 27 22:12:23.952: INFO: Created: latency-svc-qnm6l
Mar 27 22:12:23.953: INFO: Created: latency-svc-pz6fm
Mar 27 22:12:23.953: INFO: Got endpoints: latency-svc-wmqxz [603.298658ms]
Mar 27 22:12:23.953: INFO: Created: latency-svc-m6h2c
Mar 27 22:12:23.953: INFO: Created: latency-svc-rgwx4
Mar 27 22:12:23.953: INFO: Created: latency-svc-ggnjr
Mar 27 22:12:23.954: INFO: Created: latency-svc-kkqm2
Mar 27 22:12:23.991: INFO: Got endpoints: latency-svc-sndxn [708.970556ms]
Mar 27 22:12:23.995: INFO: Got endpoints: latency-svc-pz6fm [568.66136ms]
Mar 27 22:12:23.995: INFO: Got endpoints: latency-svc-s9kkn [686.096529ms]
Mar 27 22:12:23.998: INFO: Got endpoints: latency-svc-25gql [393.78264ms]
Mar 27 22:12:23.998: INFO: Got endpoints: latency-svc-qnm6l [416.521715ms]
Mar 27 22:12:24.002: INFO: Created: latency-svc-rhts5
Mar 27 22:12:24.009: INFO: Got endpoints: latency-svc-2pvbs [520.872519ms]
Mar 27 22:12:24.023: INFO: Got endpoints: latency-svc-czgpv [558.350368ms]
Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-fvvp8 [696.984029ms]
Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-kkqm2 [626.715371ms]
Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-sfnzg [512.146168ms]
Mar 27 22:12:24.026: INFO: Got endpoints: latency-svc-ggnjr [492.331687ms]
Mar 27 22:12:24.030: INFO: Got endpoints: latency-svc-rgwx4 [654.695083ms]
Mar 27 22:12:24.032: INFO: Got endpoints: latency-svc-9mtb8 [473.218066ms]
Mar 27 22:12:24.032: INFO: Got endpoints: latency-svc-m6h2c [591.8571ms]
Mar 27 22:12:24.033: INFO: Got endpoints: latency-svc-rhts5 [79.63614ms]
Mar 27 22:12:24.042: INFO: Created: latency-svc-nppz5
Mar 27 22:12:24.048: INFO: Got endpoints: latency-svc-nppz5 [56.835384ms]
Mar 27 22:12:24.404: INFO: Created: latency-svc-8pgtc
Mar 27 22:12:24.412: INFO: Created: latency-svc-jgfcp
Mar 27 22:12:24.412: INFO: Created: latency-svc-l8mcc
Mar 27 22:12:24.414: INFO: Created: latency-svc-9f8dk
Mar 27 22:12:24.414: INFO: Created: latency-svc-g8dzm
Mar 27 22:12:24.415: INFO: Created: latency-svc-x5fwk
Mar 27 22:12:24.415: INFO: Created: latency-svc-8ldx8
Mar 27 22:12:24.415: INFO: Created: latency-svc-fqncb
Mar 27 22:12:24.415: INFO: Created: latency-svc-xrmnn
Mar 27 22:12:24.416: INFO: Created: latency-svc-sfvgm
Mar 27 22:12:24.416: INFO: Created: latency-svc-gpq4q
Mar 27 22:12:24.417: INFO: Created: latency-svc-qtc77
Mar 27 22:12:24.418: INFO: Created: latency-svc-dgpv5
Mar 27 22:12:24.417: INFO: Created: latency-svc-hb9nc
Mar 27 22:12:24.417: INFO: Got endpoints: latency-svc-x5fwk [422.166363ms]
Mar 27 22:12:24.417: INFO: Got endpoints: latency-svc-8pgtc [385.154122ms]
Mar 27 22:12:24.417: INFO: Created: latency-svc-k2rd2
Mar 27 22:12:24.419: INFO: Got endpoints: latency-svc-l8mcc [371.532236ms]
Mar 27 22:12:24.420: INFO: Got endpoints: latency-svc-g8dzm [397.112968ms]
Mar 27 22:12:24.420: INFO: Got endpoints: latency-svc-9f8dk [395.930071ms]
Mar 27 22:12:24.421: INFO: Got endpoints: latency-svc-jgfcp [411.830583ms]
Mar 27 22:12:24.421: INFO: Got endpoints: latency-svc-8ldx8 [389.293255ms]
Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-gpq4q [397.631946ms]
Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-dgpv5 [403.675517ms]
Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-hb9nc [430.575129ms]
Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-qtc77 [430.511751ms]
Mar 27 22:12:24.429: INFO: Got endpoints: latency-svc-sfvgm [433.149999ms]
Mar 27 22:12:24.435: INFO: Got endpoints: latency-svc-k2rd2 [402.627446ms]
Mar 27 22:12:24.437: INFO: Got endpoints: latency-svc-xrmnn [410.632666ms]
Mar 27 22:12:24.437: INFO: Got endpoints: latency-svc-fqncb [413.081887ms]
Mar 27 22:12:24.437: INFO: Latencies: [48.817786ms 52.130574ms 53.620068ms 56.835384ms 62.33737ms 69.808096ms 75.959429ms 76.350437ms 77.793035ms 79.119683ms 79.63614ms 90.14967ms 100.06448ms 101.647799ms 105.039669ms 113.286835ms 113.557499ms 120.908076ms 124.877532ms 132.230667ms 134.89405ms 138.981021ms 140.875869ms 148.376079ms 156.825258ms 157.188128ms 158.500659ms 160.98778ms 172.769006ms 177.664049ms 179.782244ms 182.584332ms 184.298977ms 186.211213ms 200.835009ms 200.916732ms 202.216617ms 204.399706ms 217.133299ms 219.557626ms 223.506734ms 225.152236ms 226.143184ms 242.698522ms 243.69425ms 252.092388ms 257.161856ms 268.733778ms 271.848706ms 277.770714ms 285.431263ms 286.823566ms 293.48329ms 301.516291ms 311.219917ms 315.940163ms 320.034148ms 334.893834ms 336.511552ms 337.112026ms 337.368305ms 342.991735ms 343.411066ms 343.646818ms 344.258442ms 355.361022ms 360.520564ms 362.190585ms 367.601879ms 371.532236ms 375.593303ms 375.916768ms 376.305501ms 376.581299ms 376.744333ms 378.594587ms 378.853265ms 379.649189ms 379.95732ms 380.808524ms 381.444062ms 382.831236ms 384.976042ms 385.154122ms 387.425704ms 387.801575ms 388.04675ms 388.115018ms 388.677198ms 389.288081ms 389.293255ms 389.830199ms 389.841832ms 390.379785ms 391.334836ms 391.733848ms 392.428498ms 393.105161ms 393.78264ms 394.680096ms 395.930071ms 397.112968ms 397.515987ms 397.631946ms 399.414069ms 402.627446ms 403.675517ms 406.089304ms 406.375245ms 406.780132ms 407.247091ms 408.278639ms 410.632666ms 410.955048ms 411.003573ms 411.651522ms 411.830583ms 412.548392ms 412.629974ms 413.081887ms 414.622037ms 414.677833ms 416.187707ms 416.521715ms 417.593356ms 419.024729ms 420.44616ms 422.166363ms 422.979896ms 423.781227ms 424.306203ms 427.731747ms 430.511751ms 430.575129ms 431.443068ms 433.149999ms 435.63456ms 442.246368ms 453.493865ms 460.492967ms 466.225092ms 469.472729ms 471.053201ms 472.022226ms 473.218066ms 481.758441ms 491.235905ms 492.331687ms 495.221907ms 504.076785ms 512.146168ms 515.608842ms 518.608351ms 520.872519ms 530.035581ms 533.420407ms 533.736189ms 551.653876ms 552.692598ms 557.988649ms 558.350368ms 568.66136ms 574.211697ms 576.549154ms 586.452699ms 586.840873ms 591.8571ms 597.227682ms 603.298658ms 608.639095ms 611.060898ms 612.290805ms 612.418284ms 622.370372ms 624.224864ms 626.275718ms 626.715371ms 629.57974ms 634.575113ms 637.912014ms 647.669248ms 652.75907ms 654.369589ms 654.695083ms 673.276605ms 677.978384ms 683.184572ms 684.296167ms 686.096529ms 696.984029ms 697.830164ms 706.087752ms 708.59894ms 708.970556ms 709.425495ms 732.424536ms 736.696376ms 762.992503ms 795.711871ms 813.891632ms]
Mar 27 22:12:24.438: INFO: 50 %ile: 395.930071ms
Mar 27 22:12:24.438: INFO: 90 %ile: 647.669248ms
Mar 27 22:12:24.438: INFO: 99 %ile: 795.711871ms
Mar 27 22:12:24.438: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Mar 27 22:12:24.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-574" for this suite. 03/27/23 22:12:24.474
------------------------------
• [SLOW TEST] [7.634 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:12:16.863
    Mar 27 22:12:16.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svc-latency 03/27/23 22:12:16.865
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:16.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:16.927
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 27 22:12:16.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-574 03/27/23 22:12:16.941
    I0327 22:12:16.959092      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-574, replica count: 1
    I0327 22:12:18.009866      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0327 22:12:19.010248      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 22:12:19.151: INFO: Created: latency-svc-wp74v
    Mar 27 22:12:19.158: INFO: Got endpoints: latency-svc-wp74v [47.442012ms]
    Mar 27 22:12:19.201: INFO: Created: latency-svc-6f64f
    Mar 27 22:12:19.207: INFO: Got endpoints: latency-svc-6f64f [48.817786ms]
    Mar 27 22:12:19.227: INFO: Created: latency-svc-bpvl9
    Mar 27 22:12:19.235: INFO: Got endpoints: latency-svc-bpvl9 [76.350437ms]
    Mar 27 22:12:19.267: INFO: Created: latency-svc-bq4cm
    Mar 27 22:12:19.272: INFO: Got endpoints: latency-svc-bq4cm [113.286835ms]
    Mar 27 22:12:19.294: INFO: Created: latency-svc-fdfcm
    Mar 27 22:12:19.300: INFO: Got endpoints: latency-svc-fdfcm [140.875869ms]
    Mar 27 22:12:19.314: INFO: Created: latency-svc-58cp5
    Mar 27 22:12:19.320: INFO: Got endpoints: latency-svc-58cp5 [160.98778ms]
    Mar 27 22:12:19.338: INFO: Created: latency-svc-wjvm5
    Mar 27 22:12:19.344: INFO: Got endpoints: latency-svc-wjvm5 [184.298977ms]
    Mar 27 22:12:19.370: INFO: Created: latency-svc-sxtgl
    Mar 27 22:12:19.377: INFO: Got endpoints: latency-svc-sxtgl [217.133299ms]
    Mar 27 22:12:19.396: INFO: Created: latency-svc-vd92s
    Mar 27 22:12:19.404: INFO: Got endpoints: latency-svc-vd92s [243.69425ms]
    Mar 27 22:12:19.771: INFO: Created: latency-svc-wvmx4
    Mar 27 22:12:19.771: INFO: Got endpoints: latency-svc-wvmx4 [471.053201ms]
    Mar 27 22:12:19.771: INFO: Created: latency-svc-pczdn
    Mar 27 22:12:19.771: INFO: Got endpoints: latency-svc-pczdn [394.680096ms]
    Mar 27 22:12:19.772: INFO: Created: latency-svc-5gcpm
    Mar 27 22:12:19.772: INFO: Got endpoints: latency-svc-5gcpm [612.290805ms]
    Mar 27 22:12:19.772: INFO: Created: latency-svc-qv72v
    Mar 27 22:12:19.772: INFO: Got endpoints: latency-svc-qv72v [612.418284ms]
    Mar 27 22:12:19.774: INFO: Created: latency-svc-5k8v6
    Mar 27 22:12:19.774: INFO: Created: latency-svc-9r9kz
    Mar 27 22:12:19.775: INFO: Created: latency-svc-wf2pl
    Mar 27 22:12:19.775: INFO: Created: latency-svc-wwphz
    Mar 27 22:12:19.775: INFO: Created: latency-svc-x6rsw
    Mar 27 22:12:19.775: INFO: Created: latency-svc-qhxrd
    Mar 27 22:12:19.775: INFO: Created: latency-svc-2fp7l
    Mar 27 22:12:19.776: INFO: Created: latency-svc-kqvcb
    Mar 27 22:12:19.776: INFO: Created: latency-svc-zt9jm
    Mar 27 22:12:19.776: INFO: Created: latency-svc-z5nch
    Mar 27 22:12:19.776: INFO: Created: latency-svc-pqcgl
    Mar 27 22:12:19.780: INFO: Got endpoints: latency-svc-z5nch [435.63456ms]
    Mar 27 22:12:19.781: INFO: Got endpoints: latency-svc-x6rsw [622.370372ms]
    Mar 27 22:12:19.781: INFO: Got endpoints: latency-svc-zt9jm [574.211697ms]
    Mar 27 22:12:19.784: INFO: Got endpoints: latency-svc-kqvcb [624.224864ms]
    Mar 27 22:12:19.784: INFO: Got endpoints: latency-svc-5k8v6 [379.95732ms]
    Mar 27 22:12:19.786: INFO: Got endpoints: latency-svc-9r9kz [626.275718ms]
    Mar 27 22:12:19.788: INFO: Got endpoints: latency-svc-wwphz [515.608842ms]
    Mar 27 22:12:19.787: INFO: Got endpoints: latency-svc-pqcgl [466.225092ms]
    Mar 27 22:12:19.787: INFO: Got endpoints: latency-svc-qhxrd [551.653876ms]
    Mar 27 22:12:19.789: INFO: Got endpoints: latency-svc-wf2pl [629.57974ms]
    Mar 27 22:12:19.794: INFO: Got endpoints: latency-svc-2fp7l [634.575113ms]
    Mar 27 22:12:19.816: INFO: Created: latency-svc-hxf5x
    Mar 27 22:12:19.823: INFO: Got endpoints: latency-svc-hxf5x [52.130574ms]
    Mar 27 22:12:19.841: INFO: Created: latency-svc-f9sgj
    Mar 27 22:12:19.847: INFO: Got endpoints: latency-svc-f9sgj [75.959429ms]
    Mar 27 22:12:19.866: INFO: Created: latency-svc-59qlq
    Mar 27 22:12:19.872: INFO: Got endpoints: latency-svc-59qlq [100.06448ms]
    Mar 27 22:12:19.888: INFO: Created: latency-svc-9lr8k
    Mar 27 22:12:19.893: INFO: Got endpoints: latency-svc-9lr8k [120.908076ms]
    Mar 27 22:12:19.913: INFO: Created: latency-svc-sfqvv
    Mar 27 22:12:19.919: INFO: Got endpoints: latency-svc-sfqvv [138.981021ms]
    Mar 27 22:12:19.936: INFO: Created: latency-svc-hjfnv
    Mar 27 22:12:19.943: INFO: Got endpoints: latency-svc-hjfnv [158.500659ms]
    Mar 27 22:12:19.971: INFO: Created: latency-svc-jf2dz
    Mar 27 22:12:19.971: INFO: Got endpoints: latency-svc-jf2dz [186.211213ms]
    Mar 27 22:12:19.981: INFO: Created: latency-svc-szkpc
    Mar 27 22:12:19.986: INFO: Got endpoints: latency-svc-szkpc [200.916732ms]
    Mar 27 22:12:20.007: INFO: Created: latency-svc-d7fk9
    Mar 27 22:12:20.012: INFO: Got endpoints: latency-svc-d7fk9 [225.152236ms]
    Mar 27 22:12:20.037: INFO: Created: latency-svc-sqb4b
    Mar 27 22:12:20.045: INFO: Got endpoints: latency-svc-sqb4b [257.161856ms]
    Mar 27 22:12:20.077: INFO: Created: latency-svc-69h2s
    Mar 27 22:12:20.082: INFO: Got endpoints: latency-svc-69h2s [293.48329ms]
    Mar 27 22:12:20.098: INFO: Created: latency-svc-vt25d
    Mar 27 22:12:20.103: INFO: Got endpoints: latency-svc-vt25d [315.940163ms]
    Mar 27 22:12:20.121: INFO: Created: latency-svc-z4fmr
    Mar 27 22:12:20.126: INFO: Got endpoints: latency-svc-z4fmr [337.112026ms]
    Mar 27 22:12:20.152: INFO: Created: latency-svc-csshg
    Mar 27 22:12:20.157: INFO: Got endpoints: latency-svc-csshg [367.601879ms]
    Mar 27 22:12:20.173: INFO: Created: latency-svc-sfmws
    Mar 27 22:12:20.184: INFO: Got endpoints: latency-svc-sfmws [389.830199ms]
    Mar 27 22:12:20.206: INFO: Created: latency-svc-b49h7
    Mar 27 22:12:20.212: INFO: Got endpoints: latency-svc-b49h7 [388.677198ms]
    Mar 27 22:12:20.232: INFO: Created: latency-svc-6fxjz
    Mar 27 22:12:20.237: INFO: Got endpoints: latency-svc-6fxjz [389.288081ms]
    Mar 27 22:12:20.254: INFO: Created: latency-svc-k55ln
    Mar 27 22:12:20.260: INFO: Got endpoints: latency-svc-k55ln [387.801575ms]
    Mar 27 22:12:20.280: INFO: Created: latency-svc-zctw8
    Mar 27 22:12:20.285: INFO: Got endpoints: latency-svc-zctw8 [391.334836ms]
    Mar 27 22:12:20.301: INFO: Created: latency-svc-9hqnh
    Mar 27 22:12:20.309: INFO: Got endpoints: latency-svc-9hqnh [390.379785ms]
    Mar 27 22:12:20.323: INFO: Created: latency-svc-xxvvr
    Mar 27 22:12:20.333: INFO: Got endpoints: latency-svc-xxvvr [389.841832ms]
    Mar 27 22:12:20.344: INFO: Created: latency-svc-jwbqd
    Mar 27 22:12:20.350: INFO: Got endpoints: latency-svc-jwbqd [378.594587ms]
    Mar 27 22:12:20.367: INFO: Created: latency-svc-88nst
    Mar 27 22:12:20.374: INFO: Got endpoints: latency-svc-88nst [388.115018ms]
    Mar 27 22:12:20.397: INFO: Created: latency-svc-fljwg
    Mar 27 22:12:20.397: INFO: Got endpoints: latency-svc-fljwg [384.976042ms]
    Mar 27 22:12:20.742: INFO: Created: latency-svc-59mxf
    Mar 27 22:12:20.749: INFO: Created: latency-svc-rtwd8
    Mar 27 22:12:20.750: INFO: Created: latency-svc-x5fzj
    Mar 27 22:12:20.750: INFO: Created: latency-svc-ns7k6
    Mar 27 22:12:20.750: INFO: Created: latency-svc-z7szc
    Mar 27 22:12:20.750: INFO: Created: latency-svc-pxzpv
    Mar 27 22:12:20.751: INFO: Created: latency-svc-fx9gf
    Mar 27 22:12:20.751: INFO: Created: latency-svc-2t6mj
    Mar 27 22:12:20.751: INFO: Got endpoints: latency-svc-59mxf [376.744333ms]
    Mar 27 22:12:20.752: INFO: Created: latency-svc-d6hwf
    Mar 27 22:12:20.752: INFO: Created: latency-svc-g5kn8
    Mar 27 22:12:20.752: INFO: Created: latency-svc-7gxf5
    Mar 27 22:12:20.754: INFO: Created: latency-svc-g9h28
    Mar 27 22:12:20.754: INFO: Created: latency-svc-sg7hd
    Mar 27 22:12:20.754: INFO: Created: latency-svc-fkhtp
    Mar 27 22:12:20.754: INFO: Got endpoints: latency-svc-rtwd8 [469.472729ms]
    Mar 27 22:12:20.754: INFO: Got endpoints: latency-svc-2t6mj [709.425495ms]
    Mar 27 22:12:20.755: INFO: Got endpoints: latency-svc-pxzpv [597.227682ms]
    Mar 27 22:12:20.755: INFO: Got endpoints: latency-svc-fx9gf [495.221907ms]
    Mar 27 22:12:20.755: INFO: Created: latency-svc-hh475
    Mar 27 22:12:20.756: INFO: Got endpoints: latency-svc-ns7k6 [422.979896ms]
    Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-d6hwf [460.492967ms]
    Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-x5fzj [557.988649ms]
    Mar 27 22:12:20.770: INFO: Got endpoints: latency-svc-fkhtp [586.840873ms]
    Mar 27 22:12:20.771: INFO: Got endpoints: latency-svc-g9h28 [533.736189ms]
    Mar 27 22:12:20.776: INFO: Got endpoints: latency-svc-sg7hd [673.276605ms]
    Mar 27 22:12:20.777: INFO: Got endpoints: latency-svc-z7szc [427.731747ms]
    Mar 27 22:12:20.778: INFO: Got endpoints: latency-svc-7gxf5 [380.808524ms]
    Mar 27 22:12:20.780: INFO: Got endpoints: latency-svc-g5kn8 [697.830164ms]
    Mar 27 22:12:20.780: INFO: Got endpoints: latency-svc-hh475 [654.369589ms]
    Mar 27 22:12:20.805: INFO: Created: latency-svc-nv2jk
    Mar 27 22:12:20.808: INFO: Got endpoints: latency-svc-nv2jk [53.620068ms]
    Mar 27 22:12:20.827: INFO: Created: latency-svc-ws69v
    Mar 27 22:12:20.832: INFO: Got endpoints: latency-svc-ws69v [77.793035ms]
    Mar 27 22:12:20.853: INFO: Created: latency-svc-88g7l
    Mar 27 22:12:20.856: INFO: Got endpoints: latency-svc-88g7l [101.647799ms]
    Mar 27 22:12:20.875: INFO: Created: latency-svc-qpcqz
    Mar 27 22:12:20.880: INFO: Got endpoints: latency-svc-qpcqz [124.877532ms]
    Mar 27 22:12:20.898: INFO: Created: latency-svc-7qc7t
    Mar 27 22:12:20.904: INFO: Got endpoints: latency-svc-7qc7t [148.376079ms]
    Mar 27 22:12:20.922: INFO: Created: latency-svc-b2bs9
    Mar 27 22:12:20.928: INFO: Got endpoints: latency-svc-b2bs9 [172.769006ms]
    Mar 27 22:12:20.944: INFO: Created: latency-svc-mtmpb
    Mar 27 22:12:20.950: INFO: Got endpoints: latency-svc-mtmpb [179.782244ms]
    Mar 27 22:12:20.966: INFO: Created: latency-svc-47k57
    Mar 27 22:12:20.972: INFO: Got endpoints: latency-svc-47k57 [202.216617ms]
    Mar 27 22:12:20.990: INFO: Created: latency-svc-9s55j
    Mar 27 22:12:20.994: INFO: Got endpoints: latency-svc-9s55j [223.506734ms]
    Mar 27 22:12:21.035: INFO: Created: latency-svc-rmzq8
    Mar 27 22:12:21.042: INFO: Got endpoints: latency-svc-rmzq8 [271.848706ms]
    Mar 27 22:12:21.057: INFO: Created: latency-svc-b8lkg
    Mar 27 22:12:21.063: INFO: Got endpoints: latency-svc-b8lkg [285.431263ms]
    Mar 27 22:12:21.084: INFO: Created: latency-svc-nvl6s
    Mar 27 22:12:21.089: INFO: Got endpoints: latency-svc-nvl6s [311.219917ms]
    Mar 27 22:12:21.107: INFO: Created: latency-svc-h79vr
    Mar 27 22:12:21.111: INFO: Got endpoints: latency-svc-h79vr [334.893834ms]
    Mar 27 22:12:21.129: INFO: Created: latency-svc-fph82
    Mar 27 22:12:21.140: INFO: Got endpoints: latency-svc-fph82 [360.520564ms]
    Mar 27 22:12:21.157: INFO: Created: latency-svc-d7qhc
    Mar 27 22:12:21.163: INFO: Got endpoints: latency-svc-d7qhc [382.831236ms]
    Mar 27 22:12:21.181: INFO: Created: latency-svc-9drfw
    Mar 27 22:12:21.187: INFO: Got endpoints: latency-svc-9drfw [378.853265ms]
    Mar 27 22:12:21.204: INFO: Created: latency-svc-fjjxh
    Mar 27 22:12:21.209: INFO: Got endpoints: latency-svc-fjjxh [376.305501ms]
    Mar 27 22:12:21.563: INFO: Created: latency-svc-zt9r8
    Mar 27 22:12:21.571: INFO: Created: latency-svc-rqmlr
    Mar 27 22:12:21.571: INFO: Created: latency-svc-l9j5m
    Mar 27 22:12:21.571: INFO: Created: latency-svc-h62xd
    Mar 27 22:12:21.572: INFO: Created: latency-svc-5hq28
    Mar 27 22:12:21.572: INFO: Created: latency-svc-strmg
    Mar 27 22:12:21.573: INFO: Created: latency-svc-h8czn
    Mar 27 22:12:21.573: INFO: Created: latency-svc-7jp4m
    Mar 27 22:12:21.576: INFO: Created: latency-svc-svmxd
    Mar 27 22:12:21.577: INFO: Created: latency-svc-h7bnv
    Mar 27 22:12:21.577: INFO: Created: latency-svc-5fxbv
    Mar 27 22:12:21.578: INFO: Created: latency-svc-cvpm2
    Mar 27 22:12:21.577: INFO: Created: latency-svc-z6d7b
    Mar 27 22:12:21.577: INFO: Created: latency-svc-spwb2
    Mar 27 22:12:21.578: INFO: Created: latency-svc-qt7nm
    Mar 27 22:12:21.580: INFO: Got endpoints: latency-svc-rqmlr [491.235905ms]
    Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-zt9r8 [608.639095ms]
    Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-h62xd [586.452699ms]
    Mar 27 22:12:21.581: INFO: Got endpoints: latency-svc-5hq28 [652.75907ms]
    Mar 27 22:12:21.582: INFO: Got endpoints: latency-svc-h8czn [518.608351ms]
    Mar 27 22:12:21.588: INFO: Got endpoints: latency-svc-strmg [684.296167ms]
    Mar 27 22:12:21.588: INFO: Got endpoints: latency-svc-svmxd [708.59894ms]
    Mar 27 22:12:21.589: INFO: Got endpoints: latency-svc-l9j5m [732.424536ms]
    Mar 27 22:12:21.593: INFO: Got endpoints: latency-svc-z6d7b [481.758441ms]
    Mar 27 22:12:21.594: INFO: Got endpoints: latency-svc-spwb2 [453.493865ms]
    Mar 27 22:12:21.595: INFO: Got endpoints: latency-svc-7jp4m [431.443068ms]
    Mar 27 22:12:21.595: INFO: Got endpoints: latency-svc-cvpm2 [552.692598ms]
    Mar 27 22:12:21.597: INFO: Got endpoints: latency-svc-h7bnv [647.669248ms]
    Mar 27 22:12:21.599: INFO: Got endpoints: latency-svc-5fxbv [412.629974ms]
    Mar 27 22:12:21.601: INFO: Got endpoints: latency-svc-qt7nm [391.733848ms]
    Mar 27 22:12:21.645: INFO: Created: latency-svc-xf49f
    Mar 27 22:12:21.650: INFO: Got endpoints: latency-svc-xf49f [69.808096ms]
    Mar 27 22:12:21.665: INFO: Created: latency-svc-hfqfn
    Mar 27 22:12:21.671: INFO: Got endpoints: latency-svc-hfqfn [90.14967ms]
    Mar 27 22:12:21.688: INFO: Created: latency-svc-rchh4
    Mar 27 22:12:21.694: INFO: Got endpoints: latency-svc-rchh4 [113.557499ms]
    Mar 27 22:12:21.710: INFO: Created: latency-svc-twchs
    Mar 27 22:12:21.716: INFO: Got endpoints: latency-svc-twchs [134.89405ms]
    Mar 27 22:12:21.733: INFO: Created: latency-svc-bklzr
    Mar 27 22:12:21.738: INFO: Got endpoints: latency-svc-bklzr [156.825258ms]
    Mar 27 22:12:21.761: INFO: Created: latency-svc-6bvcp
    Mar 27 22:12:21.766: INFO: Got endpoints: latency-svc-6bvcp [177.664049ms]
    Mar 27 22:12:21.784: INFO: Created: latency-svc-8m5l6
    Mar 27 22:12:21.789: INFO: Got endpoints: latency-svc-8m5l6 [200.835009ms]
    Mar 27 22:12:21.808: INFO: Created: latency-svc-lgrqz
    Mar 27 22:12:21.815: INFO: Got endpoints: latency-svc-lgrqz [226.143184ms]
    Mar 27 22:12:21.844: INFO: Created: latency-svc-dbl9m
    Mar 27 22:12:21.846: INFO: Got endpoints: latency-svc-dbl9m [252.092388ms]
    Mar 27 22:12:21.867: INFO: Created: latency-svc-pqrl5
    Mar 27 22:12:21.873: INFO: Got endpoints: latency-svc-pqrl5 [277.770714ms]
    Mar 27 22:12:21.891: INFO: Created: latency-svc-kdcj9
    Mar 27 22:12:21.896: INFO: Got endpoints: latency-svc-kdcj9 [301.516291ms]
    Mar 27 22:12:21.928: INFO: Created: latency-svc-frs6b
    Mar 27 22:12:21.933: INFO: Got endpoints: latency-svc-frs6b [337.368305ms]
    Mar 27 22:12:21.947: INFO: Created: latency-svc-mlxgd
    Mar 27 22:12:21.953: INFO: Got endpoints: latency-svc-mlxgd [355.361022ms]
    Mar 27 22:12:21.969: INFO: Created: latency-svc-k6btv
    Mar 27 22:12:21.976: INFO: Got endpoints: latency-svc-k6btv [376.581299ms]
    Mar 27 22:12:21.993: INFO: Created: latency-svc-jjvls
    Mar 27 22:12:21.998: INFO: Got endpoints: latency-svc-jjvls [397.515987ms]
    Mar 27 22:12:22.021: INFO: Created: latency-svc-pfxzl
    Mar 27 22:12:22.026: INFO: Got endpoints: latency-svc-pfxzl [375.593303ms]
    Mar 27 22:12:22.049: INFO: Created: latency-svc-xcpf4
    Mar 27 22:12:22.059: INFO: Got endpoints: latency-svc-xcpf4 [387.425704ms]
    Mar 27 22:12:22.078: INFO: Created: latency-svc-vwzcz
    Mar 27 22:12:22.082: INFO: Got endpoints: latency-svc-vwzcz [388.04675ms]
    Mar 27 22:12:22.122: INFO: Created: latency-svc-4vxwl
    Mar 27 22:12:22.122: INFO: Got endpoints: latency-svc-4vxwl [406.089304ms]
    Mar 27 22:12:22.142: INFO: Created: latency-svc-bjklm
    Mar 27 22:12:22.147: INFO: Got endpoints: latency-svc-bjklm [408.278639ms]
    Mar 27 22:12:22.167: INFO: Created: latency-svc-9n6nx
    Mar 27 22:12:22.173: INFO: Got endpoints: latency-svc-9n6nx [407.247091ms]
    Mar 27 22:12:22.196: INFO: Created: latency-svc-tpllr
    Mar 27 22:12:22.204: INFO: Got endpoints: latency-svc-tpllr [414.622037ms]
    Mar 27 22:12:22.221: INFO: Created: latency-svc-xv278
    Mar 27 22:12:22.227: INFO: Got endpoints: latency-svc-xv278 [410.955048ms]
    Mar 27 22:12:22.643: INFO: Created: latency-svc-sn79r
    Mar 27 22:12:22.644: INFO: Created: latency-svc-9vzwd
    Mar 27 22:12:22.644: INFO: Created: latency-svc-jqqxl
    Mar 27 22:12:22.645: INFO: Created: latency-svc-hzkv5
    Mar 27 22:12:22.645: INFO: Created: latency-svc-mxxwj
    Mar 27 22:12:22.645: INFO: Created: latency-svc-xtd68
    Mar 27 22:12:22.647: INFO: Created: latency-svc-29lqh
    Mar 27 22:12:22.647: INFO: Created: latency-svc-wcfr4
    Mar 27 22:12:22.648: INFO: Created: latency-svc-dwcm8
    Mar 27 22:12:22.648: INFO: Created: latency-svc-8mfg5
    Mar 27 22:12:22.649: INFO: Created: latency-svc-4snzk
    Mar 27 22:12:22.649: INFO: Created: latency-svc-l96qr
    Mar 27 22:12:22.650: INFO: Created: latency-svc-rztbh
    Mar 27 22:12:22.651: INFO: Created: latency-svc-jnq8h
    Mar 27 22:12:22.652: INFO: Created: latency-svc-hv9rr
    Mar 27 22:12:22.656: INFO: Got endpoints: latency-svc-hv9rr [533.420407ms]
    Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-hzkv5 [576.549154ms]
    Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-9vzwd [706.087752ms]
    Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-mxxwj [683.184572ms]
    Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-sn79r [813.891632ms]
    Mar 27 22:12:22.659: INFO: Got endpoints: latency-svc-jqqxl [762.992503ms]
    Mar 27 22:12:22.664: INFO: Got endpoints: latency-svc-29lqh [637.912014ms]
    Mar 27 22:12:22.668: INFO: Got endpoints: latency-svc-xtd68 [795.711871ms]
    Mar 27 22:12:22.669: INFO: Got endpoints: latency-svc-dwcm8 [442.246368ms]
    Mar 27 22:12:22.670: INFO: Got endpoints: latency-svc-l96qr [736.696376ms]
    Mar 27 22:12:22.670: INFO: Got endpoints: latency-svc-wcfr4 [611.060898ms]
    Mar 27 22:12:22.676: INFO: Got endpoints: latency-svc-8mfg5 [472.022226ms]
    Mar 27 22:12:22.676: INFO: Got endpoints: latency-svc-4snzk [677.978384ms]
    Mar 27 22:12:22.677: INFO: Got endpoints: latency-svc-jnq8h [530.035581ms]
    Mar 27 22:12:22.678: INFO: Got endpoints: latency-svc-rztbh [504.076785ms]
    Mar 27 22:12:22.711: INFO: Created: latency-svc-njhl4
    Mar 27 22:12:22.719: INFO: Got endpoints: latency-svc-njhl4 [62.33737ms]
    Mar 27 22:12:22.733: INFO: Created: latency-svc-92sxk
    Mar 27 22:12:22.739: INFO: Got endpoints: latency-svc-92sxk [79.119683ms]
    Mar 27 22:12:22.760: INFO: Created: latency-svc-bp8q7
    Mar 27 22:12:22.765: INFO: Got endpoints: latency-svc-bp8q7 [105.039669ms]
    Mar 27 22:12:22.786: INFO: Created: latency-svc-6bvxk
    Mar 27 22:12:22.791: INFO: Got endpoints: latency-svc-6bvxk [132.230667ms]
    Mar 27 22:12:22.813: INFO: Created: latency-svc-pg5dn
    Mar 27 22:12:22.821: INFO: Got endpoints: latency-svc-pg5dn [157.188128ms]
    Mar 27 22:12:22.836: INFO: Created: latency-svc-p4mzc
    Mar 27 22:12:22.842: INFO: Got endpoints: latency-svc-p4mzc [182.584332ms]
    Mar 27 22:12:22.858: INFO: Created: latency-svc-4xgj6
    Mar 27 22:12:22.864: INFO: Got endpoints: latency-svc-4xgj6 [204.399706ms]
    Mar 27 22:12:22.883: INFO: Created: latency-svc-5cjzd
    Mar 27 22:12:22.888: INFO: Got endpoints: latency-svc-5cjzd [219.557626ms]
    Mar 27 22:12:22.904: INFO: Created: latency-svc-h47f9
    Mar 27 22:12:22.912: INFO: Got endpoints: latency-svc-h47f9 [242.698522ms]
    Mar 27 22:12:22.934: INFO: Created: latency-svc-kjngh
    Mar 27 22:12:22.938: INFO: Got endpoints: latency-svc-kjngh [268.733778ms]
    Mar 27 22:12:22.954: INFO: Created: latency-svc-vrvjh
    Mar 27 22:12:22.963: INFO: Got endpoints: latency-svc-vrvjh [286.823566ms]
    Mar 27 22:12:22.986: INFO: Created: latency-svc-h8d9n
    Mar 27 22:12:22.990: INFO: Got endpoints: latency-svc-h8d9n [320.034148ms]
    Mar 27 22:12:23.009: INFO: Created: latency-svc-lwcmk
    Mar 27 22:12:23.014: INFO: Got endpoints: latency-svc-lwcmk [336.511552ms]
    Mar 27 22:12:23.034: INFO: Created: latency-svc-f87d9
    Mar 27 22:12:23.039: INFO: Got endpoints: latency-svc-f87d9 [362.190585ms]
    Mar 27 22:12:23.077: INFO: Created: latency-svc-5hjnd
    Mar 27 22:12:23.083: INFO: Got endpoints: latency-svc-5hjnd [406.375245ms]
    Mar 27 22:12:23.108: INFO: Created: latency-svc-k7pbw
    Mar 27 22:12:23.112: INFO: Got endpoints: latency-svc-k7pbw [392.428498ms]
    Mar 27 22:12:23.127: INFO: Created: latency-svc-xj7r2
    Mar 27 22:12:23.132: INFO: Got endpoints: latency-svc-xj7r2 [393.105161ms]
    Mar 27 22:12:23.183: INFO: Created: latency-svc-klvw2
    Mar 27 22:12:23.189: INFO: Got endpoints: latency-svc-klvw2 [424.306203ms]
    Mar 27 22:12:23.209: INFO: Created: latency-svc-lzwmn
    Mar 27 22:12:23.215: INFO: Got endpoints: latency-svc-lzwmn [423.781227ms]
    Mar 27 22:12:23.232: INFO: Created: latency-svc-g8ktg
    Mar 27 22:12:23.238: INFO: Got endpoints: latency-svc-g8ktg [416.187707ms]
    Mar 27 22:12:23.254: INFO: Created: latency-svc-k8d4n
    Mar 27 22:12:23.261: INFO: Got endpoints: latency-svc-k8d4n [419.024729ms]
    Mar 27 22:12:23.275: INFO: Created: latency-svc-jqq6h
    Mar 27 22:12:23.282: INFO: Got endpoints: latency-svc-jqq6h [417.593356ms]
    Mar 27 22:12:23.303: INFO: Created: latency-svc-qvwkp
    Mar 27 22:12:23.309: INFO: Got endpoints: latency-svc-qvwkp [420.44616ms]
    Mar 27 22:12:23.321: INFO: Created: latency-svc-gt569
    Mar 27 22:12:23.326: INFO: Got endpoints: latency-svc-gt569 [414.677833ms]
    Mar 27 22:12:23.345: INFO: Created: latency-svc-6bn5z
    Mar 27 22:12:23.350: INFO: Got endpoints: latency-svc-6bn5z [411.003573ms]
    Mar 27 22:12:23.368: INFO: Created: latency-svc-zgnfx
    Mar 27 22:12:23.375: INFO: Got endpoints: latency-svc-zgnfx [412.548392ms]
    Mar 27 22:12:23.390: INFO: Created: latency-svc-wsxvk
    Mar 27 22:12:23.397: INFO: Got endpoints: latency-svc-wsxvk [406.780132ms]
    Mar 27 22:12:23.413: INFO: Created: latency-svc-6zzds
    Mar 27 22:12:23.426: INFO: Got endpoints: latency-svc-6zzds [411.651522ms]
    Mar 27 22:12:23.434: INFO: Created: latency-svc-rrx7w
    Mar 27 22:12:23.440: INFO: Got endpoints: latency-svc-rrx7w [399.414069ms]
    Mar 27 22:12:23.458: INFO: Created: latency-svc-7zccv
    Mar 27 22:12:23.464: INFO: Got endpoints: latency-svc-7zccv [381.444062ms]
    Mar 27 22:12:23.482: INFO: Created: latency-svc-9n9r2
    Mar 27 22:12:23.488: INFO: Got endpoints: latency-svc-9n9r2 [375.916768ms]
    Mar 27 22:12:23.506: INFO: Created: latency-svc-qjg47
    Mar 27 22:12:23.511: INFO: Got endpoints: latency-svc-qjg47 [379.649189ms]
    Mar 27 22:12:23.528: INFO: Created: latency-svc-55kzw
    Mar 27 22:12:23.534: INFO: Got endpoints: latency-svc-55kzw [344.258442ms]
    Mar 27 22:12:23.551: INFO: Created: latency-svc-5m7vm
    Mar 27 22:12:23.559: INFO: Got endpoints: latency-svc-5m7vm [343.411066ms]
    Mar 27 22:12:23.575: INFO: Created: latency-svc-b47s5
    Mar 27 22:12:23.581: INFO: Got endpoints: latency-svc-b47s5 [343.646818ms]
    Mar 27 22:12:23.599: INFO: Created: latency-svc-zhnbp
    Mar 27 22:12:23.604: INFO: Got endpoints: latency-svc-zhnbp [342.991735ms]
    Mar 27 22:12:23.942: INFO: Created: latency-svc-wmqxz
    Mar 27 22:12:23.946: INFO: Created: latency-svc-25gql
    Mar 27 22:12:23.950: INFO: Created: latency-svc-czgpv
    Mar 27 22:12:23.952: INFO: Created: latency-svc-s9kkn
    Mar 27 22:12:23.952: INFO: Created: latency-svc-fvvp8
    Mar 27 22:12:23.952: INFO: Created: latency-svc-9mtb8
    Mar 27 22:12:23.953: INFO: Created: latency-svc-sfnzg
    Mar 27 22:12:23.953: INFO: Created: latency-svc-2pvbs
    Mar 27 22:12:23.953: INFO: Created: latency-svc-sndxn
    Mar 27 22:12:23.952: INFO: Created: latency-svc-qnm6l
    Mar 27 22:12:23.953: INFO: Created: latency-svc-pz6fm
    Mar 27 22:12:23.953: INFO: Got endpoints: latency-svc-wmqxz [603.298658ms]
    Mar 27 22:12:23.953: INFO: Created: latency-svc-m6h2c
    Mar 27 22:12:23.953: INFO: Created: latency-svc-rgwx4
    Mar 27 22:12:23.953: INFO: Created: latency-svc-ggnjr
    Mar 27 22:12:23.954: INFO: Created: latency-svc-kkqm2
    Mar 27 22:12:23.991: INFO: Got endpoints: latency-svc-sndxn [708.970556ms]
    Mar 27 22:12:23.995: INFO: Got endpoints: latency-svc-pz6fm [568.66136ms]
    Mar 27 22:12:23.995: INFO: Got endpoints: latency-svc-s9kkn [686.096529ms]
    Mar 27 22:12:23.998: INFO: Got endpoints: latency-svc-25gql [393.78264ms]
    Mar 27 22:12:23.998: INFO: Got endpoints: latency-svc-qnm6l [416.521715ms]
    Mar 27 22:12:24.002: INFO: Created: latency-svc-rhts5
    Mar 27 22:12:24.009: INFO: Got endpoints: latency-svc-2pvbs [520.872519ms]
    Mar 27 22:12:24.023: INFO: Got endpoints: latency-svc-czgpv [558.350368ms]
    Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-fvvp8 [696.984029ms]
    Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-kkqm2 [626.715371ms]
    Mar 27 22:12:24.024: INFO: Got endpoints: latency-svc-sfnzg [512.146168ms]
    Mar 27 22:12:24.026: INFO: Got endpoints: latency-svc-ggnjr [492.331687ms]
    Mar 27 22:12:24.030: INFO: Got endpoints: latency-svc-rgwx4 [654.695083ms]
    Mar 27 22:12:24.032: INFO: Got endpoints: latency-svc-9mtb8 [473.218066ms]
    Mar 27 22:12:24.032: INFO: Got endpoints: latency-svc-m6h2c [591.8571ms]
    Mar 27 22:12:24.033: INFO: Got endpoints: latency-svc-rhts5 [79.63614ms]
    Mar 27 22:12:24.042: INFO: Created: latency-svc-nppz5
    Mar 27 22:12:24.048: INFO: Got endpoints: latency-svc-nppz5 [56.835384ms]
    Mar 27 22:12:24.404: INFO: Created: latency-svc-8pgtc
    Mar 27 22:12:24.412: INFO: Created: latency-svc-jgfcp
    Mar 27 22:12:24.412: INFO: Created: latency-svc-l8mcc
    Mar 27 22:12:24.414: INFO: Created: latency-svc-9f8dk
    Mar 27 22:12:24.414: INFO: Created: latency-svc-g8dzm
    Mar 27 22:12:24.415: INFO: Created: latency-svc-x5fwk
    Mar 27 22:12:24.415: INFO: Created: latency-svc-8ldx8
    Mar 27 22:12:24.415: INFO: Created: latency-svc-fqncb
    Mar 27 22:12:24.415: INFO: Created: latency-svc-xrmnn
    Mar 27 22:12:24.416: INFO: Created: latency-svc-sfvgm
    Mar 27 22:12:24.416: INFO: Created: latency-svc-gpq4q
    Mar 27 22:12:24.417: INFO: Created: latency-svc-qtc77
    Mar 27 22:12:24.418: INFO: Created: latency-svc-dgpv5
    Mar 27 22:12:24.417: INFO: Created: latency-svc-hb9nc
    Mar 27 22:12:24.417: INFO: Got endpoints: latency-svc-x5fwk [422.166363ms]
    Mar 27 22:12:24.417: INFO: Got endpoints: latency-svc-8pgtc [385.154122ms]
    Mar 27 22:12:24.417: INFO: Created: latency-svc-k2rd2
    Mar 27 22:12:24.419: INFO: Got endpoints: latency-svc-l8mcc [371.532236ms]
    Mar 27 22:12:24.420: INFO: Got endpoints: latency-svc-g8dzm [397.112968ms]
    Mar 27 22:12:24.420: INFO: Got endpoints: latency-svc-9f8dk [395.930071ms]
    Mar 27 22:12:24.421: INFO: Got endpoints: latency-svc-jgfcp [411.830583ms]
    Mar 27 22:12:24.421: INFO: Got endpoints: latency-svc-8ldx8 [389.293255ms]
    Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-gpq4q [397.631946ms]
    Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-dgpv5 [403.675517ms]
    Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-hb9nc [430.575129ms]
    Mar 27 22:12:24.428: INFO: Got endpoints: latency-svc-qtc77 [430.511751ms]
    Mar 27 22:12:24.429: INFO: Got endpoints: latency-svc-sfvgm [433.149999ms]
    Mar 27 22:12:24.435: INFO: Got endpoints: latency-svc-k2rd2 [402.627446ms]
    Mar 27 22:12:24.437: INFO: Got endpoints: latency-svc-xrmnn [410.632666ms]
    Mar 27 22:12:24.437: INFO: Got endpoints: latency-svc-fqncb [413.081887ms]
    Mar 27 22:12:24.437: INFO: Latencies: [48.817786ms 52.130574ms 53.620068ms 56.835384ms 62.33737ms 69.808096ms 75.959429ms 76.350437ms 77.793035ms 79.119683ms 79.63614ms 90.14967ms 100.06448ms 101.647799ms 105.039669ms 113.286835ms 113.557499ms 120.908076ms 124.877532ms 132.230667ms 134.89405ms 138.981021ms 140.875869ms 148.376079ms 156.825258ms 157.188128ms 158.500659ms 160.98778ms 172.769006ms 177.664049ms 179.782244ms 182.584332ms 184.298977ms 186.211213ms 200.835009ms 200.916732ms 202.216617ms 204.399706ms 217.133299ms 219.557626ms 223.506734ms 225.152236ms 226.143184ms 242.698522ms 243.69425ms 252.092388ms 257.161856ms 268.733778ms 271.848706ms 277.770714ms 285.431263ms 286.823566ms 293.48329ms 301.516291ms 311.219917ms 315.940163ms 320.034148ms 334.893834ms 336.511552ms 337.112026ms 337.368305ms 342.991735ms 343.411066ms 343.646818ms 344.258442ms 355.361022ms 360.520564ms 362.190585ms 367.601879ms 371.532236ms 375.593303ms 375.916768ms 376.305501ms 376.581299ms 376.744333ms 378.594587ms 378.853265ms 379.649189ms 379.95732ms 380.808524ms 381.444062ms 382.831236ms 384.976042ms 385.154122ms 387.425704ms 387.801575ms 388.04675ms 388.115018ms 388.677198ms 389.288081ms 389.293255ms 389.830199ms 389.841832ms 390.379785ms 391.334836ms 391.733848ms 392.428498ms 393.105161ms 393.78264ms 394.680096ms 395.930071ms 397.112968ms 397.515987ms 397.631946ms 399.414069ms 402.627446ms 403.675517ms 406.089304ms 406.375245ms 406.780132ms 407.247091ms 408.278639ms 410.632666ms 410.955048ms 411.003573ms 411.651522ms 411.830583ms 412.548392ms 412.629974ms 413.081887ms 414.622037ms 414.677833ms 416.187707ms 416.521715ms 417.593356ms 419.024729ms 420.44616ms 422.166363ms 422.979896ms 423.781227ms 424.306203ms 427.731747ms 430.511751ms 430.575129ms 431.443068ms 433.149999ms 435.63456ms 442.246368ms 453.493865ms 460.492967ms 466.225092ms 469.472729ms 471.053201ms 472.022226ms 473.218066ms 481.758441ms 491.235905ms 492.331687ms 495.221907ms 504.076785ms 512.146168ms 515.608842ms 518.608351ms 520.872519ms 530.035581ms 533.420407ms 533.736189ms 551.653876ms 552.692598ms 557.988649ms 558.350368ms 568.66136ms 574.211697ms 576.549154ms 586.452699ms 586.840873ms 591.8571ms 597.227682ms 603.298658ms 608.639095ms 611.060898ms 612.290805ms 612.418284ms 622.370372ms 624.224864ms 626.275718ms 626.715371ms 629.57974ms 634.575113ms 637.912014ms 647.669248ms 652.75907ms 654.369589ms 654.695083ms 673.276605ms 677.978384ms 683.184572ms 684.296167ms 686.096529ms 696.984029ms 697.830164ms 706.087752ms 708.59894ms 708.970556ms 709.425495ms 732.424536ms 736.696376ms 762.992503ms 795.711871ms 813.891632ms]
    Mar 27 22:12:24.438: INFO: 50 %ile: 395.930071ms
    Mar 27 22:12:24.438: INFO: 90 %ile: 647.669248ms
    Mar 27 22:12:24.438: INFO: 99 %ile: 795.711871ms
    Mar 27 22:12:24.438: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:12:24.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-574" for this suite. 03/27/23 22:12:24.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:12:24.499
Mar 27 22:12:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 22:12:24.501
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:24.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:24.57
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 22:12:24.607
Mar 27 22:12:24.643: INFO: Waiting up to 5m0s for pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7" in namespace "emptydir-1223" to be "Succeeded or Failed"
Mar 27 22:12:24.662: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.980758ms
Mar 27 22:12:26.683: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039545008s
Mar 27 22:12:28.681: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037560581s
STEP: Saw pod success 03/27/23 22:12:28.681
Mar 27 22:12:28.681: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7" satisfied condition "Succeeded or Failed"
Mar 27 22:12:28.697: INFO: Trying to get logs from node 10.176.99.177 pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 container test-container: <nil>
STEP: delete the pod 03/27/23 22:12:28.777
Mar 27 22:12:28.820: INFO: Waiting for pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 to disappear
Mar 27 22:12:28.836: INFO: Pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:12:28.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1223" for this suite. 03/27/23 22:12:28.86
------------------------------
• [4.386 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:12:24.499
    Mar 27 22:12:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 22:12:24.501
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:24.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:24.57
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/27/23 22:12:24.607
    Mar 27 22:12:24.643: INFO: Waiting up to 5m0s for pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7" in namespace "emptydir-1223" to be "Succeeded or Failed"
    Mar 27 22:12:24.662: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.980758ms
    Mar 27 22:12:26.683: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039545008s
    Mar 27 22:12:28.681: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037560581s
    STEP: Saw pod success 03/27/23 22:12:28.681
    Mar 27 22:12:28.681: INFO: Pod "pod-921e4743-e0db-425a-a728-6c1fe63dbfc7" satisfied condition "Succeeded or Failed"
    Mar 27 22:12:28.697: INFO: Trying to get logs from node 10.176.99.177 pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 container test-container: <nil>
    STEP: delete the pod 03/27/23 22:12:28.777
    Mar 27 22:12:28.820: INFO: Waiting for pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 to disappear
    Mar 27 22:12:28.836: INFO: Pod pod-921e4743-e0db-425a-a728-6c1fe63dbfc7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:12:28.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1223" for this suite. 03/27/23 22:12:28.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:12:28.888
Mar 27 22:12:28.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 22:12:28.889
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:28.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:28.962
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-4sthw" 03/27/23 22:12:28.99
Mar 27 22:12:29.021: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard cpu limit of 500m
Mar 27 22:12:29.021: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.021
STEP: Confirm /status for "e2e-rq-status-4sthw" resourceQuota via watch 03/27/23 22:12:29.051
Mar 27 22:12:29.059: INFO: observed resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList(nil)
Mar 27 22:12:29.059: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 27 22:12:29.059: INFO: ResourceQuota "e2e-rq-status-4sthw" /status was updated
STEP: Patching hard spec values for cpu & memory 03/27/23 22:12:29.073
Mar 27 22:12:29.092: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard cpu limit of 1
Mar 27 22:12:29.092: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.092
STEP: Confirm /status for "e2e-rq-status-4sthw" resourceQuota via watch 03/27/23 22:12:29.111
Mar 27 22:12:29.119: INFO: observed resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Mar 27 22:12:29.119: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Mar 27 22:12:29.119: INFO: ResourceQuota "e2e-rq-status-4sthw" /status was patched
STEP: Get "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.119
Mar 27 22:12:29.135: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard cpu of 1
Mar 27 22:12:29.135: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-4sthw" /status before checking Spec is unchanged 03/27/23 22:12:29.149
Mar 27 22:12:29.168: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard cpu of 2
Mar 27 22:12:29.168: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard memory of 2Gi
Mar 27 22:12:29.175: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Mar 27 22:13:29.207: INFO: ResourceQuota "e2e-rq-status-4sthw" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 22:13:29.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4359" for this suite. 03/27/23 22:13:29.229
------------------------------
• [SLOW TEST] [60.367 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:12:28.888
    Mar 27 22:12:28.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 22:12:28.889
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:12:28.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:12:28.962
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-4sthw" 03/27/23 22:12:28.99
    Mar 27 22:12:29.021: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard cpu limit of 500m
    Mar 27 22:12:29.021: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.021
    STEP: Confirm /status for "e2e-rq-status-4sthw" resourceQuota via watch 03/27/23 22:12:29.051
    Mar 27 22:12:29.059: INFO: observed resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList(nil)
    Mar 27 22:12:29.059: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 27 22:12:29.059: INFO: ResourceQuota "e2e-rq-status-4sthw" /status was updated
    STEP: Patching hard spec values for cpu & memory 03/27/23 22:12:29.073
    Mar 27 22:12:29.092: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard cpu limit of 1
    Mar 27 22:12:29.092: INFO: Resource quota "e2e-rq-status-4sthw" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.092
    STEP: Confirm /status for "e2e-rq-status-4sthw" resourceQuota via watch 03/27/23 22:12:29.111
    Mar 27 22:12:29.119: INFO: observed resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Mar 27 22:12:29.119: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Mar 27 22:12:29.119: INFO: ResourceQuota "e2e-rq-status-4sthw" /status was patched
    STEP: Get "e2e-rq-status-4sthw" /status 03/27/23 22:12:29.119
    Mar 27 22:12:29.135: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard cpu of 1
    Mar 27 22:12:29.135: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-4sthw" /status before checking Spec is unchanged 03/27/23 22:12:29.149
    Mar 27 22:12:29.168: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard cpu of 2
    Mar 27 22:12:29.168: INFO: Resourcequota "e2e-rq-status-4sthw" reports status: hard memory of 2Gi
    Mar 27 22:12:29.175: INFO: Found resourceQuota "e2e-rq-status-4sthw" in namespace "resourcequota-4359" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Mar 27 22:13:29.207: INFO: ResourceQuota "e2e-rq-status-4sthw" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:13:29.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4359" for this suite. 03/27/23 22:13:29.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:13:29.257
Mar 27 22:13:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 22:13:29.259
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:29.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:29.326
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 03/27/23 22:13:29.361
Mar 27 22:13:29.400: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7152" to be "running and ready"
Mar 27 22:13:29.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.666938ms
Mar 27 22:13:29.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:13:31.435: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03558636s
Mar 27 22:13:31.435: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:13:33.434: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.034033705s
Mar 27 22:13:33.434: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 27 22:13:33.434: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 03/27/23 22:13:33.45
Mar 27 22:13:33.478: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7152" to be "running and ready"
Mar 27 22:13:33.494: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.875074ms
Mar 27 22:13:33.494: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:13:35.510: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.032146537s
Mar 27 22:13:35.510: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 27 22:13:35.510: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/27/23 22:13:35.527
STEP: delete the pod with lifecycle hook 03/27/23 22:13:35.558
Mar 27 22:13:35.586: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 22:13:35.605: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 27 22:13:37.606: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 22:13:37.625: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 27 22:13:39.606: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 27 22:13:39.626: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Mar 27 22:13:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7152" for this suite. 03/27/23 22:13:39.667
------------------------------
• [SLOW TEST] [10.435 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:13:29.257
    Mar 27 22:13:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/27/23 22:13:29.259
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:29.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:29.326
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 03/27/23 22:13:29.361
    Mar 27 22:13:29.400: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7152" to be "running and ready"
    Mar 27 22:13:29.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.666938ms
    Mar 27 22:13:29.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:13:31.435: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03558636s
    Mar 27 22:13:31.435: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:13:33.434: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.034033705s
    Mar 27 22:13:33.434: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 27 22:13:33.434: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 03/27/23 22:13:33.45
    Mar 27 22:13:33.478: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7152" to be "running and ready"
    Mar 27 22:13:33.494: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.875074ms
    Mar 27 22:13:33.494: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:13:35.510: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.032146537s
    Mar 27 22:13:35.510: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 27 22:13:35.510: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/27/23 22:13:35.527
    STEP: delete the pod with lifecycle hook 03/27/23 22:13:35.558
    Mar 27 22:13:35.586: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 22:13:35.605: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 27 22:13:37.606: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 22:13:37.625: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 27 22:13:39.606: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 27 22:13:39.626: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:13:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7152" for this suite. 03/27/23 22:13:39.667
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:13:39.693
Mar 27 22:13:39.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:13:39.694
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:39.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:39.756
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1090 03/27/23 22:13:39.769
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 22:13:39.826
STEP: creating service externalsvc in namespace services-1090 03/27/23 22:13:39.826
STEP: creating replication controller externalsvc in namespace services-1090 03/27/23 22:13:39.863
I0327 22:13:39.878733      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1090, replica count: 2
I0327 22:13:42.930241      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/27/23 22:13:42.943
Mar 27 22:13:43.232: INFO: Creating new exec pod
Mar 27 22:13:43.263: INFO: Waiting up to 5m0s for pod "execpod97sjx" in namespace "services-1090" to be "running"
Mar 27 22:13:43.281: INFO: Pod "execpod97sjx": Phase="Pending", Reason="", readiness=false. Elapsed: 17.931701ms
Mar 27 22:13:45.300: INFO: Pod "execpod97sjx": Phase="Running", Reason="", readiness=true. Elapsed: 2.036292307s
Mar 27 22:13:45.300: INFO: Pod "execpod97sjx" satisfied condition "running"
Mar 27 22:13:45.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1090 exec execpod97sjx -- /bin/sh -x -c nslookup nodeport-service.services-1090.svc.cluster.local'
Mar 27 22:13:45.632: INFO: stderr: "+ nslookup nodeport-service.services-1090.svc.cluster.local\n"
Mar 27 22:13:45.632: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1090.svc.cluster.local\tcanonical name = externalsvc.services-1090.svc.cluster.local.\nName:\texternalsvc.services-1090.svc.cluster.local\nAddress: 172.21.143.76\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1090, will wait for the garbage collector to delete the pods 03/27/23 22:13:45.632
Mar 27 22:13:45.717: INFO: Deleting ReplicationController externalsvc took: 20.858427ms
Mar 27 22:13:45.818: INFO: Terminating ReplicationController externalsvc pods took: 100.864067ms
Mar 27 22:13:48.306: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:13:48.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1090" for this suite. 03/27/23 22:13:48.367
------------------------------
• [SLOW TEST] [8.698 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:13:39.693
    Mar 27 22:13:39.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:13:39.694
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:39.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:39.756
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1090 03/27/23 22:13:39.769
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 22:13:39.826
    STEP: creating service externalsvc in namespace services-1090 03/27/23 22:13:39.826
    STEP: creating replication controller externalsvc in namespace services-1090 03/27/23 22:13:39.863
    I0327 22:13:39.878733      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1090, replica count: 2
    I0327 22:13:42.930241      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/27/23 22:13:42.943
    Mar 27 22:13:43.232: INFO: Creating new exec pod
    Mar 27 22:13:43.263: INFO: Waiting up to 5m0s for pod "execpod97sjx" in namespace "services-1090" to be "running"
    Mar 27 22:13:43.281: INFO: Pod "execpod97sjx": Phase="Pending", Reason="", readiness=false. Elapsed: 17.931701ms
    Mar 27 22:13:45.300: INFO: Pod "execpod97sjx": Phase="Running", Reason="", readiness=true. Elapsed: 2.036292307s
    Mar 27 22:13:45.300: INFO: Pod "execpod97sjx" satisfied condition "running"
    Mar 27 22:13:45.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-1090 exec execpod97sjx -- /bin/sh -x -c nslookup nodeport-service.services-1090.svc.cluster.local'
    Mar 27 22:13:45.632: INFO: stderr: "+ nslookup nodeport-service.services-1090.svc.cluster.local\n"
    Mar 27 22:13:45.632: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1090.svc.cluster.local\tcanonical name = externalsvc.services-1090.svc.cluster.local.\nName:\texternalsvc.services-1090.svc.cluster.local\nAddress: 172.21.143.76\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1090, will wait for the garbage collector to delete the pods 03/27/23 22:13:45.632
    Mar 27 22:13:45.717: INFO: Deleting ReplicationController externalsvc took: 20.858427ms
    Mar 27 22:13:45.818: INFO: Terminating ReplicationController externalsvc pods took: 100.864067ms
    Mar 27 22:13:48.306: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:13:48.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1090" for this suite. 03/27/23 22:13:48.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:13:48.394
Mar 27 22:13:48.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename pod-network-test 03/27/23 22:13:48.395
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:48.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:48.463
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2998 03/27/23 22:13:48.479
STEP: creating a selector 03/27/23 22:13:48.479
STEP: Creating the service pods in kubernetes 03/27/23 22:13:48.48
Mar 27 22:13:48.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 27 22:13:48.592: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2998" to be "running and ready"
Mar 27 22:13:48.610: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.500604ms
Mar 27 22:13:48.610: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:13:50.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03434791s
Mar 27 22:13:50.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:13:52.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.035537599s
Mar 27 22:13:52.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:13:54.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035067056s
Mar 27 22:13:54.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:13:56.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.038085579s
Mar 27 22:13:56.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:13:58.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.039060257s
Mar 27 22:13:58.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:00.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.035053335s
Mar 27 22:14:00.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:02.630: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.038005461s
Mar 27 22:14:02.630: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:04.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034964466s
Mar 27 22:14:04.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:06.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034484757s
Mar 27 22:14:06.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:08.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04628904s
Mar 27 22:14:08.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 27 22:14:10.630: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.037654123s
Mar 27 22:14:10.630: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 27 22:14:10.630: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 27 22:14:10.646: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2998" to be "running and ready"
Mar 27 22:14:10.662: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.387055ms
Mar 27 22:14:10.662: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 27 22:14:10.662: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 27 22:14:10.678: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2998" to be "running and ready"
Mar 27 22:14:10.694: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 16.323946ms
Mar 27 22:14:10.695: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 27 22:14:10.695: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/27/23 22:14:10.712
Mar 27 22:14:10.754: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2998" to be "running"
Mar 27 22:14:10.777: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.270406ms
Mar 27 22:14:12.795: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040850377s
Mar 27 22:14:12.795: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 27 22:14:12.812: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2998" to be "running"
Mar 27 22:14:12.830: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.018412ms
Mar 27 22:14:12.830: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 27 22:14:12.849: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 27 22:14:12.849: INFO: Going to poll 172.30.56.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 22:14:12.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.56.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:14:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:14:12.866: INFO: ExecWithOptions: Clientset creation
Mar 27 22:14:12.866: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.56.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 22:14:14.016: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 27 22:14:14.016: INFO: Going to poll 172.30.85.131 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 22:14:14.034: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.85.131 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:14:14.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:14:14.035: INFO: ExecWithOptions: Clientset creation
Mar 27 22:14:14.035: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.85.131+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 22:14:15.189: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 27 22:14:15.189: INFO: Going to poll 172.30.4.90 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 27 22:14:15.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.4.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:14:15.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:14:15.207: INFO: ExecWithOptions: Clientset creation
Mar 27 22:14:15.207: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.4.90+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 27 22:14:16.399: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2998" for this suite. 03/27/23 22:14:16.423
------------------------------
• [SLOW TEST] [28.056 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:13:48.394
    Mar 27 22:13:48.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename pod-network-test 03/27/23 22:13:48.395
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:13:48.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:13:48.463
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2998 03/27/23 22:13:48.479
    STEP: creating a selector 03/27/23 22:13:48.479
    STEP: Creating the service pods in kubernetes 03/27/23 22:13:48.48
    Mar 27 22:13:48.480: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 27 22:13:48.592: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2998" to be "running and ready"
    Mar 27 22:13:48.610: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.500604ms
    Mar 27 22:13:48.610: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:13:50.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.03434791s
    Mar 27 22:13:50.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:13:52.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.035537599s
    Mar 27 22:13:52.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:13:54.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035067056s
    Mar 27 22:13:54.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:13:56.631: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.038085579s
    Mar 27 22:13:56.631: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:13:58.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.039060257s
    Mar 27 22:13:58.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:00.628: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.035053335s
    Mar 27 22:14:00.628: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:02.630: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.038005461s
    Mar 27 22:14:02.630: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:04.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.034964466s
    Mar 27 22:14:04.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:06.627: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.034484757s
    Mar 27 22:14:06.627: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:08.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04628904s
    Mar 27 22:14:08.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 27 22:14:10.630: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.037654123s
    Mar 27 22:14:10.630: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 27 22:14:10.630: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 27 22:14:10.646: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2998" to be "running and ready"
    Mar 27 22:14:10.662: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 16.387055ms
    Mar 27 22:14:10.662: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 27 22:14:10.662: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 27 22:14:10.678: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2998" to be "running and ready"
    Mar 27 22:14:10.694: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 16.323946ms
    Mar 27 22:14:10.695: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 27 22:14:10.695: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/27/23 22:14:10.712
    Mar 27 22:14:10.754: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2998" to be "running"
    Mar 27 22:14:10.777: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.270406ms
    Mar 27 22:14:12.795: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.040850377s
    Mar 27 22:14:12.795: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 27 22:14:12.812: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2998" to be "running"
    Mar 27 22:14:12.830: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.018412ms
    Mar 27 22:14:12.830: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 27 22:14:12.849: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 27 22:14:12.849: INFO: Going to poll 172.30.56.108 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 22:14:12.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.56.108 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:14:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:14:12.866: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:14:12.866: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.56.108+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 22:14:14.016: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 27 22:14:14.016: INFO: Going to poll 172.30.85.131 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 22:14:14.034: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.85.131 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:14:14.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:14:14.035: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:14:14.035: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.85.131+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 22:14:15.189: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 27 22:14:15.189: INFO: Going to poll 172.30.4.90 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 27 22:14:15.207: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.4.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2998 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:14:15.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:14:15.207: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:14:15.207: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2998/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.4.90+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 27 22:14:16.399: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:16.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2998" for this suite. 03/27/23 22:14:16.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:16.451
Mar 27 22:14:16.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:14:16.452
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:16.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:16.732
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4616 03/27/23 22:14:16.746
STEP: creating service affinity-nodeport in namespace services-4616 03/27/23 22:14:16.747
STEP: creating replication controller affinity-nodeport in namespace services-4616 03/27/23 22:14:16.802
I0327 22:14:16.820445      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4616, replica count: 3
I0327 22:14:19.871866      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 22:14:19.944: INFO: Creating new exec pod
Mar 27 22:14:19.981: INFO: Waiting up to 5m0s for pod "execpod-affinitygqj7w" in namespace "services-4616" to be "running"
Mar 27 22:14:19.997: INFO: Pod "execpod-affinitygqj7w": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158618ms
Mar 27 22:14:22.015: INFO: Pod "execpod-affinitygqj7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.034002623s
Mar 27 22:14:22.015: INFO: Pod "execpod-affinitygqj7w" satisfied condition "running"
Mar 27 22:14:23.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Mar 27 22:14:23.350: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 27 22:14:23.350: INFO: stdout: ""
Mar 27 22:14:23.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 172.21.146.228 80'
Mar 27 22:14:23.648: INFO: stderr: "+ nc -v -z -w 2 172.21.146.228 80\nConnection to 172.21.146.228 80 port [tcp/http] succeeded!\n"
Mar 27 22:14:23.649: INFO: stdout: ""
Mar 27 22:14:23.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 10.176.99.175 31389'
Mar 27 22:14:23.969: INFO: stderr: "+ nc -v -z -w 2 10.176.99.175 31389\nConnection to 10.176.99.175 31389 port [tcp/*] succeeded!\n"
Mar 27 22:14:23.969: INFO: stdout: ""
Mar 27 22:14:23.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 31389'
Mar 27 22:14:24.215: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 31389\nConnection to 10.176.99.177 31389 port [tcp/*] succeeded!\n"
Mar 27 22:14:24.216: INFO: stdout: ""
Mar 27 22:14:24.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:31389/ ; done'
Mar 27 22:14:24.571: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n"
Mar 27 22:14:24.571: INFO: stdout: "\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7"
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
Mar 27 22:14:24.571: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4616, will wait for the garbage collector to delete the pods 03/27/23 22:14:24.611
Mar 27 22:14:24.696: INFO: Deleting ReplicationController affinity-nodeport took: 20.76397ms
Mar 27 22:14:24.796: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.319324ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:27.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4616" for this suite. 03/27/23 22:14:27.5
------------------------------
• [SLOW TEST] [11.078 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:16.451
    Mar 27 22:14:16.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:14:16.452
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:16.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:16.732
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4616 03/27/23 22:14:16.746
    STEP: creating service affinity-nodeport in namespace services-4616 03/27/23 22:14:16.747
    STEP: creating replication controller affinity-nodeport in namespace services-4616 03/27/23 22:14:16.802
    I0327 22:14:16.820445      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4616, replica count: 3
    I0327 22:14:19.871866      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 22:14:19.944: INFO: Creating new exec pod
    Mar 27 22:14:19.981: INFO: Waiting up to 5m0s for pod "execpod-affinitygqj7w" in namespace "services-4616" to be "running"
    Mar 27 22:14:19.997: INFO: Pod "execpod-affinitygqj7w": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158618ms
    Mar 27 22:14:22.015: INFO: Pod "execpod-affinitygqj7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.034002623s
    Mar 27 22:14:22.015: INFO: Pod "execpod-affinitygqj7w" satisfied condition "running"
    Mar 27 22:14:23.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Mar 27 22:14:23.350: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 27 22:14:23.350: INFO: stdout: ""
    Mar 27 22:14:23.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 172.21.146.228 80'
    Mar 27 22:14:23.648: INFO: stderr: "+ nc -v -z -w 2 172.21.146.228 80\nConnection to 172.21.146.228 80 port [tcp/http] succeeded!\n"
    Mar 27 22:14:23.649: INFO: stdout: ""
    Mar 27 22:14:23.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 10.176.99.175 31389'
    Mar 27 22:14:23.969: INFO: stderr: "+ nc -v -z -w 2 10.176.99.175 31389\nConnection to 10.176.99.175 31389 port [tcp/*] succeeded!\n"
    Mar 27 22:14:23.969: INFO: stdout: ""
    Mar 27 22:14:23.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 31389'
    Mar 27 22:14:24.215: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 31389\nConnection to 10.176.99.177 31389 port [tcp/*] succeeded!\n"
    Mar 27 22:14:24.216: INFO: stdout: ""
    Mar 27 22:14:24.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-4616 exec execpod-affinitygqj7w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:31389/ ; done'
    Mar 27 22:14:24.571: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:31389/\n"
    Mar 27 22:14:24.571: INFO: stdout: "\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7\naffinity-nodeport-4p9z7"
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Received response from host: affinity-nodeport-4p9z7
    Mar 27 22:14:24.571: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4616, will wait for the garbage collector to delete the pods 03/27/23 22:14:24.611
    Mar 27 22:14:24.696: INFO: Deleting ReplicationController affinity-nodeport took: 20.76397ms
    Mar 27 22:14:24.796: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.319324ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:27.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4616" for this suite. 03/27/23 22:14:27.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:27.531
Mar 27 22:14:27.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 22:14:27.532
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:27.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:27.605
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 03/27/23 22:14:27.62
STEP: Ensure pods equal to parallelism count is attached to the job 03/27/23 22:14:27.637
STEP: patching /status 03/27/23 22:14:31.659
STEP: updating /status 03/27/23 22:14:31.684
STEP: get /status 03/27/23 22:14:31.718
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:31.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3958" for this suite. 03/27/23 22:14:31.755
------------------------------
• [4.250 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:27.531
    Mar 27 22:14:27.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 22:14:27.532
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:27.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:27.605
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 03/27/23 22:14:27.62
    STEP: Ensure pods equal to parallelism count is attached to the job 03/27/23 22:14:27.637
    STEP: patching /status 03/27/23 22:14:31.659
    STEP: updating /status 03/27/23 22:14:31.684
    STEP: get /status 03/27/23 22:14:31.718
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:31.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3958" for this suite. 03/27/23 22:14:31.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:31.781
Mar 27 22:14:31.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename job 03/27/23 22:14:31.782
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:31.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:31.855
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 03/27/23 22:14:31.872
STEP: Ensuring job reaches completions 03/27/23 22:14:31.891
STEP: Ensuring pods with index for job exist 03/27/23 22:14:39.909
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5812" for this suite. 03/27/23 22:14:39.958
------------------------------
• [SLOW TEST] [8.205 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:31.781
    Mar 27 22:14:31.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename job 03/27/23 22:14:31.782
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:31.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:31.855
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 03/27/23 22:14:31.872
    STEP: Ensuring job reaches completions 03/27/23 22:14:31.891
    STEP: Ensuring pods with index for job exist 03/27/23 22:14:39.909
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5812" for this suite. 03/27/23 22:14:39.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:39.989
Mar 27 22:14:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename containers 03/27/23 22:14:39.99
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:40.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:40.052
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 03/27/23 22:14:40.067
Mar 27 22:14:40.100: INFO: Waiting up to 5m0s for pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60" in namespace "containers-2721" to be "Succeeded or Failed"
Mar 27 22:14:40.117: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Pending", Reason="", readiness=false. Elapsed: 17.529772ms
Mar 27 22:14:42.138: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038259897s
Mar 27 22:14:44.136: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036484593s
STEP: Saw pod success 03/27/23 22:14:44.136
Mar 27 22:14:44.136: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60" satisfied condition "Succeeded or Failed"
Mar 27 22:14:44.155: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 22:14:44.194
Mar 27 22:14:44.248: INFO: Waiting for pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 to disappear
Mar 27 22:14:44.264: INFO: Pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:44.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2721" for this suite. 03/27/23 22:14:44.289
------------------------------
• [4.332 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:39.989
    Mar 27 22:14:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename containers 03/27/23 22:14:39.99
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:40.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:40.052
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 03/27/23 22:14:40.067
    Mar 27 22:14:40.100: INFO: Waiting up to 5m0s for pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60" in namespace "containers-2721" to be "Succeeded or Failed"
    Mar 27 22:14:40.117: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Pending", Reason="", readiness=false. Elapsed: 17.529772ms
    Mar 27 22:14:42.138: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038259897s
    Mar 27 22:14:44.136: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036484593s
    STEP: Saw pod success 03/27/23 22:14:44.136
    Mar 27 22:14:44.136: INFO: Pod "client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60" satisfied condition "Succeeded or Failed"
    Mar 27 22:14:44.155: INFO: Trying to get logs from node 10.176.99.177 pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 22:14:44.194
    Mar 27 22:14:44.248: INFO: Waiting for pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 to disappear
    Mar 27 22:14:44.264: INFO: Pod client-containers-41eaf5f4-ae6f-43f6-9782-4da79b2bde60 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:44.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2721" for this suite. 03/27/23 22:14:44.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:44.323
Mar 27 22:14:44.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:14:44.324
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:44.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:44.404
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 03/27/23 22:14:44.418
STEP: watching for the ServiceAccount to be added 03/27/23 22:14:44.452
STEP: patching the ServiceAccount 03/27/23 22:14:44.459
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/27/23 22:14:44.478
STEP: deleting the ServiceAccount 03/27/23 22:14:44.494
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:44.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8536" for this suite. 03/27/23 22:14:44.564
------------------------------
• [0.266 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:44.323
    Mar 27 22:14:44.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename svcaccounts 03/27/23 22:14:44.324
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:44.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:44.404
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 03/27/23 22:14:44.418
    STEP: watching for the ServiceAccount to be added 03/27/23 22:14:44.452
    STEP: patching the ServiceAccount 03/27/23 22:14:44.459
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/27/23 22:14:44.478
    STEP: deleting the ServiceAccount 03/27/23 22:14:44.494
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:44.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8536" for this suite. 03/27/23 22:14:44.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:44.591
Mar 27 22:14:44.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 22:14:44.592
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:44.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:44.658
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-6020/configmap-test-71654c42-3ff8-4630-a09b-f7625232204a 03/27/23 22:14:44.673
STEP: Creating a pod to test consume configMaps 03/27/23 22:14:44.692
Mar 27 22:14:44.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7" in namespace "configmap-6020" to be "Succeeded or Failed"
Mar 27 22:14:44.742: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.015416ms
Mar 27 22:14:46.760: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034506613s
Mar 27 22:14:48.766: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040229763s
STEP: Saw pod success 03/27/23 22:14:48.766
Mar 27 22:14:48.766: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7" satisfied condition "Succeeded or Failed"
Mar 27 22:14:48.783: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 container env-test: <nil>
STEP: delete the pod 03/27/23 22:14:48.827
Mar 27 22:14:48.866: INFO: Waiting for pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 to disappear
Mar 27 22:14:48.902: INFO: Pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:14:48.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6020" for this suite. 03/27/23 22:14:48.932
------------------------------
• [4.368 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:44.591
    Mar 27 22:14:44.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 22:14:44.592
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:44.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:44.658
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-6020/configmap-test-71654c42-3ff8-4630-a09b-f7625232204a 03/27/23 22:14:44.673
    STEP: Creating a pod to test consume configMaps 03/27/23 22:14:44.692
    Mar 27 22:14:44.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7" in namespace "configmap-6020" to be "Succeeded or Failed"
    Mar 27 22:14:44.742: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.015416ms
    Mar 27 22:14:46.760: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034506613s
    Mar 27 22:14:48.766: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040229763s
    STEP: Saw pod success 03/27/23 22:14:48.766
    Mar 27 22:14:48.766: INFO: Pod "pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7" satisfied condition "Succeeded or Failed"
    Mar 27 22:14:48.783: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 container env-test: <nil>
    STEP: delete the pod 03/27/23 22:14:48.827
    Mar 27 22:14:48.866: INFO: Waiting for pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 to disappear
    Mar 27 22:14:48.902: INFO: Pod pod-configmaps-565dc252-8d9c-4dc0-9ad0-0eed2efbc1a7 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:14:48.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6020" for this suite. 03/27/23 22:14:48.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:14:48.961
Mar 27 22:14:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 22:14:48.963
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:49.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:49.041
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 22:15:49.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4703" for this suite. 03/27/23 22:15:49.129
------------------------------
• [SLOW TEST] [60.194 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:14:48.961
    Mar 27 22:14:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 22:14:48.963
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:14:49.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:14:49.041
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:15:49.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4703" for this suite. 03/27/23 22:15:49.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:15:49.156
Mar 27 22:15:49.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename events 03/27/23 22:15:49.157
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:49.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:49.228
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/27/23 22:15:49.244
Mar 27 22:15:49.263: INFO: created test-event-1
Mar 27 22:15:49.281: INFO: created test-event-2
Mar 27 22:15:49.301: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/27/23 22:15:49.301
STEP: delete collection of events 03/27/23 22:15:49.318
Mar 27 22:15:49.318: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/27/23 22:15:49.426
Mar 27 22:15:49.426: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Mar 27 22:15:49.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3224" for this suite. 03/27/23 22:15:49.471
------------------------------
• [0.340 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:15:49.156
    Mar 27 22:15:49.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename events 03/27/23 22:15:49.157
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:49.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:49.228
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/27/23 22:15:49.244
    Mar 27 22:15:49.263: INFO: created test-event-1
    Mar 27 22:15:49.281: INFO: created test-event-2
    Mar 27 22:15:49.301: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/27/23 22:15:49.301
    STEP: delete collection of events 03/27/23 22:15:49.318
    Mar 27 22:15:49.318: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/27/23 22:15:49.426
    Mar 27 22:15:49.426: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:15:49.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3224" for this suite. 03/27/23 22:15:49.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:15:49.498
Mar 27 22:15:49.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 22:15:49.5
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:49.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:49.571
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 22:15:49.63
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:15:50.259
STEP: Deploying the webhook pod 03/27/23 22:15:50.288
STEP: Wait for the deployment to be ready 03/27/23 22:15:50.331
Mar 27 22:15:50.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 22:15:52.425
STEP: Verifying the service has paired with the endpoint 03/27/23 22:15:52.523
Mar 27 22:15:53.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Mar 27 22:15:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2012-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 22:15:54.083
STEP: Creating a custom resource while v1 is storage version 03/27/23 22:15:54.165
STEP: Patching Custom Resource Definition to set v2 as storage 03/27/23 22:15:56.339
STEP: Patching the custom resource while v2 is storage version 03/27/23 22:15:56.363
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:15:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8661" for this suite. 03/27/23 22:15:57.313
STEP: Destroying namespace "webhook-8661-markers" for this suite. 03/27/23 22:15:57.337
------------------------------
• [SLOW TEST] [7.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:15:49.498
    Mar 27 22:15:49.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 22:15:49.5
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:49.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:49.571
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 22:15:49.63
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:15:50.259
    STEP: Deploying the webhook pod 03/27/23 22:15:50.288
    STEP: Wait for the deployment to be ready 03/27/23 22:15:50.331
    Mar 27 22:15:50.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 22:15:52.425
    STEP: Verifying the service has paired with the endpoint 03/27/23 22:15:52.523
    Mar 27 22:15:53.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Mar 27 22:15:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2012-crds.webhook.example.com via the AdmissionRegistration API 03/27/23 22:15:54.083
    STEP: Creating a custom resource while v1 is storage version 03/27/23 22:15:54.165
    STEP: Patching Custom Resource Definition to set v2 as storage 03/27/23 22:15:56.339
    STEP: Patching the custom resource while v2 is storage version 03/27/23 22:15:56.363
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:15:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8661" for this suite. 03/27/23 22:15:57.313
    STEP: Destroying namespace "webhook-8661-markers" for this suite. 03/27/23 22:15:57.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:15:57.366
Mar 27 22:15:57.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename replication-controller 03/27/23 22:15:57.368
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:57.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:57.432
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 03/27/23 22:15:57.463
STEP: waiting for RC to be added 03/27/23 22:15:57.481
STEP: waiting for available Replicas 03/27/23 22:15:57.481
STEP: patching ReplicationController 03/27/23 22:15:58.948
STEP: waiting for RC to be modified 03/27/23 22:15:58.973
STEP: patching ReplicationController status 03/27/23 22:15:58.974
STEP: waiting for RC to be modified 03/27/23 22:15:58.992
STEP: waiting for available Replicas 03/27/23 22:15:58.992
STEP: fetching ReplicationController status 03/27/23 22:15:59
STEP: patching ReplicationController scale 03/27/23 22:15:59.014
STEP: waiting for RC to be modified 03/27/23 22:15:59.031
STEP: waiting for ReplicationController's scale to be the max amount 03/27/23 22:15:59.031
STEP: fetching ReplicationController; ensuring that it's patched 03/27/23 22:16:00.591
STEP: updating ReplicationController status 03/27/23 22:16:00.605
STEP: waiting for RC to be modified 03/27/23 22:16:00.622
STEP: listing all ReplicationControllers 03/27/23 22:16:00.622
STEP: checking that ReplicationController has expected values 03/27/23 22:16:00.636
STEP: deleting ReplicationControllers by collection 03/27/23 22:16:00.636
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/27/23 22:16:00.665
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7733" for this suite. 03/27/23 22:16:00.771
------------------------------
• [3.431 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:15:57.366
    Mar 27 22:15:57.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename replication-controller 03/27/23 22:15:57.368
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:15:57.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:15:57.432
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 03/27/23 22:15:57.463
    STEP: waiting for RC to be added 03/27/23 22:15:57.481
    STEP: waiting for available Replicas 03/27/23 22:15:57.481
    STEP: patching ReplicationController 03/27/23 22:15:58.948
    STEP: waiting for RC to be modified 03/27/23 22:15:58.973
    STEP: patching ReplicationController status 03/27/23 22:15:58.974
    STEP: waiting for RC to be modified 03/27/23 22:15:58.992
    STEP: waiting for available Replicas 03/27/23 22:15:58.992
    STEP: fetching ReplicationController status 03/27/23 22:15:59
    STEP: patching ReplicationController scale 03/27/23 22:15:59.014
    STEP: waiting for RC to be modified 03/27/23 22:15:59.031
    STEP: waiting for ReplicationController's scale to be the max amount 03/27/23 22:15:59.031
    STEP: fetching ReplicationController; ensuring that it's patched 03/27/23 22:16:00.591
    STEP: updating ReplicationController status 03/27/23 22:16:00.605
    STEP: waiting for RC to be modified 03/27/23 22:16:00.622
    STEP: listing all ReplicationControllers 03/27/23 22:16:00.622
    STEP: checking that ReplicationController has expected values 03/27/23 22:16:00.636
    STEP: deleting ReplicationControllers by collection 03/27/23 22:16:00.636
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/27/23 22:16:00.665
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7733" for this suite. 03/27/23 22:16:00.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:00.802
Mar 27 22:16:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 22:16:00.804
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:00.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:00.867
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 03/27/23 22:16:00.883
Mar 27 22:16:00.915: INFO: Waiting up to 5m0s for pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960" in namespace "downward-api-710" to be "Succeeded or Failed"
Mar 27 22:16:00.934: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Pending", Reason="", readiness=false. Elapsed: 18.660256ms
Mar 27 22:16:02.956: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040895472s
Mar 27 22:16:04.953: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038267827s
STEP: Saw pod success 03/27/23 22:16:04.953
Mar 27 22:16:04.954: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960" satisfied condition "Succeeded or Failed"
Mar 27 22:16:04.971: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 container dapi-container: <nil>
STEP: delete the pod 03/27/23 22:16:05.006
Mar 27 22:16:05.047: INFO: Waiting for pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 to disappear
Mar 27 22:16:05.062: INFO: Pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:05.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-710" for this suite. 03/27/23 22:16:05.089
------------------------------
• [4.313 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:00.802
    Mar 27 22:16:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 22:16:00.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:00.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:00.867
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 03/27/23 22:16:00.883
    Mar 27 22:16:00.915: INFO: Waiting up to 5m0s for pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960" in namespace "downward-api-710" to be "Succeeded or Failed"
    Mar 27 22:16:00.934: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Pending", Reason="", readiness=false. Elapsed: 18.660256ms
    Mar 27 22:16:02.956: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040895472s
    Mar 27 22:16:04.953: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038267827s
    STEP: Saw pod success 03/27/23 22:16:04.953
    Mar 27 22:16:04.954: INFO: Pod "downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960" satisfied condition "Succeeded or Failed"
    Mar 27 22:16:04.971: INFO: Trying to get logs from node 10.176.99.177 pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 container dapi-container: <nil>
    STEP: delete the pod 03/27/23 22:16:05.006
    Mar 27 22:16:05.047: INFO: Waiting for pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 to disappear
    Mar 27 22:16:05.062: INFO: Pod downward-api-5d4435b3-a50a-47cc-96fb-472b1f931960 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:05.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-710" for this suite. 03/27/23 22:16:05.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:05.118
Mar 27 22:16:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename init-container 03/27/23 22:16:05.119
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:05.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:05.184
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 03/27/23 22:16:05.199
Mar 27 22:16:05.199: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8125" for this suite. 03/27/23 22:16:09.009
------------------------------
• [3.917 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:05.118
    Mar 27 22:16:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename init-container 03/27/23 22:16:05.119
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:05.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:05.184
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 03/27/23 22:16:05.199
    Mar 27 22:16:05.199: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8125" for this suite. 03/27/23 22:16:09.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:09.043
Mar 27 22:16:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubelet-test 03/27/23 22:16:09.045
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:09.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:09.14
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 27 22:16:09.187: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1" in namespace "kubelet-test-7587" to be "running and ready"
Mar 27 22:16:09.206: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.952749ms
Mar 27 22:16:09.206: INFO: The phase of Pod busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:16:11.223: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.036244417s
Mar 27 22:16:11.223: INFO: The phase of Pod busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1 is Running (Ready = true)
Mar 27 22:16:11.224: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7587" for this suite. 03/27/23 22:16:11.296
------------------------------
• [2.278 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:09.043
    Mar 27 22:16:09.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubelet-test 03/27/23 22:16:09.045
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:09.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:09.14
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 27 22:16:09.187: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1" in namespace "kubelet-test-7587" to be "running and ready"
    Mar 27 22:16:09.206: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.952749ms
    Mar 27 22:16:09.206: INFO: The phase of Pod busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:16:11.223: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.036244417s
    Mar 27 22:16:11.223: INFO: The phase of Pod busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1 is Running (Ready = true)
    Mar 27 22:16:11.224: INFO: Pod "busybox-readonly-fs1224d4de-a885-4905-ab17-36a1f9e1e9d1" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7587" for this suite. 03/27/23 22:16:11.296
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:11.324
Mar 27 22:16:11.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 22:16:11.325
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:11.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:11.394
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Mar 27 22:16:11.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-9668 version'
Mar 27 22:16:11.502: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 27 22:16:11.502: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3+IKS\", GitCommit:\"c10226fa544b705dd0c734979364970b768efa19\", GitTreeState:\"clean\", BuildDate:\"2023-03-20T11:35:09Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:11.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9668" for this suite. 03/27/23 22:16:11.525
------------------------------
• [0.227 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:11.324
    Mar 27 22:16:11.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 22:16:11.325
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:11.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:11.394
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Mar 27 22:16:11.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-9668 version'
    Mar 27 22:16:11.502: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 27 22:16:11.502: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3+IKS\", GitCommit:\"c10226fa544b705dd0c734979364970b768efa19\", GitTreeState:\"clean\", BuildDate:\"2023-03-20T11:35:09Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:11.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9668" for this suite. 03/27/23 22:16:11.525
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:11.552
Mar 27 22:16:11.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 22:16:11.553
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:11.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:11.616
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7930 03/27/23 22:16:11.631
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Mar 27 22:16:11.680: INFO: Found 0 stateful pods, waiting for 1
Mar 27 22:16:21.700: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/27/23 22:16:21.729
W0327 22:16:21.761652      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 27 22:16:21.793: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:16:21.793: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Mar 27 22:16:31.811: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:16:31.811: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/27/23 22:16:31.842
STEP: Delete all of the StatefulSets 03/27/23 22:16:31.856
STEP: Verify that StatefulSets have been deleted 03/27/23 22:16:31.883
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 22:16:31.901: INFO: Deleting all statefulset in ns statefulset-7930
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:31.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7930" for this suite. 03/27/23 22:16:31.974
------------------------------
• [SLOW TEST] [20.446 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:11.552
    Mar 27 22:16:11.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 22:16:11.553
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:11.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:11.616
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7930 03/27/23 22:16:11.631
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Mar 27 22:16:11.680: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 22:16:21.700: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/27/23 22:16:21.729
    W0327 22:16:21.761652      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 27 22:16:21.793: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:16:21.793: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 27 22:16:31.811: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:16:31.811: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/27/23 22:16:31.842
    STEP: Delete all of the StatefulSets 03/27/23 22:16:31.856
    STEP: Verify that StatefulSets have been deleted 03/27/23 22:16:31.883
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 22:16:31.901: INFO: Deleting all statefulset in ns statefulset-7930
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:31.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7930" for this suite. 03/27/23 22:16:31.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:31.999
Mar 27 22:16:32.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 22:16:32.001
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:32.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:32.066
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-cf2cc377-98f8-421e-bd43-908e7dd5550e 03/27/23 22:16:32.1
STEP: Creating secret with name s-test-opt-upd-fecea92e-d870-4ee8-afa5-1fcda9766ba2 03/27/23 22:16:32.121
STEP: Creating the pod 03/27/23 22:16:32.138
Mar 27 22:16:32.180: INFO: Waiting up to 5m0s for pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37" in namespace "secrets-8711" to be "running and ready"
Mar 27 22:16:32.197: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 16.626873ms
Mar 27 22:16:32.197: INFO: The phase of Pod pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:16:34.245: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37": Phase="Running", Reason="", readiness=true. Elapsed: 2.065409986s
Mar 27 22:16:34.246: INFO: The phase of Pod pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37 is Running (Ready = true)
Mar 27 22:16:34.246: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-cf2cc377-98f8-421e-bd43-908e7dd5550e 03/27/23 22:16:34.425
STEP: Updating secret s-test-opt-upd-fecea92e-d870-4ee8-afa5-1fcda9766ba2 03/27/23 22:16:34.446
STEP: Creating secret with name s-test-opt-create-616616a9-0ba5-4a64-9df8-3009494b5da0 03/27/23 22:16:34.463
STEP: waiting to observe update in volume 03/27/23 22:16:34.481
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:36.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8711" for this suite. 03/27/23 22:16:36.629
------------------------------
• [4.654 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:31.999
    Mar 27 22:16:32.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 22:16:32.001
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:32.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:32.066
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-cf2cc377-98f8-421e-bd43-908e7dd5550e 03/27/23 22:16:32.1
    STEP: Creating secret with name s-test-opt-upd-fecea92e-d870-4ee8-afa5-1fcda9766ba2 03/27/23 22:16:32.121
    STEP: Creating the pod 03/27/23 22:16:32.138
    Mar 27 22:16:32.180: INFO: Waiting up to 5m0s for pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37" in namespace "secrets-8711" to be "running and ready"
    Mar 27 22:16:32.197: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37": Phase="Pending", Reason="", readiness=false. Elapsed: 16.626873ms
    Mar 27 22:16:32.197: INFO: The phase of Pod pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:16:34.245: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37": Phase="Running", Reason="", readiness=true. Elapsed: 2.065409986s
    Mar 27 22:16:34.246: INFO: The phase of Pod pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37 is Running (Ready = true)
    Mar 27 22:16:34.246: INFO: Pod "pod-secrets-75b02da8-8c04-491f-a558-a7120573bc37" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-cf2cc377-98f8-421e-bd43-908e7dd5550e 03/27/23 22:16:34.425
    STEP: Updating secret s-test-opt-upd-fecea92e-d870-4ee8-afa5-1fcda9766ba2 03/27/23 22:16:34.446
    STEP: Creating secret with name s-test-opt-create-616616a9-0ba5-4a64-9df8-3009494b5da0 03/27/23 22:16:34.463
    STEP: waiting to observe update in volume 03/27/23 22:16:34.481
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:36.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8711" for this suite. 03/27/23 22:16:36.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:36.655
Mar 27 22:16:36.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 22:16:36.657
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:36.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:36.733
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 27 22:16:36.828: INFO: Waiting up to 5m0s for pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263" in namespace "emptydir-wrapper-3541" to be "running and ready"
Mar 27 22:16:36.845: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263": Phase="Pending", Reason="", readiness=false. Elapsed: 17.665362ms
Mar 27 22:16:36.846: INFO: The phase of Pod pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263 is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:16:38.877: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263": Phase="Running", Reason="", readiness=true. Elapsed: 2.048856715s
Mar 27 22:16:38.877: INFO: The phase of Pod pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263 is Running (Ready = true)
Mar 27 22:16:38.877: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/27/23 22:16:38.915
STEP: Cleaning up the configmap 03/27/23 22:16:38.94
STEP: Cleaning up the pod 03/27/23 22:16:38.971
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:39.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3541" for this suite. 03/27/23 22:16:39.069
------------------------------
• [2.444 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:36.655
    Mar 27 22:16:36.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir-wrapper 03/27/23 22:16:36.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:36.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:36.733
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 27 22:16:36.828: INFO: Waiting up to 5m0s for pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263" in namespace "emptydir-wrapper-3541" to be "running and ready"
    Mar 27 22:16:36.845: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263": Phase="Pending", Reason="", readiness=false. Elapsed: 17.665362ms
    Mar 27 22:16:36.846: INFO: The phase of Pod pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263 is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:16:38.877: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263": Phase="Running", Reason="", readiness=true. Elapsed: 2.048856715s
    Mar 27 22:16:38.877: INFO: The phase of Pod pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263 is Running (Ready = true)
    Mar 27 22:16:38.877: INFO: Pod "pod-secrets-8ce02706-cc0c-4400-87d3-75c681173263" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/27/23 22:16:38.915
    STEP: Cleaning up the configmap 03/27/23 22:16:38.94
    STEP: Cleaning up the pod 03/27/23 22:16:38.971
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:39.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3541" for this suite. 03/27/23 22:16:39.069
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:39.104
Mar 27 22:16:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 22:16:39.105
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:39.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:39.179
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9500 03/27/23 22:16:39.195
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-9500 03/27/23 22:16:39.248
Mar 27 22:16:39.291: INFO: Found 0 stateful pods, waiting for 1
Mar 27 22:16:49.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/27/23 22:16:49.34
STEP: Getting /status 03/27/23 22:16:49.359
Mar 27 22:16:49.374: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/27/23 22:16:49.375
Mar 27 22:16:49.407: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/27/23 22:16:49.407
Mar 27 22:16:49.415: INFO: Observed &StatefulSet event: ADDED
Mar 27 22:16:49.415: INFO: Found Statefulset ss in namespace statefulset-9500 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 27 22:16:49.415: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/27/23 22:16:49.415
Mar 27 22:16:49.415: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 27 22:16:49.438: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/27/23 22:16:49.438
Mar 27 22:16:49.445: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 22:16:49.446: INFO: Deleting all statefulset in ns statefulset-9500
Mar 27 22:16:49.471: INFO: Scaling statefulset ss to 0
Mar 27 22:16:59.541: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 22:16:59.555: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:16:59.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9500" for this suite. 03/27/23 22:16:59.634
------------------------------
• [SLOW TEST] [20.553 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:39.104
    Mar 27 22:16:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 22:16:39.105
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:39.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:39.179
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9500 03/27/23 22:16:39.195
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-9500 03/27/23 22:16:39.248
    Mar 27 22:16:39.291: INFO: Found 0 stateful pods, waiting for 1
    Mar 27 22:16:49.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/27/23 22:16:49.34
    STEP: Getting /status 03/27/23 22:16:49.359
    Mar 27 22:16:49.374: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/27/23 22:16:49.375
    Mar 27 22:16:49.407: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/27/23 22:16:49.407
    Mar 27 22:16:49.415: INFO: Observed &StatefulSet event: ADDED
    Mar 27 22:16:49.415: INFO: Found Statefulset ss in namespace statefulset-9500 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 27 22:16:49.415: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/27/23 22:16:49.415
    Mar 27 22:16:49.415: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 27 22:16:49.438: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/27/23 22:16:49.438
    Mar 27 22:16:49.445: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 22:16:49.446: INFO: Deleting all statefulset in ns statefulset-9500
    Mar 27 22:16:49.471: INFO: Scaling statefulset ss to 0
    Mar 27 22:16:59.541: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 22:16:59.555: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:16:59.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9500" for this suite. 03/27/23 22:16:59.634
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:16:59.658
Mar 27 22:16:59.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:16:59.659
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:59.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:59.722
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6981 03/27/23 22:16:59.737
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 22:16:59.779
STEP: creating service externalsvc in namespace services-6981 03/27/23 22:16:59.78
STEP: creating replication controller externalsvc in namespace services-6981 03/27/23 22:16:59.82
I0327 22:16:59.839664      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6981, replica count: 2
I0327 22:17:02.891107      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/27/23 22:17:02.905
Mar 27 22:17:02.965: INFO: Creating new exec pod
Mar 27 22:17:03.008: INFO: Waiting up to 5m0s for pod "execpodfvgsz" in namespace "services-6981" to be "running"
Mar 27 22:17:03.024: INFO: Pod "execpodfvgsz": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032584ms
Mar 27 22:17:05.042: INFO: Pod "execpodfvgsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.033891189s
Mar 27 22:17:05.042: INFO: Pod "execpodfvgsz" satisfied condition "running"
Mar 27 22:17:05.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6981 exec execpodfvgsz -- /bin/sh -x -c nslookup clusterip-service.services-6981.svc.cluster.local'
Mar 27 22:17:05.383: INFO: stderr: "+ nslookup clusterip-service.services-6981.svc.cluster.local\n"
Mar 27 22:17:05.383: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-6981.svc.cluster.local\tcanonical name = externalsvc.services-6981.svc.cluster.local.\nName:\texternalsvc.services-6981.svc.cluster.local\nAddress: 172.21.65.10\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6981, will wait for the garbage collector to delete the pods 03/27/23 22:17:05.383
Mar 27 22:17:05.472: INFO: Deleting ReplicationController externalsvc took: 23.868623ms
Mar 27 22:17:05.573: INFO: Terminating ReplicationController externalsvc pods took: 100.709236ms
Mar 27 22:17:07.940: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:17:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6981" for this suite. 03/27/23 22:17:08.003
------------------------------
• [SLOW TEST] [8.371 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:16:59.658
    Mar 27 22:16:59.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:16:59.659
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:16:59.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:16:59.722
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6981 03/27/23 22:16:59.737
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/27/23 22:16:59.779
    STEP: creating service externalsvc in namespace services-6981 03/27/23 22:16:59.78
    STEP: creating replication controller externalsvc in namespace services-6981 03/27/23 22:16:59.82
    I0327 22:16:59.839664      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6981, replica count: 2
    I0327 22:17:02.891107      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/27/23 22:17:02.905
    Mar 27 22:17:02.965: INFO: Creating new exec pod
    Mar 27 22:17:03.008: INFO: Waiting up to 5m0s for pod "execpodfvgsz" in namespace "services-6981" to be "running"
    Mar 27 22:17:03.024: INFO: Pod "execpodfvgsz": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032584ms
    Mar 27 22:17:05.042: INFO: Pod "execpodfvgsz": Phase="Running", Reason="", readiness=true. Elapsed: 2.033891189s
    Mar 27 22:17:05.042: INFO: Pod "execpodfvgsz" satisfied condition "running"
    Mar 27 22:17:05.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-6981 exec execpodfvgsz -- /bin/sh -x -c nslookup clusterip-service.services-6981.svc.cluster.local'
    Mar 27 22:17:05.383: INFO: stderr: "+ nslookup clusterip-service.services-6981.svc.cluster.local\n"
    Mar 27 22:17:05.383: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-6981.svc.cluster.local\tcanonical name = externalsvc.services-6981.svc.cluster.local.\nName:\texternalsvc.services-6981.svc.cluster.local\nAddress: 172.21.65.10\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6981, will wait for the garbage collector to delete the pods 03/27/23 22:17:05.383
    Mar 27 22:17:05.472: INFO: Deleting ReplicationController externalsvc took: 23.868623ms
    Mar 27 22:17:05.573: INFO: Terminating ReplicationController externalsvc pods took: 100.709236ms
    Mar 27 22:17:07.940: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:17:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6981" for this suite. 03/27/23 22:17:08.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:17:08.034
Mar 27 22:17:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename sched-preemption 03/27/23 22:17:08.035
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:17:08.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:17:08.097
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Mar 27 22:17:08.170: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 27 22:18:08.299: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 03/27/23 22:18:08.317
Mar 27 22:18:08.388: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 27 22:18:08.412: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 27 22:18:08.486: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 27 22:18:08.507: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 27 22:18:08.558: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 27 22:18:08.579: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/27/23 22:18:08.579
Mar 27 22:18:08.580: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:08.610: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 30.121499ms
Mar 27 22:18:10.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.049644139s
Mar 27 22:18:10.630: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 27 22:18:10.630: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.650: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.255941ms
Mar 27 22:18:10.650: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 22:18:10.650: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.666: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.220197ms
Mar 27 22:18:10.667: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 22:18:10.667: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.692: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.658899ms
Mar 27 22:18:10.692: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 22:18:10.692: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.710: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.679879ms
Mar 27 22:18:10.710: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 27 22:18:10.710: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.728: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.230701ms
Mar 27 22:18:10.728: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/27/23 22:18:10.728
Mar 27 22:18:10.748: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9926" to be "running"
Mar 27 22:18:10.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.927672ms
Mar 27 22:18:12.785: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036609227s
Mar 27 22:18:14.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034560707s
Mar 27 22:18:16.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035345357s
Mar 27 22:18:18.783: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.034557545s
Mar 27 22:18:18.783: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:18:18.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9926" for this suite. 03/27/23 22:18:19.129
------------------------------
• [SLOW TEST] [71.120 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:17:08.034
    Mar 27 22:17:08.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename sched-preemption 03/27/23 22:17:08.035
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:17:08.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:17:08.097
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Mar 27 22:17:08.170: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 27 22:18:08.299: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 03/27/23 22:18:08.317
    Mar 27 22:18:08.388: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 27 22:18:08.412: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 27 22:18:08.486: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 27 22:18:08.507: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 27 22:18:08.558: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 27 22:18:08.579: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/27/23 22:18:08.579
    Mar 27 22:18:08.580: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:08.610: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 30.121499ms
    Mar 27 22:18:10.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.049644139s
    Mar 27 22:18:10.630: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 27 22:18:10.630: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.650: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.255941ms
    Mar 27 22:18:10.650: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 22:18:10.650: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.666: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.220197ms
    Mar 27 22:18:10.667: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 22:18:10.667: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.692: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.658899ms
    Mar 27 22:18:10.692: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 22:18:10.692: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.710: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.679879ms
    Mar 27 22:18:10.710: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 27 22:18:10.710: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.728: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.230701ms
    Mar 27 22:18:10.728: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/27/23 22:18:10.728
    Mar 27 22:18:10.748: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9926" to be "running"
    Mar 27 22:18:10.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.927672ms
    Mar 27 22:18:12.785: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036609227s
    Mar 27 22:18:14.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034560707s
    Mar 27 22:18:16.783: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035345357s
    Mar 27 22:18:18.783: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.034557545s
    Mar 27 22:18:18.783: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:18:18.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9926" for this suite. 03/27/23 22:18:19.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:18:19.157
Mar 27 22:18:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 22:18:19.159
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:18:19.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:18:19.223
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 27 22:18:19.238: INFO: Creating deployment "webserver-deployment"
Mar 27 22:18:19.264: INFO: Waiting for observed generation 1
Mar 27 22:18:21.306: INFO: Waiting for all required pods to come up
Mar 27 22:18:21.333: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/27/23 22:18:21.333
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ztzh2" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rhghd" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qbl7w" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bxnr" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dkwwd" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2fp9d" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z4bpx" in namespace "deployment-3670" to be "running"
Mar 27 22:18:21.350: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.330614ms
Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.570113ms
Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr": Phase="Pending", Reason="", readiness=false. Elapsed: 21.459252ms
Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx": Phase="Pending", Reason="", readiness=false. Elapsed: 20.829067ms
Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.605584ms
Mar 27 22:18:21.357: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w": Phase="Pending", Reason="", readiness=false. Elapsed: 22.648873ms
Mar 27 22:18:21.357: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.000716ms
Mar 27 22:18:23.372: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.037747992s
Mar 27 22:18:23.372: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2" satisfied condition "running"
Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd": Phase="Running", Reason="", readiness=true. Elapsed: 2.047636651s
Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd" satisfied condition "running"
Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.046919446s
Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d" satisfied condition "running"
Mar 27 22:18:23.386: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.051447665s
Mar 27 22:18:23.386: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr" satisfied condition "running"
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.052294676s
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w" satisfied condition "running"
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.051302162s
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx" satisfied condition "running"
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.052000222s
Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd" satisfied condition "running"
Mar 27 22:18:23.387: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 27 22:18:23.420: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 27 22:18:23.457: INFO: Updating deployment webserver-deployment
Mar 27 22:18:23.457: INFO: Waiting for observed generation 2
Mar 27 22:18:25.493: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 27 22:18:25.510: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 27 22:18:25.530: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 27 22:18:25.589: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 27 22:18:25.589: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 27 22:18:25.837: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 27 22:18:25.877: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 27 22:18:25.877: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 27 22:18:25.916: INFO: Updating deployment webserver-deployment
Mar 27 22:18:25.916: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 27 22:18:25.982: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 27 22:18:28.052: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 22:18:28.148: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3670  cf596e6b-18b9-4332-b97c-4563fff679d8 50399 3 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005048368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 22:18:25 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-27 22:18:28 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Mar 27 22:18:28.167: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3670  03ae2342-19e0-4f9b-88f5-32b56f4782f1 50224 3 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cf596e6b-18b9-4332-b97c-4563fff679d8 0xc0004319d7 0xc0004319d8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf596e6b-18b9-4332-b97c-4563fff679d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000431dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:18:28.167: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 27 22:18:28.167: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3670  f9bf6375-d3f5-4817-a160-b00105a6e156 50397 3 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cf596e6b-18b9-4332-b97c-4563fff679d8 0xc000431067 0xc000431068}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf596e6b-18b9-4332-b97c-4563fff679d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000431918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:18:28.208: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2fp9d webserver-deployment-7f5969cbc7- deployment-3670  0871eede-8147-4647-9e96-cef93cea1a90 49987 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba49dd6e572bcf6804bbe9246e94b0b045568c66cef82fe8209a3f48303fa4cb cni.projectcalico.org/podIP:172.30.85.136/32 cni.projectcalico.org/podIPs:172.30.85.136/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048767 0xc005048768}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cmbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cmbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.136,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5947193b17b9b8d6fe423b371b0d2a0adb2ad2ebd8eb1569c0eaf747629b4993,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.209: INFO: Pod "webserver-deployment-7f5969cbc7-7gr5j" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7gr5j webserver-deployment-7f5969cbc7- deployment-3670  0e221174-a5b0-4c6f-bf5c-3faf39c56100 49970 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dffae8800eb1884c9fafdc23cc5c201226a52ba1253a5c1b1344ff1fe9e2755f cni.projectcalico.org/podIP:172.30.56.120/32 cni.projectcalico.org/podIPs:172.30.56.120/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048c77 0xc005048c78}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99szk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99szk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.120,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://30d65101f3bbe2bc3785095d2f9187284888042abfb8466ed097da81cbc59068,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.210: INFO: Pod "webserver-deployment-7f5969cbc7-dhgnw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dhgnw webserver-deployment-7f5969cbc7- deployment-3670  dc775022-1e96-4e6f-8b0f-845c7c7d752e 50389 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:17fd0844811758e1b4472d16e3ea3359f413e628ed512cf8ef49662554cdbc31 cni.projectcalico.org/podIP:172.30.85.175/32 cni.projectcalico.org/podIPs:172.30.85.175/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048ed7 0xc005048ed8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6npn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6npn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.210: INFO: Pod "webserver-deployment-7f5969cbc7-dtcj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dtcj8 webserver-deployment-7f5969cbc7- deployment-3670  d9671f2f-febd-41a0-bd37-e7ca123bdc82 50361 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3220c7f5272773576d013c206db7b188cee9d7423598423e685aac720f8f271b cni.projectcalico.org/podIP:172.30.4.103/32 cni.projectcalico.org/podIPs:172.30.4.103/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049117 0xc005049118}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb6dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb6dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.211: INFO: Pod "webserver-deployment-7f5969cbc7-fcv8h" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fcv8h webserver-deployment-7f5969cbc7- deployment-3670  4393f6d2-52c7-4058-b5ea-dc1b199a2807 50273 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049327 0xc005049328}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8mt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8mt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.211: INFO: Pod "webserver-deployment-7f5969cbc7-jjplb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jjplb webserver-deployment-7f5969cbc7- deployment-3670  f9ae9658-58e1-487b-9a87-5e5f2a338bc1 49977 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:72875614e71ac22416217221bd0f088ed3f24c1c56f31244ce7bb14a4f50f92d cni.projectcalico.org/podIP:172.30.4.81/32 cni.projectcalico.org/podIPs:172.30.4.81/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049557 0xc005049558}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcczg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcczg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.81,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dc25e86eeca7ee18b69a78926d1de49b97093dac1b2a8118484257eefb89b54c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.212: INFO: Pod "webserver-deployment-7f5969cbc7-p5qhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p5qhc webserver-deployment-7f5969cbc7- deployment-3670  8f51b33b-9578-40af-8c4b-2d6a003778d9 50339 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6480271fba2b77e5bcb63513630695a018aa3359fd4d55d504931e45b36dca4b cni.projectcalico.org/podIP:172.30.56.93/32 cni.projectcalico.org/podIPs:172.30.56.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0050497f0 0xc0050497f1}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9tsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9tsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.212: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qbl7w webserver-deployment-7f5969cbc7- deployment-3670  b32062c0-901e-48bd-9a2f-c3fc05b5041f 49995 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1be28af2762b238d09f13fa67411b713f79bc8d427c7f85990c8a46f14327c2e cni.projectcalico.org/podIP:172.30.85.145/32 cni.projectcalico.org/podIPs:172.30.85.145/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049a17 0xc005049a18}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l64js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l64js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.145,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3c564f703d3004cd21cdb24695fadce45053baa73c202e774bda257dd5e1e284,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.213: INFO: Pod "webserver-deployment-7f5969cbc7-rc5wc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rc5wc webserver-deployment-7f5969cbc7- deployment-3670  634a4b8e-d4b1-459d-85a2-8b79b276fe72 50358 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:93409301ebb44af5af81d016d2cd7a01eba938ee92efb40690be410dcceef40b cni.projectcalico.org/podIP:172.30.85.172/32 cni.projectcalico.org/podIPs:172.30.85.172/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049c57 0xc005049c58}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvbql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvbql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.213: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rhghd webserver-deployment-7f5969cbc7- deployment-3670  ff6e654b-0ee7-4dc2-b247-5e09f0fab29b 50000 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d90723ca29df18594037315be953e12c663f38b102e9d1e4a8f183d51d04f2a cni.projectcalico.org/podIP:172.30.56.89/32 cni.projectcalico.org/podIPs:172.30.56.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049e67 0xc005049e68}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5p28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5p28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.89,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9138981cb70edac37339ed0d127949fe23126c034ff38fbe1cc39b535aba7708,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.214: INFO: Pod "webserver-deployment-7f5969cbc7-rpp7d" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rpp7d webserver-deployment-7f5969cbc7- deployment-3670  58706424-1944-45cc-88c8-3918eeea2ee6 50283 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8087 0xc0044e8088}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fmdzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fmdzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.214: INFO: Pod "webserver-deployment-7f5969cbc7-s9r5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s9r5k webserver-deployment-7f5969cbc7- deployment-3670  bf17a075-613e-40f8-8d38-b923087e3c7d 50394 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6f79f1c52b66e59bffa79dc7f90e977956660cfec433102f59bcb686c8208cc4 cni.projectcalico.org/podIP:172.30.4.111/32 cni.projectcalico.org/podIPs:172.30.4.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8287 0xc0044e8288}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tb69v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.215: INFO: Pod "webserver-deployment-7f5969cbc7-shf9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shf9l webserver-deployment-7f5969cbc7- deployment-3670  0153230a-70e6-4ec6-a0d3-b8469e41ef84 50325 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7b6ff378c7cb84011f34996df098e533e51c4d882ccec3ac51cbb2c887d5451b cni.projectcalico.org/podIP:172.30.85.169/32 cni.projectcalico.org/podIPs:172.30.85.169/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e84b7 0xc0044e84b8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnzmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnzmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.215: INFO: Pod "webserver-deployment-7f5969cbc7-shhdm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shhdm webserver-deployment-7f5969cbc7- deployment-3670  374fb0ba-edd7-40b7-9a77-b5721b86b82c 50338 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:34911aac612f5c982dd877297967e2b44216f09e81205d237a0ef83a2e9e1afb cni.projectcalico.org/podIP:172.30.85.157/32 cni.projectcalico.org/podIPs:172.30.85.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e86f7 0xc0044e86f8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dcjj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dcjj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.216: INFO: Pod "webserver-deployment-7f5969cbc7-vlq8c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vlq8c webserver-deployment-7f5969cbc7- deployment-3670  60371ec3-d693-49ec-8567-fb80f8131cc6 50326 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1760d983b96d3978880fc2f922a7d0106c0f80acc984d9b7389ef80d6ec5bee cni.projectcalico.org/podIP:172.30.4.98/32 cni.projectcalico.org/podIPs:172.30.4.98/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8907 0xc0044e8908}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhmgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhmgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.217: INFO: Pod "webserver-deployment-7f5969cbc7-wgfsg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wgfsg webserver-deployment-7f5969cbc7- deployment-3670  7ad20518-adc6-4f80-a702-0c68778b8356 50402 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4d44f746cd20b29d7be562442db41a4d65bc484f0c7f30ceb4db1b3ad666ff0b cni.projectcalico.org/podIP:172.30.56.94/32 cni.projectcalico.org/podIPs:172.30.56.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8b27 0xc0044e8b28}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6kzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6kzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.94,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://80349be1b087fa961dce023b051b1041534ba6d5861fc3c4b9cb052e1e31bbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.218: INFO: Pod "webserver-deployment-7f5969cbc7-wj8xq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wj8xq webserver-deployment-7f5969cbc7- deployment-3670  24085b3b-fa93-4bc5-8a9e-6c250f988a24 50395 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:556b73a87396c1646b821ab9850df461ba3a61c270d53ab323a11446c61cfaab cni.projectcalico.org/podIP:172.30.56.126/32 cni.projectcalico.org/podIPs:172.30.56.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8d57 0xc0044e8d58}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdffq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdffq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.126,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c1fa9a41af8b2982dcaf62c24ebaa3792348ca6bf96821bd9d85d19a2f48e767,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.218: INFO: Pod "webserver-deployment-7f5969cbc7-wmfpd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wmfpd webserver-deployment-7f5969cbc7- deployment-3670  92583f7e-af6e-4e60-b3e3-5afbb9bc895b 49974 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5b8e57c3cfbb57688efe314d7eb9026b7a265801f9427b022568b85ce1ad5d7a cni.projectcalico.org/podIP:172.30.56.119/32 cni.projectcalico.org/podIPs:172.30.56.119/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8f87 0xc0044e8f88}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdpnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdpnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.119,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3887c4b9842927e09ce3e8378e146ff5d10f646694fd4aa42540c43f0ca440f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.219: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z4bpx webserver-deployment-7f5969cbc7- deployment-3670  0049a24d-7cb6-42d0-9f17-c615ca4cc9ee 49985 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1c29b06e149eff7f741a38d56668d9d509ba075dc3fd5ff127450399da28fde0 cni.projectcalico.org/podIP:172.30.4.93/32 cni.projectcalico.org/podIPs:172.30.4.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e91b7 0xc0044e91b8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkfzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkfzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.93,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff4d2c832f317fb75e1d86d25433631e6f146f26a8b0a774efb13f2cfc2c788c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.219: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ztzh2 webserver-deployment-7f5969cbc7- deployment-3670  05782e5a-1073-4c7d-9400-81e3403ec8e0 49980 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6e57de5c8decd40df1c0a373c963e33fac20e2a52aae8f7440bfaddf5737e9a3 cni.projectcalico.org/podIP:172.30.4.96/32 cni.projectcalico.org/podIPs:172.30.4.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e93e0 0xc0044e93e1}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-547pp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-547pp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.96,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://739f6a5edc76c5182476113e25346c77dd114dbc06ba3a8e9e90c8c3c2c4565a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.220: INFO: Pod "webserver-deployment-d9f79cb5-48vt5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-48vt5 webserver-deployment-d9f79cb5- deployment-3670  92acef13-f9de-4b04-8072-19076563b4e7 50298 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7da3987b9b4f835250b13a22d0cd223c63646d079b932301a31846ddf2162bd7 cni.projectcalico.org/podIP:172.30.4.97/32 cni.projectcalico.org/podIPs:172.30.4.97/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0044e95cf 0xc0044e9600}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdzsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdzsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.97,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.220: INFO: Pod "webserver-deployment-d9f79cb5-4xpd5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4xpd5 webserver-deployment-d9f79cb5- deployment-3670  2ab964d2-4380-44a6-be5d-d66a50718d83 50341 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c14dd1d88d5b68bbc420fe9d2e973bdd2f1cede653bc445f28a2ae061f6bbf4e cni.projectcalico.org/podIP:172.30.4.99/32 cni.projectcalico.org/podIPs:172.30.4.99/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0044e986f 0xc0044e98f0}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cfr4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfr4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.221: INFO: Pod "webserver-deployment-d9f79cb5-682q4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-682q4 webserver-deployment-d9f79cb5- deployment-3670  9d7d3c62-1cac-4942-b477-ba3e9511ba68 50377 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1e8d69db8cc36513868bf14efde25cbe5e33e8e2fdc195911af665c27f239346 cni.projectcalico.org/podIP:172.30.4.107/32 cni.projectcalico.org/podIPs:172.30.4.107/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be127 0xc0045be128}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thtp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thtp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.222: INFO: Pod "webserver-deployment-d9f79cb5-98gwn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-98gwn webserver-deployment-d9f79cb5- deployment-3670  a0a06a1f-4dc0-4744-b007-41939f866191 50299 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2b4b65b516369b40a0034b382c0f2b23e4a0056ec82d94388983e71e531f7bde cni.projectcalico.org/podIP:172.30.85.179/32 cni.projectcalico.org/podIPs:172.30.85.179/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be527 0xc0045be528}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8thql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8thql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.179,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.226: INFO: Pod "webserver-deployment-d9f79cb5-dkd4q" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dkd4q webserver-deployment-d9f79cb5- deployment-3670  a96230c4-eee0-4d67-8e51-a90d28bff0a8 50259 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:dd2374a71aadb77753959cccbddca299295affbbd2a0ff138df13076293d2bf2 cni.projectcalico.org/podIP:172.30.56.76/32 cni.projectcalico.org/podIPs:172.30.56.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be787 0xc0045be788}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j692h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j692h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.76,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.227: INFO: Pod "webserver-deployment-d9f79cb5-f9vhg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f9vhg webserver-deployment-d9f79cb5- deployment-3670  9b338637-3572-419f-b2cc-2ce1cb5aff9f 50290 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be9c7 0xc0045be9c8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qct7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qct7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.227: INFO: Pod "webserver-deployment-d9f79cb5-lz9t5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lz9t5 webserver-deployment-d9f79cb5- deployment-3670  78dbf3af-c3fc-47bf-9087-9b987858dacc 50304 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e64e40f08e8c02c3d7a529958e693ab053f31573e4f883b7668ab96cb1a20b9 cni.projectcalico.org/podIP:172.30.85.171/32 cni.projectcalico.org/podIPs:172.30.85.171/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bebe7 0xc0045bebe8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tftn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tftn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.171,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-m224k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m224k webserver-deployment-d9f79cb5- deployment-3670  2280622f-0387-4efd-b349-5685c8f374b5 50381 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:02d153d6962e551cda0ca78e88a66dbf342d07a1b0a8332cc585499f6620636d cni.projectcalico.org/podIP:172.30.56.69/32 cni.projectcalico.org/podIPs:172.30.56.69/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bee47 0xc0045bee48}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26wgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26wgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-qrjzd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qrjzd webserver-deployment-d9f79cb5- deployment-3670  2331e331-3d33-4db1-b033-6c740a1b8cc6 50312 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4f915cc3ea4191a6cce70d75387b58396a36c9af2006c6e4310e96f1ca3293ab cni.projectcalico.org/podIP:172.30.85.170/32 cni.projectcalico.org/podIPs:172.30.85.170/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf077 0xc0045bf078}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tplqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tplqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-qxp5k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qxp5k webserver-deployment-d9f79cb5- deployment-3670  b54edbe8-1c7f-45bb-a2e5-5ddc4981cdb7 50375 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:99510827b02416d02b412ccb1c84fd6d26a87a02d48c57ca484ced9f2b3427d1 cni.projectcalico.org/podIP:172.30.85.167/32 cni.projectcalico.org/podIPs:172.30.85.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf2a7 0xc0045bf2a8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-sbjlx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sbjlx webserver-deployment-d9f79cb5- deployment-3670  a912390f-fe14-43a2-880a-824bab84df67 50264 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:43cae989773991cd15203c272a399ddcb916a5095d3bfb6e27b3c0ee0509d0e9 cni.projectcalico.org/podIP:172.30.56.96/32 cni.projectcalico.org/podIPs:172.30.56.96/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf507 0xc0045bf508}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78xn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78xn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.96,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-v88bw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v88bw webserver-deployment-d9f79cb5- deployment-3670  6971a090-28cf-482b-a14c-018cec12fe57 50362 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2b0592ddf4beaf975cb2f7954656cf81439653db9be8ef1b9e50ed6dd4a311e4 cni.projectcalico.org/podIP:172.30.56.78/32 cni.projectcalico.org/podIPs:172.30.56.78/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf767 0xc0045bf768}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-674wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-674wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-x7fpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x7fpn webserver-deployment-d9f79cb5- deployment-3670  8ddfb1a8-adcd-4e1a-a671-a6208907c040 50313 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:83d030b748de5bff3e0988e67faa84e4f79451b58be4bdb472205fa43501ccea cni.projectcalico.org/podIP:172.30.4.95/32 cni.projectcalico.org/podIPs:172.30.4.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf9a7 0xc0045bf9a8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4j7mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4j7mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 22:18:28.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3670" for this suite. 03/27/23 22:18:28.253
------------------------------
• [SLOW TEST] [9.121 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:18:19.157
    Mar 27 22:18:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 22:18:19.159
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:18:19.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:18:19.223
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 27 22:18:19.238: INFO: Creating deployment "webserver-deployment"
    Mar 27 22:18:19.264: INFO: Waiting for observed generation 1
    Mar 27 22:18:21.306: INFO: Waiting for all required pods to come up
    Mar 27 22:18:21.333: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/27/23 22:18:21.333
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ztzh2" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rhghd" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qbl7w" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9bxnr" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dkwwd" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2fp9d" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.334: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z4bpx" in namespace "deployment-3670" to be "running"
    Mar 27 22:18:21.350: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.330614ms
    Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.570113ms
    Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr": Phase="Pending", Reason="", readiness=false. Elapsed: 21.459252ms
    Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx": Phase="Pending", Reason="", readiness=false. Elapsed: 20.829067ms
    Mar 27 22:18:21.356: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.605584ms
    Mar 27 22:18:21.357: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w": Phase="Pending", Reason="", readiness=false. Elapsed: 22.648873ms
    Mar 27 22:18:21.357: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.000716ms
    Mar 27 22:18:23.372: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.037747992s
    Mar 27 22:18:23.372: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2" satisfied condition "running"
    Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd": Phase="Running", Reason="", readiness=true. Elapsed: 2.047636651s
    Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd" satisfied condition "running"
    Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.046919446s
    Mar 27 22:18:23.382: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d" satisfied condition "running"
    Mar 27 22:18:23.386: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.051447665s
    Mar 27 22:18:23.386: INFO: Pod "webserver-deployment-7f5969cbc7-9bxnr" satisfied condition "running"
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w": Phase="Running", Reason="", readiness=true. Elapsed: 2.052294676s
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w" satisfied condition "running"
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.051302162s
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx" satisfied condition "running"
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.052000222s
    Mar 27 22:18:23.387: INFO: Pod "webserver-deployment-7f5969cbc7-dkwwd" satisfied condition "running"
    Mar 27 22:18:23.387: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 27 22:18:23.420: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 27 22:18:23.457: INFO: Updating deployment webserver-deployment
    Mar 27 22:18:23.457: INFO: Waiting for observed generation 2
    Mar 27 22:18:25.493: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 27 22:18:25.510: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 27 22:18:25.530: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 22:18:25.589: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 27 22:18:25.589: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 27 22:18:25.837: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 22:18:25.877: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 27 22:18:25.877: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 27 22:18:25.916: INFO: Updating deployment webserver-deployment
    Mar 27 22:18:25.916: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 27 22:18:25.982: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 27 22:18:28.052: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 22:18:28.148: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3670  cf596e6b-18b9-4332-b97c-4563fff679d8 50399 3 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005048368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 22:18:25 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-03-27 22:18:28 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

    Mar 27 22:18:28.167: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3670  03ae2342-19e0-4f9b-88f5-32b56f4782f1 50224 3 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cf596e6b-18b9-4332-b97c-4563fff679d8 0xc0004319d7 0xc0004319d8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf596e6b-18b9-4332-b97c-4563fff679d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000431dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:18:28.167: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 27 22:18:28.167: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3670  f9bf6375-d3f5-4817-a160-b00105a6e156 50397 3 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cf596e6b-18b9-4332-b97c-4563fff679d8 0xc000431067 0xc000431068}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf596e6b-18b9-4332-b97c-4563fff679d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000431918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:18:28.208: INFO: Pod "webserver-deployment-7f5969cbc7-2fp9d" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2fp9d webserver-deployment-7f5969cbc7- deployment-3670  0871eede-8147-4647-9e96-cef93cea1a90 49987 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ba49dd6e572bcf6804bbe9246e94b0b045568c66cef82fe8209a3f48303fa4cb cni.projectcalico.org/podIP:172.30.85.136/32 cni.projectcalico.org/podIPs:172.30.85.136/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048767 0xc005048768}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cmbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cmbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.136,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5947193b17b9b8d6fe423b371b0d2a0adb2ad2ebd8eb1569c0eaf747629b4993,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.209: INFO: Pod "webserver-deployment-7f5969cbc7-7gr5j" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7gr5j webserver-deployment-7f5969cbc7- deployment-3670  0e221174-a5b0-4c6f-bf5c-3faf39c56100 49970 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:dffae8800eb1884c9fafdc23cc5c201226a52ba1253a5c1b1344ff1fe9e2755f cni.projectcalico.org/podIP:172.30.56.120/32 cni.projectcalico.org/podIPs:172.30.56.120/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048c77 0xc005048c78}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99szk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99szk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.120,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://30d65101f3bbe2bc3785095d2f9187284888042abfb8466ed097da81cbc59068,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.210: INFO: Pod "webserver-deployment-7f5969cbc7-dhgnw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dhgnw webserver-deployment-7f5969cbc7- deployment-3670  dc775022-1e96-4e6f-8b0f-845c7c7d752e 50389 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:17fd0844811758e1b4472d16e3ea3359f413e628ed512cf8ef49662554cdbc31 cni.projectcalico.org/podIP:172.30.85.175/32 cni.projectcalico.org/podIPs:172.30.85.175/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005048ed7 0xc005048ed8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6npn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6npn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.210: INFO: Pod "webserver-deployment-7f5969cbc7-dtcj8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dtcj8 webserver-deployment-7f5969cbc7- deployment-3670  d9671f2f-febd-41a0-bd37-e7ca123bdc82 50361 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3220c7f5272773576d013c206db7b188cee9d7423598423e685aac720f8f271b cni.projectcalico.org/podIP:172.30.4.103/32 cni.projectcalico.org/podIPs:172.30.4.103/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049117 0xc005049118}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb6dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb6dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.211: INFO: Pod "webserver-deployment-7f5969cbc7-fcv8h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fcv8h webserver-deployment-7f5969cbc7- deployment-3670  4393f6d2-52c7-4058-b5ea-dc1b199a2807 50273 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049327 0xc005049328}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8mt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8mt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.211: INFO: Pod "webserver-deployment-7f5969cbc7-jjplb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jjplb webserver-deployment-7f5969cbc7- deployment-3670  f9ae9658-58e1-487b-9a87-5e5f2a338bc1 49977 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:72875614e71ac22416217221bd0f088ed3f24c1c56f31244ce7bb14a4f50f92d cni.projectcalico.org/podIP:172.30.4.81/32 cni.projectcalico.org/podIPs:172.30.4.81/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049557 0xc005049558}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcczg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcczg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.81,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dc25e86eeca7ee18b69a78926d1de49b97093dac1b2a8118484257eefb89b54c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.212: INFO: Pod "webserver-deployment-7f5969cbc7-p5qhc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p5qhc webserver-deployment-7f5969cbc7- deployment-3670  8f51b33b-9578-40af-8c4b-2d6a003778d9 50339 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6480271fba2b77e5bcb63513630695a018aa3359fd4d55d504931e45b36dca4b cni.projectcalico.org/podIP:172.30.56.93/32 cni.projectcalico.org/podIPs:172.30.56.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0050497f0 0xc0050497f1}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x9tsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x9tsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.212: INFO: Pod "webserver-deployment-7f5969cbc7-qbl7w" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qbl7w webserver-deployment-7f5969cbc7- deployment-3670  b32062c0-901e-48bd-9a2f-c3fc05b5041f 49995 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1be28af2762b238d09f13fa67411b713f79bc8d427c7f85990c8a46f14327c2e cni.projectcalico.org/podIP:172.30.85.145/32 cni.projectcalico.org/podIPs:172.30.85.145/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049a17 0xc005049a18}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l64js,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l64js,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.145,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3c564f703d3004cd21cdb24695fadce45053baa73c202e774bda257dd5e1e284,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.213: INFO: Pod "webserver-deployment-7f5969cbc7-rc5wc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rc5wc webserver-deployment-7f5969cbc7- deployment-3670  634a4b8e-d4b1-459d-85a2-8b79b276fe72 50358 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:93409301ebb44af5af81d016d2cd7a01eba938ee92efb40690be410dcceef40b cni.projectcalico.org/podIP:172.30.85.172/32 cni.projectcalico.org/podIPs:172.30.85.172/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049c57 0xc005049c58}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvbql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvbql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.213: INFO: Pod "webserver-deployment-7f5969cbc7-rhghd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rhghd webserver-deployment-7f5969cbc7- deployment-3670  ff6e654b-0ee7-4dc2-b247-5e09f0fab29b 50000 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2d90723ca29df18594037315be953e12c663f38b102e9d1e4a8f183d51d04f2a cni.projectcalico.org/podIP:172.30.56.89/32 cni.projectcalico.org/podIPs:172.30.56.89/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc005049e67 0xc005049e68}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5p28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5p28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.89,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9138981cb70edac37339ed0d127949fe23126c034ff38fbe1cc39b535aba7708,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.214: INFO: Pod "webserver-deployment-7f5969cbc7-rpp7d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rpp7d webserver-deployment-7f5969cbc7- deployment-3670  58706424-1944-45cc-88c8-3918eeea2ee6 50283 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8087 0xc0044e8088}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fmdzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fmdzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.214: INFO: Pod "webserver-deployment-7f5969cbc7-s9r5k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-s9r5k webserver-deployment-7f5969cbc7- deployment-3670  bf17a075-613e-40f8-8d38-b923087e3c7d 50394 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6f79f1c52b66e59bffa79dc7f90e977956660cfec433102f59bcb686c8208cc4 cni.projectcalico.org/podIP:172.30.4.111/32 cni.projectcalico.org/podIPs:172.30.4.111/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8287 0xc0044e8288}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tb69v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tb69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.215: INFO: Pod "webserver-deployment-7f5969cbc7-shf9l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shf9l webserver-deployment-7f5969cbc7- deployment-3670  0153230a-70e6-4ec6-a0d3-b8469e41ef84 50325 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7b6ff378c7cb84011f34996df098e533e51c4d882ccec3ac51cbb2c887d5451b cni.projectcalico.org/podIP:172.30.85.169/32 cni.projectcalico.org/podIPs:172.30.85.169/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e84b7 0xc0044e84b8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnzmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnzmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.215: INFO: Pod "webserver-deployment-7f5969cbc7-shhdm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shhdm webserver-deployment-7f5969cbc7- deployment-3670  374fb0ba-edd7-40b7-9a77-b5721b86b82c 50338 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:34911aac612f5c982dd877297967e2b44216f09e81205d237a0ef83a2e9e1afb cni.projectcalico.org/podIP:172.30.85.157/32 cni.projectcalico.org/podIPs:172.30.85.157/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e86f7 0xc0044e86f8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dcjj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dcjj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.216: INFO: Pod "webserver-deployment-7f5969cbc7-vlq8c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vlq8c webserver-deployment-7f5969cbc7- deployment-3670  60371ec3-d693-49ec-8567-fb80f8131cc6 50326 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1760d983b96d3978880fc2f922a7d0106c0f80acc984d9b7389ef80d6ec5bee cni.projectcalico.org/podIP:172.30.4.98/32 cni.projectcalico.org/podIPs:172.30.4.98/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8907 0xc0044e8908}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhmgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhmgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.217: INFO: Pod "webserver-deployment-7f5969cbc7-wgfsg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wgfsg webserver-deployment-7f5969cbc7- deployment-3670  7ad20518-adc6-4f80-a702-0c68778b8356 50402 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:4d44f746cd20b29d7be562442db41a4d65bc484f0c7f30ceb4db1b3ad666ff0b cni.projectcalico.org/podIP:172.30.56.94/32 cni.projectcalico.org/podIPs:172.30.56.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8b27 0xc0044e8b28}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6kzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6kzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.94,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://80349be1b087fa961dce023b051b1041534ba6d5861fc3c4b9cb052e1e31bbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.218: INFO: Pod "webserver-deployment-7f5969cbc7-wj8xq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wj8xq webserver-deployment-7f5969cbc7- deployment-3670  24085b3b-fa93-4bc5-8a9e-6c250f988a24 50395 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:556b73a87396c1646b821ab9850df461ba3a61c270d53ab323a11446c61cfaab cni.projectcalico.org/podIP:172.30.56.126/32 cni.projectcalico.org/podIPs:172.30.56.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8d57 0xc0044e8d58}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdffq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdffq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.126,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c1fa9a41af8b2982dcaf62c24ebaa3792348ca6bf96821bd9d85d19a2f48e767,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.218: INFO: Pod "webserver-deployment-7f5969cbc7-wmfpd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wmfpd webserver-deployment-7f5969cbc7- deployment-3670  92583f7e-af6e-4e60-b3e3-5afbb9bc895b 49974 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5b8e57c3cfbb57688efe314d7eb9026b7a265801f9427b022568b85ce1ad5d7a cni.projectcalico.org/podIP:172.30.56.119/32 cni.projectcalico.org/podIPs:172.30.56.119/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e8f87 0xc0044e8f88}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdpnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdpnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.119,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3887c4b9842927e09ce3e8378e146ff5d10f646694fd4aa42540c43f0ca440f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.219: INFO: Pod "webserver-deployment-7f5969cbc7-z4bpx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z4bpx webserver-deployment-7f5969cbc7- deployment-3670  0049a24d-7cb6-42d0-9f17-c615ca4cc9ee 49985 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1c29b06e149eff7f741a38d56668d9d509ba075dc3fd5ff127450399da28fde0 cni.projectcalico.org/podIP:172.30.4.93/32 cni.projectcalico.org/podIPs:172.30.4.93/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e91b7 0xc0044e91b8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkfzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkfzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.93,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff4d2c832f317fb75e1d86d25433631e6f146f26a8b0a774efb13f2cfc2c788c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.219: INFO: Pod "webserver-deployment-7f5969cbc7-ztzh2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ztzh2 webserver-deployment-7f5969cbc7- deployment-3670  05782e5a-1073-4c7d-9400-81e3403ec8e0 49980 0 2023-03-27 22:18:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6e57de5c8decd40df1c0a373c963e33fac20e2a52aae8f7440bfaddf5737e9a3 cni.projectcalico.org/podIP:172.30.4.96/32 cni.projectcalico.org/podIPs:172.30.4.96/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f9bf6375-d3f5-4817-a160-b00105a6e156 0xc0044e93e0 0xc0044e93e1}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9bf6375-d3f5-4817-a160-b00105a6e156\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-547pp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-547pp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.96,StartTime:2023-03-27 22:18:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:18:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://739f6a5edc76c5182476113e25346c77dd114dbc06ba3a8e9e90c8c3c2c4565a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.220: INFO: Pod "webserver-deployment-d9f79cb5-48vt5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-48vt5 webserver-deployment-d9f79cb5- deployment-3670  92acef13-f9de-4b04-8072-19076563b4e7 50298 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:7da3987b9b4f835250b13a22d0cd223c63646d079b932301a31846ddf2162bd7 cni.projectcalico.org/podIP:172.30.4.97/32 cni.projectcalico.org/podIPs:172.30.4.97/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0044e95cf 0xc0044e9600}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.4.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdzsj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdzsj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:172.30.4.97,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.4.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.220: INFO: Pod "webserver-deployment-d9f79cb5-4xpd5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4xpd5 webserver-deployment-d9f79cb5- deployment-3670  2ab964d2-4380-44a6-be5d-d66a50718d83 50341 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c14dd1d88d5b68bbc420fe9d2e973bdd2f1cede653bc445f28a2ae061f6bbf4e cni.projectcalico.org/podIP:172.30.4.99/32 cni.projectcalico.org/podIPs:172.30.4.99/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0044e986f 0xc0044e98f0}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cfr4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfr4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.221: INFO: Pod "webserver-deployment-d9f79cb5-682q4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-682q4 webserver-deployment-d9f79cb5- deployment-3670  9d7d3c62-1cac-4942-b477-ba3e9511ba68 50377 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1e8d69db8cc36513868bf14efde25cbe5e33e8e2fdc195911af665c27f239346 cni.projectcalico.org/podIP:172.30.4.107/32 cni.projectcalico.org/podIPs:172.30.4.107/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be127 0xc0045be128}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thtp4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thtp4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.222: INFO: Pod "webserver-deployment-d9f79cb5-98gwn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-98gwn webserver-deployment-d9f79cb5- deployment-3670  a0a06a1f-4dc0-4744-b007-41939f866191 50299 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2b4b65b516369b40a0034b382c0f2b23e4a0056ec82d94388983e71e531f7bde cni.projectcalico.org/podIP:172.30.85.179/32 cni.projectcalico.org/podIPs:172.30.85.179/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be527 0xc0045be528}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8thql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8thql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.179,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.226: INFO: Pod "webserver-deployment-d9f79cb5-dkd4q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dkd4q webserver-deployment-d9f79cb5- deployment-3670  a96230c4-eee0-4d67-8e51-a90d28bff0a8 50259 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:dd2374a71aadb77753959cccbddca299295affbbd2a0ff138df13076293d2bf2 cni.projectcalico.org/podIP:172.30.56.76/32 cni.projectcalico.org/podIPs:172.30.56.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be787 0xc0045be788}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j692h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j692h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.76,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.227: INFO: Pod "webserver-deployment-d9f79cb5-f9vhg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f9vhg webserver-deployment-d9f79cb5- deployment-3670  9b338637-3572-419f-b2cc-2ce1cb5aff9f 50290 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045be9c7 0xc0045be9c8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qct7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qct7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.227: INFO: Pod "webserver-deployment-d9f79cb5-lz9t5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lz9t5 webserver-deployment-d9f79cb5- deployment-3670  78dbf3af-c3fc-47bf-9087-9b987858dacc 50304 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6e64e40f08e8c02c3d7a529958e693ab053f31573e4f883b7668ab96cb1a20b9 cni.projectcalico.org/podIP:172.30.85.171/32 cni.projectcalico.org/podIPs:172.30.85.171/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bebe7 0xc0045bebe8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tftn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tftn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.171,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-m224k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m224k webserver-deployment-d9f79cb5- deployment-3670  2280622f-0387-4efd-b349-5685c8f374b5 50381 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:02d153d6962e551cda0ca78e88a66dbf342d07a1b0a8332cc585499f6620636d cni.projectcalico.org/podIP:172.30.56.69/32 cni.projectcalico.org/podIPs:172.30.56.69/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bee47 0xc0045bee48}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26wgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26wgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-qrjzd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qrjzd webserver-deployment-d9f79cb5- deployment-3670  2331e331-3d33-4db1-b033-6c740a1b8cc6 50312 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4f915cc3ea4191a6cce70d75387b58396a36c9af2006c6e4310e96f1ca3293ab cni.projectcalico.org/podIP:172.30.85.170/32 cni.projectcalico.org/podIPs:172.30.85.170/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf077 0xc0045bf078}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tplqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tplqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.228: INFO: Pod "webserver-deployment-d9f79cb5-qxp5k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-qxp5k webserver-deployment-d9f79cb5- deployment-3670  b54edbe8-1c7f-45bb-a2e5-5ddc4981cdb7 50375 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:99510827b02416d02b412ccb1c84fd6d26a87a02d48c57ca484ced9f2b3427d1 cni.projectcalico.org/podIP:172.30.85.167/32 cni.projectcalico.org/podIPs:172.30.85.167/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf2a7 0xc0045bf2a8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-sbjlx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sbjlx webserver-deployment-d9f79cb5- deployment-3670  a912390f-fe14-43a2-880a-824bab84df67 50264 0 2023-03-27 22:18:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:43cae989773991cd15203c272a399ddcb916a5095d3bfb6e27b3c0ee0509d0e9 cni.projectcalico.org/podIP:172.30.56.96/32 cni.projectcalico.org/podIPs:172.30.56.96/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf507 0xc0045bf508}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.56.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78xn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78xn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:172.30.56.96,StartTime:2023-03-27 22:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:8518f45b86b6e93c8e6be280b412b177212ea59787707e891f52590bf0a042d1: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.56.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-v88bw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v88bw webserver-deployment-d9f79cb5- deployment-3670  6971a090-28cf-482b-a14c-018cec12fe57 50362 0 2023-03-27 22:18:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:2b0592ddf4beaf975cb2f7954656cf81439653db9be8ef1b9e50ed6dd4a311e4 cni.projectcalico.org/podIP:172.30.56.78/32 cni.projectcalico.org/podIPs:172.30.56.78/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf767 0xc0045bf768}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-03-27 22:18:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-674wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-674wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.175,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.175,PodIP:,StartTime:2023-03-27 22:18:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 27 22:18:28.229: INFO: Pod "webserver-deployment-d9f79cb5-x7fpn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x7fpn webserver-deployment-d9f79cb5- deployment-3670  8ddfb1a8-adcd-4e1a-a671-a6208907c040 50313 0 2023-03-27 22:18:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:83d030b748de5bff3e0988e67faa84e4f79451b58be4bdb472205fa43501ccea cni.projectcalico.org/podIP:172.30.4.95/32 cni.projectcalico.org/podIPs:172.30.4.95/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 03ae2342-19e0-4f9b-88f5-32b56f4782f1 0xc0045bf9a7 0xc0045bf9a8}] [] [{kube-controller-manager Update v1 2023-03-27 22:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"03ae2342-19e0-4f9b-88f5-32b56f4782f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4j7mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4j7mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:18:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.178,PodIP:,StartTime:2023-03-27 22:18:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:18:28.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3670" for this suite. 03/27/23 22:18:28.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:18:28.281
Mar 27 22:18:28.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename cronjob 03/27/23 22:18:28.283
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:18:28.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:18:28.351
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/27/23 22:18:28.366
STEP: Ensuring a job is scheduled 03/27/23 22:18:28.387
STEP: Ensuring exactly one is scheduled 03/27/23 22:19:00.404
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 22:19:00.419
STEP: Ensuring the job is replaced with a new one 03/27/23 22:19:00.437
STEP: Removing cronjob 03/27/23 22:20:00.453
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:00.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7486" for this suite. 03/27/23 22:20:00.51
------------------------------
• [SLOW TEST] [92.259 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:18:28.281
    Mar 27 22:18:28.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename cronjob 03/27/23 22:18:28.283
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:18:28.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:18:28.351
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/27/23 22:18:28.366
    STEP: Ensuring a job is scheduled 03/27/23 22:18:28.387
    STEP: Ensuring exactly one is scheduled 03/27/23 22:19:00.404
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/27/23 22:19:00.419
    STEP: Ensuring the job is replaced with a new one 03/27/23 22:19:00.437
    STEP: Removing cronjob 03/27/23 22:20:00.453
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:00.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7486" for this suite. 03/27/23 22:20:00.51
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:00.542
Mar 27 22:20:00.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename endpointslicemirroring 03/27/23 22:20:00.544
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:00.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:00.603
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/27/23 22:20:00.657
Mar 27 22:20:00.683: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/27/23 22:20:02.71
STEP: mirroring deletion of a custom Endpoint 03/27/23 22:20:02.745
Mar 27 22:20:02.779: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:04.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5264" for this suite. 03/27/23 22:20:04.819
------------------------------
• [4.304 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:00.542
    Mar 27 22:20:00.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename endpointslicemirroring 03/27/23 22:20:00.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:00.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:00.603
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/27/23 22:20:00.657
    Mar 27 22:20:00.683: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/27/23 22:20:02.71
    STEP: mirroring deletion of a custom Endpoint 03/27/23 22:20:02.745
    Mar 27 22:20:02.779: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:04.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5264" for this suite. 03/27/23 22:20:04.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:04.85
Mar 27 22:20:04.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 22:20:04.851
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:04.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:04.915
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 22:20:04.973
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:20:05.458
STEP: Deploying the webhook pod 03/27/23 22:20:05.487
STEP: Wait for the deployment to be ready 03/27/23 22:20:05.529
Mar 27 22:20:05.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 22:20:07.622
STEP: Verifying the service has paired with the endpoint 03/27/23 22:20:07.67
Mar 27 22:20:08.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 03/27/23 22:20:08.685
STEP: Creating a custom resource definition that should be denied by the webhook 03/27/23 22:20:08.77
Mar 27 22:20:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4553" for this suite. 03/27/23 22:20:09.058
STEP: Destroying namespace "webhook-4553-markers" for this suite. 03/27/23 22:20:09.085
------------------------------
• [4.267 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:04.85
    Mar 27 22:20:04.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 22:20:04.851
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:04.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:04.915
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 22:20:04.973
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:20:05.458
    STEP: Deploying the webhook pod 03/27/23 22:20:05.487
    STEP: Wait for the deployment to be ready 03/27/23 22:20:05.529
    Mar 27 22:20:05.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 22:20:07.622
    STEP: Verifying the service has paired with the endpoint 03/27/23 22:20:07.67
    Mar 27 22:20:08.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/27/23 22:20:08.685
    STEP: Creating a custom resource definition that should be denied by the webhook 03/27/23 22:20:08.77
    Mar 27 22:20:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:08.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4553" for this suite. 03/27/23 22:20:09.058
    STEP: Destroying namespace "webhook-4553-markers" for this suite. 03/27/23 22:20:09.085
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:09.119
Mar 27 22:20:09.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:20:09.121
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:09.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:09.184
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 03/27/23 22:20:09.2
Mar 27 22:20:09.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d" in namespace "projected-238" to be "Succeeded or Failed"
Mar 27 22:20:09.262: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.565989ms
Mar 27 22:20:11.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043392246s
Mar 27 22:20:13.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043806413s
STEP: Saw pod success 03/27/23 22:20:13.279
Mar 27 22:20:13.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d" satisfied condition "Succeeded or Failed"
Mar 27 22:20:13.295: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d container client-container: <nil>
STEP: delete the pod 03/27/23 22:20:13.377
Mar 27 22:20:13.429: INFO: Waiting for pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d to disappear
Mar 27 22:20:13.443: INFO: Pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:13.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-238" for this suite. 03/27/23 22:20:13.467
------------------------------
• [4.373 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:09.119
    Mar 27 22:20:09.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:20:09.121
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:09.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:09.184
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 03/27/23 22:20:09.2
    Mar 27 22:20:09.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d" in namespace "projected-238" to be "Succeeded or Failed"
    Mar 27 22:20:09.262: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.565989ms
    Mar 27 22:20:11.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043392246s
    Mar 27 22:20:13.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043806413s
    STEP: Saw pod success 03/27/23 22:20:13.279
    Mar 27 22:20:13.279: INFO: Pod "downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d" satisfied condition "Succeeded or Failed"
    Mar 27 22:20:13.295: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d container client-container: <nil>
    STEP: delete the pod 03/27/23 22:20:13.377
    Mar 27 22:20:13.429: INFO: Waiting for pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d to disappear
    Mar 27 22:20:13.443: INFO: Pod downwardapi-volume-3a409537-ad2d-4428-8f18-e816acce952d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:13.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-238" for this suite. 03/27/23 22:20:13.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:13.497
Mar 27 22:20:13.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename watch 03/27/23 22:20:13.499
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:13.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:13.574
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/27/23 22:20:13.59
STEP: creating a new configmap 03/27/23 22:20:13.597
STEP: modifying the configmap once 03/27/23 22:20:13.617
STEP: closing the watch once it receives two notifications 03/27/23 22:20:13.648
Mar 27 22:20:13.648: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51069 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:20:13.649: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51070 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/27/23 22:20:13.649
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/27/23 22:20:13.686
STEP: deleting the configmap 03/27/23 22:20:13.694
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/27/23 22:20:13.718
Mar 27 22:20:13.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51071 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 27 22:20:13.719: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51072 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6476" for this suite. 03/27/23 22:20:13.745
------------------------------
• [0.273 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:13.497
    Mar 27 22:20:13.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename watch 03/27/23 22:20:13.499
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:13.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:13.574
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/27/23 22:20:13.59
    STEP: creating a new configmap 03/27/23 22:20:13.597
    STEP: modifying the configmap once 03/27/23 22:20:13.617
    STEP: closing the watch once it receives two notifications 03/27/23 22:20:13.648
    Mar 27 22:20:13.648: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51069 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:20:13.649: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51070 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/27/23 22:20:13.649
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/27/23 22:20:13.686
    STEP: deleting the configmap 03/27/23 22:20:13.694
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/27/23 22:20:13.718
    Mar 27 22:20:13.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51071 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 27 22:20:13.719: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6476  076f095f-7c40-43cf-a155-841f94c890d9 51072 0 2023-03-27 22:20:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-27 22:20:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6476" for this suite. 03/27/23 22:20:13.745
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:13.772
Mar 27 22:20:13.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 22:20:13.773
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:13.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:13.846
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-1fa17249-fc4e-4e36-a2db-c8cd515bd97b 03/27/23 22:20:13.86
STEP: Creating a pod to test consume configMaps 03/27/23 22:20:13.879
Mar 27 22:20:13.912: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c" in namespace "configmap-1809" to be "Succeeded or Failed"
Mar 27 22:20:13.930: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.394615ms
Mar 27 22:20:15.954: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041988836s
Mar 27 22:20:17.950: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037201227s
STEP: Saw pod success 03/27/23 22:20:17.95
Mar 27 22:20:17.950: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c" satisfied condition "Succeeded or Failed"
Mar 27 22:20:17.967: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c container configmap-volume-test: <nil>
STEP: delete the pod 03/27/23 22:20:18.001
Mar 27 22:20:18.053: INFO: Waiting for pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c to disappear
Mar 27 22:20:18.070: INFO: Pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:20:18.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1809" for this suite. 03/27/23 22:20:18.092
------------------------------
• [4.346 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:13.772
    Mar 27 22:20:13.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 22:20:13.773
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:13.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:13.846
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-1fa17249-fc4e-4e36-a2db-c8cd515bd97b 03/27/23 22:20:13.86
    STEP: Creating a pod to test consume configMaps 03/27/23 22:20:13.879
    Mar 27 22:20:13.912: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c" in namespace "configmap-1809" to be "Succeeded or Failed"
    Mar 27 22:20:13.930: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.394615ms
    Mar 27 22:20:15.954: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041988836s
    Mar 27 22:20:17.950: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037201227s
    STEP: Saw pod success 03/27/23 22:20:17.95
    Mar 27 22:20:17.950: INFO: Pod "pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c" satisfied condition "Succeeded or Failed"
    Mar 27 22:20:17.967: INFO: Trying to get logs from node 10.176.99.177 pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c container configmap-volume-test: <nil>
    STEP: delete the pod 03/27/23 22:20:18.001
    Mar 27 22:20:18.053: INFO: Waiting for pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c to disappear
    Mar 27 22:20:18.070: INFO: Pod pod-configmaps-6e23512f-5f60-45a7-9ac6-9a3f9c1a8d5c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:20:18.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1809" for this suite. 03/27/23 22:20:18.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:20:18.125
Mar 27 22:20:18.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename statefulset 03/27/23 22:20:18.127
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:18.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:18.204
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8480 03/27/23 22:20:18.219
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 03/27/23 22:20:18.237
Mar 27 22:20:18.273: INFO: Found 0 stateful pods, waiting for 3
Mar 27 22:20:28.297: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:20:28.297: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:20:28.297: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/27/23 22:20:28.367
Mar 27 22:20:28.418: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/27/23 22:20:28.418
STEP: Not applying an update when the partition is greater than the number of replicas 03/27/23 22:20:38.485
STEP: Performing a canary update 03/27/23 22:20:38.485
Mar 27 22:20:38.528: INFO: Updating stateful set ss2
Mar 27 22:20:38.562: INFO: Waiting for Pod statefulset-8480/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 03/27/23 22:20:48.599
Mar 27 22:20:48.750: INFO: Found 2 stateful pods, waiting for 3
Mar 27 22:20:58.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:20:58.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 27 22:20:58.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/27/23 22:20:58.802
Mar 27 22:20:58.843: INFO: Updating stateful set ss2
Mar 27 22:20:58.874: INFO: Waiting for Pod statefulset-8480/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Mar 27 22:21:08.955: INFO: Updating stateful set ss2
Mar 27 22:21:08.989: INFO: Waiting for StatefulSet statefulset-8480/ss2 to complete update
Mar 27 22:21:08.989: INFO: Waiting for Pod statefulset-8480/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Mar 27 22:21:19.026: INFO: Deleting all statefulset in ns statefulset-8480
Mar 27 22:21:19.039: INFO: Scaling statefulset ss2 to 0
Mar 27 22:21:29.111: INFO: Waiting for statefulset status.replicas updated to 0
Mar 27 22:21:29.124: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Mar 27 22:21:29.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8480" for this suite. 03/27/23 22:21:29.205
------------------------------
• [SLOW TEST] [71.107 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:20:18.125
    Mar 27 22:20:18.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename statefulset 03/27/23 22:20:18.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:20:18.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:20:18.204
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8480 03/27/23 22:20:18.219
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 03/27/23 22:20:18.237
    Mar 27 22:20:18.273: INFO: Found 0 stateful pods, waiting for 3
    Mar 27 22:20:28.297: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:20:28.297: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:20:28.297: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 03/27/23 22:20:28.367
    Mar 27 22:20:28.418: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/27/23 22:20:28.418
    STEP: Not applying an update when the partition is greater than the number of replicas 03/27/23 22:20:38.485
    STEP: Performing a canary update 03/27/23 22:20:38.485
    Mar 27 22:20:38.528: INFO: Updating stateful set ss2
    Mar 27 22:20:38.562: INFO: Waiting for Pod statefulset-8480/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 03/27/23 22:20:48.599
    Mar 27 22:20:48.750: INFO: Found 2 stateful pods, waiting for 3
    Mar 27 22:20:58.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:20:58.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 27 22:20:58.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/27/23 22:20:58.802
    Mar 27 22:20:58.843: INFO: Updating stateful set ss2
    Mar 27 22:20:58.874: INFO: Waiting for Pod statefulset-8480/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Mar 27 22:21:08.955: INFO: Updating stateful set ss2
    Mar 27 22:21:08.989: INFO: Waiting for StatefulSet statefulset-8480/ss2 to complete update
    Mar 27 22:21:08.989: INFO: Waiting for Pod statefulset-8480/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Mar 27 22:21:19.026: INFO: Deleting all statefulset in ns statefulset-8480
    Mar 27 22:21:19.039: INFO: Scaling statefulset ss2 to 0
    Mar 27 22:21:29.111: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 27 22:21:29.124: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:21:29.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8480" for this suite. 03/27/23 22:21:29.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:21:29.237
Mar 27 22:21:29.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename emptydir 03/27/23 22:21:29.238
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:29.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:29.298
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 22:21:29.33
Mar 27 22:21:29.363: INFO: Waiting up to 5m0s for pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa" in namespace "emptydir-5343" to be "Succeeded or Failed"
Mar 27 22:21:29.380: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.192673ms
Mar 27 22:21:31.398: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035133972s
Mar 27 22:21:33.406: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043354827s
STEP: Saw pod success 03/27/23 22:21:33.406
Mar 27 22:21:33.406: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa" satisfied condition "Succeeded or Failed"
Mar 27 22:21:33.422: INFO: Trying to get logs from node 10.176.99.177 pod pod-2b9357da-f08b-495f-8246-4e808679e4aa container test-container: <nil>
STEP: delete the pod 03/27/23 22:21:33.452
Mar 27 22:21:33.509: INFO: Waiting for pod pod-2b9357da-f08b-495f-8246-4e808679e4aa to disappear
Mar 27 22:21:33.526: INFO: Pod pod-2b9357da-f08b-495f-8246-4e808679e4aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Mar 27 22:21:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5343" for this suite. 03/27/23 22:21:33.547
------------------------------
• [4.337 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:21:29.237
    Mar 27 22:21:29.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename emptydir 03/27/23 22:21:29.238
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:29.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:29.298
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/27/23 22:21:29.33
    Mar 27 22:21:29.363: INFO: Waiting up to 5m0s for pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa" in namespace "emptydir-5343" to be "Succeeded or Failed"
    Mar 27 22:21:29.380: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.192673ms
    Mar 27 22:21:31.398: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035133972s
    Mar 27 22:21:33.406: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043354827s
    STEP: Saw pod success 03/27/23 22:21:33.406
    Mar 27 22:21:33.406: INFO: Pod "pod-2b9357da-f08b-495f-8246-4e808679e4aa" satisfied condition "Succeeded or Failed"
    Mar 27 22:21:33.422: INFO: Trying to get logs from node 10.176.99.177 pod pod-2b9357da-f08b-495f-8246-4e808679e4aa container test-container: <nil>
    STEP: delete the pod 03/27/23 22:21:33.452
    Mar 27 22:21:33.509: INFO: Waiting for pod pod-2b9357da-f08b-495f-8246-4e808679e4aa to disappear
    Mar 27 22:21:33.526: INFO: Pod pod-2b9357da-f08b-495f-8246-4e808679e4aa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:21:33.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5343" for this suite. 03/27/23 22:21:33.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:21:33.578
Mar 27 22:21:33.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename kubectl 03/27/23 22:21:33.58
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:33.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:33.645
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 03/27/23 22:21:33.66
Mar 27 22:21:33.660: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1014 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/27/23 22:21:33.733
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Mar 27 22:21:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1014" for this suite. 03/27/23 22:21:33.77
------------------------------
• [0.219 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:21:33.578
    Mar 27 22:21:33.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename kubectl 03/27/23 22:21:33.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:33.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:33.645
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 03/27/23 22:21:33.66
    Mar 27 22:21:33.660: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=kubectl-1014 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/27/23 22:21:33.733
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:21:33.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1014" for this suite. 03/27/23 22:21:33.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:21:33.8
Mar 27 22:21:33.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename container-probe 03/27/23 22:21:33.801
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:33.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:33.872
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a in namespace container-probe-3696 03/27/23 22:21:33.886
Mar 27 22:21:33.924: INFO: Waiting up to 5m0s for pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a" in namespace "container-probe-3696" to be "not pending"
Mar 27 22:21:33.941: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.518888ms
Mar 27 22:21:35.970: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045923901s
Mar 27 22:21:35.970: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a" satisfied condition "not pending"
Mar 27 22:21:35.971: INFO: Started pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a in namespace container-probe-3696
STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 22:21:35.971
Mar 27 22:21:36.002: INFO: Initial restart count of pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a is 0
Mar 27 22:22:26.691: INFO: Restart count of pod container-probe-3696/busybox-952afe14-1b50-43db-8119-2df31bdf8d7a is now 1 (50.689238477s elapsed)
STEP: deleting the pod 03/27/23 22:22:26.691
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:26.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3696" for this suite. 03/27/23 22:22:26.769
------------------------------
• [SLOW TEST] [53.036 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:21:33.8
    Mar 27 22:21:33.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename container-probe 03/27/23 22:21:33.801
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:21:33.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:21:33.872
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a in namespace container-probe-3696 03/27/23 22:21:33.886
    Mar 27 22:21:33.924: INFO: Waiting up to 5m0s for pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a" in namespace "container-probe-3696" to be "not pending"
    Mar 27 22:21:33.941: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.518888ms
    Mar 27 22:21:35.970: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.045923901s
    Mar 27 22:21:35.970: INFO: Pod "busybox-952afe14-1b50-43db-8119-2df31bdf8d7a" satisfied condition "not pending"
    Mar 27 22:21:35.971: INFO: Started pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a in namespace container-probe-3696
    STEP: checking the pod's current state and verifying that restartCount is present 03/27/23 22:21:35.971
    Mar 27 22:21:36.002: INFO: Initial restart count of pod busybox-952afe14-1b50-43db-8119-2df31bdf8d7a is 0
    Mar 27 22:22:26.691: INFO: Restart count of pod container-probe-3696/busybox-952afe14-1b50-43db-8119-2df31bdf8d7a is now 1 (50.689238477s elapsed)
    STEP: deleting the pod 03/27/23 22:22:26.691
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:26.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3696" for this suite. 03/27/23 22:22:26.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:26.839
Mar 27 22:22:26.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename daemonsets 03/27/23 22:22:26.84
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:26.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:26.93
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Mar 27 22:22:27.043: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/27/23 22:22:27.06
Mar 27 22:22:27.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:27.078: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/27/23 22:22:27.078
Mar 27 22:22:27.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:27.213: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:28.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:28.234: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:29.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 22:22:29.231: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/27/23 22:22:29.243
Mar 27 22:22:29.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 22:22:29.322: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar 27 22:22:30.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:30.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/27/23 22:22:30.34
Mar 27 22:22:30.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:30.380: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:31.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:31.401: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:32.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:32.397: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:33.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:33.396: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
Mar 27 22:22:34.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 27 22:22:34.398: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 03/27/23 22:22:34.424
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-714, will wait for the garbage collector to delete the pods 03/27/23 22:22:34.424
Mar 27 22:22:34.525: INFO: Deleting DaemonSet.extensions daemon-set took: 37.974139ms
Mar 27 22:22:34.626: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.080055ms
Mar 27 22:22:37.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 27 22:22:37.344: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 27 22:22:37.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51813"},"items":null}

Mar 27 22:22:37.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51813"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-714" for this suite. 03/27/23 22:22:37.628
------------------------------
• [SLOW TEST] [10.853 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:26.839
    Mar 27 22:22:26.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename daemonsets 03/27/23 22:22:26.84
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:26.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:26.93
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Mar 27 22:22:27.043: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/27/23 22:22:27.06
    Mar 27 22:22:27.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:27.078: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/27/23 22:22:27.078
    Mar 27 22:22:27.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:27.213: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:28.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:28.234: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:29.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 22:22:29.231: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/27/23 22:22:29.243
    Mar 27 22:22:29.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 22:22:29.322: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Mar 27 22:22:30.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:30.340: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/27/23 22:22:30.34
    Mar 27 22:22:30.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:30.380: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:31.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:31.401: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:32.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:32.397: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:33.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:33.396: INFO: Node 10.176.99.177 is running 0 daemon pod, expected 1
    Mar 27 22:22:34.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 27 22:22:34.398: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 03/27/23 22:22:34.424
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-714, will wait for the garbage collector to delete the pods 03/27/23 22:22:34.424
    Mar 27 22:22:34.525: INFO: Deleting DaemonSet.extensions daemon-set took: 37.974139ms
    Mar 27 22:22:34.626: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.080055ms
    Mar 27 22:22:37.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 27 22:22:37.344: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 27 22:22:37.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51813"},"items":null}

    Mar 27 22:22:37.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51813"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-714" for this suite. 03/27/23 22:22:37.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:37.692
Mar 27 22:22:37.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename runtimeclass 03/27/23 22:22:37.693
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:37.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:37.762
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5791-delete-me 03/27/23 22:22:37.792
STEP: Waiting for the RuntimeClass to disappear 03/27/23 22:22:37.813
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:37.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5791" for this suite. 03/27/23 22:22:37.898
------------------------------
• [0.242 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:37.692
    Mar 27 22:22:37.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename runtimeclass 03/27/23 22:22:37.693
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:37.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:37.762
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5791-delete-me 03/27/23 22:22:37.792
    STEP: Waiting for the RuntimeClass to disappear 03/27/23 22:22:37.813
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:37.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5791" for this suite. 03/27/23 22:22:37.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:37.941
Mar 27 22:22:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename downward-api 03/27/23 22:22:37.942
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:37.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:38.005
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 03/27/23 22:22:38.019
Mar 27 22:22:38.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd" in namespace "downward-api-2239" to be "Succeeded or Failed"
Mar 27 22:22:38.073: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.474666ms
Mar 27 22:22:40.091: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037210362s
Mar 27 22:22:42.096: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042456707s
STEP: Saw pod success 03/27/23 22:22:42.096
Mar 27 22:22:42.096: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd" satisfied condition "Succeeded or Failed"
Mar 27 22:22:42.112: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd container client-container: <nil>
STEP: delete the pod 03/27/23 22:22:42.142
Mar 27 22:22:42.190: INFO: Waiting for pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd to disappear
Mar 27 22:22:42.207: INFO: Pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2239" for this suite. 03/27/23 22:22:42.231
------------------------------
• [4.321 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:37.941
    Mar 27 22:22:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename downward-api 03/27/23 22:22:37.942
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:37.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:38.005
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 03/27/23 22:22:38.019
    Mar 27 22:22:38.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd" in namespace "downward-api-2239" to be "Succeeded or Failed"
    Mar 27 22:22:38.073: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.474666ms
    Mar 27 22:22:40.091: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037210362s
    Mar 27 22:22:42.096: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042456707s
    STEP: Saw pod success 03/27/23 22:22:42.096
    Mar 27 22:22:42.096: INFO: Pod "downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd" satisfied condition "Succeeded or Failed"
    Mar 27 22:22:42.112: INFO: Trying to get logs from node 10.176.99.177 pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd container client-container: <nil>
    STEP: delete the pod 03/27/23 22:22:42.142
    Mar 27 22:22:42.190: INFO: Waiting for pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd to disappear
    Mar 27 22:22:42.207: INFO: Pod downwardapi-volume-7f44b766-dda5-4155-9875-165a9fa267cd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:42.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2239" for this suite. 03/27/23 22:22:42.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:42.266
Mar 27 22:22:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename projected 03/27/23 22:22:42.267
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:42.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:42.326
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-be757038-fde1-412c-abc1-86d29d0576de 03/27/23 22:22:42.341
STEP: Creating a pod to test consume configMaps 03/27/23 22:22:42.358
Mar 27 22:22:42.388: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7" in namespace "projected-1086" to be "Succeeded or Failed"
Mar 27 22:22:42.406: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.090234ms
Mar 27 22:22:44.422: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033557767s
Mar 27 22:22:46.423: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034072373s
STEP: Saw pod success 03/27/23 22:22:46.423
Mar 27 22:22:46.423: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7" satisfied condition "Succeeded or Failed"
Mar 27 22:22:46.440: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 container agnhost-container: <nil>
STEP: delete the pod 03/27/23 22:22:46.501
Mar 27 22:22:46.541: INFO: Waiting for pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 to disappear
Mar 27 22:22:46.557: INFO: Pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:46.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1086" for this suite. 03/27/23 22:22:46.58
------------------------------
• [4.340 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:42.266
    Mar 27 22:22:42.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename projected 03/27/23 22:22:42.267
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:42.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:42.326
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-be757038-fde1-412c-abc1-86d29d0576de 03/27/23 22:22:42.341
    STEP: Creating a pod to test consume configMaps 03/27/23 22:22:42.358
    Mar 27 22:22:42.388: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7" in namespace "projected-1086" to be "Succeeded or Failed"
    Mar 27 22:22:42.406: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.090234ms
    Mar 27 22:22:44.422: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033557767s
    Mar 27 22:22:46.423: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034072373s
    STEP: Saw pod success 03/27/23 22:22:46.423
    Mar 27 22:22:46.423: INFO: Pod "pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7" satisfied condition "Succeeded or Failed"
    Mar 27 22:22:46.440: INFO: Trying to get logs from node 10.176.99.177 pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 container agnhost-container: <nil>
    STEP: delete the pod 03/27/23 22:22:46.501
    Mar 27 22:22:46.541: INFO: Waiting for pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 to disappear
    Mar 27 22:22:46.557: INFO: Pod pod-projected-configmaps-2755ce25-3e6b-48ff-b9d8-dbffb7d309d7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:46.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1086" for this suite. 03/27/23 22:22:46.58
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:46.606
Mar 27 22:22:46.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename gc 03/27/23 22:22:46.608
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:46.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:46.675
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 27 22:22:46.808: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"62697e1b-10f5-4543-817e-595b2977ec6d", Controller:(*bool)(0xc004a40646), BlockOwnerDeletion:(*bool)(0xc004a40647)}}
Mar 27 22:22:46.855: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4e627dd4-ffb3-474b-b742-5cfe64177277", Controller:(*bool)(0xc0045bfd7e), BlockOwnerDeletion:(*bool)(0xc0045bfd7f)}}
Mar 27 22:22:46.880: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3b3fa03c-1cde-4eb6-94f8-1f7444f703ff", Controller:(*bool)(0xc005238006), BlockOwnerDeletion:(*bool)(0xc005238007)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:51.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5347" for this suite. 03/27/23 22:22:51.951
------------------------------
• [SLOW TEST] [5.371 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:46.606
    Mar 27 22:22:46.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename gc 03/27/23 22:22:46.608
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:46.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:46.675
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 27 22:22:46.808: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"62697e1b-10f5-4543-817e-595b2977ec6d", Controller:(*bool)(0xc004a40646), BlockOwnerDeletion:(*bool)(0xc004a40647)}}
    Mar 27 22:22:46.855: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4e627dd4-ffb3-474b-b742-5cfe64177277", Controller:(*bool)(0xc0045bfd7e), BlockOwnerDeletion:(*bool)(0xc0045bfd7f)}}
    Mar 27 22:22:46.880: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3b3fa03c-1cde-4eb6-94f8-1f7444f703ff", Controller:(*bool)(0xc005238006), BlockOwnerDeletion:(*bool)(0xc005238007)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:51.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5347" for this suite. 03/27/23 22:22:51.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:51.981
Mar 27 22:22:51.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename controllerrevisions 03/27/23 22:22:51.983
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:52.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:52.092
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-xl9gr-daemon-set" 03/27/23 22:22:52.213
STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 22:22:52.229
Mar 27 22:22:52.264: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
Mar 27 22:22:52.264: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 22:22:53.310: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
Mar 27 22:22:53.310: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
Mar 27 22:22:54.302: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 2
Mar 27 22:22:54.302: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
Mar 27 22:22:55.304: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 3
Mar 27 22:22:55.304: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-xl9gr-daemon-set
STEP: Confirm DaemonSet "e2e-xl9gr-daemon-set" successfully created with "daemonset-name=e2e-xl9gr-daemon-set" label 03/27/23 22:22:55.317
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xl9gr-daemon-set" 03/27/23 22:22:55.35
Mar 27 22:22:55.373: INFO: Located ControllerRevision: "e2e-xl9gr-daemon-set-d6b7ddfcf"
STEP: Patching ControllerRevision "e2e-xl9gr-daemon-set-d6b7ddfcf" 03/27/23 22:22:55.39
Mar 27 22:22:55.415: INFO: e2e-xl9gr-daemon-set-d6b7ddfcf has been patched
STEP: Create a new ControllerRevision 03/27/23 22:22:55.415
Mar 27 22:22:55.439: INFO: Created ControllerRevision: e2e-xl9gr-daemon-set-768bd44895
STEP: Confirm that there are two ControllerRevisions 03/27/23 22:22:55.439
Mar 27 22:22:55.439: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 22:22:55.456: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-xl9gr-daemon-set-d6b7ddfcf" 03/27/23 22:22:55.456
STEP: Confirm that there is only one ControllerRevision 03/27/23 22:22:55.483
Mar 27 22:22:55.484: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 22:22:55.501: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-xl9gr-daemon-set-768bd44895" 03/27/23 22:22:55.517
Mar 27 22:22:55.556: INFO: e2e-xl9gr-daemon-set-768bd44895 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/27/23 22:22:55.556
W0327 22:22:55.578032      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/27/23 22:22:55.578
Mar 27 22:22:55.578: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 22:22:56.596: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 22:22:56.612: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xl9gr-daemon-set-768bd44895=updated" 03/27/23 22:22:56.612
STEP: Confirm that there is only one ControllerRevision 03/27/23 22:22:56.654
Mar 27 22:22:56.654: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 27 22:22:56.670: INFO: Found 1 ControllerRevisions
Mar 27 22:22:56.686: INFO: ControllerRevision "e2e-xl9gr-daemon-set-585b6bbdd6" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-xl9gr-daemon-set" 03/27/23 22:22:56.699
STEP: deleting DaemonSet.extensions e2e-xl9gr-daemon-set in namespace controllerrevisions-3413, will wait for the garbage collector to delete the pods 03/27/23 22:22:56.699
Mar 27 22:22:56.784: INFO: Deleting DaemonSet.extensions e2e-xl9gr-daemon-set took: 21.268266ms
Mar 27 22:22:56.886: INFO: Terminating DaemonSet.extensions e2e-xl9gr-daemon-set pods took: 102.04899ms
Mar 27 22:22:58.504: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
Mar 27 22:22:58.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xl9gr-daemon-set
Mar 27 22:22:58.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52051"},"items":null}

Mar 27 22:22:58.532: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52051"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:58.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3413" for this suite. 03/27/23 22:22:58.623
------------------------------
• [SLOW TEST] [6.667 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:51.981
    Mar 27 22:22:51.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename controllerrevisions 03/27/23 22:22:51.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:52.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:52.092
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-xl9gr-daemon-set" 03/27/23 22:22:52.213
    STEP: Check that daemon pods launch on every node of the cluster. 03/27/23 22:22:52.229
    Mar 27 22:22:52.264: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
    Mar 27 22:22:52.264: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 22:22:53.310: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
    Mar 27 22:22:53.310: INFO: Node 10.176.99.175 is running 0 daemon pod, expected 1
    Mar 27 22:22:54.302: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 2
    Mar 27 22:22:54.302: INFO: Node 10.176.99.178 is running 0 daemon pod, expected 1
    Mar 27 22:22:55.304: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 3
    Mar 27 22:22:55.304: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-xl9gr-daemon-set
    STEP: Confirm DaemonSet "e2e-xl9gr-daemon-set" successfully created with "daemonset-name=e2e-xl9gr-daemon-set" label 03/27/23 22:22:55.317
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xl9gr-daemon-set" 03/27/23 22:22:55.35
    Mar 27 22:22:55.373: INFO: Located ControllerRevision: "e2e-xl9gr-daemon-set-d6b7ddfcf"
    STEP: Patching ControllerRevision "e2e-xl9gr-daemon-set-d6b7ddfcf" 03/27/23 22:22:55.39
    Mar 27 22:22:55.415: INFO: e2e-xl9gr-daemon-set-d6b7ddfcf has been patched
    STEP: Create a new ControllerRevision 03/27/23 22:22:55.415
    Mar 27 22:22:55.439: INFO: Created ControllerRevision: e2e-xl9gr-daemon-set-768bd44895
    STEP: Confirm that there are two ControllerRevisions 03/27/23 22:22:55.439
    Mar 27 22:22:55.439: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 22:22:55.456: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-xl9gr-daemon-set-d6b7ddfcf" 03/27/23 22:22:55.456
    STEP: Confirm that there is only one ControllerRevision 03/27/23 22:22:55.483
    Mar 27 22:22:55.484: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 22:22:55.501: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-xl9gr-daemon-set-768bd44895" 03/27/23 22:22:55.517
    Mar 27 22:22:55.556: INFO: e2e-xl9gr-daemon-set-768bd44895 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/27/23 22:22:55.556
    W0327 22:22:55.578032      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/27/23 22:22:55.578
    Mar 27 22:22:55.578: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 22:22:56.596: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 22:22:56.612: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xl9gr-daemon-set-768bd44895=updated" 03/27/23 22:22:56.612
    STEP: Confirm that there is only one ControllerRevision 03/27/23 22:22:56.654
    Mar 27 22:22:56.654: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 27 22:22:56.670: INFO: Found 1 ControllerRevisions
    Mar 27 22:22:56.686: INFO: ControllerRevision "e2e-xl9gr-daemon-set-585b6bbdd6" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-xl9gr-daemon-set" 03/27/23 22:22:56.699
    STEP: deleting DaemonSet.extensions e2e-xl9gr-daemon-set in namespace controllerrevisions-3413, will wait for the garbage collector to delete the pods 03/27/23 22:22:56.699
    Mar 27 22:22:56.784: INFO: Deleting DaemonSet.extensions e2e-xl9gr-daemon-set took: 21.268266ms
    Mar 27 22:22:56.886: INFO: Terminating DaemonSet.extensions e2e-xl9gr-daemon-set pods took: 102.04899ms
    Mar 27 22:22:58.504: INFO: Number of nodes with available pods controlled by daemonset e2e-xl9gr-daemon-set: 0
    Mar 27 22:22:58.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xl9gr-daemon-set
    Mar 27 22:22:58.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52051"},"items":null}

    Mar 27 22:22:58.532: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52051"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:58.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3413" for this suite. 03/27/23 22:22:58.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:58.655
Mar 27 22:22:58.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename configmap 03/27/23 22:22:58.656
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:58.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:58.754
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-bd06879a-4356-496c-a4bb-ce16078de0c9 03/27/23 22:22:58.769
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Mar 27 22:22:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6260" for this suite. 03/27/23 22:22:58.8
------------------------------
• [0.199 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:58.655
    Mar 27 22:22:58.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename configmap 03/27/23 22:22:58.656
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:58.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:58.754
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-bd06879a-4356-496c-a4bb-ce16078de0c9 03/27/23 22:22:58.769
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:22:58.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6260" for this suite. 03/27/23 22:22:58.8
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:22:58.855
Mar 27 22:22:58.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename resourcequota 03/27/23 22:22:58.856
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:58.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:58.933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 03/27/23 22:23:15.973
STEP: Creating a ResourceQuota 03/27/23 22:23:20.988
STEP: Ensuring resource quota status is calculated 03/27/23 22:23:21.007
STEP: Creating a ConfigMap 03/27/23 22:23:23.021
STEP: Ensuring resource quota status captures configMap creation 03/27/23 22:23:23.073
STEP: Deleting a ConfigMap 03/27/23 22:23:25.092
STEP: Ensuring resource quota status released usage 03/27/23 22:23:25.117
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Mar 27 22:23:27.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3047" for this suite. 03/27/23 22:23:27.156
------------------------------
• [SLOW TEST] [28.328 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:22:58.855
    Mar 27 22:22:58.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename resourcequota 03/27/23 22:22:58.856
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:22:58.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:22:58.933
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 03/27/23 22:23:15.973
    STEP: Creating a ResourceQuota 03/27/23 22:23:20.988
    STEP: Ensuring resource quota status is calculated 03/27/23 22:23:21.007
    STEP: Creating a ConfigMap 03/27/23 22:23:23.021
    STEP: Ensuring resource quota status captures configMap creation 03/27/23 22:23:23.073
    STEP: Deleting a ConfigMap 03/27/23 22:23:25.092
    STEP: Ensuring resource quota status released usage 03/27/23 22:23:25.117
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:23:27.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3047" for this suite. 03/27/23 22:23:27.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:23:27.185
Mar 27 22:23:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename security-context-test 03/27/23 22:23:27.186
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:27.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:27.254
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Mar 27 22:23:27.297: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1" in namespace "security-context-test-1559" to be "Succeeded or Failed"
Mar 27 22:23:27.315: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117999ms
Mar 27 22:23:29.331: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034645385s
Mar 27 22:23:31.333: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036310949s
Mar 27 22:23:33.334: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037407672s
Mar 27 22:23:33.335: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Mar 27 22:23:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1559" for this suite. 03/27/23 22:23:33.419
------------------------------
• [SLOW TEST] [6.259 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:23:27.185
    Mar 27 22:23:27.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename security-context-test 03/27/23 22:23:27.186
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:27.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:27.254
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Mar 27 22:23:27.297: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1" in namespace "security-context-test-1559" to be "Succeeded or Failed"
    Mar 27 22:23:27.315: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117999ms
    Mar 27 22:23:29.331: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034645385s
    Mar 27 22:23:31.333: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036310949s
    Mar 27 22:23:33.334: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037407672s
    Mar 27 22:23:33.335: INFO: Pod "alpine-nnp-false-372dbb95-33d9-42a2-b595-a191e965ead1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:23:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1559" for this suite. 03/27/23 22:23:33.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:23:33.447
Mar 27 22:23:33.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/27/23 22:23:33.448
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:33.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:33.537
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/27/23 22:23:33.552
STEP: Creating hostNetwork=false pod 03/27/23 22:23:33.552
Mar 27 22:23:33.585: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-690" to be "running and ready"
Mar 27 22:23:33.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 43.35696ms
Mar 27 22:23:33.628: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:23:35.647: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.062310832s
Mar 27 22:23:35.647: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 27 22:23:35.647: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/27/23 22:23:35.664
Mar 27 22:23:35.687: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-690" to be "running and ready"
Mar 27 22:23:35.705: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.028797ms
Mar 27 22:23:35.705: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 27 22:23:37.722: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035440227s
Mar 27 22:23:37.722: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 27 22:23:37.722: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/27/23 22:23:37.739
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/27/23 22:23:37.739
Mar 27 22:23:37.739: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:37.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:37.740: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:37.740: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 22:23:37.896: INFO: Exec stderr: ""
Mar 27 22:23:37.896: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:37.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:37.897: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:37.897: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 22:23:38.087: INFO: Exec stderr: ""
Mar 27 22:23:38.088: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.088: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.089: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 22:23:38.253: INFO: Exec stderr: ""
Mar 27 22:23:38.253: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.255: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.255: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 22:23:38.391: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/27/23 22:23:38.391
Mar 27 22:23:38.392: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.393: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.393: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 27 22:23:38.558: INFO: Exec stderr: ""
Mar 27 22:23:38.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.559: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.559: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 27 22:23:38.728: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/27/23 22:23:38.728
Mar 27 22:23:38.729: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.729: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.729: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 22:23:38.869: INFO: Exec stderr: ""
Mar 27 22:23:38.869: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:38.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:38.870: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:38.871: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 27 22:23:39.060: INFO: Exec stderr: ""
Mar 27 22:23:39.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:39.061: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:39.062: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 22:23:39.248: INFO: Exec stderr: ""
Mar 27 22:23:39.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 27 22:23:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:23:39.249: INFO: ExecWithOptions: Clientset creation
Mar 27 22:23:39.249: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 27 22:23:39.385: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Mar 27 22:23:39.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-690" for this suite. 03/27/23 22:23:39.409
------------------------------
• [SLOW TEST] [5.987 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:23:33.447
    Mar 27 22:23:33.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/27/23 22:23:33.448
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:33.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:33.537
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/27/23 22:23:33.552
    STEP: Creating hostNetwork=false pod 03/27/23 22:23:33.552
    Mar 27 22:23:33.585: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-690" to be "running and ready"
    Mar 27 22:23:33.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 43.35696ms
    Mar 27 22:23:33.628: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:23:35.647: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.062310832s
    Mar 27 22:23:35.647: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 27 22:23:35.647: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/27/23 22:23:35.664
    Mar 27 22:23:35.687: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-690" to be "running and ready"
    Mar 27 22:23:35.705: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.028797ms
    Mar 27 22:23:35.705: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 27 22:23:37.722: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035440227s
    Mar 27 22:23:37.722: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 27 22:23:37.722: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/27/23 22:23:37.739
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/27/23 22:23:37.739
    Mar 27 22:23:37.739: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:37.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:37.740: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:37.740: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 22:23:37.896: INFO: Exec stderr: ""
    Mar 27 22:23:37.896: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:37.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:37.897: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:37.897: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 22:23:38.087: INFO: Exec stderr: ""
    Mar 27 22:23:38.088: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.088: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.089: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 22:23:38.253: INFO: Exec stderr: ""
    Mar 27 22:23:38.253: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.255: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.255: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 22:23:38.391: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/27/23 22:23:38.391
    Mar 27 22:23:38.392: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.393: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.393: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 27 22:23:38.558: INFO: Exec stderr: ""
    Mar 27 22:23:38.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.559: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.559: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 27 22:23:38.728: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/27/23 22:23:38.728
    Mar 27 22:23:38.729: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.729: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.729: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 22:23:38.869: INFO: Exec stderr: ""
    Mar 27 22:23:38.869: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:38.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:38.870: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:38.871: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 27 22:23:39.060: INFO: Exec stderr: ""
    Mar 27 22:23:39.060: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:39.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:39.061: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:39.062: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 22:23:39.248: INFO: Exec stderr: ""
    Mar 27 22:23:39.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 27 22:23:39.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:23:39.249: INFO: ExecWithOptions: Clientset creation
    Mar 27 22:23:39.249: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 27 22:23:39.385: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:23:39.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-690" for this suite. 03/27/23 22:23:39.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:23:39.466
Mar 27 22:23:39.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename services 03/27/23 22:23:39.468
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:39.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:39.532
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9489 03/27/23 22:23:39.545
STEP: creating service affinity-nodeport-transition in namespace services-9489 03/27/23 22:23:39.545
STEP: creating replication controller affinity-nodeport-transition in namespace services-9489 03/27/23 22:23:39.604
I0327 22:23:39.621276      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9489, replica count: 3
I0327 22:23:42.673430      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 27 22:23:42.722: INFO: Creating new exec pod
Mar 27 22:23:42.752: INFO: Waiting up to 5m0s for pod "execpod-affinity7524v" in namespace "services-9489" to be "running"
Mar 27 22:23:42.767: INFO: Pod "execpod-affinity7524v": Phase="Pending", Reason="", readiness=false. Elapsed: 15.48138ms
Mar 27 22:23:44.818: INFO: Pod "execpod-affinity7524v": Phase="Running", Reason="", readiness=true. Elapsed: 2.065940937s
Mar 27 22:23:44.818: INFO: Pod "execpod-affinity7524v" satisfied condition "running"
Mar 27 22:23:45.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Mar 27 22:23:46.133: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 27 22:23:46.133: INFO: stdout: ""
Mar 27 22:23:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 172.21.187.104 80'
Mar 27 22:23:46.415: INFO: stderr: "+ nc -v -z -w 2 172.21.187.104 80\nConnection to 172.21.187.104 80 port [tcp/http] succeeded!\n"
Mar 27 22:23:46.415: INFO: stdout: ""
Mar 27 22:23:46.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 10.176.99.178 30011'
Mar 27 22:23:46.707: INFO: stderr: "+ nc -v -z -w 2 10.176.99.178 30011\nConnection to 10.176.99.178 30011 port [tcp/*] succeeded!\n"
Mar 27 22:23:46.707: INFO: stdout: ""
Mar 27 22:23:46.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 30011'
Mar 27 22:23:47.011: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 30011\nConnection to 10.176.99.177 30011 port [tcp/*] succeeded!\n"
Mar 27 22:23:47.011: INFO: stdout: ""
Mar 27 22:23:47.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
Mar 27 22:23:47.370: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
Mar 27 22:23:47.370: INFO: stdout: "\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt"
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
Mar 27 22:24:17.725: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
Mar 27 22:24:17.725: INFO: stdout: "\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt"
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
Mar 27 22:24:17.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
Mar 27 22:24:18.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
Mar 27 22:24:18.114: INFO: stdout: "\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d"
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
Mar 27 22:24:18.114: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9489, will wait for the garbage collector to delete the pods 03/27/23 22:24:18.166
Mar 27 22:24:18.255: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.56604ms
Mar 27 22:24:18.356: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.046348ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Mar 27 22:24:20.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9489" for this suite. 03/27/23 22:24:20.861
------------------------------
• [SLOW TEST] [41.418 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:23:39.466
    Mar 27 22:23:39.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename services 03/27/23 22:23:39.468
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:23:39.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:23:39.532
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9489 03/27/23 22:23:39.545
    STEP: creating service affinity-nodeport-transition in namespace services-9489 03/27/23 22:23:39.545
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9489 03/27/23 22:23:39.604
    I0327 22:23:39.621276      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9489, replica count: 3
    I0327 22:23:42.673430      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 27 22:23:42.722: INFO: Creating new exec pod
    Mar 27 22:23:42.752: INFO: Waiting up to 5m0s for pod "execpod-affinity7524v" in namespace "services-9489" to be "running"
    Mar 27 22:23:42.767: INFO: Pod "execpod-affinity7524v": Phase="Pending", Reason="", readiness=false. Elapsed: 15.48138ms
    Mar 27 22:23:44.818: INFO: Pod "execpod-affinity7524v": Phase="Running", Reason="", readiness=true. Elapsed: 2.065940937s
    Mar 27 22:23:44.818: INFO: Pod "execpod-affinity7524v" satisfied condition "running"
    Mar 27 22:23:45.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Mar 27 22:23:46.133: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 27 22:23:46.133: INFO: stdout: ""
    Mar 27 22:23:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 172.21.187.104 80'
    Mar 27 22:23:46.415: INFO: stderr: "+ nc -v -z -w 2 172.21.187.104 80\nConnection to 172.21.187.104 80 port [tcp/http] succeeded!\n"
    Mar 27 22:23:46.415: INFO: stdout: ""
    Mar 27 22:23:46.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 10.176.99.178 30011'
    Mar 27 22:23:46.707: INFO: stderr: "+ nc -v -z -w 2 10.176.99.178 30011\nConnection to 10.176.99.178 30011 port [tcp/*] succeeded!\n"
    Mar 27 22:23:46.707: INFO: stdout: ""
    Mar 27 22:23:46.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c nc -v -z -w 2 10.176.99.177 30011'
    Mar 27 22:23:47.011: INFO: stderr: "+ nc -v -z -w 2 10.176.99.177 30011\nConnection to 10.176.99.177 30011 port [tcp/*] succeeded!\n"
    Mar 27 22:23:47.011: INFO: stdout: ""
    Mar 27 22:23:47.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
    Mar 27 22:23:47.370: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
    Mar 27 22:23:47.370: INFO: stdout: "\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt"
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:23:47.370: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
    Mar 27 22:24:17.725: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
    Mar 27 22:24:17.725: INFO: stdout: "\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-qxjk5\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt\naffinity-nodeport-transition-cbhjt"
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-qxjk5
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.726: INFO: Received response from host: affinity-nodeport-transition-cbhjt
    Mar 27 22:24:17.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3231223527 --namespace=services-9489 exec execpod-affinity7524v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.176.99.175:30011/ ; done'
    Mar 27 22:24:18.114: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.176.99.175:30011/\n"
    Mar 27 22:24:18.114: INFO: stdout: "\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d\naffinity-nodeport-transition-zkr9d"
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Received response from host: affinity-nodeport-transition-zkr9d
    Mar 27 22:24:18.114: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9489, will wait for the garbage collector to delete the pods 03/27/23 22:24:18.166
    Mar 27 22:24:18.255: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.56604ms
    Mar 27 22:24:18.356: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.046348ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:24:20.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9489" for this suite. 03/27/23 22:24:20.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:24:20.887
Mar 27 22:24:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 22:24:20.889
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:20.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:20.954
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 27 22:24:21.014: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 27 22:24:26.033: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/27/23 22:24:26.033
Mar 27 22:24:26.034: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 27 22:24:28.055: INFO: Creating deployment "test-rollover-deployment"
Mar 27 22:24:28.092: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 27 22:24:30.125: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 27 22:24:30.160: INFO: Ensure that both replica sets have 1 created replica
Mar 27 22:24:30.195: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 27 22:24:30.233: INFO: Updating deployment test-rollover-deployment
Mar 27 22:24:30.233: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 27 22:24:32.269: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 27 22:24:32.304: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 27 22:24:32.342: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 22:24:32.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:24:34.379: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 22:24:34.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:24:36.378: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 22:24:36.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:24:38.378: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 22:24:38.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:24:40.376: INFO: all replica sets need to contain the pod-template-hash label
Mar 27 22:24:40.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 27 22:24:42.377: INFO: 
Mar 27 22:24:42.377: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 22:24:42.425: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6744  a7fdd337-ad68-4914-8a00-0f9485be2e6f 52564 2 2023-03-27 22:24:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e15db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 22:24:28 +0000 UTC,LastTransitionTime:2023-03-27 22:24:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-27 22:24:41 +0000 UTC,LastTransitionTime:2023-03-27 22:24:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 27 22:24:42.454: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6744  f72739f6-0d5c-470f-8b2b-8eff0a94b1f6 52554 2 2023-03-27 22:24:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4337 0xc004cc4338}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc43e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:24:42.454: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 27 22:24:42.454: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6744  23126c32-6644-47db-b027-c4c48401df72 52563 2 2023-03-27 22:24:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4207 0xc004cc4208}] [] [{e2e.test Update apps/v1 2023-03-27 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004cc42c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:24:42.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6744  1f82b3b5-60b1-45f3-981f-07df0d5de79a 52524 2 2023-03-27 22:24:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4457 0xc004cc4458}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc4508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:24:42.471: INFO: Pod "test-rollover-deployment-6c6df9974f-24k2r" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-24k2r test-rollover-deployment-6c6df9974f- deployment-6744  8bfcf5f2-2a87-4090-9734-f65b022ec4ae 52537 0 2023-03-27 22:24:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:6bdfeb5a004a9b16513c47c5cf23b83639b38ee6540355c1a71d4efa8c89737a cni.projectcalico.org/podIP:172.30.85.137/32 cni.projectcalico.org/podIPs:172.30.85.137/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f f72739f6-0d5c-470f-8b2b-8eff0a94b1f6 0xc004cc4a77 0xc004cc4a78}] [] [{kube-controller-manager Update v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f72739f6-0d5c-470f-8b2b-8eff0a94b1f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:24:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grm65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grm65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.137,StartTime:2023-03-27 22:24:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://523f4c6d8a7ec2f1a866e92dc0242a0f0aeec4b077b48baa56889d78c8a054f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 22:24:42.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6744" for this suite. 03/27/23 22:24:42.493
------------------------------
• [SLOW TEST] [21.631 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:24:20.887
    Mar 27 22:24:20.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 22:24:20.889
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:20.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:20.954
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 27 22:24:21.014: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 27 22:24:26.033: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/27/23 22:24:26.033
    Mar 27 22:24:26.034: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 27 22:24:28.055: INFO: Creating deployment "test-rollover-deployment"
    Mar 27 22:24:28.092: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 27 22:24:30.125: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 27 22:24:30.160: INFO: Ensure that both replica sets have 1 created replica
    Mar 27 22:24:30.195: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 27 22:24:30.233: INFO: Updating deployment test-rollover-deployment
    Mar 27 22:24:30.233: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 27 22:24:32.269: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 27 22:24:32.304: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 27 22:24:32.342: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 22:24:32.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:24:34.379: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 22:24:34.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:24:36.378: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 22:24:36.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:24:38.378: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 22:24:38.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:24:40.376: INFO: all replica sets need to contain the pod-template-hash label
    Mar 27 22:24:40.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 27, 22, 24, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 27, 22, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 27 22:24:42.377: INFO: 
    Mar 27 22:24:42.377: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 22:24:42.425: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6744  a7fdd337-ad68-4914-8a00-0f9485be2e6f 52564 2 2023-03-27 22:24:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e15db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-27 22:24:28 +0000 UTC,LastTransitionTime:2023-03-27 22:24:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-03-27 22:24:41 +0000 UTC,LastTransitionTime:2023-03-27 22:24:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 27 22:24:42.454: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6744  f72739f6-0d5c-470f-8b2b-8eff0a94b1f6 52554 2 2023-03-27 22:24:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4337 0xc004cc4338}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc43e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:24:42.454: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 27 22:24:42.454: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6744  23126c32-6644-47db-b027-c4c48401df72 52563 2 2023-03-27 22:24:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4207 0xc004cc4208}] [] [{e2e.test Update apps/v1 2023-03-27 22:24:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004cc42c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:24:42.455: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6744  1f82b3b5-60b1-45f3-981f-07df0d5de79a 52524 2 2023-03-27 22:24:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a7fdd337-ad68-4914-8a00-0f9485be2e6f 0xc004cc4457 0xc004cc4458}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7fdd337-ad68-4914-8a00-0f9485be2e6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cc4508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:24:42.471: INFO: Pod "test-rollover-deployment-6c6df9974f-24k2r" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-24k2r test-rollover-deployment-6c6df9974f- deployment-6744  8bfcf5f2-2a87-4090-9734-f65b022ec4ae 52537 0 2023-03-27 22:24:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:6bdfeb5a004a9b16513c47c5cf23b83639b38ee6540355c1a71d4efa8c89737a cni.projectcalico.org/podIP:172.30.85.137/32 cni.projectcalico.org/podIPs:172.30.85.137/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f f72739f6-0d5c-470f-8b2b-8eff0a94b1f6 0xc004cc4a77 0xc004cc4a78}] [] [{kube-controller-manager Update v1 2023-03-27 22:24:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f72739f6-0d5c-470f-8b2b-8eff0a94b1f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-03-27 22:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-27 22:24:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.85.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grm65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grm65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:24:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:172.30.85.137,StartTime:2023-03-27 22:24:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-27 22:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://523f4c6d8a7ec2f1a866e92dc0242a0f0aeec4b077b48baa56889d78c8a054f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.85.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:24:42.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6744" for this suite. 03/27/23 22:24:42.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:24:42.532
Mar 27 22:24:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename secrets 03/27/23 22:24:42.533
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:42.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:42.598
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-11dee1e6-8857-4b16-bc40-d1b8cbf6f7fa 03/27/23 22:24:42.676
STEP: Creating a pod to test consume secrets 03/27/23 22:24:42.694
Mar 27 22:24:42.728: INFO: Waiting up to 5m0s for pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af" in namespace "secrets-1941" to be "Succeeded or Failed"
Mar 27 22:24:42.745: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Pending", Reason="", readiness=false. Elapsed: 17.20871ms
Mar 27 22:24:44.768: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040753633s
Mar 27 22:24:46.763: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035380252s
STEP: Saw pod success 03/27/23 22:24:46.763
Mar 27 22:24:46.763: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af" satisfied condition "Succeeded or Failed"
Mar 27 22:24:46.779: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af container secret-volume-test: <nil>
STEP: delete the pod 03/27/23 22:24:46.812
Mar 27 22:24:46.863: INFO: Waiting for pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af to disappear
Mar 27 22:24:46.878: INFO: Pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Mar 27 22:24:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1941" for this suite. 03/27/23 22:24:46.9
STEP: Destroying namespace "secret-namespace-4483" for this suite. 03/27/23 22:24:46.926
------------------------------
• [4.418 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:24:42.532
    Mar 27 22:24:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename secrets 03/27/23 22:24:42.533
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:42.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:42.598
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-11dee1e6-8857-4b16-bc40-d1b8cbf6f7fa 03/27/23 22:24:42.676
    STEP: Creating a pod to test consume secrets 03/27/23 22:24:42.694
    Mar 27 22:24:42.728: INFO: Waiting up to 5m0s for pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af" in namespace "secrets-1941" to be "Succeeded or Failed"
    Mar 27 22:24:42.745: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Pending", Reason="", readiness=false. Elapsed: 17.20871ms
    Mar 27 22:24:44.768: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040753633s
    Mar 27 22:24:46.763: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035380252s
    STEP: Saw pod success 03/27/23 22:24:46.763
    Mar 27 22:24:46.763: INFO: Pod "pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af" satisfied condition "Succeeded or Failed"
    Mar 27 22:24:46.779: INFO: Trying to get logs from node 10.176.99.177 pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af container secret-volume-test: <nil>
    STEP: delete the pod 03/27/23 22:24:46.812
    Mar 27 22:24:46.863: INFO: Waiting for pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af to disappear
    Mar 27 22:24:46.878: INFO: Pod pod-secrets-3cafa8e5-8767-41cb-9447-93cb2b0c91af no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:24:46.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1941" for this suite. 03/27/23 22:24:46.9
    STEP: Destroying namespace "secret-namespace-4483" for this suite. 03/27/23 22:24:46.926
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:24:46.956
Mar 27 22:24:46.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:24:46.957
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:47.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:47.02
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/27/23 22:24:47.034
Mar 27 22:24:47.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/27/23 22:24:54.948
Mar 27 22:24:54.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
Mar 27 22:24:57.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:25:06.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5167" for this suite. 03/27/23 22:25:06.131
------------------------------
• [SLOW TEST] [19.200 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:24:46.956
    Mar 27 22:24:46.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:24:46.957
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:24:47.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:24:47.02
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/27/23 22:24:47.034
    Mar 27 22:24:47.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/27/23 22:24:54.948
    Mar 27 22:24:54.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    Mar 27 22:24:57.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:25:06.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5167" for this suite. 03/27/23 22:25:06.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:25:06.157
Mar 27 22:25:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename webhook 03/27/23 22:25:06.158
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:06.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:06.204
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 03/27/23 22:25:06.255
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:25:07.041
STEP: Deploying the webhook pod 03/27/23 22:25:07.067
STEP: Wait for the deployment to be ready 03/27/23 22:25:07.101
Mar 27 22:25:07.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/27/23 22:25:09.176
STEP: Verifying the service has paired with the endpoint 03/27/23 22:25:09.196
Mar 27 22:25:10.197: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 22:25:10.205
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 22:25:10.257
STEP: Creating a dummy validating-webhook-configuration object 03/27/23 22:25:10.302
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/27/23 22:25:10.323
STEP: Creating a dummy mutating-webhook-configuration object 03/27/23 22:25:10.336
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/27/23 22:25:10.361
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:25:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4561" for this suite. 03/27/23 22:25:10.555
STEP: Destroying namespace "webhook-4561-markers" for this suite. 03/27/23 22:25:10.586
------------------------------
• [4.461 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:25:06.157
    Mar 27 22:25:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename webhook 03/27/23 22:25:06.158
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:06.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:06.204
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 03/27/23 22:25:06.255
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/27/23 22:25:07.041
    STEP: Deploying the webhook pod 03/27/23 22:25:07.067
    STEP: Wait for the deployment to be ready 03/27/23 22:25:07.101
    Mar 27 22:25:07.130: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/27/23 22:25:09.176
    STEP: Verifying the service has paired with the endpoint 03/27/23 22:25:09.196
    Mar 27 22:25:10.197: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 22:25:10.205
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/27/23 22:25:10.257
    STEP: Creating a dummy validating-webhook-configuration object 03/27/23 22:25:10.302
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/27/23 22:25:10.323
    STEP: Creating a dummy mutating-webhook-configuration object 03/27/23 22:25:10.336
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/27/23 22:25:10.361
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:25:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4561" for this suite. 03/27/23 22:25:10.555
    STEP: Destroying namespace "webhook-4561-markers" for this suite. 03/27/23 22:25:10.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:25:10.619
Mar 27 22:25:10.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:25:10.622
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:10.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:10.667
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 03/27/23 22:25:10.675
Mar 27 22:25:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: mark a version not serverd 03/27/23 22:25:15.6
STEP: check the unserved version gets removed 03/27/23 22:25:15.668
STEP: check the other version is not changed 03/27/23 22:25:17.984
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Mar 27 22:25:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5622" for this suite. 03/27/23 22:25:21.957
------------------------------
• [SLOW TEST] [11.370 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:25:10.619
    Mar 27 22:25:10.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename crd-publish-openapi 03/27/23 22:25:10.622
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:10.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:10.667
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 03/27/23 22:25:10.675
    Mar 27 22:25:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: mark a version not serverd 03/27/23 22:25:15.6
    STEP: check the unserved version gets removed 03/27/23 22:25:15.668
    STEP: check the other version is not changed 03/27/23 22:25:17.984
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:25:21.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5622" for this suite. 03/27/23 22:25:21.957
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 03/27/23 22:25:21.991
Mar 27 22:25:21.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
STEP: Building a namespace api object, basename deployment 03/27/23 22:25:21.992
STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:22.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:22.073
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 27 22:25:22.090: INFO: Creating deployment "test-recreate-deployment"
Mar 27 22:25:22.106: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 27 22:25:22.139: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 27 22:25:24.184: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 27 22:25:24.199: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 27 22:25:24.235: INFO: Updating deployment test-recreate-deployment
Mar 27 22:25:24.235: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 27 22:25:24.415: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2110  b48f2617-0fb9-4ced-b961-baa699169b7b 52875 2 2023-03-27 22:25:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea69e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 22:25:24 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-27 22:25:24 +0000 UTC,LastTransitionTime:2023-03-27 22:25:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 27 22:25:24.427: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2110  1c23b06f-4500-482c-86a5-89034f180538 52873 1 2023-03-27 22:25:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b48f2617-0fb9-4ced-b961-baa699169b7b 0xc001ea6fa0 0xc001ea6fa1}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f2617-0fb9-4ced-b961-baa699169b7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea7108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:25:24.427: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 27 22:25:24.427: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2110  8ba36485-9025-4d2a-8260-85110d1d8812 52864 2 2023-03-27 22:25:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b48f2617-0fb9-4ced-b961-baa699169b7b 0xc001ea6da7 0xc001ea6da8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f2617-0fb9-4ced-b961-baa699169b7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 27 22:25:24.442: INFO: Pod "test-recreate-deployment-cff6dc657-4pkvp" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4pkvp test-recreate-deployment-cff6dc657- deployment-2110  4734c88e-dc3a-441a-9253-b6c2a3194d31 52876 0 2023-03-27 22:25:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1c23b06f-4500-482c-86a5-89034f180538 0xc0016a4ce0 0xc0016a4ce1}] [] [{kube-controller-manager Update v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c23b06f-4500-482c-86a5-89034f180538\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7sjxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7sjxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:25:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Mar 27 22:25:24.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2110" for this suite. 03/27/23 22:25:24.459
------------------------------
• [2.488 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 03/27/23 22:25:21.991
    Mar 27 22:25:21.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3231223527
    STEP: Building a namespace api object, basename deployment 03/27/23 22:25:21.992
    STEP: Waiting for a default service account to be provisioned in namespace 03/27/23 22:25:22.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/27/23 22:25:22.073
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 27 22:25:22.090: INFO: Creating deployment "test-recreate-deployment"
    Mar 27 22:25:22.106: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 27 22:25:22.139: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 27 22:25:24.184: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 27 22:25:24.199: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 27 22:25:24.235: INFO: Updating deployment test-recreate-deployment
    Mar 27 22:25:24.235: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 27 22:25:24.415: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2110  b48f2617-0fb9-4ced-b961-baa699169b7b 52875 2 2023-03-27 22:25:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea69e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-27 22:25:24 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-03-27 22:25:24 +0000 UTC,LastTransitionTime:2023-03-27 22:25:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 27 22:25:24.427: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2110  1c23b06f-4500-482c-86a5-89034f180538 52873 1 2023-03-27 22:25:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b48f2617-0fb9-4ced-b961-baa699169b7b 0xc001ea6fa0 0xc001ea6fa1}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f2617-0fb9-4ced-b961-baa699169b7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea7108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:25:24.427: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 27 22:25:24.427: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2110  8ba36485-9025-4d2a-8260-85110d1d8812 52864 2 2023-03-27 22:25:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b48f2617-0fb9-4ced-b961-baa699169b7b 0xc001ea6da7 0xc001ea6da8}] [] [{kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b48f2617-0fb9-4ced-b961-baa699169b7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ea6e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 27 22:25:24.442: INFO: Pod "test-recreate-deployment-cff6dc657-4pkvp" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-4pkvp test-recreate-deployment-cff6dc657- deployment-2110  4734c88e-dc3a-441a-9253-b6c2a3194d31 52876 0 2023-03-27 22:25:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1c23b06f-4500-482c-86a5-89034f180538 0xc0016a4ce0 0xc0016a4ce1}] [] [{kube-controller-manager Update v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c23b06f-4500-482c-86a5-89034f180538\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-27 22:25:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7sjxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7sjxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.176.99.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-27 22:25:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.176.99.177,PodIP:,StartTime:2023-03-27 22:25:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Mar 27 22:25:24.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2110" for this suite. 03/27/23 22:25:24.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Mar 27 22:25:24.482: INFO: Running AfterSuite actions on node 1
Mar 27 22:25:24.482: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Mar 27 22:25:24.482: INFO: Running AfterSuite actions on node 1
    Mar 27 22:25:24.482: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.138 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5739.602 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h35m40.251901061s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

