I0130 22:43:51.719421      23 e2e.go:126] Starting e2e run "55d643dd-b598-41d5-b9a7-a15abad2b29e" on Ginkgo node 1
Jan 30 22:43:51.749: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1675118631 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 30 22:43:51.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
E0130 22:43:51.940946      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 30 22:43:51.942: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 30 22:43:51.995: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 30 22:43:52.158: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 30 22:43:52.158: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
Jan 30 22:43:52.158: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jan 30 22:43:52.194: INFO: e2e test version: v1.26.1
Jan 30 22:43:52.200: INFO: kube-apiserver version: v1.26.1+IKS
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 30 22:43:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:43:52.219: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.280 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 30 22:43:51.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    E0130 22:43:51.940946      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 30 22:43:51.942: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 30 22:43:51.995: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 30 22:43:52.158: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 30 22:43:52.158: INFO: expected 21 pod replicas in namespace 'kube-system', 21 are Running and Ready.
    Jan 30 22:43:52.158: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
    Jan 30 22:43:52.194: INFO: e2e test version: v1.26.1
    Jan 30 22:43:52.200: INFO: kube-apiserver version: v1.26.1+IKS
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 30 22:43:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:43:52.219: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:43:52.268
Jan 30 22:43:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/30/23 22:43:52.269
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:43:52.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:43:52.332
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/30/23 22:43:52.346
Jan 30 22:43:52.425: INFO: Waiting up to 5m0s for pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0" in namespace "var-expansion-161" to be "Succeeded or Failed"
Jan 30 22:43:52.438: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.10081ms
Jan 30 22:43:54.454: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029402876s
Jan 30 22:43:56.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030307026s
Jan 30 22:43:58.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030146765s
Jan 30 22:44:00.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030271748s
STEP: Saw pod success 01/30/23 22:44:00.456
Jan 30 22:44:00.456: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0" satisfied condition "Succeeded or Failed"
Jan 30 22:44:00.470: INFO: Trying to get logs from node 10.15.28.237 pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 container dapi-container: <nil>
STEP: delete the pod 01/30/23 22:44:00.592
Jan 30 22:44:00.656: INFO: Waiting for pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 to disappear
Jan 30 22:44:00.669: INFO: Pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:00.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-161" for this suite. 01/30/23 22:44:00.692
------------------------------
• [SLOW TEST] [8.459 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:43:52.268
    Jan 30 22:43:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/30/23 22:43:52.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:43:52.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:43:52.332
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/30/23 22:43:52.346
    Jan 30 22:43:52.425: INFO: Waiting up to 5m0s for pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0" in namespace "var-expansion-161" to be "Succeeded or Failed"
    Jan 30 22:43:52.438: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.10081ms
    Jan 30 22:43:54.454: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029402876s
    Jan 30 22:43:56.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030307026s
    Jan 30 22:43:58.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030146765s
    Jan 30 22:44:00.455: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.030271748s
    STEP: Saw pod success 01/30/23 22:44:00.456
    Jan 30 22:44:00.456: INFO: Pod "var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0" satisfied condition "Succeeded or Failed"
    Jan 30 22:44:00.470: INFO: Trying to get logs from node 10.15.28.237 pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 22:44:00.592
    Jan 30 22:44:00.656: INFO: Waiting for pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 to disappear
    Jan 30 22:44:00.669: INFO: Pod var-expansion-f8d5500b-7e88-4196-bb8a-2f31a0e280c0 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:00.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-161" for this suite. 01/30/23 22:44:00.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:00.743
Jan 30 22:44:00.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/30/23 22:44:00.753
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:00.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:00.827
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3348" 01/30/23 22:44:00.841
Jan 30 22:44:00.906: INFO: Namespace "namespaces-3348" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"55d643dd-b598-41d5-b9a7-a15abad2b29e", "kubernetes.io/metadata.name":"namespaces-3348", "namespaces-3348":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:00.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3348" for this suite. 01/30/23 22:44:00.929
------------------------------
• [0.217 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:00.743
    Jan 30 22:44:00.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/30/23 22:44:00.753
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:00.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:00.827
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3348" 01/30/23 22:44:00.841
    Jan 30 22:44:00.906: INFO: Namespace "namespaces-3348" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"55d643dd-b598-41d5-b9a7-a15abad2b29e", "kubernetes.io/metadata.name":"namespaces-3348", "namespaces-3348":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:00.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3348" for this suite. 01/30/23 22:44:00.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:00.971
Jan 30 22:44:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:44:00.973
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:01.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:01.05
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/30/23 22:44:01.062
Jan 30 22:44:01.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 create -f -'
Jan 30 22:44:01.595: INFO: stderr: ""
Jan 30 22:44:01.595: INFO: stdout: "pod/pause created\n"
Jan 30 22:44:01.595: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 30 22:44:01.595: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8062" to be "running and ready"
Jan 30 22:44:01.620: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.903884ms
Jan 30 22:44:01.620: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.28.227' to be 'Running' but was 'Pending'
Jan 30 22:44:03.637: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041952096s
Jan 30 22:44:03.637: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.28.227' to be 'Running' but was 'Pending'
Jan 30 22:44:05.636: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.040830619s
Jan 30 22:44:05.636: INFO: Pod "pause" satisfied condition "running and ready"
Jan 30 22:44:05.636: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/30/23 22:44:05.636
Jan 30 22:44:05.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 label pods pause testing-label=testing-label-value'
Jan 30 22:44:05.827: INFO: stderr: ""
Jan 30 22:44:05.827: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/30/23 22:44:05.827
Jan 30 22:44:05.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pod pause -L testing-label'
Jan 30 22:44:05.958: INFO: stderr: ""
Jan 30 22:44:05.958: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/30/23 22:44:05.958
Jan 30 22:44:05.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 label pods pause testing-label-'
Jan 30 22:44:06.169: INFO: stderr: ""
Jan 30 22:44:06.169: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/30/23 22:44:06.169
Jan 30 22:44:06.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pod pause -L testing-label'
Jan 30 22:44:06.348: INFO: stderr: ""
Jan 30 22:44:06.348: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/30/23 22:44:06.348
Jan 30 22:44:06.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 delete --grace-period=0 --force -f -'
Jan 30 22:44:06.557: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 22:44:06.558: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 30 22:44:06.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get rc,svc -l name=pause --no-headers'
Jan 30 22:44:06.717: INFO: stderr: "No resources found in kubectl-8062 namespace.\n"
Jan 30 22:44:06.717: INFO: stdout: ""
Jan 30 22:44:06.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 22:44:06.847: INFO: stderr: ""
Jan 30 22:44:06.847: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:06.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8062" for this suite. 01/30/23 22:44:06.896
------------------------------
• [SLOW TEST] [5.948 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:00.971
    Jan 30 22:44:00.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:44:00.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:01.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:01.05
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/30/23 22:44:01.062
    Jan 30 22:44:01.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 create -f -'
    Jan 30 22:44:01.595: INFO: stderr: ""
    Jan 30 22:44:01.595: INFO: stdout: "pod/pause created\n"
    Jan 30 22:44:01.595: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 30 22:44:01.595: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8062" to be "running and ready"
    Jan 30 22:44:01.620: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.903884ms
    Jan 30 22:44:01.620: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.28.227' to be 'Running' but was 'Pending'
    Jan 30 22:44:03.637: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041952096s
    Jan 30 22:44:03.637: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.15.28.227' to be 'Running' but was 'Pending'
    Jan 30 22:44:05.636: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.040830619s
    Jan 30 22:44:05.636: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 30 22:44:05.636: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/30/23 22:44:05.636
    Jan 30 22:44:05.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 label pods pause testing-label=testing-label-value'
    Jan 30 22:44:05.827: INFO: stderr: ""
    Jan 30 22:44:05.827: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/30/23 22:44:05.827
    Jan 30 22:44:05.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pod pause -L testing-label'
    Jan 30 22:44:05.958: INFO: stderr: ""
    Jan 30 22:44:05.958: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/30/23 22:44:05.958
    Jan 30 22:44:05.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 label pods pause testing-label-'
    Jan 30 22:44:06.169: INFO: stderr: ""
    Jan 30 22:44:06.169: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/30/23 22:44:06.169
    Jan 30 22:44:06.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pod pause -L testing-label'
    Jan 30 22:44:06.348: INFO: stderr: ""
    Jan 30 22:44:06.348: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/30/23 22:44:06.348
    Jan 30 22:44:06.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 delete --grace-period=0 --force -f -'
    Jan 30 22:44:06.557: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 22:44:06.558: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 30 22:44:06.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get rc,svc -l name=pause --no-headers'
    Jan 30 22:44:06.717: INFO: stderr: "No resources found in kubectl-8062 namespace.\n"
    Jan 30 22:44:06.717: INFO: stdout: ""
    Jan 30 22:44:06.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8062 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 22:44:06.847: INFO: stderr: ""
    Jan 30 22:44:06.847: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:06.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8062" for this suite. 01/30/23 22:44:06.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:06.928
Jan 30 22:44:06.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 22:44:06.93
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:06.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:07.008
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/30/23 22:44:07.021
Jan 30 22:44:07.049: INFO: Waiting up to 5m0s for pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900" in namespace "emptydir-2002" to be "Succeeded or Failed"
Jan 30 22:44:07.064: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 15.329745ms
Jan 30 22:44:09.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03087388s
Jan 30 22:44:11.079: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030591208s
Jan 30 22:44:13.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031234747s
Jan 30 22:44:15.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030771361s
Jan 30 22:44:17.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031532242s
Jan 30 22:44:19.111: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 12.062344694s
Jan 30 22:44:21.082: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.033139895s
STEP: Saw pod success 01/30/23 22:44:21.082
Jan 30 22:44:21.082: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900" satisfied condition "Succeeded or Failed"
Jan 30 22:44:21.098: INFO: Trying to get logs from node 10.15.28.227 pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 container test-container: <nil>
STEP: delete the pod 01/30/23 22:44:21.195
Jan 30 22:44:21.232: INFO: Waiting for pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 to disappear
Jan 30 22:44:21.247: INFO: Pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2002" for this suite. 01/30/23 22:44:21.266
------------------------------
• [SLOW TEST] [14.362 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:06.928
    Jan 30 22:44:06.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 22:44:06.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:06.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:07.008
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/30/23 22:44:07.021
    Jan 30 22:44:07.049: INFO: Waiting up to 5m0s for pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900" in namespace "emptydir-2002" to be "Succeeded or Failed"
    Jan 30 22:44:07.064: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 15.329745ms
    Jan 30 22:44:09.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03087388s
    Jan 30 22:44:11.079: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030591208s
    Jan 30 22:44:13.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031234747s
    Jan 30 22:44:15.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030771361s
    Jan 30 22:44:17.080: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031532242s
    Jan 30 22:44:19.111: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Pending", Reason="", readiness=false. Elapsed: 12.062344694s
    Jan 30 22:44:21.082: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.033139895s
    STEP: Saw pod success 01/30/23 22:44:21.082
    Jan 30 22:44:21.082: INFO: Pod "pod-76889fab-2a3d-467f-8f9d-f84cb62e4900" satisfied condition "Succeeded or Failed"
    Jan 30 22:44:21.098: INFO: Trying to get logs from node 10.15.28.227 pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 container test-container: <nil>
    STEP: delete the pod 01/30/23 22:44:21.195
    Jan 30 22:44:21.232: INFO: Waiting for pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 to disappear
    Jan 30 22:44:21.247: INFO: Pod pod-76889fab-2a3d-467f-8f9d-f84cb62e4900 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2002" for this suite. 01/30/23 22:44:21.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:21.305
Jan 30 22:44:21.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename cronjob 01/30/23 22:44:21.307
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:21.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:21.375
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/30/23 22:44:21.388
STEP: creating 01/30/23 22:44:21.389
STEP: getting 01/30/23 22:44:21.435
STEP: listing 01/30/23 22:44:21.45
STEP: watching 01/30/23 22:44:21.467
Jan 30 22:44:21.467: INFO: starting watch
STEP: cluster-wide listing 01/30/23 22:44:21.472
STEP: cluster-wide watching 01/30/23 22:44:21.487
Jan 30 22:44:21.488: INFO: starting watch
STEP: patching 01/30/23 22:44:21.494
STEP: updating 01/30/23 22:44:21.538
Jan 30 22:44:21.577: INFO: waiting for watch events with expected annotations
Jan 30 22:44:21.577: INFO: saw patched and updated annotations
STEP: patching /status 01/30/23 22:44:21.577
STEP: updating /status 01/30/23 22:44:21.597
STEP: get /status 01/30/23 22:44:21.63
STEP: deleting 01/30/23 22:44:21.646
STEP: deleting a collection 01/30/23 22:44:21.704
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:21.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3063" for this suite. 01/30/23 22:44:21.827
------------------------------
• [0.572 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:21.305
    Jan 30 22:44:21.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename cronjob 01/30/23 22:44:21.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:21.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:21.375
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/30/23 22:44:21.388
    STEP: creating 01/30/23 22:44:21.389
    STEP: getting 01/30/23 22:44:21.435
    STEP: listing 01/30/23 22:44:21.45
    STEP: watching 01/30/23 22:44:21.467
    Jan 30 22:44:21.467: INFO: starting watch
    STEP: cluster-wide listing 01/30/23 22:44:21.472
    STEP: cluster-wide watching 01/30/23 22:44:21.487
    Jan 30 22:44:21.488: INFO: starting watch
    STEP: patching 01/30/23 22:44:21.494
    STEP: updating 01/30/23 22:44:21.538
    Jan 30 22:44:21.577: INFO: waiting for watch events with expected annotations
    Jan 30 22:44:21.577: INFO: saw patched and updated annotations
    STEP: patching /status 01/30/23 22:44:21.577
    STEP: updating /status 01/30/23 22:44:21.597
    STEP: get /status 01/30/23 22:44:21.63
    STEP: deleting 01/30/23 22:44:21.646
    STEP: deleting a collection 01/30/23 22:44:21.704
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:21.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3063" for this suite. 01/30/23 22:44:21.827
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:21.881
Jan 30 22:44:21.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 22:44:21.883
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:21.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:21.958
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-fb5e3c36-7aef-4754-8177-63e8767b0a6d 01/30/23 22:44:21.973
STEP: Creating a pod to test consume configMaps 01/30/23 22:44:21.995
Jan 30 22:44:22.066: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca" in namespace "projected-9658" to be "Succeeded or Failed"
Jan 30 22:44:22.081: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 14.907855ms
Jan 30 22:44:24.096: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029674792s
Jan 30 22:44:26.098: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031210324s
Jan 30 22:44:28.096: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029695907s
Jan 30 22:44:30.104: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038015405s
Jan 30 22:44:32.099: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Running", Reason="", readiness=true. Elapsed: 10.032991333s
Jan 30 22:44:34.103: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Running", Reason="", readiness=false. Elapsed: 12.036164481s
Jan 30 22:44:36.097: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.030463044s
STEP: Saw pod success 01/30/23 22:44:36.097
Jan 30 22:44:36.097: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca" satisfied condition "Succeeded or Failed"
Jan 30 22:44:36.114: INFO: Trying to get logs from node 10.15.28.237 pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/30/23 22:44:36.155
Jan 30 22:44:36.204: INFO: Waiting for pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca to disappear
Jan 30 22:44:36.225: INFO: Pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:44:36.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9658" for this suite. 01/30/23 22:44:36.249
------------------------------
• [SLOW TEST] [14.397 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:21.881
    Jan 30 22:44:21.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 22:44:21.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:21.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:21.958
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-fb5e3c36-7aef-4754-8177-63e8767b0a6d 01/30/23 22:44:21.973
    STEP: Creating a pod to test consume configMaps 01/30/23 22:44:21.995
    Jan 30 22:44:22.066: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca" in namespace "projected-9658" to be "Succeeded or Failed"
    Jan 30 22:44:22.081: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 14.907855ms
    Jan 30 22:44:24.096: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029674792s
    Jan 30 22:44:26.098: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031210324s
    Jan 30 22:44:28.096: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029695907s
    Jan 30 22:44:30.104: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038015405s
    Jan 30 22:44:32.099: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Running", Reason="", readiness=true. Elapsed: 10.032991333s
    Jan 30 22:44:34.103: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Running", Reason="", readiness=false. Elapsed: 12.036164481s
    Jan 30 22:44:36.097: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.030463044s
    STEP: Saw pod success 01/30/23 22:44:36.097
    Jan 30 22:44:36.097: INFO: Pod "pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca" satisfied condition "Succeeded or Failed"
    Jan 30 22:44:36.114: INFO: Trying to get logs from node 10.15.28.237 pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/30/23 22:44:36.155
    Jan 30 22:44:36.204: INFO: Waiting for pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca to disappear
    Jan 30 22:44:36.225: INFO: Pod pod-projected-configmaps-cd4343f1-3c1c-47b1-880e-89e93d6c3aca no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:44:36.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9658" for this suite. 01/30/23 22:44:36.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:44:36.292
Jan 30 22:44:36.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pod-network-test 01/30/23 22:44:36.294
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:36.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:36.398
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-2415 01/30/23 22:44:36.412
STEP: creating a selector 01/30/23 22:44:36.413
STEP: Creating the service pods in kubernetes 01/30/23 22:44:36.414
Jan 30 22:44:36.414: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 22:44:36.530: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2415" to be "running and ready"
Jan 30 22:44:36.565: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.512601ms
Jan 30 22:44:36.565: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:44:38.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052487035s
Jan 30 22:44:38.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:44:40.584: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053028346s
Jan 30 22:44:40.584: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:44:42.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052300897s
Jan 30 22:44:42.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:44:44.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.051240677s
Jan 30 22:44:44.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:46.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.061124887s
Jan 30 22:44:46.592: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:48.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050913773s
Jan 30 22:44:48.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:50.583: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052003276s
Jan 30 22:44:50.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:52.584: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.052882582s
Jan 30 22:44:52.584: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:54.585: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05391452s
Jan 30 22:44:54.585: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:56.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.049512465s
Jan 30 22:44:56.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:44:58.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.049659384s
Jan 30 22:44:58.581: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 22:44:58.581: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 22:44:58.595: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2415" to be "running and ready"
Jan 30 22:44:58.610: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.891489ms
Jan 30 22:44:58.610: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 22:44:58.610: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 30 22:44:58.623: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2415" to be "running and ready"
Jan 30 22:44:58.636: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.251665ms
Jan 30 22:44:58.636: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 30 22:44:58.636: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 22:44:58.65
Jan 30 22:44:58.670: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2415" to be "running"
Jan 30 22:44:58.683: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.897079ms
Jan 30 22:45:00.700: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029572266s
Jan 30 22:45:00.700: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 22:45:00.718: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 30 22:45:00.718: INFO: Breadth first check of 172.30.237.177 on host 10.15.28.225...
Jan 30 22:45:00.734: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.237.177&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:45:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:45:00.736: INFO: ExecWithOptions: Clientset creation
Jan 30 22:45:00.736: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.237.177%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 22:45:01.003: INFO: Waiting for responses: map[]
Jan 30 22:45:01.003: INFO: reached 172.30.237.177 after 0/1 tries
Jan 30 22:45:01.003: INFO: Breadth first check of 172.30.199.26 on host 10.15.28.227...
Jan 30 22:45:01.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.199.26&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:45:01.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:45:01.018: INFO: ExecWithOptions: Clientset creation
Jan 30 22:45:01.019: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.199.26%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 22:45:01.325: INFO: Waiting for responses: map[]
Jan 30 22:45:01.325: INFO: reached 172.30.199.26 after 0/1 tries
Jan 30 22:45:01.325: INFO: Breadth first check of 172.30.248.47 on host 10.15.28.237...
Jan 30 22:45:01.384: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.248.47&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:45:01.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:45:01.385: INFO: ExecWithOptions: Clientset creation
Jan 30 22:45:01.385: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.248.47%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 30 22:45:01.626: INFO: Waiting for responses: map[]
Jan 30 22:45:01.626: INFO: reached 172.30.248.47 after 0/1 tries
Jan 30 22:45:01.626: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2415" for this suite. 01/30/23 22:45:01.645
------------------------------
• [SLOW TEST] [25.377 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:44:36.292
    Jan 30 22:44:36.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 22:44:36.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:44:36.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:44:36.398
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-2415 01/30/23 22:44:36.412
    STEP: creating a selector 01/30/23 22:44:36.413
    STEP: Creating the service pods in kubernetes 01/30/23 22:44:36.414
    Jan 30 22:44:36.414: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 22:44:36.530: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2415" to be "running and ready"
    Jan 30 22:44:36.565: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.512601ms
    Jan 30 22:44:36.565: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:44:38.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052487035s
    Jan 30 22:44:38.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:44:40.584: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053028346s
    Jan 30 22:44:40.584: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:44:42.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052300897s
    Jan 30 22:44:42.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:44:44.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.051240677s
    Jan 30 22:44:44.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:46.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.061124887s
    Jan 30 22:44:46.592: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:48.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.050913773s
    Jan 30 22:44:48.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:50.583: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052003276s
    Jan 30 22:44:50.583: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:52.584: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.052882582s
    Jan 30 22:44:52.584: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:54.585: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05391452s
    Jan 30 22:44:54.585: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:56.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.049512465s
    Jan 30 22:44:56.580: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:44:58.580: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.049659384s
    Jan 30 22:44:58.581: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 22:44:58.581: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 22:44:58.595: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2415" to be "running and ready"
    Jan 30 22:44:58.610: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.891489ms
    Jan 30 22:44:58.610: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 22:44:58.610: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 30 22:44:58.623: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2415" to be "running and ready"
    Jan 30 22:44:58.636: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.251665ms
    Jan 30 22:44:58.636: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 30 22:44:58.636: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 22:44:58.65
    Jan 30 22:44:58.670: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2415" to be "running"
    Jan 30 22:44:58.683: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.897079ms
    Jan 30 22:45:00.700: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.029572266s
    Jan 30 22:45:00.700: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 22:45:00.718: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 30 22:45:00.718: INFO: Breadth first check of 172.30.237.177 on host 10.15.28.225...
    Jan 30 22:45:00.734: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.237.177&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:45:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:45:00.736: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:45:00.736: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.237.177%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 22:45:01.003: INFO: Waiting for responses: map[]
    Jan 30 22:45:01.003: INFO: reached 172.30.237.177 after 0/1 tries
    Jan 30 22:45:01.003: INFO: Breadth first check of 172.30.199.26 on host 10.15.28.227...
    Jan 30 22:45:01.017: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.199.26&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:45:01.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:45:01.018: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:45:01.019: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.199.26%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 22:45:01.325: INFO: Waiting for responses: map[]
    Jan 30 22:45:01.325: INFO: reached 172.30.199.26 after 0/1 tries
    Jan 30 22:45:01.325: INFO: Breadth first check of 172.30.248.47 on host 10.15.28.237...
    Jan 30 22:45:01.384: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.248.48:9080/dial?request=hostname&protocol=http&host=172.30.248.47&port=8083&tries=1'] Namespace:pod-network-test-2415 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:45:01.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:45:01.385: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:45:01.385: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2415/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.248.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.248.47%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 30 22:45:01.626: INFO: Waiting for responses: map[]
    Jan 30 22:45:01.626: INFO: reached 172.30.248.47 after 0/1 tries
    Jan 30 22:45:01.626: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2415" for this suite. 01/30/23 22:45:01.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:01.672
Jan 30 22:45:01.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/30/23 22:45:01.674
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:01.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:01.781
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 30 22:45:01.795: INFO: Creating deployment "test-recreate-deployment"
Jan 30 22:45:01.815: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 30 22:45:01.862: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 30 22:45:03.921: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 30 22:45:03.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 22:45:05.952: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 30 22:45:05.988: INFO: Updating deployment test-recreate-deployment
Jan 30 22:45:05.988: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 22:45:06.247: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1350  f9543b37-fb1e-43c9-85f3-bbb72445d8b4 19357 2 2023-01-30 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 22:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e658c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 22:45:06 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-30 22:45:06 +0000 UTC,LastTransitionTime:2023-01-30 22:45:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 22:45:06.266: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1350  7d4f6c28-bfdf-456e-915f-6419f62f4db3 19354 1 2023-01-30 22:45:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f9543b37-fb1e-43c9-85f3-bbb72445d8b4 0xc00471cdc0 0xc00471cdc1}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9543b37-fb1e-43c9-85f3-bbb72445d8b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471ce58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 22:45:06.266: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 30 22:45:06.267: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1350  9fce4715-6702-450f-8342-130f7a67c601 19346 2 2023-01-30 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f9543b37-fb1e-43c9-85f3-bbb72445d8b4 0xc00471cca7 0xc00471cca8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9543b37-fb1e-43c9-85f3-bbb72445d8b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471cd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 22:45:06.284: INFO: Pod "test-recreate-deployment-cff6dc657-c6gqg" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-c6gqg test-recreate-deployment-cff6dc657- deployment-1350  6e7e4cc7-0ed0-412c-ba5c-013e39818111 19358 0 2023-01-30 22:45:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7d4f6c28-bfdf-456e-915f-6419f62f4db3 0xc00471d2e0 0xc00471d2e1}] [] [{kube-controller-manager Update v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d4f6c28-bfdf-456e-915f-6419f62f4db3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtzs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtzs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-30 22:45:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:06.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1350" for this suite. 01/30/23 22:45:06.303
------------------------------
• [4.654 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:01.672
    Jan 30 22:45:01.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/30/23 22:45:01.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:01.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:01.781
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 30 22:45:01.795: INFO: Creating deployment "test-recreate-deployment"
    Jan 30 22:45:01.815: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 30 22:45:01.862: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 30 22:45:03.921: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 30 22:45:03.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 22:45:05.952: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 30 22:45:05.988: INFO: Updating deployment test-recreate-deployment
    Jan 30 22:45:05.988: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 22:45:06.247: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1350  f9543b37-fb1e-43c9-85f3-bbb72445d8b4 19357 2 2023-01-30 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 22:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e658c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 22:45:06 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-30 22:45:06 +0000 UTC,LastTransitionTime:2023-01-30 22:45:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 30 22:45:06.266: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-1350  7d4f6c28-bfdf-456e-915f-6419f62f4db3 19354 1 2023-01-30 22:45:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f9543b37-fb1e-43c9-85f3-bbb72445d8b4 0xc00471cdc0 0xc00471cdc1}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9543b37-fb1e-43c9-85f3-bbb72445d8b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471ce58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 22:45:06.266: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 30 22:45:06.267: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-1350  9fce4715-6702-450f-8342-130f7a67c601 19346 2 2023-01-30 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f9543b37-fb1e-43c9-85f3-bbb72445d8b4 0xc00471cca7 0xc00471cca8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9543b37-fb1e-43c9-85f3-bbb72445d8b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00471cd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 22:45:06.284: INFO: Pod "test-recreate-deployment-cff6dc657-c6gqg" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-c6gqg test-recreate-deployment-cff6dc657- deployment-1350  6e7e4cc7-0ed0-412c-ba5c-013e39818111 19358 0 2023-01-30 22:45:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7d4f6c28-bfdf-456e-915f-6419f62f4db3 0xc00471d2e0 0xc00471d2e1}] [] [{kube-controller-manager Update v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d4f6c28-bfdf-456e-915f-6419f62f4db3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 22:45:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mtzs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mtzs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-30 22:45:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:06.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1350" for this suite. 01/30/23 22:45:06.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:06.335
Jan 30 22:45:06.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename ephemeral-containers-test 01/30/23 22:45:06.337
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:06.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:06.443
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/30/23 22:45:06.457
Jan 30 22:45:06.486: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6320" to be "running and ready"
Jan 30 22:45:06.501: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.70095ms
Jan 30 22:45:06.501: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:45:08.518: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031972462s
Jan 30 22:45:08.518: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:45:10.520: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.03383042s
Jan 30 22:45:10.520: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 30 22:45:10.520: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/30/23 22:45:10.533
Jan 30 22:45:10.568: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6320" to be "container debugger running"
Jan 30 22:45:10.584: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.24995ms
Jan 30 22:45:12.600: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031483658s
Jan 30 22:45:12.600: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/30/23 22:45:12.6
Jan 30 22:45:12.601: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6320 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:45:12.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:45:12.602: INFO: ExecWithOptions: Clientset creation
Jan 30 22:45:12.603: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-6320/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 30 22:45:12.847: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:12.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-6320" for this suite. 01/30/23 22:45:12.902
------------------------------
• [SLOW TEST] [6.591 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:06.335
    Jan 30 22:45:06.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/30/23 22:45:06.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:06.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:06.443
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/30/23 22:45:06.457
    Jan 30 22:45:06.486: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6320" to be "running and ready"
    Jan 30 22:45:06.501: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.70095ms
    Jan 30 22:45:06.501: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:45:08.518: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031972462s
    Jan 30 22:45:08.518: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:45:10.520: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.03383042s
    Jan 30 22:45:10.520: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 30 22:45:10.520: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/30/23 22:45:10.533
    Jan 30 22:45:10.568: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6320" to be "container debugger running"
    Jan 30 22:45:10.584: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 15.24995ms
    Jan 30 22:45:12.600: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031483658s
    Jan 30 22:45:12.600: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/30/23 22:45:12.6
    Jan 30 22:45:12.601: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6320 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:45:12.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:45:12.602: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:45:12.603: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-6320/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 30 22:45:12.847: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:12.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-6320" for this suite. 01/30/23 22:45:12.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:12.929
Jan 30 22:45:12.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename csistoragecapacity 01/30/23 22:45:12.931
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:12.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:13.008
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/30/23 22:45:13.021
STEP: getting /apis/storage.k8s.io 01/30/23 22:45:13.036
STEP: getting /apis/storage.k8s.io/v1 01/30/23 22:45:13.042
STEP: creating 01/30/23 22:45:13.048
STEP: watching 01/30/23 22:45:13.119
Jan 30 22:45:13.120: INFO: starting watch
STEP: getting 01/30/23 22:45:13.158
STEP: listing in namespace 01/30/23 22:45:13.171
STEP: listing across namespaces 01/30/23 22:45:13.185
STEP: patching 01/30/23 22:45:13.199
STEP: updating 01/30/23 22:45:13.218
Jan 30 22:45:13.240: INFO: waiting for watch events with expected annotations in namespace
Jan 30 22:45:13.240: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/30/23 22:45:13.241
STEP: deleting a collection 01/30/23 22:45:13.298
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:13.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-4728" for this suite. 01/30/23 22:45:13.417
------------------------------
• [0.525 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:12.929
    Jan 30 22:45:12.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename csistoragecapacity 01/30/23 22:45:12.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:12.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:13.008
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/30/23 22:45:13.021
    STEP: getting /apis/storage.k8s.io 01/30/23 22:45:13.036
    STEP: getting /apis/storage.k8s.io/v1 01/30/23 22:45:13.042
    STEP: creating 01/30/23 22:45:13.048
    STEP: watching 01/30/23 22:45:13.119
    Jan 30 22:45:13.120: INFO: starting watch
    STEP: getting 01/30/23 22:45:13.158
    STEP: listing in namespace 01/30/23 22:45:13.171
    STEP: listing across namespaces 01/30/23 22:45:13.185
    STEP: patching 01/30/23 22:45:13.199
    STEP: updating 01/30/23 22:45:13.218
    Jan 30 22:45:13.240: INFO: waiting for watch events with expected annotations in namespace
    Jan 30 22:45:13.240: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/30/23 22:45:13.241
    STEP: deleting a collection 01/30/23 22:45:13.298
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:13.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-4728" for this suite. 01/30/23 22:45:13.417
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:13.456
Jan 30 22:45:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 22:45:13.459
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:13.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:13.56
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 30 22:45:13.683: INFO: Create a RollingUpdate DaemonSet
Jan 30 22:45:13.703: INFO: Check that daemon pods launch on every node of the cluster
Jan 30 22:45:13.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:13.738: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:14.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:14.782: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:15.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:15.774: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:16.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:16.773: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:17.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:17.805: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:18.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:18.776: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:19.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:19.777: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:20.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 22:45:20.810: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:45:21.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 22:45:21.779: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 30 22:45:21.779: INFO: Update the DaemonSet to trigger a rollout
Jan 30 22:45:21.818: INFO: Updating DaemonSet daemon-set
Jan 30 22:45:25.888: INFO: Roll back the DaemonSet before rollout is complete
Jan 30 22:45:25.922: INFO: Updating DaemonSet daemon-set
Jan 30 22:45:25.922: INFO: Make sure DaemonSet rollback is complete
Jan 30 22:45:25.941: INFO: Wrong image for pod: daemon-set-bjrmb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 30 22:45:25.941: INFO: Pod daemon-set-bjrmb is not available
Jan 30 22:45:33.977: INFO: Pod daemon-set-qhqsn is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 22:45:34.057
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8138, will wait for the garbage collector to delete the pods 01/30/23 22:45:34.058
Jan 30 22:45:34.148: INFO: Deleting DaemonSet.extensions daemon-set took: 26.489308ms
Jan 30 22:45:34.350: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.061555ms
Jan 30 22:45:37.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:45:37.090: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 22:45:37.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19659"},"items":null}

Jan 30 22:45:37.128: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19659"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8138" for this suite. 01/30/23 22:45:37.222
------------------------------
• [SLOW TEST] [23.828 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:13.456
    Jan 30 22:45:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 22:45:13.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:13.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:13.56
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 30 22:45:13.683: INFO: Create a RollingUpdate DaemonSet
    Jan 30 22:45:13.703: INFO: Check that daemon pods launch on every node of the cluster
    Jan 30 22:45:13.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:13.738: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:14.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:14.782: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:15.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:15.774: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:16.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:16.773: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:17.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:17.805: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:18.776: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:18.776: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:19.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:19.777: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:20.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 22:45:20.810: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:45:21.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 22:45:21.779: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan 30 22:45:21.779: INFO: Update the DaemonSet to trigger a rollout
    Jan 30 22:45:21.818: INFO: Updating DaemonSet daemon-set
    Jan 30 22:45:25.888: INFO: Roll back the DaemonSet before rollout is complete
    Jan 30 22:45:25.922: INFO: Updating DaemonSet daemon-set
    Jan 30 22:45:25.922: INFO: Make sure DaemonSet rollback is complete
    Jan 30 22:45:25.941: INFO: Wrong image for pod: daemon-set-bjrmb. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 30 22:45:25.941: INFO: Pod daemon-set-bjrmb is not available
    Jan 30 22:45:33.977: INFO: Pod daemon-set-qhqsn is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 22:45:34.057
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8138, will wait for the garbage collector to delete the pods 01/30/23 22:45:34.058
    Jan 30 22:45:34.148: INFO: Deleting DaemonSet.extensions daemon-set took: 26.489308ms
    Jan 30 22:45:34.350: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.061555ms
    Jan 30 22:45:37.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:45:37.090: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 22:45:37.105: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19659"},"items":null}

    Jan 30 22:45:37.128: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19659"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8138" for this suite. 01/30/23 22:45:37.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:37.285
Jan 30 22:45:37.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 22:45:37.287
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:37.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:37.377
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/30/23 22:45:37.399
Jan 30 22:45:37.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a" in namespace "projected-6087" to be "Succeeded or Failed"
Jan 30 22:45:37.450: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.253097ms
Jan 30 22:45:39.480: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046614109s
Jan 30 22:45:41.477: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043565944s
STEP: Saw pod success 01/30/23 22:45:41.477
Jan 30 22:45:41.478: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a" satisfied condition "Succeeded or Failed"
Jan 30 22:45:41.493: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a container client-container: <nil>
STEP: delete the pod 01/30/23 22:45:41.534
Jan 30 22:45:41.585: INFO: Waiting for pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a to disappear
Jan 30 22:45:41.598: INFO: Pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:41.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6087" for this suite. 01/30/23 22:45:41.617
------------------------------
• [4.356 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:37.285
    Jan 30 22:45:37.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 22:45:37.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:37.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:37.377
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/30/23 22:45:37.399
    Jan 30 22:45:37.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a" in namespace "projected-6087" to be "Succeeded or Failed"
    Jan 30 22:45:37.450: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.253097ms
    Jan 30 22:45:39.480: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046614109s
    Jan 30 22:45:41.477: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043565944s
    STEP: Saw pod success 01/30/23 22:45:41.477
    Jan 30 22:45:41.478: INFO: Pod "downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a" satisfied condition "Succeeded or Failed"
    Jan 30 22:45:41.493: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a container client-container: <nil>
    STEP: delete the pod 01/30/23 22:45:41.534
    Jan 30 22:45:41.585: INFO: Waiting for pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a to disappear
    Jan 30 22:45:41.598: INFO: Pod downwardapi-volume-ddbba8dc-007c-4558-b672-4f45be72197a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:41.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6087" for this suite. 01/30/23 22:45:41.617
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:41.643
Jan 30 22:45:41.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 22:45:41.646
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:41.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:41.718
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/30/23 22:45:41.73
STEP: Creating a ResourceQuota 01/30/23 22:45:46.745
STEP: Ensuring resource quota status is calculated 01/30/23 22:45:46.772
STEP: Creating a ReplicaSet 01/30/23 22:45:48.794
STEP: Ensuring resource quota status captures replicaset creation 01/30/23 22:45:48.831
STEP: Deleting a ReplicaSet 01/30/23 22:45:50.848
STEP: Ensuring resource quota status released usage 01/30/23 22:45:50.886
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:52.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2711" for this suite. 01/30/23 22:45:52.924
------------------------------
• [SLOW TEST] [11.308 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:41.643
    Jan 30 22:45:41.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 22:45:41.646
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:41.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:41.718
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/30/23 22:45:41.73
    STEP: Creating a ResourceQuota 01/30/23 22:45:46.745
    STEP: Ensuring resource quota status is calculated 01/30/23 22:45:46.772
    STEP: Creating a ReplicaSet 01/30/23 22:45:48.794
    STEP: Ensuring resource quota status captures replicaset creation 01/30/23 22:45:48.831
    STEP: Deleting a ReplicaSet 01/30/23 22:45:50.848
    STEP: Ensuring resource quota status released usage 01/30/23 22:45:50.886
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:52.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2711" for this suite. 01/30/23 22:45:52.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:52.957
Jan 30 22:45:52.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/30/23 22:45:52.959
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:53.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:53.048
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/30/23 22:45:53.082
Jan 30 22:45:53.083: INFO: Creating simple deployment test-deployment-r4j85
Jan 30 22:45:53.169: INFO: deployment "test-deployment-r4j85" doesn't have the required revision set
Jan 30 22:45:55.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-r4j85-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/30/23 22:45:57.244
Jan 30 22:45:57.259: INFO: Deployment test-deployment-r4j85 has Conditions: [{Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/30/23 22:45:57.259
Jan 30 22:45:57.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-r4j85-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/30/23 22:45:57.29
Jan 30 22:45:57.300: INFO: Observed &Deployment event: ADDED
Jan 30 22:45:57.300: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
Jan 30 22:45:57.301: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 22:45:57.301: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4j85-54bc444df" is progressing.}
Jan 30 22:45:57.302: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
Jan 30 22:45:57.302: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
Jan 30 22:45:57.302: INFO: Found Deployment test-deployment-r4j85 in namespace deployment-2179 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 22:45:57.302: INFO: Deployment test-deployment-r4j85 has an updated status
STEP: patching the Statefulset Status 01/30/23 22:45:57.302
Jan 30 22:45:57.302: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 30 22:45:57.328: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/30/23 22:45:57.328
Jan 30 22:45:57.336: INFO: Observed &Deployment event: ADDED
Jan 30 22:45:57.336: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4j85-54bc444df" is progressing.}
Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.338: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 22:45:57.338: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
Jan 30 22:45:57.339: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.340: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 30 22:45:57.340: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
Jan 30 22:45:57.341: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 22:45:57.341: INFO: Observed &Deployment event: MODIFIED
Jan 30 22:45:57.342: INFO: Found deployment test-deployment-r4j85 in namespace deployment-2179 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 30 22:45:57.343: INFO: Deployment test-deployment-r4j85 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 22:45:57.357: INFO: Deployment "test-deployment-r4j85":
&Deployment{ObjectMeta:{test-deployment-r4j85  deployment-2179  c5dd45ad-24d4-40a0-953e-d5c46ff2670a 19801 1 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-30 22:45:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-30 22:45:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467e558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-r4j85-54bc444df",LastUpdateTime:2023-01-30 22:45:57 +0000 UTC,LastTransitionTime:2023-01-30 22:45:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 22:45:57.410: INFO: New ReplicaSet "test-deployment-r4j85-54bc444df" of Deployment "test-deployment-r4j85":
&ReplicaSet{ObjectMeta:{test-deployment-r4j85-54bc444df  deployment-2179  9dc499e2-a949-495b-8a37-ada5695f2cd8 19794 1 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-r4j85 c5dd45ad-24d4-40a0-953e-d5c46ff2670a 0xc00467e940 0xc00467e941}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5dd45ad-24d4-40a0-953e-d5c46ff2670a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467e9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 22:45:57.429: INFO: Pod "test-deployment-r4j85-54bc444df-c7sxm" is available:
&Pod{ObjectMeta:{test-deployment-r4j85-54bc444df-c7sxm test-deployment-r4j85-54bc444df- deployment-2179  fd06ff39-2402-4034-b9bc-5a4c103c503a 19793 0 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:857ddb5b37572e468283647e8d9e509cdaeefee123a989694a82a9ed453731f7 cni.projectcalico.org/podIP:172.30.199.32/32 cni.projectcalico.org/podIPs:172.30.199.32/32] [{apps/v1 ReplicaSet test-deployment-r4j85-54bc444df 9dc499e2-a949-495b-8a37-ada5695f2cd8 0xc00467edb0 0xc00467edb1}] [] [{kube-controller-manager Update v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9dc499e2-a949-495b-8a37-ada5695f2cd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 22:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 22:45:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgknc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgknc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.32,StartTime:2023-01-30 22:45:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 22:45:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://42bb1271bccddb891b4c276c617bfa769ee4f719f6e1c6ddc2f80e9a3a979b14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:57.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2179" for this suite. 01/30/23 22:45:57.457
------------------------------
• [4.527 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:52.957
    Jan 30 22:45:52.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/30/23 22:45:52.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:53.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:53.048
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/30/23 22:45:53.082
    Jan 30 22:45:53.083: INFO: Creating simple deployment test-deployment-r4j85
    Jan 30 22:45:53.169: INFO: deployment "test-deployment-r4j85" doesn't have the required revision set
    Jan 30 22:45:55.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-r4j85-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/30/23 22:45:57.244
    Jan 30 22:45:57.259: INFO: Deployment test-deployment-r4j85 has Conditions: [{Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/30/23 22:45:57.259
    Jan 30 22:45:57.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 45, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 45, 53, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-r4j85-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/30/23 22:45:57.29
    Jan 30 22:45:57.300: INFO: Observed &Deployment event: ADDED
    Jan 30 22:45:57.300: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
    Jan 30 22:45:57.301: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
    Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 22:45:57.301: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 22:45:57.301: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4j85-54bc444df" is progressing.}
    Jan 30 22:45:57.302: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
    Jan 30 22:45:57.302: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 22:45:57.302: INFO: Observed Deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
    Jan 30 22:45:57.302: INFO: Found Deployment test-deployment-r4j85 in namespace deployment-2179 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 22:45:57.302: INFO: Deployment test-deployment-r4j85 has an updated status
    STEP: patching the Statefulset Status 01/30/23 22:45:57.302
    Jan 30 22:45:57.302: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 30 22:45:57.328: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/30/23 22:45:57.328
    Jan 30 22:45:57.336: INFO: Observed &Deployment event: ADDED
    Jan 30 22:45:57.336: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
    Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-r4j85-54bc444df"}
    Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 30 22:45:57.337: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:53 +0000 UTC 2023-01-30 22:45:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-r4j85-54bc444df" is progressing.}
    Jan 30 22:45:57.337: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.338: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 22:45:57.338: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
    Jan 30 22:45:57.339: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.340: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 30 22:45:57.340: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-30 22:45:55 +0000 UTC 2023-01-30 22:45:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-r4j85-54bc444df" has successfully progressed.}
    Jan 30 22:45:57.341: INFO: Observed deployment test-deployment-r4j85 in namespace deployment-2179 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 22:45:57.341: INFO: Observed &Deployment event: MODIFIED
    Jan 30 22:45:57.342: INFO: Found deployment test-deployment-r4j85 in namespace deployment-2179 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 30 22:45:57.343: INFO: Deployment test-deployment-r4j85 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 22:45:57.357: INFO: Deployment "test-deployment-r4j85":
    &Deployment{ObjectMeta:{test-deployment-r4j85  deployment-2179  c5dd45ad-24d4-40a0-953e-d5c46ff2670a 19801 1 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-30 22:45:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-30 22:45:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467e558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-r4j85-54bc444df",LastUpdateTime:2023-01-30 22:45:57 +0000 UTC,LastTransitionTime:2023-01-30 22:45:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 22:45:57.410: INFO: New ReplicaSet "test-deployment-r4j85-54bc444df" of Deployment "test-deployment-r4j85":
    &ReplicaSet{ObjectMeta:{test-deployment-r4j85-54bc444df  deployment-2179  9dc499e2-a949-495b-8a37-ada5695f2cd8 19794 1 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-r4j85 c5dd45ad-24d4-40a0-953e-d5c46ff2670a 0xc00467e940 0xc00467e941}] [] [{kube-controller-manager Update apps/v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5dd45ad-24d4-40a0-953e-d5c46ff2670a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 22:45:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467e9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 22:45:57.429: INFO: Pod "test-deployment-r4j85-54bc444df-c7sxm" is available:
    &Pod{ObjectMeta:{test-deployment-r4j85-54bc444df-c7sxm test-deployment-r4j85-54bc444df- deployment-2179  fd06ff39-2402-4034-b9bc-5a4c103c503a 19793 0 2023-01-30 22:45:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:857ddb5b37572e468283647e8d9e509cdaeefee123a989694a82a9ed453731f7 cni.projectcalico.org/podIP:172.30.199.32/32 cni.projectcalico.org/podIPs:172.30.199.32/32] [{apps/v1 ReplicaSet test-deployment-r4j85-54bc444df 9dc499e2-a949-495b-8a37-ada5695f2cd8 0xc00467edb0 0xc00467edb1}] [] [{kube-controller-manager Update v1 2023-01-30 22:45:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9dc499e2-a949-495b-8a37-ada5695f2cd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 22:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 22:45:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgknc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgknc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 22:45:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.32,StartTime:2023-01-30 22:45:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 22:45:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://42bb1271bccddb891b4c276c617bfa769ee4f719f6e1c6ddc2f80e9a3a979b14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:57.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2179" for this suite. 01/30/23 22:45:57.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:57.505
Jan 30 22:45:57.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 22:45:57.506
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:57.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:57.603
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/30/23 22:45:57.613
STEP: Getting a ResourceQuota 01/30/23 22:45:57.64
STEP: Updating a ResourceQuota 01/30/23 22:45:57.653
STEP: Verifying a ResourceQuota was modified 01/30/23 22:45:57.713
STEP: Deleting a ResourceQuota 01/30/23 22:45:57.731
STEP: Verifying the deleted ResourceQuota 01/30/23 22:45:57.761
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3703" for this suite. 01/30/23 22:45:57.799
------------------------------
• [0.318 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:57.505
    Jan 30 22:45:57.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 22:45:57.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:57.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:57.603
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/30/23 22:45:57.613
    STEP: Getting a ResourceQuota 01/30/23 22:45:57.64
    STEP: Updating a ResourceQuota 01/30/23 22:45:57.653
    STEP: Verifying a ResourceQuota was modified 01/30/23 22:45:57.713
    STEP: Deleting a ResourceQuota 01/30/23 22:45:57.731
    STEP: Verifying the deleted ResourceQuota 01/30/23 22:45:57.761
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:57.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3703" for this suite. 01/30/23 22:45:57.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:57.831
Jan 30 22:45:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/30/23 22:45:57.834
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:57.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:57.961
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/30/23 22:45:57.975
STEP: watching for the ServiceAccount to be added 01/30/23 22:45:58.038
STEP: patching the ServiceAccount 01/30/23 22:45:58.045
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/30/23 22:45:58.065
STEP: deleting the ServiceAccount 01/30/23 22:45:58.08
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 22:45:58.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9902" for this suite. 01/30/23 22:45:58.152
------------------------------
• [0.345 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:57.831
    Jan 30 22:45:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 22:45:57.834
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:57.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:57.961
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/30/23 22:45:57.975
    STEP: watching for the ServiceAccount to be added 01/30/23 22:45:58.038
    STEP: patching the ServiceAccount 01/30/23 22:45:58.045
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/30/23 22:45:58.065
    STEP: deleting the ServiceAccount 01/30/23 22:45:58.08
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:45:58.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9902" for this suite. 01/30/23 22:45:58.152
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:45:58.177
Jan 30 22:45:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/30/23 22:45:58.181
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:58.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:58.255
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/30/23 22:45:58.267
STEP: Creating hostNetwork=false pod 01/30/23 22:45:58.268
Jan 30 22:45:58.297: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5258" to be "running and ready"
Jan 30 22:45:58.310: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344688ms
Jan 30 22:45:58.310: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:46:00.327: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029646427s
Jan 30 22:46:00.327: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:46:02.324: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027138698s
Jan 30 22:46:02.325: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 30 22:46:02.325: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/30/23 22:46:02.338
Jan 30 22:46:02.357: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5258" to be "running and ready"
Jan 30 22:46:02.374: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743108ms
Jan 30 22:46:02.374: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:46:04.392: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035366326s
Jan 30 22:46:04.393: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:46:06.390: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032718976s
Jan 30 22:46:06.390: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 30 22:46:06.390: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/30/23 22:46:06.404
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/30/23 22:46:06.404
Jan 30 22:46:06.405: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:06.406: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:06.406: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 22:46:06.668: INFO: Exec stderr: ""
Jan 30 22:46:06.668: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:06.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:06.670: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:06.670: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 22:46:06.950: INFO: Exec stderr: ""
Jan 30 22:46:06.951: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:06.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:06.954: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:06.954: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 22:46:07.279: INFO: Exec stderr: ""
Jan 30 22:46:07.279: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:07.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:07.282: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:07.282: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 22:46:07.524: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/30/23 22:46:07.524
Jan 30 22:46:07.525: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:07.526: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:07.526: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 30 22:46:07.778: INFO: Exec stderr: ""
Jan 30 22:46:07.778: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:07.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:07.781: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:07.782: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 30 22:46:08.058: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/30/23 22:46:08.058
Jan 30 22:46:08.059: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:08.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:08.061: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:08.061: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 22:46:08.377: INFO: Exec stderr: ""
Jan 30 22:46:08.377: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:08.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:08.379: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:08.379: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 30 22:46:08.627: INFO: Exec stderr: ""
Jan 30 22:46:08.627: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:08.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:08.628: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:08.628: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 22:46:08.888: INFO: Exec stderr: ""
Jan 30 22:46:08.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:46:08.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:46:08.890: INFO: ExecWithOptions: Clientset creation
Jan 30 22:46:08.890: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 30 22:46:09.123: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:09.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5258" for this suite. 01/30/23 22:46:09.144
------------------------------
• [SLOW TEST] [10.993 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:45:58.177
    Jan 30 22:45:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/30/23 22:45:58.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:45:58.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:45:58.255
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/30/23 22:45:58.267
    STEP: Creating hostNetwork=false pod 01/30/23 22:45:58.268
    Jan 30 22:45:58.297: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5258" to be "running and ready"
    Jan 30 22:45:58.310: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344688ms
    Jan 30 22:45:58.310: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:46:00.327: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029646427s
    Jan 30 22:46:00.327: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:46:02.324: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.027138698s
    Jan 30 22:46:02.325: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 30 22:46:02.325: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/30/23 22:46:02.338
    Jan 30 22:46:02.357: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5258" to be "running and ready"
    Jan 30 22:46:02.374: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.743108ms
    Jan 30 22:46:02.374: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:46:04.392: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035366326s
    Jan 30 22:46:04.393: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:46:06.390: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032718976s
    Jan 30 22:46:06.390: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 30 22:46:06.390: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/30/23 22:46:06.404
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/30/23 22:46:06.404
    Jan 30 22:46:06.405: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:06.406: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:06.406: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 22:46:06.668: INFO: Exec stderr: ""
    Jan 30 22:46:06.668: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:06.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:06.670: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:06.670: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 22:46:06.950: INFO: Exec stderr: ""
    Jan 30 22:46:06.951: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:06.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:06.954: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:06.954: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 22:46:07.279: INFO: Exec stderr: ""
    Jan 30 22:46:07.279: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:07.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:07.282: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:07.282: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 22:46:07.524: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/30/23 22:46:07.524
    Jan 30 22:46:07.525: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:07.526: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:07.526: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 30 22:46:07.778: INFO: Exec stderr: ""
    Jan 30 22:46:07.778: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:07.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:07.781: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:07.782: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 30 22:46:08.058: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/30/23 22:46:08.058
    Jan 30 22:46:08.059: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:08.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:08.061: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:08.061: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 22:46:08.377: INFO: Exec stderr: ""
    Jan 30 22:46:08.377: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:08.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:08.379: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:08.379: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 30 22:46:08.627: INFO: Exec stderr: ""
    Jan 30 22:46:08.627: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:08.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:08.628: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:08.628: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 22:46:08.888: INFO: Exec stderr: ""
    Jan 30 22:46:08.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5258 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:46:08.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:46:08.890: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:46:08.890: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5258/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 30 22:46:09.123: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:09.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5258" for this suite. 01/30/23 22:46:09.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:09.179
Jan 30 22:46:09.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/30/23 22:46:09.18
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:09.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:09.247
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/30/23 22:46:09.259
Jan 30 22:46:09.295: INFO: Waiting up to 5m0s for pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695" in namespace "var-expansion-2953" to be "Succeeded or Failed"
Jan 30 22:46:09.308: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 13.162107ms
Jan 30 22:46:11.325: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030058595s
Jan 30 22:46:13.328: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033016708s
Jan 30 22:46:15.324: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02958335s
Jan 30 22:46:17.322: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027071126s
STEP: Saw pod success 01/30/23 22:46:17.322
Jan 30 22:46:17.322: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695" satisfied condition "Succeeded or Failed"
Jan 30 22:46:17.335: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 container dapi-container: <nil>
STEP: delete the pod 01/30/23 22:46:17.371
Jan 30 22:46:17.428: INFO: Waiting for pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 to disappear
Jan 30 22:46:17.440: INFO: Pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:17.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2953" for this suite. 01/30/23 22:46:17.461
------------------------------
• [SLOW TEST] [8.311 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:09.179
    Jan 30 22:46:09.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/30/23 22:46:09.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:09.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:09.247
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/30/23 22:46:09.259
    Jan 30 22:46:09.295: INFO: Waiting up to 5m0s for pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695" in namespace "var-expansion-2953" to be "Succeeded or Failed"
    Jan 30 22:46:09.308: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 13.162107ms
    Jan 30 22:46:11.325: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030058595s
    Jan 30 22:46:13.328: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033016708s
    Jan 30 22:46:15.324: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02958335s
    Jan 30 22:46:17.322: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.027071126s
    STEP: Saw pod success 01/30/23 22:46:17.322
    Jan 30 22:46:17.322: INFO: Pod "var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695" satisfied condition "Succeeded or Failed"
    Jan 30 22:46:17.335: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 22:46:17.371
    Jan 30 22:46:17.428: INFO: Waiting for pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 to disappear
    Jan 30 22:46:17.440: INFO: Pod var-expansion-eb0f5519-3b98-41fa-8ed0-d71366ff8695 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:17.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2953" for this suite. 01/30/23 22:46:17.461
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:17.493
Jan 30 22:46:17.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename limitrange 01/30/23 22:46:17.496
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:17.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:17.58
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-bfqbv" in namespace "limitrange-423" 01/30/23 22:46:17.652
STEP: Creating another limitRange in another namespace 01/30/23 22:46:17.673
Jan 30 22:46:17.740: INFO: Namespace "e2e-limitrange-bfqbv-7080" created
Jan 30 22:46:17.740: INFO: Creating LimitRange "e2e-limitrange-bfqbv" in namespace "e2e-limitrange-bfqbv-7080"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-bfqbv" 01/30/23 22:46:17.757
Jan 30 22:46:17.799: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-bfqbv" in "limitrange-423" namespace 01/30/23 22:46:17.799
Jan 30 22:46:17.824: INFO: LimitRange "e2e-limitrange-bfqbv" has been patched
STEP: Delete LimitRange "e2e-limitrange-bfqbv" by Collection with labelSelector: "e2e-limitrange-bfqbv=patched" 01/30/23 22:46:17.824
STEP: Confirm that the limitRange "e2e-limitrange-bfqbv" has been deleted 01/30/23 22:46:17.861
Jan 30 22:46:17.861: INFO: Requesting list of LimitRange to confirm quantity
Jan 30 22:46:17.878: INFO: Found 0 LimitRange with label "e2e-limitrange-bfqbv=patched"
Jan 30 22:46:17.878: INFO: LimitRange "e2e-limitrange-bfqbv" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-bfqbv" 01/30/23 22:46:17.878
Jan 30 22:46:17.927: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-423" for this suite. 01/30/23 22:46:17.948
STEP: Destroying namespace "e2e-limitrange-bfqbv-7080" for this suite. 01/30/23 22:46:17.972
------------------------------
• [0.507 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:17.493
    Jan 30 22:46:17.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename limitrange 01/30/23 22:46:17.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:17.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:17.58
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-bfqbv" in namespace "limitrange-423" 01/30/23 22:46:17.652
    STEP: Creating another limitRange in another namespace 01/30/23 22:46:17.673
    Jan 30 22:46:17.740: INFO: Namespace "e2e-limitrange-bfqbv-7080" created
    Jan 30 22:46:17.740: INFO: Creating LimitRange "e2e-limitrange-bfqbv" in namespace "e2e-limitrange-bfqbv-7080"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-bfqbv" 01/30/23 22:46:17.757
    Jan 30 22:46:17.799: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-bfqbv" in "limitrange-423" namespace 01/30/23 22:46:17.799
    Jan 30 22:46:17.824: INFO: LimitRange "e2e-limitrange-bfqbv" has been patched
    STEP: Delete LimitRange "e2e-limitrange-bfqbv" by Collection with labelSelector: "e2e-limitrange-bfqbv=patched" 01/30/23 22:46:17.824
    STEP: Confirm that the limitRange "e2e-limitrange-bfqbv" has been deleted 01/30/23 22:46:17.861
    Jan 30 22:46:17.861: INFO: Requesting list of LimitRange to confirm quantity
    Jan 30 22:46:17.878: INFO: Found 0 LimitRange with label "e2e-limitrange-bfqbv=patched"
    Jan 30 22:46:17.878: INFO: LimitRange "e2e-limitrange-bfqbv" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-bfqbv" 01/30/23 22:46:17.878
    Jan 30 22:46:17.927: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-423" for this suite. 01/30/23 22:46:17.948
    STEP: Destroying namespace "e2e-limitrange-bfqbv-7080" for this suite. 01/30/23 22:46:17.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:18.009
Jan 30 22:46:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 22:46:18.01
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:18.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:18.086
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 22:46:18.097
Jan 30 22:46:18.124: INFO: Waiting up to 5m0s for pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35" in namespace "emptydir-3810" to be "Succeeded or Failed"
Jan 30 22:46:18.138: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 13.21115ms
Jan 30 22:46:20.157: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031943784s
Jan 30 22:46:22.155: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030579484s
Jan 30 22:46:24.154: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029504621s
STEP: Saw pod success 01/30/23 22:46:24.154
Jan 30 22:46:24.154: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35" satisfied condition "Succeeded or Failed"
Jan 30 22:46:24.169: INFO: Trying to get logs from node 10.15.28.227 pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 container test-container: <nil>
STEP: delete the pod 01/30/23 22:46:24.205
Jan 30 22:46:24.238: INFO: Waiting for pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 to disappear
Jan 30 22:46:24.251: INFO: Pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:24.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3810" for this suite. 01/30/23 22:46:24.271
------------------------------
• [SLOW TEST] [6.288 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:18.009
    Jan 30 22:46:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 22:46:18.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:18.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:18.086
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 22:46:18.097
    Jan 30 22:46:18.124: INFO: Waiting up to 5m0s for pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35" in namespace "emptydir-3810" to be "Succeeded or Failed"
    Jan 30 22:46:18.138: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 13.21115ms
    Jan 30 22:46:20.157: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031943784s
    Jan 30 22:46:22.155: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030579484s
    Jan 30 22:46:24.154: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029504621s
    STEP: Saw pod success 01/30/23 22:46:24.154
    Jan 30 22:46:24.154: INFO: Pod "pod-42136f41-a15f-4371-9c21-ed4e84905c35" satisfied condition "Succeeded or Failed"
    Jan 30 22:46:24.169: INFO: Trying to get logs from node 10.15.28.227 pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 container test-container: <nil>
    STEP: delete the pod 01/30/23 22:46:24.205
    Jan 30 22:46:24.238: INFO: Waiting for pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 to disappear
    Jan 30 22:46:24.251: INFO: Pod pod-42136f41-a15f-4371-9c21-ed4e84905c35 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:24.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3810" for this suite. 01/30/23 22:46:24.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:24.309
Jan 30 22:46:24.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 22:46:24.311
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:24.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:24.386
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/30/23 22:46:24.401
STEP: getting 01/30/23 22:46:24.485
STEP: listing in namespace 01/30/23 22:46:24.51
STEP: patching 01/30/23 22:46:24.526
STEP: deleting 01/30/23 22:46:24.551
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:24.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-59" for this suite. 01/30/23 22:46:24.622
------------------------------
• [0.337 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:24.309
    Jan 30 22:46:24.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 22:46:24.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:24.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:24.386
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/30/23 22:46:24.401
    STEP: getting 01/30/23 22:46:24.485
    STEP: listing in namespace 01/30/23 22:46:24.51
    STEP: patching 01/30/23 22:46:24.526
    STEP: deleting 01/30/23 22:46:24.551
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:24.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-59" for this suite. 01/30/23 22:46:24.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:24.652
Jan 30 22:46:24.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename proxy 01/30/23 22:46:24.657
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:24.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:24.786
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 30 22:46:24.798: INFO: Creating pod...
Jan 30 22:46:24.830: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6008" to be "running"
Jan 30 22:46:24.849: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.719253ms
Jan 30 22:46:26.864: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033573295s
Jan 30 22:46:28.864: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.034208683s
Jan 30 22:46:28.865: INFO: Pod "agnhost" satisfied condition "running"
Jan 30 22:46:28.865: INFO: Creating service...
Jan 30 22:46:28.914: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=DELETE
Jan 30 22:46:28.980: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 22:46:28.980: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=OPTIONS
Jan 30 22:46:29.003: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 22:46:29.003: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PATCH
Jan 30 22:46:29.025: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 22:46:29.025: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=POST
Jan 30 22:46:29.050: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 22:46:29.050: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PUT
Jan 30 22:46:29.072: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 30 22:46:29.072: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 30 22:46:29.101: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 30 22:46:29.101: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 30 22:46:29.133: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 30 22:46:29.133: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 30 22:46:29.163: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 30 22:46:29.163: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=POST
Jan 30 22:46:29.195: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 30 22:46:29.195: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PUT
Jan 30 22:46:29.223: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 30 22:46:29.223: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=GET
Jan 30 22:46:29.236: INFO: http.Client request:GET StatusCode:301
Jan 30 22:46:29.236: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=GET
Jan 30 22:46:29.256: INFO: http.Client request:GET StatusCode:301
Jan 30 22:46:29.256: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=HEAD
Jan 30 22:46:29.271: INFO: http.Client request:HEAD StatusCode:301
Jan 30 22:46:29.271: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 30 22:46:29.293: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:29.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6008" for this suite. 01/30/23 22:46:29.313
------------------------------
• [4.701 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:24.652
    Jan 30 22:46:24.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename proxy 01/30/23 22:46:24.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:24.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:24.786
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 30 22:46:24.798: INFO: Creating pod...
    Jan 30 22:46:24.830: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6008" to be "running"
    Jan 30 22:46:24.849: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.719253ms
    Jan 30 22:46:26.864: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033573295s
    Jan 30 22:46:28.864: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.034208683s
    Jan 30 22:46:28.865: INFO: Pod "agnhost" satisfied condition "running"
    Jan 30 22:46:28.865: INFO: Creating service...
    Jan 30 22:46:28.914: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=DELETE
    Jan 30 22:46:28.980: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 22:46:28.980: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=OPTIONS
    Jan 30 22:46:29.003: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 22:46:29.003: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PATCH
    Jan 30 22:46:29.025: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 22:46:29.025: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=POST
    Jan 30 22:46:29.050: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 22:46:29.050: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=PUT
    Jan 30 22:46:29.072: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 30 22:46:29.072: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 30 22:46:29.101: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 30 22:46:29.101: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 30 22:46:29.133: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 30 22:46:29.133: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 30 22:46:29.163: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 30 22:46:29.163: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=POST
    Jan 30 22:46:29.195: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 30 22:46:29.195: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 30 22:46:29.223: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 30 22:46:29.223: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=GET
    Jan 30 22:46:29.236: INFO: http.Client request:GET StatusCode:301
    Jan 30 22:46:29.236: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=GET
    Jan 30 22:46:29.256: INFO: http.Client request:GET StatusCode:301
    Jan 30 22:46:29.256: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/pods/agnhost/proxy?method=HEAD
    Jan 30 22:46:29.271: INFO: http.Client request:HEAD StatusCode:301
    Jan 30 22:46:29.271: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6008/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 30 22:46:29.293: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:29.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6008" for this suite. 01/30/23 22:46:29.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:29.363
Jan 30 22:46:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 22:46:29.365
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:29.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:29.447
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 30 22:46:29.460: INFO: Creating ReplicaSet my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0
Jan 30 22:46:29.503: INFO: Pod name my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Found 0 pods out of 1
Jan 30 22:46:34.520: INFO: Pod name my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Found 1 pods out of 1
Jan 30 22:46:34.520: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0" is running
Jan 30 22:46:34.520: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" in namespace "replicaset-8275" to be "running"
Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c": Phase="Running", Reason="", readiness=true. Elapsed: 16.126596ms
Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" satisfied condition "running"
Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:29 +0000 UTC Reason: Message:}])
Jan 30 22:46:34.536: INFO: Trying to dial the pod
Jan 30 22:46:39.639: INFO: Controller my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Got expected result from replica 1 [my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c]: "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:39.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8275" for this suite. 01/30/23 22:46:39.658
------------------------------
• [SLOW TEST] [10.320 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:29.363
    Jan 30 22:46:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 22:46:29.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:29.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:29.447
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 30 22:46:29.460: INFO: Creating ReplicaSet my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0
    Jan 30 22:46:29.503: INFO: Pod name my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Found 0 pods out of 1
    Jan 30 22:46:34.520: INFO: Pod name my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Found 1 pods out of 1
    Jan 30 22:46:34.520: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0" is running
    Jan 30 22:46:34.520: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" in namespace "replicaset-8275" to be "running"
    Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c": Phase="Running", Reason="", readiness=true. Elapsed: 16.126596ms
    Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" satisfied condition "running"
    Jan 30 22:46:34.536: INFO: Pod "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:29 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-30 22:46:29 +0000 UTC Reason: Message:}])
    Jan 30 22:46:34.536: INFO: Trying to dial the pod
    Jan 30 22:46:39.639: INFO: Controller my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0: Got expected result from replica 1 [my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c]: "my-hostname-basic-f9bcf72b-ff90-41d6-988c-4e50793b01c0-8rz9c", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:39.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8275" for this suite. 01/30/23 22:46:39.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:39.697
Jan 30 22:46:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 22:46:39.699
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:39.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:39.768
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/30/23 22:46:39.779
Jan 30 22:46:39.812: INFO: Waiting up to 5m0s for pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8" in namespace "downward-api-426" to be "running and ready"
Jan 30 22:46:39.826: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.888354ms
Jan 30 22:46:39.826: INFO: The phase of Pod annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:46:41.846: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.033884676s
Jan 30 22:46:41.846: INFO: The phase of Pod annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8 is Running (Ready = true)
Jan 30 22:46:41.846: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8" satisfied condition "running and ready"
Jan 30 22:46:42.502: INFO: Successfully updated pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 22:46:44.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-426" for this suite. 01/30/23 22:46:44.599
------------------------------
• [4.938 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:39.697
    Jan 30 22:46:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 22:46:39.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:39.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:39.768
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/30/23 22:46:39.779
    Jan 30 22:46:39.812: INFO: Waiting up to 5m0s for pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8" in namespace "downward-api-426" to be "running and ready"
    Jan 30 22:46:39.826: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.888354ms
    Jan 30 22:46:39.826: INFO: The phase of Pod annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:46:41.846: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.033884676s
    Jan 30 22:46:41.846: INFO: The phase of Pod annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8 is Running (Ready = true)
    Jan 30 22:46:41.846: INFO: Pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8" satisfied condition "running and ready"
    Jan 30 22:46:42.502: INFO: Successfully updated pod "annotationupdatedcf9d537-360e-474f-b8bf-e5999ecc86b8"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:46:44.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-426" for this suite. 01/30/23 22:46:44.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:46:44.638
Jan 30 22:46:44.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/30/23 22:46:44.64
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:44.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:44.763
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/30/23 22:46:44.794
STEP: delete the rc 01/30/23 22:46:49.85
STEP: wait for the rc to be deleted 01/30/23 22:46:49.884
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/30/23 22:46:54.935
STEP: Gathering metrics 01/30/23 22:47:24.985
W0130 22:47:25.047555      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 30 22:47:25.047: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 30 22:47:25.047: INFO: Deleting pod "simpletest.rc-24jt5" in namespace "gc-8752"
Jan 30 22:47:25.112: INFO: Deleting pod "simpletest.rc-27j52" in namespace "gc-8752"
Jan 30 22:47:25.172: INFO: Deleting pod "simpletest.rc-27nrh" in namespace "gc-8752"
Jan 30 22:47:25.221: INFO: Deleting pod "simpletest.rc-2c776" in namespace "gc-8752"
Jan 30 22:47:25.289: INFO: Deleting pod "simpletest.rc-2ckkf" in namespace "gc-8752"
Jan 30 22:47:25.340: INFO: Deleting pod "simpletest.rc-2f27t" in namespace "gc-8752"
Jan 30 22:47:25.373: INFO: Deleting pod "simpletest.rc-2k6w5" in namespace "gc-8752"
Jan 30 22:47:25.416: INFO: Deleting pod "simpletest.rc-2t2rf" in namespace "gc-8752"
Jan 30 22:47:25.461: INFO: Deleting pod "simpletest.rc-454wd" in namespace "gc-8752"
Jan 30 22:47:25.508: INFO: Deleting pod "simpletest.rc-496rt" in namespace "gc-8752"
Jan 30 22:47:25.547: INFO: Deleting pod "simpletest.rc-49vcx" in namespace "gc-8752"
Jan 30 22:47:25.598: INFO: Deleting pod "simpletest.rc-4tmlr" in namespace "gc-8752"
Jan 30 22:47:25.646: INFO: Deleting pod "simpletest.rc-4w87h" in namespace "gc-8752"
Jan 30 22:47:25.716: INFO: Deleting pod "simpletest.rc-55bwz" in namespace "gc-8752"
Jan 30 22:47:25.790: INFO: Deleting pod "simpletest.rc-5dgtq" in namespace "gc-8752"
Jan 30 22:47:25.836: INFO: Deleting pod "simpletest.rc-658rz" in namespace "gc-8752"
Jan 30 22:47:25.881: INFO: Deleting pod "simpletest.rc-6h88h" in namespace "gc-8752"
Jan 30 22:47:25.930: INFO: Deleting pod "simpletest.rc-6wsj2" in namespace "gc-8752"
Jan 30 22:47:25.981: INFO: Deleting pod "simpletest.rc-7545s" in namespace "gc-8752"
Jan 30 22:47:26.018: INFO: Deleting pod "simpletest.rc-76rvf" in namespace "gc-8752"
Jan 30 22:47:26.060: INFO: Deleting pod "simpletest.rc-7vsjp" in namespace "gc-8752"
Jan 30 22:47:26.111: INFO: Deleting pod "simpletest.rc-7wl4x" in namespace "gc-8752"
Jan 30 22:47:26.143: INFO: Deleting pod "simpletest.rc-886hr" in namespace "gc-8752"
Jan 30 22:47:26.199: INFO: Deleting pod "simpletest.rc-8tdqc" in namespace "gc-8752"
Jan 30 22:47:26.240: INFO: Deleting pod "simpletest.rc-8z6mm" in namespace "gc-8752"
Jan 30 22:47:26.298: INFO: Deleting pod "simpletest.rc-94t24" in namespace "gc-8752"
Jan 30 22:47:26.342: INFO: Deleting pod "simpletest.rc-9c8rs" in namespace "gc-8752"
Jan 30 22:47:26.396: INFO: Deleting pod "simpletest.rc-9lf9m" in namespace "gc-8752"
Jan 30 22:47:26.496: INFO: Deleting pod "simpletest.rc-9sdfp" in namespace "gc-8752"
Jan 30 22:47:26.539: INFO: Deleting pod "simpletest.rc-9xx5g" in namespace "gc-8752"
Jan 30 22:47:26.581: INFO: Deleting pod "simpletest.rc-b2sqw" in namespace "gc-8752"
Jan 30 22:47:26.637: INFO: Deleting pod "simpletest.rc-bd2kw" in namespace "gc-8752"
Jan 30 22:47:26.691: INFO: Deleting pod "simpletest.rc-c9br5" in namespace "gc-8752"
Jan 30 22:47:26.732: INFO: Deleting pod "simpletest.rc-cbvll" in namespace "gc-8752"
Jan 30 22:47:26.832: INFO: Deleting pod "simpletest.rc-crcxg" in namespace "gc-8752"
Jan 30 22:47:26.896: INFO: Deleting pod "simpletest.rc-cwm2s" in namespace "gc-8752"
Jan 30 22:47:26.933: INFO: Deleting pod "simpletest.rc-dbdjs" in namespace "gc-8752"
Jan 30 22:47:26.987: INFO: Deleting pod "simpletest.rc-f4hsw" in namespace "gc-8752"
Jan 30 22:47:27.030: INFO: Deleting pod "simpletest.rc-f4vxb" in namespace "gc-8752"
Jan 30 22:47:27.080: INFO: Deleting pod "simpletest.rc-fd622" in namespace "gc-8752"
Jan 30 22:47:27.119: INFO: Deleting pod "simpletest.rc-ffjnz" in namespace "gc-8752"
Jan 30 22:47:27.183: INFO: Deleting pod "simpletest.rc-flhgd" in namespace "gc-8752"
Jan 30 22:47:27.228: INFO: Deleting pod "simpletest.rc-fpcqz" in namespace "gc-8752"
Jan 30 22:47:27.301: INFO: Deleting pod "simpletest.rc-gg6bl" in namespace "gc-8752"
Jan 30 22:47:27.357: INFO: Deleting pod "simpletest.rc-gt89z" in namespace "gc-8752"
Jan 30 22:47:27.409: INFO: Deleting pod "simpletest.rc-h29jf" in namespace "gc-8752"
Jan 30 22:47:27.469: INFO: Deleting pod "simpletest.rc-hqclt" in namespace "gc-8752"
Jan 30 22:47:27.519: INFO: Deleting pod "simpletest.rc-hwnrf" in namespace "gc-8752"
Jan 30 22:47:27.565: INFO: Deleting pod "simpletest.rc-hwnzv" in namespace "gc-8752"
Jan 30 22:47:27.608: INFO: Deleting pod "simpletest.rc-j9srn" in namespace "gc-8752"
Jan 30 22:47:27.660: INFO: Deleting pod "simpletest.rc-j9xn6" in namespace "gc-8752"
Jan 30 22:47:27.701: INFO: Deleting pod "simpletest.rc-jfxbc" in namespace "gc-8752"
Jan 30 22:47:27.743: INFO: Deleting pod "simpletest.rc-jgjs2" in namespace "gc-8752"
Jan 30 22:47:27.778: INFO: Deleting pod "simpletest.rc-k4mxt" in namespace "gc-8752"
Jan 30 22:47:27.852: INFO: Deleting pod "simpletest.rc-kbn8n" in namespace "gc-8752"
Jan 30 22:47:27.898: INFO: Deleting pod "simpletest.rc-kc6p9" in namespace "gc-8752"
Jan 30 22:47:27.936: INFO: Deleting pod "simpletest.rc-kc72b" in namespace "gc-8752"
Jan 30 22:47:27.970: INFO: Deleting pod "simpletest.rc-l7pnk" in namespace "gc-8752"
Jan 30 22:47:28.017: INFO: Deleting pod "simpletest.rc-lpkph" in namespace "gc-8752"
Jan 30 22:47:28.075: INFO: Deleting pod "simpletest.rc-m2frv" in namespace "gc-8752"
Jan 30 22:47:28.131: INFO: Deleting pod "simpletest.rc-m4nbk" in namespace "gc-8752"
Jan 30 22:47:28.167: INFO: Deleting pod "simpletest.rc-mh7s5" in namespace "gc-8752"
Jan 30 22:47:28.211: INFO: Deleting pod "simpletest.rc-mvrtq" in namespace "gc-8752"
Jan 30 22:47:28.246: INFO: Deleting pod "simpletest.rc-n699m" in namespace "gc-8752"
Jan 30 22:47:28.298: INFO: Deleting pod "simpletest.rc-ng9gr" in namespace "gc-8752"
Jan 30 22:47:28.344: INFO: Deleting pod "simpletest.rc-nj4hj" in namespace "gc-8752"
Jan 30 22:47:28.403: INFO: Deleting pod "simpletest.rc-nkkjh" in namespace "gc-8752"
Jan 30 22:47:28.483: INFO: Deleting pod "simpletest.rc-ntsgv" in namespace "gc-8752"
Jan 30 22:47:28.525: INFO: Deleting pod "simpletest.rc-nzqjz" in namespace "gc-8752"
Jan 30 22:47:28.601: INFO: Deleting pod "simpletest.rc-p9d4j" in namespace "gc-8752"
Jan 30 22:47:28.635: INFO: Deleting pod "simpletest.rc-ps8hr" in namespace "gc-8752"
Jan 30 22:47:28.698: INFO: Deleting pod "simpletest.rc-pv5j6" in namespace "gc-8752"
Jan 30 22:47:28.736: INFO: Deleting pod "simpletest.rc-qdkbj" in namespace "gc-8752"
Jan 30 22:47:28.782: INFO: Deleting pod "simpletest.rc-qht99" in namespace "gc-8752"
Jan 30 22:47:28.824: INFO: Deleting pod "simpletest.rc-qs2k6" in namespace "gc-8752"
Jan 30 22:47:28.867: INFO: Deleting pod "simpletest.rc-rcwtv" in namespace "gc-8752"
Jan 30 22:47:28.904: INFO: Deleting pod "simpletest.rc-rdzss" in namespace "gc-8752"
Jan 30 22:47:28.942: INFO: Deleting pod "simpletest.rc-rf7hs" in namespace "gc-8752"
Jan 30 22:47:28.988: INFO: Deleting pod "simpletest.rc-rql5m" in namespace "gc-8752"
Jan 30 22:47:29.039: INFO: Deleting pod "simpletest.rc-sjfsb" in namespace "gc-8752"
Jan 30 22:47:29.091: INFO: Deleting pod "simpletest.rc-sp42s" in namespace "gc-8752"
Jan 30 22:47:29.124: INFO: Deleting pod "simpletest.rc-ssgkv" in namespace "gc-8752"
Jan 30 22:47:29.170: INFO: Deleting pod "simpletest.rc-swkxv" in namespace "gc-8752"
Jan 30 22:47:29.213: INFO: Deleting pod "simpletest.rc-tnx5f" in namespace "gc-8752"
Jan 30 22:47:29.258: INFO: Deleting pod "simpletest.rc-ttznd" in namespace "gc-8752"
Jan 30 22:47:29.299: INFO: Deleting pod "simpletest.rc-tzq6l" in namespace "gc-8752"
Jan 30 22:47:29.348: INFO: Deleting pod "simpletest.rc-v48bw" in namespace "gc-8752"
Jan 30 22:47:29.382: INFO: Deleting pod "simpletest.rc-vkrzk" in namespace "gc-8752"
Jan 30 22:47:29.428: INFO: Deleting pod "simpletest.rc-vprmk" in namespace "gc-8752"
Jan 30 22:47:29.476: INFO: Deleting pod "simpletest.rc-vv5x8" in namespace "gc-8752"
Jan 30 22:47:29.511: INFO: Deleting pod "simpletest.rc-wkdr8" in namespace "gc-8752"
Jan 30 22:47:29.549: INFO: Deleting pod "simpletest.rc-ww2dj" in namespace "gc-8752"
Jan 30 22:47:29.583: INFO: Deleting pod "simpletest.rc-xk2b8" in namespace "gc-8752"
Jan 30 22:47:29.631: INFO: Deleting pod "simpletest.rc-xnksm" in namespace "gc-8752"
Jan 30 22:47:29.678: INFO: Deleting pod "simpletest.rc-xns5k" in namespace "gc-8752"
Jan 30 22:47:29.717: INFO: Deleting pod "simpletest.rc-z26hq" in namespace "gc-8752"
Jan 30 22:47:29.772: INFO: Deleting pod "simpletest.rc-z2jtn" in namespace "gc-8752"
Jan 30 22:47:29.822: INFO: Deleting pod "simpletest.rc-zj9ff" in namespace "gc-8752"
Jan 30 22:47:29.861: INFO: Deleting pod "simpletest.rc-zpf4q" in namespace "gc-8752"
Jan 30 22:47:29.909: INFO: Deleting pod "simpletest.rc-zzlxh" in namespace "gc-8752"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 22:47:29.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8752" for this suite. 01/30/23 22:47:30.004
------------------------------
• [SLOW TEST] [45.405 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:46:44.638
    Jan 30 22:46:44.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/30/23 22:46:44.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:46:44.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:46:44.763
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/30/23 22:46:44.794
    STEP: delete the rc 01/30/23 22:46:49.85
    STEP: wait for the rc to be deleted 01/30/23 22:46:49.884
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/30/23 22:46:54.935
    STEP: Gathering metrics 01/30/23 22:47:24.985
    W0130 22:47:25.047555      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 30 22:47:25.047: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 30 22:47:25.047: INFO: Deleting pod "simpletest.rc-24jt5" in namespace "gc-8752"
    Jan 30 22:47:25.112: INFO: Deleting pod "simpletest.rc-27j52" in namespace "gc-8752"
    Jan 30 22:47:25.172: INFO: Deleting pod "simpletest.rc-27nrh" in namespace "gc-8752"
    Jan 30 22:47:25.221: INFO: Deleting pod "simpletest.rc-2c776" in namespace "gc-8752"
    Jan 30 22:47:25.289: INFO: Deleting pod "simpletest.rc-2ckkf" in namespace "gc-8752"
    Jan 30 22:47:25.340: INFO: Deleting pod "simpletest.rc-2f27t" in namespace "gc-8752"
    Jan 30 22:47:25.373: INFO: Deleting pod "simpletest.rc-2k6w5" in namespace "gc-8752"
    Jan 30 22:47:25.416: INFO: Deleting pod "simpletest.rc-2t2rf" in namespace "gc-8752"
    Jan 30 22:47:25.461: INFO: Deleting pod "simpletest.rc-454wd" in namespace "gc-8752"
    Jan 30 22:47:25.508: INFO: Deleting pod "simpletest.rc-496rt" in namespace "gc-8752"
    Jan 30 22:47:25.547: INFO: Deleting pod "simpletest.rc-49vcx" in namespace "gc-8752"
    Jan 30 22:47:25.598: INFO: Deleting pod "simpletest.rc-4tmlr" in namespace "gc-8752"
    Jan 30 22:47:25.646: INFO: Deleting pod "simpletest.rc-4w87h" in namespace "gc-8752"
    Jan 30 22:47:25.716: INFO: Deleting pod "simpletest.rc-55bwz" in namespace "gc-8752"
    Jan 30 22:47:25.790: INFO: Deleting pod "simpletest.rc-5dgtq" in namespace "gc-8752"
    Jan 30 22:47:25.836: INFO: Deleting pod "simpletest.rc-658rz" in namespace "gc-8752"
    Jan 30 22:47:25.881: INFO: Deleting pod "simpletest.rc-6h88h" in namespace "gc-8752"
    Jan 30 22:47:25.930: INFO: Deleting pod "simpletest.rc-6wsj2" in namespace "gc-8752"
    Jan 30 22:47:25.981: INFO: Deleting pod "simpletest.rc-7545s" in namespace "gc-8752"
    Jan 30 22:47:26.018: INFO: Deleting pod "simpletest.rc-76rvf" in namespace "gc-8752"
    Jan 30 22:47:26.060: INFO: Deleting pod "simpletest.rc-7vsjp" in namespace "gc-8752"
    Jan 30 22:47:26.111: INFO: Deleting pod "simpletest.rc-7wl4x" in namespace "gc-8752"
    Jan 30 22:47:26.143: INFO: Deleting pod "simpletest.rc-886hr" in namespace "gc-8752"
    Jan 30 22:47:26.199: INFO: Deleting pod "simpletest.rc-8tdqc" in namespace "gc-8752"
    Jan 30 22:47:26.240: INFO: Deleting pod "simpletest.rc-8z6mm" in namespace "gc-8752"
    Jan 30 22:47:26.298: INFO: Deleting pod "simpletest.rc-94t24" in namespace "gc-8752"
    Jan 30 22:47:26.342: INFO: Deleting pod "simpletest.rc-9c8rs" in namespace "gc-8752"
    Jan 30 22:47:26.396: INFO: Deleting pod "simpletest.rc-9lf9m" in namespace "gc-8752"
    Jan 30 22:47:26.496: INFO: Deleting pod "simpletest.rc-9sdfp" in namespace "gc-8752"
    Jan 30 22:47:26.539: INFO: Deleting pod "simpletest.rc-9xx5g" in namespace "gc-8752"
    Jan 30 22:47:26.581: INFO: Deleting pod "simpletest.rc-b2sqw" in namespace "gc-8752"
    Jan 30 22:47:26.637: INFO: Deleting pod "simpletest.rc-bd2kw" in namespace "gc-8752"
    Jan 30 22:47:26.691: INFO: Deleting pod "simpletest.rc-c9br5" in namespace "gc-8752"
    Jan 30 22:47:26.732: INFO: Deleting pod "simpletest.rc-cbvll" in namespace "gc-8752"
    Jan 30 22:47:26.832: INFO: Deleting pod "simpletest.rc-crcxg" in namespace "gc-8752"
    Jan 30 22:47:26.896: INFO: Deleting pod "simpletest.rc-cwm2s" in namespace "gc-8752"
    Jan 30 22:47:26.933: INFO: Deleting pod "simpletest.rc-dbdjs" in namespace "gc-8752"
    Jan 30 22:47:26.987: INFO: Deleting pod "simpletest.rc-f4hsw" in namespace "gc-8752"
    Jan 30 22:47:27.030: INFO: Deleting pod "simpletest.rc-f4vxb" in namespace "gc-8752"
    Jan 30 22:47:27.080: INFO: Deleting pod "simpletest.rc-fd622" in namespace "gc-8752"
    Jan 30 22:47:27.119: INFO: Deleting pod "simpletest.rc-ffjnz" in namespace "gc-8752"
    Jan 30 22:47:27.183: INFO: Deleting pod "simpletest.rc-flhgd" in namespace "gc-8752"
    Jan 30 22:47:27.228: INFO: Deleting pod "simpletest.rc-fpcqz" in namespace "gc-8752"
    Jan 30 22:47:27.301: INFO: Deleting pod "simpletest.rc-gg6bl" in namespace "gc-8752"
    Jan 30 22:47:27.357: INFO: Deleting pod "simpletest.rc-gt89z" in namespace "gc-8752"
    Jan 30 22:47:27.409: INFO: Deleting pod "simpletest.rc-h29jf" in namespace "gc-8752"
    Jan 30 22:47:27.469: INFO: Deleting pod "simpletest.rc-hqclt" in namespace "gc-8752"
    Jan 30 22:47:27.519: INFO: Deleting pod "simpletest.rc-hwnrf" in namespace "gc-8752"
    Jan 30 22:47:27.565: INFO: Deleting pod "simpletest.rc-hwnzv" in namespace "gc-8752"
    Jan 30 22:47:27.608: INFO: Deleting pod "simpletest.rc-j9srn" in namespace "gc-8752"
    Jan 30 22:47:27.660: INFO: Deleting pod "simpletest.rc-j9xn6" in namespace "gc-8752"
    Jan 30 22:47:27.701: INFO: Deleting pod "simpletest.rc-jfxbc" in namespace "gc-8752"
    Jan 30 22:47:27.743: INFO: Deleting pod "simpletest.rc-jgjs2" in namespace "gc-8752"
    Jan 30 22:47:27.778: INFO: Deleting pod "simpletest.rc-k4mxt" in namespace "gc-8752"
    Jan 30 22:47:27.852: INFO: Deleting pod "simpletest.rc-kbn8n" in namespace "gc-8752"
    Jan 30 22:47:27.898: INFO: Deleting pod "simpletest.rc-kc6p9" in namespace "gc-8752"
    Jan 30 22:47:27.936: INFO: Deleting pod "simpletest.rc-kc72b" in namespace "gc-8752"
    Jan 30 22:47:27.970: INFO: Deleting pod "simpletest.rc-l7pnk" in namespace "gc-8752"
    Jan 30 22:47:28.017: INFO: Deleting pod "simpletest.rc-lpkph" in namespace "gc-8752"
    Jan 30 22:47:28.075: INFO: Deleting pod "simpletest.rc-m2frv" in namespace "gc-8752"
    Jan 30 22:47:28.131: INFO: Deleting pod "simpletest.rc-m4nbk" in namespace "gc-8752"
    Jan 30 22:47:28.167: INFO: Deleting pod "simpletest.rc-mh7s5" in namespace "gc-8752"
    Jan 30 22:47:28.211: INFO: Deleting pod "simpletest.rc-mvrtq" in namespace "gc-8752"
    Jan 30 22:47:28.246: INFO: Deleting pod "simpletest.rc-n699m" in namespace "gc-8752"
    Jan 30 22:47:28.298: INFO: Deleting pod "simpletest.rc-ng9gr" in namespace "gc-8752"
    Jan 30 22:47:28.344: INFO: Deleting pod "simpletest.rc-nj4hj" in namespace "gc-8752"
    Jan 30 22:47:28.403: INFO: Deleting pod "simpletest.rc-nkkjh" in namespace "gc-8752"
    Jan 30 22:47:28.483: INFO: Deleting pod "simpletest.rc-ntsgv" in namespace "gc-8752"
    Jan 30 22:47:28.525: INFO: Deleting pod "simpletest.rc-nzqjz" in namespace "gc-8752"
    Jan 30 22:47:28.601: INFO: Deleting pod "simpletest.rc-p9d4j" in namespace "gc-8752"
    Jan 30 22:47:28.635: INFO: Deleting pod "simpletest.rc-ps8hr" in namespace "gc-8752"
    Jan 30 22:47:28.698: INFO: Deleting pod "simpletest.rc-pv5j6" in namespace "gc-8752"
    Jan 30 22:47:28.736: INFO: Deleting pod "simpletest.rc-qdkbj" in namespace "gc-8752"
    Jan 30 22:47:28.782: INFO: Deleting pod "simpletest.rc-qht99" in namespace "gc-8752"
    Jan 30 22:47:28.824: INFO: Deleting pod "simpletest.rc-qs2k6" in namespace "gc-8752"
    Jan 30 22:47:28.867: INFO: Deleting pod "simpletest.rc-rcwtv" in namespace "gc-8752"
    Jan 30 22:47:28.904: INFO: Deleting pod "simpletest.rc-rdzss" in namespace "gc-8752"
    Jan 30 22:47:28.942: INFO: Deleting pod "simpletest.rc-rf7hs" in namespace "gc-8752"
    Jan 30 22:47:28.988: INFO: Deleting pod "simpletest.rc-rql5m" in namespace "gc-8752"
    Jan 30 22:47:29.039: INFO: Deleting pod "simpletest.rc-sjfsb" in namespace "gc-8752"
    Jan 30 22:47:29.091: INFO: Deleting pod "simpletest.rc-sp42s" in namespace "gc-8752"
    Jan 30 22:47:29.124: INFO: Deleting pod "simpletest.rc-ssgkv" in namespace "gc-8752"
    Jan 30 22:47:29.170: INFO: Deleting pod "simpletest.rc-swkxv" in namespace "gc-8752"
    Jan 30 22:47:29.213: INFO: Deleting pod "simpletest.rc-tnx5f" in namespace "gc-8752"
    Jan 30 22:47:29.258: INFO: Deleting pod "simpletest.rc-ttznd" in namespace "gc-8752"
    Jan 30 22:47:29.299: INFO: Deleting pod "simpletest.rc-tzq6l" in namespace "gc-8752"
    Jan 30 22:47:29.348: INFO: Deleting pod "simpletest.rc-v48bw" in namespace "gc-8752"
    Jan 30 22:47:29.382: INFO: Deleting pod "simpletest.rc-vkrzk" in namespace "gc-8752"
    Jan 30 22:47:29.428: INFO: Deleting pod "simpletest.rc-vprmk" in namespace "gc-8752"
    Jan 30 22:47:29.476: INFO: Deleting pod "simpletest.rc-vv5x8" in namespace "gc-8752"
    Jan 30 22:47:29.511: INFO: Deleting pod "simpletest.rc-wkdr8" in namespace "gc-8752"
    Jan 30 22:47:29.549: INFO: Deleting pod "simpletest.rc-ww2dj" in namespace "gc-8752"
    Jan 30 22:47:29.583: INFO: Deleting pod "simpletest.rc-xk2b8" in namespace "gc-8752"
    Jan 30 22:47:29.631: INFO: Deleting pod "simpletest.rc-xnksm" in namespace "gc-8752"
    Jan 30 22:47:29.678: INFO: Deleting pod "simpletest.rc-xns5k" in namespace "gc-8752"
    Jan 30 22:47:29.717: INFO: Deleting pod "simpletest.rc-z26hq" in namespace "gc-8752"
    Jan 30 22:47:29.772: INFO: Deleting pod "simpletest.rc-z2jtn" in namespace "gc-8752"
    Jan 30 22:47:29.822: INFO: Deleting pod "simpletest.rc-zj9ff" in namespace "gc-8752"
    Jan 30 22:47:29.861: INFO: Deleting pod "simpletest.rc-zpf4q" in namespace "gc-8752"
    Jan 30 22:47:29.909: INFO: Deleting pod "simpletest.rc-zzlxh" in namespace "gc-8752"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:47:29.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8752" for this suite. 01/30/23 22:47:30.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:47:30.045
Jan 30 22:47:30.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 22:47:30.046
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:30.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:30.161
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/30/23 22:47:30.175
STEP: getting 01/30/23 22:47:30.241
STEP: listing 01/30/23 22:47:30.27
STEP: deleting 01/30/23 22:47:30.285
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:47:30.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-762" for this suite. 01/30/23 22:47:30.462
------------------------------
• [0.440 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:47:30.045
    Jan 30 22:47:30.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename csiinlinevolumes 01/30/23 22:47:30.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:30.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:30.161
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/30/23 22:47:30.175
    STEP: getting 01/30/23 22:47:30.241
    STEP: listing 01/30/23 22:47:30.27
    STEP: deleting 01/30/23 22:47:30.285
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:47:30.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-762" for this suite. 01/30/23 22:47:30.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:47:30.485
Jan 30 22:47:30.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 22:47:30.487
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:30.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:30.579
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 22:47:30.62
Jan 30 22:47:30.653: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1858" to be "running and ready"
Jan 30 22:47:30.671: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.05581ms
Jan 30 22:47:30.671: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:32.688: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034719226s
Jan 30 22:47:32.688: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:34.686: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032584787s
Jan 30 22:47:34.686: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:36.715: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061044931s
Jan 30 22:47:36.715: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:38.689: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.035010216s
Jan 30 22:47:38.689: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 22:47:38.689: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/30/23 22:47:38.713
Jan 30 22:47:38.753: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1858" to be "running and ready"
Jan 30 22:47:38.768: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.572615ms
Jan 30 22:47:38.768: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:40.786: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032225387s
Jan 30 22:47:40.786: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:42.800: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046353667s
Jan 30 22:47:42.800: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:47:44.785: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 6.031922169s
Jan 30 22:47:44.785: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 30 22:47:44.785: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/30/23 22:47:44.802
STEP: delete the pod with lifecycle hook 01/30/23 22:47:44.853
Jan 30 22:47:44.880: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 22:47:44.903: INFO: Pod pod-with-poststart-http-hook still exists
Jan 30 22:47:46.904: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 30 22:47:46.929: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 22:47:46.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1858" for this suite. 01/30/23 22:47:46.949
------------------------------
• [SLOW TEST] [16.487 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:47:30.485
    Jan 30 22:47:30.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 22:47:30.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:30.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:30.579
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 22:47:30.62
    Jan 30 22:47:30.653: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1858" to be "running and ready"
    Jan 30 22:47:30.671: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.05581ms
    Jan 30 22:47:30.671: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:32.688: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034719226s
    Jan 30 22:47:32.688: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:34.686: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032584787s
    Jan 30 22:47:34.686: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:36.715: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061044931s
    Jan 30 22:47:36.715: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:38.689: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.035010216s
    Jan 30 22:47:38.689: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 22:47:38.689: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/30/23 22:47:38.713
    Jan 30 22:47:38.753: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1858" to be "running and ready"
    Jan 30 22:47:38.768: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 14.572615ms
    Jan 30 22:47:38.768: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:40.786: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032225387s
    Jan 30 22:47:40.786: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:42.800: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046353667s
    Jan 30 22:47:42.800: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:47:44.785: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 6.031922169s
    Jan 30 22:47:44.785: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 30 22:47:44.785: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/30/23 22:47:44.802
    STEP: delete the pod with lifecycle hook 01/30/23 22:47:44.853
    Jan 30 22:47:44.880: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 30 22:47:44.903: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 30 22:47:46.904: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 30 22:47:46.929: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:47:46.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1858" for this suite. 01/30/23 22:47:46.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:47:46.974
Jan 30 22:47:46.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:47:46.976
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:47.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:47.083
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/30/23 22:47:47.098
Jan 30 22:47:47.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 create -f -'
Jan 30 22:47:47.446: INFO: stderr: ""
Jan 30 22:47:47.446: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/30/23 22:47:47.446
Jan 30 22:47:47.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 diff -f -'
Jan 30 22:47:47.824: INFO: rc: 1
Jan 30 22:47:47.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 delete -f -'
Jan 30 22:47:48.013: INFO: stderr: ""
Jan 30 22:47:48.013: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:47:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2569" for this suite. 01/30/23 22:47:48.032
------------------------------
• [1.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:47:46.974
    Jan 30 22:47:46.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:47:46.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:47.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:47.083
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/30/23 22:47:47.098
    Jan 30 22:47:47.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 create -f -'
    Jan 30 22:47:47.446: INFO: stderr: ""
    Jan 30 22:47:47.446: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/30/23 22:47:47.446
    Jan 30 22:47:47.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 diff -f -'
    Jan 30 22:47:47.824: INFO: rc: 1
    Jan 30 22:47:47.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2569 delete -f -'
    Jan 30 22:47:48.013: INFO: stderr: ""
    Jan 30 22:47:48.013: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:47:48.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2569" for this suite. 01/30/23 22:47:48.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:47:48.061
Jan 30 22:47:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/30/23 22:47:48.064
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:48.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:48.15
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1755 01/30/23 22:47:48.162
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-1755 01/30/23 22:47:48.179
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1755 01/30/23 22:47:48.202
Jan 30 22:47:48.227: INFO: Found 0 stateful pods, waiting for 1
Jan 30 22:47:58.245: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/30/23 22:47:58.245
Jan 30 22:47:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 22:47:58.694: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 22:47:58.694: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 22:47:58.694: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 22:47:58.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 22:48:08.736: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 22:48:08.736: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 22:48:08.827: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 30 22:48:08.827: INFO: ss-0  10.15.28.237  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
Jan 30 22:48:08.827: INFO: 
Jan 30 22:48:08.827: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 30 22:48:09.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981787645s
Jan 30 22:48:10.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.957556947s
Jan 30 22:48:11.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.922059145s
Jan 30 22:48:12.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.905513912s
Jan 30 22:48:13.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.858838274s
Jan 30 22:48:14.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.841316088s
Jan 30 22:48:16.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.824957179s
Jan 30 22:48:17.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.779178257s
Jan 30 22:48:18.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 764.264532ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1755 01/30/23 22:48:19.065
Jan 30 22:48:19.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 22:48:19.605: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 22:48:19.605: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 22:48:19.606: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 22:48:19.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 22:48:20.091: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 22:48:20.091: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 22:48:20.091: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 22:48:20.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 22:48:20.484: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 30 22:48:20.484: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 22:48:20.484: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 22:48:20.498: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 22:48:20.498: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 22:48:20.498: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/30/23 22:48:20.498
Jan 30 22:48:20.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 22:48:20.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 22:48:20.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 22:48:20.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 22:48:20.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 22:48:21.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 22:48:21.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 22:48:21.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 22:48:21.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 22:48:21.710: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 22:48:21.710: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 22:48:21.710: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 22:48:21.710: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 22:48:21.725: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 30 22:48:31.765: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 22:48:31.765: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 22:48:31.765: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 22:48:31.818: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 30 22:48:31.818: INFO: ss-0  10.15.28.237  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
Jan 30 22:48:31.818: INFO: ss-1  10.15.28.227  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
Jan 30 22:48:31.818: INFO: ss-2  10.15.28.225  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
Jan 30 22:48:31.818: INFO: 
Jan 30 22:48:31.818: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 22:48:32.841: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jan 30 22:48:32.842: INFO: ss-0  10.15.28.237  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
Jan 30 22:48:32.842: INFO: ss-1  10.15.28.227  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
Jan 30 22:48:32.842: INFO: ss-2  10.15.28.225  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
Jan 30 22:48:32.842: INFO: 
Jan 30 22:48:32.842: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 30 22:48:33.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.957761549s
Jan 30 22:48:34.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.940186986s
Jan 30 22:48:35.886: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.925966168s
Jan 30 22:48:36.902: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.913348914s
Jan 30 22:48:37.917: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.897503574s
Jan 30 22:48:38.934: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.882334967s
Jan 30 22:48:39.948: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.865930783s
Jan 30 22:48:40.965: INFO: Verifying statefulset ss doesn't scale past 0 for another 851.54851ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1755 01/30/23 22:48:41.965
Jan 30 22:48:41.984: INFO: Scaling statefulset ss to 0
Jan 30 22:48:42.030: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 22:48:42.044: INFO: Deleting all statefulset in ns statefulset-1755
Jan 30 22:48:42.059: INFO: Scaling statefulset ss to 0
Jan 30 22:48:42.102: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 22:48:42.115: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 22:48:42.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1755" for this suite. 01/30/23 22:48:42.197
------------------------------
• [SLOW TEST] [54.165 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:47:48.061
    Jan 30 22:47:48.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/30/23 22:47:48.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:47:48.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:47:48.15
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1755 01/30/23 22:47:48.162
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-1755 01/30/23 22:47:48.179
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1755 01/30/23 22:47:48.202
    Jan 30 22:47:48.227: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 22:47:58.245: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/30/23 22:47:58.245
    Jan 30 22:47:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 22:47:58.694: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 22:47:58.694: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 22:47:58.694: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 22:47:58.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 30 22:48:08.736: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 22:48:08.736: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 22:48:08.827: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jan 30 22:48:08.827: INFO: ss-0  10.15.28.237  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
    Jan 30 22:48:08.827: INFO: 
    Jan 30 22:48:08.827: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 30 22:48:09.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981787645s
    Jan 30 22:48:10.886: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.957556947s
    Jan 30 22:48:11.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.922059145s
    Jan 30 22:48:12.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.905513912s
    Jan 30 22:48:13.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.858838274s
    Jan 30 22:48:14.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.841316088s
    Jan 30 22:48:16.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.824957179s
    Jan 30 22:48:17.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.779178257s
    Jan 30 22:48:18.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 764.264532ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1755 01/30/23 22:48:19.065
    Jan 30 22:48:19.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 22:48:19.605: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 22:48:19.605: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 22:48:19.606: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 22:48:19.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 22:48:20.091: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 30 22:48:20.091: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 22:48:20.091: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 22:48:20.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 22:48:20.484: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 30 22:48:20.484: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 22:48:20.484: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 22:48:20.498: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 22:48:20.498: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 22:48:20.498: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/30/23 22:48:20.498
    Jan 30 22:48:20.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 22:48:20.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 22:48:20.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 22:48:20.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 22:48:20.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 22:48:21.267: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 22:48:21.267: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 22:48:21.267: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 22:48:21.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1755 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 22:48:21.710: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 22:48:21.710: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 22:48:21.710: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 22:48:21.710: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 22:48:21.725: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 30 22:48:31.765: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 22:48:31.765: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 22:48:31.765: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 22:48:31.818: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jan 30 22:48:31.818: INFO: ss-0  10.15.28.237  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
    Jan 30 22:48:31.818: INFO: ss-1  10.15.28.227  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
    Jan 30 22:48:31.818: INFO: ss-2  10.15.28.225  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
    Jan 30 22:48:31.818: INFO: 
    Jan 30 22:48:31.818: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 30 22:48:32.841: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jan 30 22:48:32.842: INFO: ss-0  10.15.28.237  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:47:48 +0000 UTC  }]
    Jan 30 22:48:32.842: INFO: ss-1  10.15.28.227  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
    Jan 30 22:48:32.842: INFO: ss-2  10.15.28.225  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:48:08 +0000 UTC  }]
    Jan 30 22:48:32.842: INFO: 
    Jan 30 22:48:32.842: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 30 22:48:33.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.957761549s
    Jan 30 22:48:34.873: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.940186986s
    Jan 30 22:48:35.886: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.925966168s
    Jan 30 22:48:36.902: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.913348914s
    Jan 30 22:48:37.917: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.897503574s
    Jan 30 22:48:38.934: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.882334967s
    Jan 30 22:48:39.948: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.865930783s
    Jan 30 22:48:40.965: INFO: Verifying statefulset ss doesn't scale past 0 for another 851.54851ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1755 01/30/23 22:48:41.965
    Jan 30 22:48:41.984: INFO: Scaling statefulset ss to 0
    Jan 30 22:48:42.030: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 22:48:42.044: INFO: Deleting all statefulset in ns statefulset-1755
    Jan 30 22:48:42.059: INFO: Scaling statefulset ss to 0
    Jan 30 22:48:42.102: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 22:48:42.115: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:48:42.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1755" for this suite. 01/30/23 22:48:42.197
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:48:42.23
Jan 30 22:48:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename endpointslicemirroring 01/30/23 22:48:42.233
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:48:42.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:48:42.311
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/30/23 22:48:42.372
Jan 30 22:48:42.407: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/30/23 22:48:44.425
Jan 30 22:48:44.468: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/30/23 22:48:46.482
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 30 22:48:46.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2310" for this suite. 01/30/23 22:48:46.577
------------------------------
• [4.373 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:48:42.23
    Jan 30 22:48:42.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename endpointslicemirroring 01/30/23 22:48:42.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:48:42.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:48:42.311
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/30/23 22:48:42.372
    Jan 30 22:48:42.407: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/30/23 22:48:44.425
    Jan 30 22:48:44.468: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/30/23 22:48:46.482
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:48:46.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2310" for this suite. 01/30/23 22:48:46.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:48:46.606
Jan 30 22:48:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename subpath 01/30/23 22:48:46.609
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:48:46.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:48:46.728
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 22:48:46.743
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-2bht 01/30/23 22:48:46.78
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 22:48:46.78
Jan 30 22:48:46.810: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2bht" in namespace "subpath-4013" to be "Succeeded or Failed"
Jan 30 22:48:46.824: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Pending", Reason="", readiness=false. Elapsed: 13.33493ms
Jan 30 22:48:48.843: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032531582s
Jan 30 22:48:50.839: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 4.028737809s
Jan 30 22:48:52.843: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 6.032550443s
Jan 30 22:48:54.840: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 8.02910139s
Jan 30 22:48:56.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 10.02798788s
Jan 30 22:48:58.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 12.027241737s
Jan 30 22:49:00.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 14.02710114s
Jan 30 22:49:02.844: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 16.033732775s
Jan 30 22:49:04.842: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 18.03178362s
Jan 30 22:49:06.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 20.030780903s
Jan 30 22:49:08.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 22.030849553s
Jan 30 22:49:10.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=false. Elapsed: 24.030030287s
Jan 30 22:49:12.840: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029453346s
STEP: Saw pod success 01/30/23 22:49:12.84
Jan 30 22:49:12.841: INFO: Pod "pod-subpath-test-secret-2bht" satisfied condition "Succeeded or Failed"
Jan 30 22:49:12.856: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-secret-2bht container test-container-subpath-secret-2bht: <nil>
STEP: delete the pod 01/30/23 22:49:12.988
Jan 30 22:49:13.034: INFO: Waiting for pod pod-subpath-test-secret-2bht to disappear
Jan 30 22:49:13.048: INFO: Pod pod-subpath-test-secret-2bht no longer exists
STEP: Deleting pod pod-subpath-test-secret-2bht 01/30/23 22:49:13.049
Jan 30 22:49:13.049: INFO: Deleting pod "pod-subpath-test-secret-2bht" in namespace "subpath-4013"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:13.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4013" for this suite. 01/30/23 22:49:13.095
------------------------------
• [SLOW TEST] [26.516 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:48:46.606
    Jan 30 22:48:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename subpath 01/30/23 22:48:46.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:48:46.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:48:46.728
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 22:48:46.743
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-2bht 01/30/23 22:48:46.78
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 22:48:46.78
    Jan 30 22:48:46.810: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2bht" in namespace "subpath-4013" to be "Succeeded or Failed"
    Jan 30 22:48:46.824: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Pending", Reason="", readiness=false. Elapsed: 13.33493ms
    Jan 30 22:48:48.843: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032531582s
    Jan 30 22:48:50.839: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 4.028737809s
    Jan 30 22:48:52.843: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 6.032550443s
    Jan 30 22:48:54.840: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 8.02910139s
    Jan 30 22:48:56.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 10.02798788s
    Jan 30 22:48:58.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 12.027241737s
    Jan 30 22:49:00.838: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 14.02710114s
    Jan 30 22:49:02.844: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 16.033732775s
    Jan 30 22:49:04.842: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 18.03178362s
    Jan 30 22:49:06.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 20.030780903s
    Jan 30 22:49:08.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=true. Elapsed: 22.030849553s
    Jan 30 22:49:10.841: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Running", Reason="", readiness=false. Elapsed: 24.030030287s
    Jan 30 22:49:12.840: INFO: Pod "pod-subpath-test-secret-2bht": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029453346s
    STEP: Saw pod success 01/30/23 22:49:12.84
    Jan 30 22:49:12.841: INFO: Pod "pod-subpath-test-secret-2bht" satisfied condition "Succeeded or Failed"
    Jan 30 22:49:12.856: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-secret-2bht container test-container-subpath-secret-2bht: <nil>
    STEP: delete the pod 01/30/23 22:49:12.988
    Jan 30 22:49:13.034: INFO: Waiting for pod pod-subpath-test-secret-2bht to disappear
    Jan 30 22:49:13.048: INFO: Pod pod-subpath-test-secret-2bht no longer exists
    STEP: Deleting pod pod-subpath-test-secret-2bht 01/30/23 22:49:13.049
    Jan 30 22:49:13.049: INFO: Deleting pod "pod-subpath-test-secret-2bht" in namespace "subpath-4013"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:13.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4013" for this suite. 01/30/23 22:49:13.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:13.123
Jan 30 22:49:13.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 22:49:13.125
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:13.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:13.241
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/30/23 22:49:13.316
STEP: watching for Pod to be ready 01/30/23 22:49:13.346
Jan 30 22:49:13.353: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 30 22:49:13.366: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
Jan 30 22:49:13.432: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
Jan 30 22:49:14.658: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
Jan 30 22:49:15.975: INFO: Found Pod pod-test in namespace pods-9832 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/30/23 22:49:15.989
STEP: getting the Pod and ensuring that it's patched 01/30/23 22:49:16.021
STEP: replacing the Pod's status Ready condition to False 01/30/23 22:49:16.036
STEP: check the Pod again to ensure its Ready conditions are False 01/30/23 22:49:16.078
STEP: deleting the Pod via a Collection with a LabelSelector 01/30/23 22:49:16.078
STEP: watching for the Pod to be deleted 01/30/23 22:49:16.116
Jan 30 22:49:16.125: INFO: observed event type MODIFIED
Jan 30 22:49:17.983: INFO: observed event type MODIFIED
Jan 30 22:49:18.433: INFO: observed event type MODIFIED
Jan 30 22:49:19.002: INFO: observed event type MODIFIED
Jan 30 22:49:19.326: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9832" for this suite. 01/30/23 22:49:19.376
------------------------------
• [SLOW TEST] [6.277 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:13.123
    Jan 30 22:49:13.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 22:49:13.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:13.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:13.241
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/30/23 22:49:13.316
    STEP: watching for Pod to be ready 01/30/23 22:49:13.346
    Jan 30 22:49:13.353: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 30 22:49:13.366: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
    Jan 30 22:49:13.432: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
    Jan 30 22:49:14.658: INFO: observed Pod pod-test in namespace pods-9832 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
    Jan 30 22:49:15.975: INFO: Found Pod pod-test in namespace pods-9832 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 22:49:13 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/30/23 22:49:15.989
    STEP: getting the Pod and ensuring that it's patched 01/30/23 22:49:16.021
    STEP: replacing the Pod's status Ready condition to False 01/30/23 22:49:16.036
    STEP: check the Pod again to ensure its Ready conditions are False 01/30/23 22:49:16.078
    STEP: deleting the Pod via a Collection with a LabelSelector 01/30/23 22:49:16.078
    STEP: watching for the Pod to be deleted 01/30/23 22:49:16.116
    Jan 30 22:49:16.125: INFO: observed event type MODIFIED
    Jan 30 22:49:17.983: INFO: observed event type MODIFIED
    Jan 30 22:49:18.433: INFO: observed event type MODIFIED
    Jan 30 22:49:19.002: INFO: observed event type MODIFIED
    Jan 30 22:49:19.326: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:19.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9832" for this suite. 01/30/23 22:49:19.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:19.405
Jan 30 22:49:19.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 22:49:19.407
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:19.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:19.513
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:19.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4667" for this suite. 01/30/23 22:49:19.785
------------------------------
• [0.412 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:19.405
    Jan 30 22:49:19.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 22:49:19.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:19.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:19.513
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:19.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4667" for this suite. 01/30/23 22:49:19.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:19.828
Jan 30 22:49:19.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 22:49:19.831
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:19.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:19.924
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/30/23 22:49:19.937
Jan 30 22:49:19.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244" in namespace "projected-8264" to be "Succeeded or Failed"
Jan 30 22:49:20.008: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 15.205361ms
Jan 30 22:49:22.025: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03273796s
Jan 30 22:49:24.023: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029857162s
Jan 30 22:49:26.023: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03076463s
STEP: Saw pod success 01/30/23 22:49:26.024
Jan 30 22:49:26.024: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244" satisfied condition "Succeeded or Failed"
Jan 30 22:49:26.039: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 container client-container: <nil>
STEP: delete the pod 01/30/23 22:49:26.08
Jan 30 22:49:26.125: INFO: Waiting for pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 to disappear
Jan 30 22:49:26.140: INFO: Pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:26.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8264" for this suite. 01/30/23 22:49:26.161
------------------------------
• [SLOW TEST] [6.364 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:19.828
    Jan 30 22:49:19.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 22:49:19.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:19.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:19.924
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/30/23 22:49:19.937
    Jan 30 22:49:19.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244" in namespace "projected-8264" to be "Succeeded or Failed"
    Jan 30 22:49:20.008: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 15.205361ms
    Jan 30 22:49:22.025: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03273796s
    Jan 30 22:49:24.023: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029857162s
    Jan 30 22:49:26.023: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03076463s
    STEP: Saw pod success 01/30/23 22:49:26.024
    Jan 30 22:49:26.024: INFO: Pod "downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244" satisfied condition "Succeeded or Failed"
    Jan 30 22:49:26.039: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 container client-container: <nil>
    STEP: delete the pod 01/30/23 22:49:26.08
    Jan 30 22:49:26.125: INFO: Waiting for pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 to disappear
    Jan 30 22:49:26.140: INFO: Pod downwardapi-volume-b33d7159-09f8-4921-9999-0a0ff6032244 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:26.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8264" for this suite. 01/30/23 22:49:26.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:26.195
Jan 30 22:49:26.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename endpointslice 01/30/23 22:49:26.198
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:26.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:26.282
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:26.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4110" for this suite. 01/30/23 22:49:26.537
------------------------------
• [0.367 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:26.195
    Jan 30 22:49:26.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename endpointslice 01/30/23 22:49:26.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:26.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:26.282
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:26.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4110" for this suite. 01/30/23 22:49:26.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:26.574
Jan 30 22:49:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 22:49:26.576
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:26.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:26.667
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 30 22:49:26.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:33.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1069" for this suite. 01/30/23 22:49:33.653
------------------------------
• [SLOW TEST] [7.105 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:26.574
    Jan 30 22:49:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 22:49:26.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:26.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:26.667
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 30 22:49:26.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:33.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1069" for this suite. 01/30/23 22:49:33.653
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:33.681
Jan 30 22:49:33.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename watch 01/30/23 22:49:33.684
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:33.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:33.787
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/30/23 22:49:33.804
STEP: creating a new configmap 01/30/23 22:49:33.81
STEP: modifying the configmap once 01/30/23 22:49:33.828
STEP: closing the watch once it receives two notifications 01/30/23 22:49:33.863
Jan 30 22:49:33.864: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23339 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 22:49:33.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23340 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/30/23 22:49:33.864
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/30/23 22:49:33.894
STEP: deleting the configmap 01/30/23 22:49:33.9
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/30/23 22:49:33.928
Jan 30 22:49:33.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23341 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 22:49:33.930: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23342 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:33.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-263" for this suite. 01/30/23 22:49:33.957
------------------------------
• [0.302 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:33.681
    Jan 30 22:49:33.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename watch 01/30/23 22:49:33.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:33.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:33.787
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/30/23 22:49:33.804
    STEP: creating a new configmap 01/30/23 22:49:33.81
    STEP: modifying the configmap once 01/30/23 22:49:33.828
    STEP: closing the watch once it receives two notifications 01/30/23 22:49:33.863
    Jan 30 22:49:33.864: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23339 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 22:49:33.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23340 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/30/23 22:49:33.864
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/30/23 22:49:33.894
    STEP: deleting the configmap 01/30/23 22:49:33.9
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/30/23 22:49:33.928
    Jan 30 22:49:33.929: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23341 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 22:49:33.930: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-263  b9bfe628-edbf-4a02-b938-f124ef72659a 23342 0 2023-01-30 22:49:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-30 22:49:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:33.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-263" for this suite. 01/30/23 22:49:33.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:33.998
Jan 30 22:49:33.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 22:49:34
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:34.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:34.099
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1525 01/30/23 22:49:34.115
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 22:49:34.173
STEP: creating service externalsvc in namespace services-1525 01/30/23 22:49:34.174
STEP: creating replication controller externalsvc in namespace services-1525 01/30/23 22:49:34.254
I0130 22:49:34.281084      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1525, replica count: 2
I0130 22:49:37.332375      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/30/23 22:49:37.345
Jan 30 22:49:37.418: INFO: Creating new exec pod
Jan 30 22:49:37.450: INFO: Waiting up to 5m0s for pod "execpodv6nmr" in namespace "services-1525" to be "running"
Jan 30 22:49:37.468: INFO: Pod "execpodv6nmr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.272887ms
Jan 30 22:49:39.488: INFO: Pod "execpodv6nmr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037165636s
Jan 30 22:49:41.485: INFO: Pod "execpodv6nmr": Phase="Running", Reason="", readiness=true. Elapsed: 4.034849155s
Jan 30 22:49:41.485: INFO: Pod "execpodv6nmr" satisfied condition "running"
Jan 30 22:49:41.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-1525 exec execpodv6nmr -- /bin/sh -x -c nslookup clusterip-service.services-1525.svc.cluster.local'
Jan 30 22:49:41.880: INFO: stderr: "+ nslookup clusterip-service.services-1525.svc.cluster.local\n"
Jan 30 22:49:41.880: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1525.svc.cluster.local\tcanonical name = externalsvc.services-1525.svc.cluster.local.\nName:\texternalsvc.services-1525.svc.cluster.local\nAddress: 172.21.113.34\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1525, will wait for the garbage collector to delete the pods 01/30/23 22:49:41.88
Jan 30 22:49:41.981: INFO: Deleting ReplicationController externalsvc took: 30.095112ms
Jan 30 22:49:42.081: INFO: Terminating ReplicationController externalsvc pods took: 100.735589ms
Jan 30 22:49:44.879: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:44.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1525" for this suite. 01/30/23 22:49:44.953
------------------------------
• [SLOW TEST] [10.983 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:33.998
    Jan 30 22:49:33.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 22:49:34
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:34.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:34.099
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1525 01/30/23 22:49:34.115
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/30/23 22:49:34.173
    STEP: creating service externalsvc in namespace services-1525 01/30/23 22:49:34.174
    STEP: creating replication controller externalsvc in namespace services-1525 01/30/23 22:49:34.254
    I0130 22:49:34.281084      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1525, replica count: 2
    I0130 22:49:37.332375      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/30/23 22:49:37.345
    Jan 30 22:49:37.418: INFO: Creating new exec pod
    Jan 30 22:49:37.450: INFO: Waiting up to 5m0s for pod "execpodv6nmr" in namespace "services-1525" to be "running"
    Jan 30 22:49:37.468: INFO: Pod "execpodv6nmr": Phase="Pending", Reason="", readiness=false. Elapsed: 17.272887ms
    Jan 30 22:49:39.488: INFO: Pod "execpodv6nmr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037165636s
    Jan 30 22:49:41.485: INFO: Pod "execpodv6nmr": Phase="Running", Reason="", readiness=true. Elapsed: 4.034849155s
    Jan 30 22:49:41.485: INFO: Pod "execpodv6nmr" satisfied condition "running"
    Jan 30 22:49:41.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-1525 exec execpodv6nmr -- /bin/sh -x -c nslookup clusterip-service.services-1525.svc.cluster.local'
    Jan 30 22:49:41.880: INFO: stderr: "+ nslookup clusterip-service.services-1525.svc.cluster.local\n"
    Jan 30 22:49:41.880: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1525.svc.cluster.local\tcanonical name = externalsvc.services-1525.svc.cluster.local.\nName:\texternalsvc.services-1525.svc.cluster.local\nAddress: 172.21.113.34\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1525, will wait for the garbage collector to delete the pods 01/30/23 22:49:41.88
    Jan 30 22:49:41.981: INFO: Deleting ReplicationController externalsvc took: 30.095112ms
    Jan 30 22:49:42.081: INFO: Terminating ReplicationController externalsvc pods took: 100.735589ms
    Jan 30 22:49:44.879: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:44.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1525" for this suite. 01/30/23 22:49:44.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:45
Jan 30 22:49:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 22:49:45.002
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:45.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:45.069
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 22:49:45.133
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:49:45.317
STEP: Deploying the webhook pod 01/30/23 22:49:45.352
STEP: Wait for the deployment to be ready 01/30/23 22:49:45.397
Jan 30 22:49:45.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 30 22:49:47.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 22:49:49.516
STEP: Verifying the service has paired with the endpoint 01/30/23 22:49:49.585
Jan 30 22:49:50.587: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/30/23 22:49:50.603
STEP: Creating a custom resource definition that should be denied by the webhook 01/30/23 22:49:50.705
Jan 30 22:49:50.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-27" for this suite. 01/30/23 22:49:51.021
STEP: Destroying namespace "webhook-27-markers" for this suite. 01/30/23 22:49:51.053
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:45
    Jan 30 22:49:45.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 22:49:45.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:45.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:45.069
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 22:49:45.133
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:49:45.317
    STEP: Deploying the webhook pod 01/30/23 22:49:45.352
    STEP: Wait for the deployment to be ready 01/30/23 22:49:45.397
    Jan 30 22:49:45.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 30 22:49:47.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 49, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 22:49:49.516
    STEP: Verifying the service has paired with the endpoint 01/30/23 22:49:49.585
    Jan 30 22:49:50.587: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/30/23 22:49:50.603
    STEP: Creating a custom resource definition that should be denied by the webhook 01/30/23 22:49:50.705
    Jan 30 22:49:50.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-27" for this suite. 01/30/23 22:49:51.021
    STEP: Destroying namespace "webhook-27-markers" for this suite. 01/30/23 22:49:51.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:51.097
Jan 30 22:49:51.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 22:49:51.099
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:51.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:51.194
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-807/configmap-test-51817656-e4c8-4a23-8692-6ced1a06d98d 01/30/23 22:49:51.207
STEP: Creating a pod to test consume configMaps 01/30/23 22:49:51.225
Jan 30 22:49:51.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c" in namespace "configmap-807" to be "Succeeded or Failed"
Jan 30 22:49:51.278: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.106102ms
Jan 30 22:49:53.296: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Running", Reason="", readiness=true. Elapsed: 2.033430912s
Jan 30 22:49:55.296: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033270162s
Jan 30 22:49:57.294: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031643518s
STEP: Saw pod success 01/30/23 22:49:57.294
Jan 30 22:49:57.295: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c" satisfied condition "Succeeded or Failed"
Jan 30 22:49:57.308: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c container env-test: <nil>
STEP: delete the pod 01/30/23 22:49:57.427
Jan 30 22:49:57.500: INFO: Waiting for pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c to disappear
Jan 30 22:49:57.514: INFO: Pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:49:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-807" for this suite. 01/30/23 22:49:57.533
------------------------------
• [SLOW TEST] [6.460 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:51.097
    Jan 30 22:49:51.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 22:49:51.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:51.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:51.194
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-807/configmap-test-51817656-e4c8-4a23-8692-6ced1a06d98d 01/30/23 22:49:51.207
    STEP: Creating a pod to test consume configMaps 01/30/23 22:49:51.225
    Jan 30 22:49:51.262: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c" in namespace "configmap-807" to be "Succeeded or Failed"
    Jan 30 22:49:51.278: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.106102ms
    Jan 30 22:49:53.296: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Running", Reason="", readiness=true. Elapsed: 2.033430912s
    Jan 30 22:49:55.296: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033270162s
    Jan 30 22:49:57.294: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031643518s
    STEP: Saw pod success 01/30/23 22:49:57.294
    Jan 30 22:49:57.295: INFO: Pod "pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c" satisfied condition "Succeeded or Failed"
    Jan 30 22:49:57.308: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c container env-test: <nil>
    STEP: delete the pod 01/30/23 22:49:57.427
    Jan 30 22:49:57.500: INFO: Waiting for pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c to disappear
    Jan 30 22:49:57.514: INFO: Pod pod-configmaps-3fa29686-6d19-4cda-b0ad-5607dfdff81c no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:49:57.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-807" for this suite. 01/30/23 22:49:57.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:49:57.562
Jan 30 22:49:57.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:49:57.566
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:57.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:57.642
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 22:49:57.653
Jan 30 22:49:57.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 30 22:49:57.795: INFO: stderr: ""
Jan 30 22:49:57.795: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/30/23 22:49:57.795
STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 22:50:02.847
Jan 30 22:50:02.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 get pod e2e-test-httpd-pod -o json'
Jan 30 22:50:02.974: INFO: stderr: ""
Jan 30 22:50:02.974: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2242278907a265b043948b97dd0f6d5d7ba4895a8bb7ab3a36d0905cddd07030\",\n            \"cni.projectcalico.org/podIP\": \"172.30.248.41/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.248.41/32\"\n        },\n        \"creationTimestamp\": \"2023-01-30T22:49:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-909\",\n        \"resourceVersion\": \"23611\",\n        \"uid\": \"79fe47e8-b216-45bf-b255-095acc64a474\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-484jx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.15.28.237\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-484jx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:50:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:50:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0cb2e26d2404687f912ca03cd57631857bf4a4a061bd432aa2f8b7a0027e4b92\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-30T22:49:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.15.28.237\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.248.41\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.248.41\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-30T22:49:57Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/30/23 22:50:02.975
Jan 30 22:50:02.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 replace -f -'
Jan 30 22:50:04.144: INFO: stderr: ""
Jan 30 22:50:04.144: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/30/23 22:50:04.144
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 30 22:50:04.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 delete pods e2e-test-httpd-pod'
Jan 30 22:50:06.356: INFO: stderr: ""
Jan 30 22:50:06.356: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:50:06.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-909" for this suite. 01/30/23 22:50:06.374
------------------------------
• [SLOW TEST] [8.840 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:49:57.562
    Jan 30 22:49:57.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:49:57.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:49:57.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:49:57.642
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 22:49:57.653
    Jan 30 22:49:57.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 30 22:49:57.795: INFO: stderr: ""
    Jan 30 22:49:57.795: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/30/23 22:49:57.795
    STEP: verifying the pod e2e-test-httpd-pod was created 01/30/23 22:50:02.847
    Jan 30 22:50:02.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 get pod e2e-test-httpd-pod -o json'
    Jan 30 22:50:02.974: INFO: stderr: ""
    Jan 30 22:50:02.974: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2242278907a265b043948b97dd0f6d5d7ba4895a8bb7ab3a36d0905cddd07030\",\n            \"cni.projectcalico.org/podIP\": \"172.30.248.41/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.248.41/32\"\n        },\n        \"creationTimestamp\": \"2023-01-30T22:49:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-909\",\n        \"resourceVersion\": \"23611\",\n        \"uid\": \"79fe47e8-b216-45bf-b255-095acc64a474\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-484jx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.15.28.237\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-484jx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:50:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:50:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-30T22:49:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0cb2e26d2404687f912ca03cd57631857bf4a4a061bd432aa2f8b7a0027e4b92\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-30T22:49:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.15.28.237\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.248.41\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.248.41\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-30T22:49:57Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/30/23 22:50:02.975
    Jan 30 22:50:02.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 replace -f -'
    Jan 30 22:50:04.144: INFO: stderr: ""
    Jan 30 22:50:04.144: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/30/23 22:50:04.144
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 30 22:50:04.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-909 delete pods e2e-test-httpd-pod'
    Jan 30 22:50:06.356: INFO: stderr: ""
    Jan 30 22:50:06.356: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:50:06.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-909" for this suite. 01/30/23 22:50:06.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:50:06.406
Jan 30 22:50:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 22:50:06.408
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:06.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:06.499
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 22:50:06.631
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 22:50:06.654
Jan 30 22:50:06.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:50:06.702: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:50:07.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 22:50:07.752: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 22:50:08.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 22:50:08.744: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
Jan 30 22:50:09.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 22:50:09.738: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/30/23 22:50:09.752
STEP: DeleteCollection of the DaemonSets 01/30/23 22:50:09.767
STEP: Verify that ReplicaSets have been deleted 01/30/23 22:50:09.819
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 30 22:50:09.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23694"},"items":null}

Jan 30 22:50:09.909: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23694"},"items":[{"metadata":{"name":"daemon-set-5rmwq","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"af228b46-98d5-4d32-838b-9c251c79df0b","resourceVersion":"23688","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4562e640deac78f90f765adfd7082105007e0eaa0ef0bd9210eef75280561948","cni.projectcalico.org/podIP":"172.30.199.22/32","cni.projectcalico.org/podIPs":"172.30.199.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q5rt2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q5rt2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.227","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.227"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.227","podIP":"172.30.199.22","podIPs":[{"ip":"172.30.199.22"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0761ed997fb2d928feb256389c6e713ae8bb3bab05547e1a0a8e395132816d70","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k5r6t","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"de8d871b-185f-4cff-a770-a4e5b98660e2","resourceVersion":"23690","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c7d1ba444c8ff27a0c7dfa41378145ec79f340d87bdfc17ec52c08fbdf2b5421","cni.projectcalico.org/podIP":"172.30.248.43/32","cni.projectcalico.org/podIPs":"172.30.248.43/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-m25pk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-m25pk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.237","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.237"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.237","podIP":"172.30.248.43","podIPs":[{"ip":"172.30.248.43"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5e4d2c2e038747660d4150bbddf9261f773f50b009ff0c8c9560f48d38c7ad81","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vwqvx","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"24924763-9b6b-4d45-80f9-4acda0b79716","resourceVersion":"23686","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a7a96bff9e0193ebfa235824af101524672ee3719faa5545884a96a09f4e1826","cni.projectcalico.org/podIP":"172.30.237.158/32","cni.projectcalico.org/podIPs":"172.30.237.158/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rpbb6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rpbb6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.225","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.225"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.225","podIP":"172.30.237.158","podIPs":[{"ip":"172.30.237.158"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d21635dca387f22dd604c05d842d18f697e6f4e480863486b53c763e2b3f2d17","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:50:09.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1376" for this suite. 01/30/23 22:50:10.019
------------------------------
• [3.666 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:50:06.406
    Jan 30 22:50:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 22:50:06.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:06.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:06.499
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 22:50:06.631
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 22:50:06.654
    Jan 30 22:50:06.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:50:06.702: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:50:07.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 22:50:07.752: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 22:50:08.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 22:50:08.744: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
    Jan 30 22:50:09.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 22:50:09.738: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/30/23 22:50:09.752
    STEP: DeleteCollection of the DaemonSets 01/30/23 22:50:09.767
    STEP: Verify that ReplicaSets have been deleted 01/30/23 22:50:09.819
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 30 22:50:09.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23694"},"items":null}

    Jan 30 22:50:09.909: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23694"},"items":[{"metadata":{"name":"daemon-set-5rmwq","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"af228b46-98d5-4d32-838b-9c251c79df0b","resourceVersion":"23688","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4562e640deac78f90f765adfd7082105007e0eaa0ef0bd9210eef75280561948","cni.projectcalico.org/podIP":"172.30.199.22/32","cni.projectcalico.org/podIPs":"172.30.199.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q5rt2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q5rt2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.227","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.227"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.227","podIP":"172.30.199.22","podIPs":[{"ip":"172.30.199.22"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0761ed997fb2d928feb256389c6e713ae8bb3bab05547e1a0a8e395132816d70","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k5r6t","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"de8d871b-185f-4cff-a770-a4e5b98660e2","resourceVersion":"23690","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c7d1ba444c8ff27a0c7dfa41378145ec79f340d87bdfc17ec52c08fbdf2b5421","cni.projectcalico.org/podIP":"172.30.248.43/32","cni.projectcalico.org/podIPs":"172.30.248.43/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-m25pk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-m25pk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.237","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.237"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.237","podIP":"172.30.248.43","podIPs":[{"ip":"172.30.248.43"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5e4d2c2e038747660d4150bbddf9261f773f50b009ff0c8c9560f48d38c7ad81","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vwqvx","generateName":"daemon-set-","namespace":"daemonsets-1376","uid":"24924763-9b6b-4d45-80f9-4acda0b79716","resourceVersion":"23686","creationTimestamp":"2023-01-30T22:50:06Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a7a96bff9e0193ebfa235824af101524672ee3719faa5545884a96a09f4e1826","cni.projectcalico.org/podIP":"172.30.237.158/32","cni.projectcalico.org/podIPs":"172.30.237.158/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9b5f6f7b-4f7c-437e-837c-78e884a1170f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b5f6f7b-4f7c-437e-837c-78e884a1170f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-30T22:50:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rpbb6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rpbb6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.15.28.225","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.15.28.225"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-30T22:50:06Z"}],"hostIP":"10.15.28.225","podIP":"172.30.237.158","podIPs":[{"ip":"172.30.237.158"}],"startTime":"2023-01-30T22:50:06Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-30T22:50:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d21635dca387f22dd604c05d842d18f697e6f4e480863486b53c763e2b3f2d17","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:50:09.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1376" for this suite. 01/30/23 22:50:10.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:50:10.076
Jan 30 22:50:10.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pod-network-test 01/30/23 22:50:10.08
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:10.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:10.176
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4812 01/30/23 22:50:10.19
STEP: creating a selector 01/30/23 22:50:10.191
STEP: Creating the service pods in kubernetes 01/30/23 22:50:10.191
Jan 30 22:50:10.192: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 30 22:50:10.293: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4812" to be "running and ready"
Jan 30 22:50:10.324: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.811583ms
Jan 30 22:50:10.324: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:50:12.338: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045337879s
Jan 30 22:50:12.338: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:50:14.338: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.045465096s
Jan 30 22:50:14.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:16.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046226114s
Jan 30 22:50:16.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:18.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.050612886s
Jan 30 22:50:18.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:20.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.04847519s
Jan 30 22:50:20.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:22.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046726954s
Jan 30 22:50:22.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:24.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.047525227s
Jan 30 22:50:24.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:26.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.045877286s
Jan 30 22:50:26.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:28.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.048536195s
Jan 30 22:50:28.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:30.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048185705s
Jan 30 22:50:30.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 30 22:50:32.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.047745204s
Jan 30 22:50:32.341: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 30 22:50:32.341: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 30 22:50:32.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4812" to be "running and ready"
Jan 30 22:50:32.367: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.60879ms
Jan 30 22:50:32.367: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 30 22:50:32.367: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 30 22:50:32.380: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4812" to be "running and ready"
Jan 30 22:50:32.395: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.177413ms
Jan 30 22:50:32.395: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 30 22:50:32.395: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/30/23 22:50:32.408
Jan 30 22:50:32.448: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4812" to be "running"
Jan 30 22:50:32.466: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.472551ms
Jan 30 22:50:34.485: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037150658s
Jan 30 22:50:36.482: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.03382806s
Jan 30 22:50:36.482: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 30 22:50:36.496: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4812" to be "running"
Jan 30 22:50:36.512: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 16.068213ms
Jan 30 22:50:36.512: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 30 22:50:36.527: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 30 22:50:36.527: INFO: Going to poll 172.30.237.159 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 30 22:50:36.541: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.237.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:50:36.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:50:36.544: INFO: ExecWithOptions: Clientset creation
Jan 30 22:50:36.545: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.237.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 22:50:36.840: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 30 22:50:36.840: INFO: Going to poll 172.30.199.23 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 30 22:50:36.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.199.23:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:50:36.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:50:36.857: INFO: ExecWithOptions: Clientset creation
Jan 30 22:50:36.857: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.199.23%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 22:50:37.155: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 30 22:50:37.155: INFO: Going to poll 172.30.248.44 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 30 22:50:37.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.248.44:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:50:37.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:50:37.170: INFO: ExecWithOptions: Clientset creation
Jan 30 22:50:37.170: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.248.44%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 30 22:50:37.446: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 30 22:50:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4812" for this suite. 01/30/23 22:50:37.466
------------------------------
• [SLOW TEST] [27.424 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:50:10.076
    Jan 30 22:50:10.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pod-network-test 01/30/23 22:50:10.08
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:10.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:10.176
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4812 01/30/23 22:50:10.19
    STEP: creating a selector 01/30/23 22:50:10.191
    STEP: Creating the service pods in kubernetes 01/30/23 22:50:10.191
    Jan 30 22:50:10.192: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 30 22:50:10.293: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4812" to be "running and ready"
    Jan 30 22:50:10.324: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.811583ms
    Jan 30 22:50:10.324: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:50:12.338: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045337879s
    Jan 30 22:50:12.338: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:50:14.338: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.045465096s
    Jan 30 22:50:14.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:16.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046226114s
    Jan 30 22:50:16.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:18.343: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.050612886s
    Jan 30 22:50:18.344: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:20.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.04847519s
    Jan 30 22:50:20.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:22.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.046726954s
    Jan 30 22:50:22.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:24.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.047525227s
    Jan 30 22:50:24.340: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:26.339: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.045877286s
    Jan 30 22:50:26.339: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:28.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.048536195s
    Jan 30 22:50:28.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:30.341: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048185705s
    Jan 30 22:50:30.341: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 30 22:50:32.340: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.047745204s
    Jan 30 22:50:32.341: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 30 22:50:32.341: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 30 22:50:32.354: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4812" to be "running and ready"
    Jan 30 22:50:32.367: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.60879ms
    Jan 30 22:50:32.367: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 30 22:50:32.367: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 30 22:50:32.380: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4812" to be "running and ready"
    Jan 30 22:50:32.395: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.177413ms
    Jan 30 22:50:32.395: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 30 22:50:32.395: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/30/23 22:50:32.408
    Jan 30 22:50:32.448: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4812" to be "running"
    Jan 30 22:50:32.466: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.472551ms
    Jan 30 22:50:34.485: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037150658s
    Jan 30 22:50:36.482: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.03382806s
    Jan 30 22:50:36.482: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 30 22:50:36.496: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4812" to be "running"
    Jan 30 22:50:36.512: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 16.068213ms
    Jan 30 22:50:36.512: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 30 22:50:36.527: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 30 22:50:36.527: INFO: Going to poll 172.30.237.159 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 30 22:50:36.541: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.237.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:50:36.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:50:36.544: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:50:36.545: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.237.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 22:50:36.840: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 30 22:50:36.840: INFO: Going to poll 172.30.199.23 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 30 22:50:36.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.199.23:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:50:36.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:50:36.857: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:50:36.857: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.199.23%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 22:50:37.155: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 30 22:50:37.155: INFO: Going to poll 172.30.248.44 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 30 22:50:37.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.248.44:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:50:37.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:50:37.170: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:50:37.170: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4812/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.248.44%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 30 22:50:37.446: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:50:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4812" for this suite. 01/30/23 22:50:37.466
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:50:37.5
Jan 30 22:50:37.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 22:50:37.502
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:37.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:37.575
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/30/23 22:50:37.604
STEP: Patching the Job 01/30/23 22:50:37.622
STEP: Watching for Job to be patched 01/30/23 22:50:37.663
Jan 30 22:50:37.669: INFO: Event ADDED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 30 22:50:37.670: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 30 22:50:37.670: INFO: Event MODIFIED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/30/23 22:50:37.67
STEP: Watching for Job to be updated 01/30/23 22:50:37.706
Jan 30 22:50:37.714: INFO: Event MODIFIED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:37.714: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/30/23 22:50:37.714
Jan 30 22:50:37.730: INFO: Job: e2e-rdw4f as labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched]
STEP: Waiting for job to complete 01/30/23 22:50:37.73
STEP: Delete a job collection with a labelselector 01/30/23 22:50:49.774
STEP: Watching for Job to be deleted 01/30/23 22:50:49.839
Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.849: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.849: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.850: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.850: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 30 22:50:49.850: INFO: Event DELETED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/30/23 22:50:49.85
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 22:50:49.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8959" for this suite. 01/30/23 22:50:49.893
------------------------------
• [SLOW TEST] [12.417 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:50:37.5
    Jan 30 22:50:37.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 22:50:37.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:37.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:37.575
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/30/23 22:50:37.604
    STEP: Patching the Job 01/30/23 22:50:37.622
    STEP: Watching for Job to be patched 01/30/23 22:50:37.663
    Jan 30 22:50:37.669: INFO: Event ADDED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 30 22:50:37.670: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 30 22:50:37.670: INFO: Event MODIFIED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/30/23 22:50:37.67
    STEP: Watching for Job to be updated 01/30/23 22:50:37.706
    Jan 30 22:50:37.714: INFO: Event MODIFIED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:37.714: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/30/23 22:50:37.714
    Jan 30 22:50:37.730: INFO: Job: e2e-rdw4f as labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched]
    STEP: Waiting for job to complete 01/30/23 22:50:37.73
    STEP: Delete a job collection with a labelselector 01/30/23 22:50:49.774
    STEP: Watching for Job to be deleted 01/30/23 22:50:49.839
    Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.848: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.849: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.849: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.850: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.850: INFO: Event MODIFIED observed for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 30 22:50:49.850: INFO: Event DELETED found for Job e2e-rdw4f in namespace job-8959 with labels: map[e2e-job-label:e2e-rdw4f e2e-rdw4f:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/30/23 22:50:49.85
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:50:49.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8959" for this suite. 01/30/23 22:50:49.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:50:49.921
Jan 30 22:50:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 22:50:49.925
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:50.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:50.077
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-9098a471-eeec-42b0-b451-6a0aed09cd31 01/30/23 22:50:50.109
STEP: Creating the pod 01/30/23 22:50:50.128
Jan 30 22:50:50.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8" in namespace "configmap-2771" to be "running"
Jan 30 22:50:50.168: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.712937ms
Jan 30 22:50:52.213: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05827457s
Jan 30 22:50:54.187: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.031544439s
Jan 30 22:50:54.187: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8" satisfied condition "running"
STEP: Waiting for pod with text data 01/30/23 22:50:54.187
STEP: Waiting for pod with binary data 01/30/23 22:50:54.222
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:50:54.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2771" for this suite. 01/30/23 22:50:54.281
------------------------------
• [4.385 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:50:49.921
    Jan 30 22:50:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 22:50:49.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:50.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:50.077
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-9098a471-eeec-42b0-b451-6a0aed09cd31 01/30/23 22:50:50.109
    STEP: Creating the pod 01/30/23 22:50:50.128
    Jan 30 22:50:50.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8" in namespace "configmap-2771" to be "running"
    Jan 30 22:50:50.168: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.712937ms
    Jan 30 22:50:52.213: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05827457s
    Jan 30 22:50:54.187: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8": Phase="Running", Reason="", readiness=false. Elapsed: 4.031544439s
    Jan 30 22:50:54.187: INFO: Pod "pod-configmaps-eee56d1a-1fe8-426b-a5a9-01baf05359d8" satisfied condition "running"
    STEP: Waiting for pod with text data 01/30/23 22:50:54.187
    STEP: Waiting for pod with binary data 01/30/23 22:50:54.222
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:50:54.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2771" for this suite. 01/30/23 22:50:54.281
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:50:54.312
Jan 30 22:50:54.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 22:50:54.314
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:54.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:54.391
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 22:50:54.404
Jan 30 22:50:54.437: INFO: Waiting up to 5m0s for pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a" in namespace "emptydir-2927" to be "Succeeded or Failed"
Jan 30 22:50:54.452: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.040245ms
Jan 30 22:50:56.466: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028974225s
Jan 30 22:50:58.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030555705s
Jan 30 22:51:00.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029995112s
STEP: Saw pod success 01/30/23 22:51:00.467
Jan 30 22:51:00.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a" satisfied condition "Succeeded or Failed"
Jan 30 22:51:00.483: INFO: Trying to get logs from node 10.15.28.237 pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a container test-container: <nil>
STEP: delete the pod 01/30/23 22:51:00.52
Jan 30 22:51:00.560: INFO: Waiting for pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a to disappear
Jan 30 22:51:00.577: INFO: Pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:51:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2927" for this suite. 01/30/23 22:51:00.601
------------------------------
• [SLOW TEST] [6.312 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:50:54.312
    Jan 30 22:50:54.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 22:50:54.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:50:54.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:50:54.391
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/30/23 22:50:54.404
    Jan 30 22:50:54.437: INFO: Waiting up to 5m0s for pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a" in namespace "emptydir-2927" to be "Succeeded or Failed"
    Jan 30 22:50:54.452: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.040245ms
    Jan 30 22:50:56.466: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028974225s
    Jan 30 22:50:58.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030555705s
    Jan 30 22:51:00.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029995112s
    STEP: Saw pod success 01/30/23 22:51:00.467
    Jan 30 22:51:00.467: INFO: Pod "pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a" satisfied condition "Succeeded or Failed"
    Jan 30 22:51:00.483: INFO: Trying to get logs from node 10.15.28.237 pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a container test-container: <nil>
    STEP: delete the pod 01/30/23 22:51:00.52
    Jan 30 22:51:00.560: INFO: Waiting for pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a to disappear
    Jan 30 22:51:00.577: INFO: Pod pod-bac37340-a7d7-4fed-9da4-dbb585b0aa4a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:51:00.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2927" for this suite. 01/30/23 22:51:00.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:51:00.633
Jan 30 22:51:00.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 22:51:00.635
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:00.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:00.736
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 22:51:00.825
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:51:01.451
STEP: Deploying the webhook pod 01/30/23 22:51:01.488
STEP: Wait for the deployment to be ready 01/30/23 22:51:01.549
Jan 30 22:51:01.612: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 30 22:51:03.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 22:51:05.708
STEP: Verifying the service has paired with the endpoint 01/30/23 22:51:05.831
Jan 30 22:51:06.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 30 22:51:06.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3923-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 22:51:07.413
STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 22:51:07.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:51:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5030" for this suite. 01/30/23 22:51:10.577
STEP: Destroying namespace "webhook-5030-markers" for this suite. 01/30/23 22:51:10.613
------------------------------
• [SLOW TEST] [10.036 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:51:00.633
    Jan 30 22:51:00.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 22:51:00.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:00.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:00.736
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 22:51:00.825
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:51:01.451
    STEP: Deploying the webhook pod 01/30/23 22:51:01.488
    STEP: Wait for the deployment to be ready 01/30/23 22:51:01.549
    Jan 30 22:51:01.612: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 30 22:51:03.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 51, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 22:51:05.708
    STEP: Verifying the service has paired with the endpoint 01/30/23 22:51:05.831
    Jan 30 22:51:06.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 30 22:51:06.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3923-crds.webhook.example.com via the AdmissionRegistration API 01/30/23 22:51:07.413
    STEP: Creating a custom resource that should be mutated by the webhook 01/30/23 22:51:07.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:51:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5030" for this suite. 01/30/23 22:51:10.577
    STEP: Destroying namespace "webhook-5030-markers" for this suite. 01/30/23 22:51:10.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:51:10.696
Jan 30 22:51:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 22:51:10.697
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:10.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:10.799
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b7a842b7-3aa0-4ce6-b121-0d36f99f0207 01/30/23 22:51:10.816
STEP: Creating secret with name secret-projected-all-test-volume-25b24010-baca-4214-8db6-5ae8e4e67a6c 01/30/23 22:51:10.843
STEP: Creating a pod to test Check all projections for projected volume plugin 01/30/23 22:51:10.871
Jan 30 22:51:10.900: INFO: Waiting up to 5m0s for pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6" in namespace "projected-3066" to be "Succeeded or Failed"
Jan 30 22:51:10.928: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.674626ms
Jan 30 22:51:12.948: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047829675s
Jan 30 22:51:14.950: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050436292s
Jan 30 22:51:16.978: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078088939s
STEP: Saw pod success 01/30/23 22:51:16.978
Jan 30 22:51:16.978: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6" satisfied condition "Succeeded or Failed"
Jan 30 22:51:17.002: INFO: Trying to get logs from node 10.15.28.237 pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 container projected-all-volume-test: <nil>
STEP: delete the pod 01/30/23 22:51:17.137
Jan 30 22:51:17.318: INFO: Waiting for pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 to disappear
Jan 30 22:51:17.344: INFO: Pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 30 22:51:17.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3066" for this suite. 01/30/23 22:51:17.365
------------------------------
• [SLOW TEST] [6.710 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:51:10.696
    Jan 30 22:51:10.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 22:51:10.697
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:10.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:10.799
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b7a842b7-3aa0-4ce6-b121-0d36f99f0207 01/30/23 22:51:10.816
    STEP: Creating secret with name secret-projected-all-test-volume-25b24010-baca-4214-8db6-5ae8e4e67a6c 01/30/23 22:51:10.843
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/30/23 22:51:10.871
    Jan 30 22:51:10.900: INFO: Waiting up to 5m0s for pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6" in namespace "projected-3066" to be "Succeeded or Failed"
    Jan 30 22:51:10.928: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.674626ms
    Jan 30 22:51:12.948: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047829675s
    Jan 30 22:51:14.950: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050436292s
    Jan 30 22:51:16.978: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.078088939s
    STEP: Saw pod success 01/30/23 22:51:16.978
    Jan 30 22:51:16.978: INFO: Pod "projected-volume-97468255-5016-45b8-9195-888a8dce26b6" satisfied condition "Succeeded or Failed"
    Jan 30 22:51:17.002: INFO: Trying to get logs from node 10.15.28.237 pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/30/23 22:51:17.137
    Jan 30 22:51:17.318: INFO: Waiting for pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 to disappear
    Jan 30 22:51:17.344: INFO: Pod projected-volume-97468255-5016-45b8-9195-888a8dce26b6 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:51:17.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3066" for this suite. 01/30/23 22:51:17.365
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:51:17.407
Jan 30 22:51:17.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 22:51:17.41
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:17.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:17.524
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba in namespace container-probe-9564 01/30/23 22:51:17.544
Jan 30 22:51:17.595: INFO: Waiting up to 5m0s for pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba" in namespace "container-probe-9564" to be "not pending"
Jan 30 22:51:17.641: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Pending", Reason="", readiness=false. Elapsed: 46.197253ms
Jan 30 22:51:19.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066742275s
Jan 30 22:51:21.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Running", Reason="", readiness=true. Elapsed: 4.066535796s
Jan 30 22:51:21.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba" satisfied condition "not pending"
Jan 30 22:51:21.662: INFO: Started pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba in namespace container-probe-9564
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 22:51:21.662
Jan 30 22:51:21.681: INFO: Initial restart count of pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba is 0
Jan 30 22:52:10.350: INFO: Restart count of pod container-probe-9564/busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba is now 1 (48.668618023s elapsed)
STEP: deleting the pod 01/30/23 22:52:10.35
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 22:52:10.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9564" for this suite. 01/30/23 22:52:10.445
------------------------------
• [SLOW TEST] [53.087 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:51:17.407
    Jan 30 22:51:17.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 22:51:17.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:51:17.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:51:17.524
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba in namespace container-probe-9564 01/30/23 22:51:17.544
    Jan 30 22:51:17.595: INFO: Waiting up to 5m0s for pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba" in namespace "container-probe-9564" to be "not pending"
    Jan 30 22:51:17.641: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Pending", Reason="", readiness=false. Elapsed: 46.197253ms
    Jan 30 22:51:19.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066742275s
    Jan 30 22:51:21.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba": Phase="Running", Reason="", readiness=true. Elapsed: 4.066535796s
    Jan 30 22:51:21.662: INFO: Pod "busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba" satisfied condition "not pending"
    Jan 30 22:51:21.662: INFO: Started pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba in namespace container-probe-9564
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 22:51:21.662
    Jan 30 22:51:21.681: INFO: Initial restart count of pod busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba is 0
    Jan 30 22:52:10.350: INFO: Restart count of pod container-probe-9564/busybox-f67a1dd8-6d51-422d-a82e-0b56b8f9ebba is now 1 (48.668618023s elapsed)
    STEP: deleting the pod 01/30/23 22:52:10.35
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:52:10.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9564" for this suite. 01/30/23 22:52:10.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:52:10.495
Jan 30 22:52:10.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 22:52:10.498
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:10.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:10.588
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/30/23 22:52:10.604
STEP: Ensuring job reaches completions 01/30/23 22:52:10.643
STEP: Ensuring pods with index for job exist 01/30/23 22:52:22.663
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 22:52:22.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1980" for this suite. 01/30/23 22:52:22.714
------------------------------
• [SLOW TEST] [12.257 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:52:10.495
    Jan 30 22:52:10.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 22:52:10.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:10.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:10.588
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/30/23 22:52:10.604
    STEP: Ensuring job reaches completions 01/30/23 22:52:10.643
    STEP: Ensuring pods with index for job exist 01/30/23 22:52:22.663
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:52:22.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1980" for this suite. 01/30/23 22:52:22.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:52:22.759
Jan 30 22:52:22.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename init-container 01/30/23 22:52:22.761
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:22.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:22.842
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/30/23 22:52:22.858
Jan 30 22:52:22.859: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:52:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3652" for this suite. 01/30/23 22:52:27.681
------------------------------
• [4.955 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:52:22.759
    Jan 30 22:52:22.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename init-container 01/30/23 22:52:22.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:22.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:22.842
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/30/23 22:52:22.858
    Jan 30 22:52:22.859: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:52:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3652" for this suite. 01/30/23 22:52:27.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:52:27.719
Jan 30 22:52:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename hostport 01/30/23 22:52:27.721
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:27.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:27.836
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/30/23 22:52:27.928
Jan 30 22:52:27.996: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8482" to be "running and ready"
Jan 30 22:52:28.021: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.917821ms
Jan 30 22:52:28.021: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:30.042: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045782071s
Jan 30 22:52:30.042: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:32.041: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.04425887s
Jan 30 22:52:32.041: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 22:52:32.041: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.15.28.227 on the node which pod1 resides and expect scheduled 01/30/23 22:52:32.041
Jan 30 22:52:32.065: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8482" to be "running and ready"
Jan 30 22:52:32.084: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.16812ms
Jan 30 22:52:32.084: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:34.102: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037168229s
Jan 30 22:52:34.102: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:36.105: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.039665094s
Jan 30 22:52:36.105: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 22:52:36.105: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.15.28.227 but use UDP protocol on the node which pod2 resides 01/30/23 22:52:36.105
Jan 30 22:52:36.131: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8482" to be "running and ready"
Jan 30 22:52:36.149: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.204019ms
Jan 30 22:52:36.149: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:38.170: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038423649s
Jan 30 22:52:38.170: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:40.171: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.039815002s
Jan 30 22:52:40.171: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 30 22:52:40.171: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 30 22:52:40.195: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8482" to be "running and ready"
Jan 30 22:52:40.213: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.226492ms
Jan 30 22:52:40.213: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:52:42.235: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.039927303s
Jan 30 22:52:42.235: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 30 22:52:42.235: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/30/23 22:52:42.253
Jan 30 22:52:42.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.15.28.227 http://127.0.0.1:54323/hostname] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:52:42.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:52:42.255: INFO: ExecWithOptions: Clientset creation
Jan 30 22:52:42.256: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.15.28.227+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.28.227, port: 54323 01/30/23 22:52:42.571
Jan 30 22:52:42.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.15.28.227:54323/hostname] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:52:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:52:42.573: INFO: ExecWithOptions: Clientset creation
Jan 30 22:52:42.573: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.15.28.227%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.28.227, port: 54323 UDP 01/30/23 22:52:42.858
Jan 30 22:52:42.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.15.28.227 54323] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 30 22:52:42.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 22:52:42.861: INFO: ExecWithOptions: Clientset creation
Jan 30 22:52:42.861: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.15.28.227+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 30 22:52:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-8482" for this suite. 01/30/23 22:52:48.156
------------------------------
• [SLOW TEST] [20.472 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:52:27.719
    Jan 30 22:52:27.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename hostport 01/30/23 22:52:27.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:27.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:27.836
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/30/23 22:52:27.928
    Jan 30 22:52:27.996: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8482" to be "running and ready"
    Jan 30 22:52:28.021: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.917821ms
    Jan 30 22:52:28.021: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:30.042: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045782071s
    Jan 30 22:52:30.042: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:32.041: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.04425887s
    Jan 30 22:52:32.041: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 22:52:32.041: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.15.28.227 on the node which pod1 resides and expect scheduled 01/30/23 22:52:32.041
    Jan 30 22:52:32.065: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8482" to be "running and ready"
    Jan 30 22:52:32.084: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.16812ms
    Jan 30 22:52:32.084: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:34.102: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037168229s
    Jan 30 22:52:34.102: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:36.105: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.039665094s
    Jan 30 22:52:36.105: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 22:52:36.105: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.15.28.227 but use UDP protocol on the node which pod2 resides 01/30/23 22:52:36.105
    Jan 30 22:52:36.131: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8482" to be "running and ready"
    Jan 30 22:52:36.149: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.204019ms
    Jan 30 22:52:36.149: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:38.170: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038423649s
    Jan 30 22:52:38.170: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:40.171: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.039815002s
    Jan 30 22:52:40.171: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 30 22:52:40.171: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 30 22:52:40.195: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8482" to be "running and ready"
    Jan 30 22:52:40.213: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.226492ms
    Jan 30 22:52:40.213: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:52:42.235: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.039927303s
    Jan 30 22:52:42.235: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 30 22:52:42.235: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/30/23 22:52:42.253
    Jan 30 22:52:42.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.15.28.227 http://127.0.0.1:54323/hostname] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:52:42.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:52:42.255: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:52:42.256: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.15.28.227+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.28.227, port: 54323 01/30/23 22:52:42.571
    Jan 30 22:52:42.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.15.28.227:54323/hostname] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:52:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:52:42.573: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:52:42.573: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.15.28.227%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.15.28.227, port: 54323 UDP 01/30/23 22:52:42.858
    Jan 30 22:52:42.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.15.28.227 54323] Namespace:hostport-8482 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 30 22:52:42.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 22:52:42.861: INFO: ExecWithOptions: Clientset creation
    Jan 30 22:52:42.861: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-8482/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.15.28.227+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:52:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-8482" for this suite. 01/30/23 22:52:48.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:52:48.195
Jan 30 22:52:48.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 22:52:48.198
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:48.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:48.293
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-62940077-6510-40ab-b21c-8e3f4109f8ae 01/30/23 22:52:48.309
STEP: Creating a pod to test consume configMaps 01/30/23 22:52:48.334
Jan 30 22:52:48.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c" in namespace "configmap-8689" to be "Succeeded or Failed"
Jan 30 22:52:48.382: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.428509ms
Jan 30 22:52:50.402: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03842436s
Jan 30 22:52:52.402: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Running", Reason="", readiness=false. Elapsed: 4.037932274s
Jan 30 22:52:54.404: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039783628s
STEP: Saw pod success 01/30/23 22:52:54.404
Jan 30 22:52:54.404: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c" satisfied condition "Succeeded or Failed"
Jan 30 22:52:54.424: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c container agnhost-container: <nil>
STEP: delete the pod 01/30/23 22:52:54.542
Jan 30 22:52:54.651: INFO: Waiting for pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c to disappear
Jan 30 22:52:54.671: INFO: Pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:52:54.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8689" for this suite. 01/30/23 22:52:54.693
------------------------------
• [SLOW TEST] [6.537 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:52:48.195
    Jan 30 22:52:48.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 22:52:48.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:48.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:48.293
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-62940077-6510-40ab-b21c-8e3f4109f8ae 01/30/23 22:52:48.309
    STEP: Creating a pod to test consume configMaps 01/30/23 22:52:48.334
    Jan 30 22:52:48.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c" in namespace "configmap-8689" to be "Succeeded or Failed"
    Jan 30 22:52:48.382: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.428509ms
    Jan 30 22:52:50.402: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03842436s
    Jan 30 22:52:52.402: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Running", Reason="", readiness=false. Elapsed: 4.037932274s
    Jan 30 22:52:54.404: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039783628s
    STEP: Saw pod success 01/30/23 22:52:54.404
    Jan 30 22:52:54.404: INFO: Pod "pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c" satisfied condition "Succeeded or Failed"
    Jan 30 22:52:54.424: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 22:52:54.542
    Jan 30 22:52:54.651: INFO: Waiting for pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c to disappear
    Jan 30 22:52:54.671: INFO: Pod pod-configmaps-bdd2dada-6ff3-4699-9ab9-5e9037808b4c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:52:54.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8689" for this suite. 01/30/23 22:52:54.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:52:54.749
Jan 30 22:52:54.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 22:52:54.75
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:54.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:54.863
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/30/23 22:52:54.881
STEP: Verify that the required pods have come up 01/30/23 22:52:54.904
Jan 30 22:52:54.932: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 30 22:52:59.957: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/30/23 22:52:59.957
Jan 30 22:52:59.976: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/30/23 22:52:59.976
STEP: DeleteCollection of the ReplicaSets 01/30/23 22:53:00.018
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/30/23 22:53:00.102
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8294" for this suite. 01/30/23 22:53:00.151
------------------------------
• [SLOW TEST] [5.433 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:52:54.749
    Jan 30 22:52:54.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 22:52:54.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:52:54.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:52:54.863
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/30/23 22:52:54.881
    STEP: Verify that the required pods have come up 01/30/23 22:52:54.904
    Jan 30 22:52:54.932: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 30 22:52:59.957: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/30/23 22:52:59.957
    Jan 30 22:52:59.976: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/30/23 22:52:59.976
    STEP: DeleteCollection of the ReplicaSets 01/30/23 22:53:00.018
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/30/23 22:53:00.102
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8294" for this suite. 01/30/23 22:53:00.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:00.195
Jan 30 22:53:00.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 22:53:00.198
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:00.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:00.318
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/30/23 22:53:00.358
STEP: watching for the Service to be added 01/30/23 22:53:00.438
Jan 30 22:53:00.447: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 30 22:53:00.447: INFO: Service test-service-jrrsj created
STEP: Getting /status 01/30/23 22:53:00.448
Jan 30 22:53:00.482: INFO: Service test-service-jrrsj has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/30/23 22:53:00.483
STEP: watching for the Service to be patched 01/30/23 22:53:00.508
Jan 30 22:53:00.516: INFO: observed Service test-service-jrrsj in namespace services-1964 with annotations: map[] & LoadBalancer: {[]}
Jan 30 22:53:00.517: INFO: Found Service test-service-jrrsj in namespace services-1964 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 30 22:53:00.517: INFO: Service test-service-jrrsj has service status patched
STEP: updating the ServiceStatus 01/30/23 22:53:00.517
Jan 30 22:53:00.570: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/30/23 22:53:00.57
Jan 30 22:53:00.579: INFO: Observed Service test-service-jrrsj in namespace services-1964 with annotations: map[] & Conditions: {[]}
Jan 30 22:53:00.586: INFO: Observed event: &Service{ObjectMeta:{test-service-jrrsj  services-1964  8fd3c105-24f4-4ab5-a986-df677100eb0f 24764 0 2023-01-30 22:53:00 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-30 22:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-30 22:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.255.99,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.255.99],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 30 22:53:00.588: INFO: Found Service test-service-jrrsj in namespace services-1964 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 22:53:00.588: INFO: Service test-service-jrrsj has service status updated
STEP: patching the service 01/30/23 22:53:00.588
STEP: watching for the Service to be patched 01/30/23 22:53:00.615
Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
Jan 30 22:53:00.624: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service:patched test-service-static:true]
Jan 30 22:53:00.624: INFO: Service test-service-jrrsj patched
STEP: deleting the service 01/30/23 22:53:00.624
STEP: watching for the Service to be deleted 01/30/23 22:53:00.719
Jan 30 22:53:00.728: INFO: Observed event: ADDED
Jan 30 22:53:00.728: INFO: Observed event: MODIFIED
Jan 30 22:53:00.729: INFO: Observed event: MODIFIED
Jan 30 22:53:00.729: INFO: Observed event: MODIFIED
Jan 30 22:53:00.729: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 30 22:53:00.729: INFO: Service test-service-jrrsj deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:00.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1964" for this suite. 01/30/23 22:53:00.752
------------------------------
• [0.590 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:00.195
    Jan 30 22:53:00.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 22:53:00.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:00.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:00.318
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/30/23 22:53:00.358
    STEP: watching for the Service to be added 01/30/23 22:53:00.438
    Jan 30 22:53:00.447: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 30 22:53:00.447: INFO: Service test-service-jrrsj created
    STEP: Getting /status 01/30/23 22:53:00.448
    Jan 30 22:53:00.482: INFO: Service test-service-jrrsj has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/30/23 22:53:00.483
    STEP: watching for the Service to be patched 01/30/23 22:53:00.508
    Jan 30 22:53:00.516: INFO: observed Service test-service-jrrsj in namespace services-1964 with annotations: map[] & LoadBalancer: {[]}
    Jan 30 22:53:00.517: INFO: Found Service test-service-jrrsj in namespace services-1964 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 30 22:53:00.517: INFO: Service test-service-jrrsj has service status patched
    STEP: updating the ServiceStatus 01/30/23 22:53:00.517
    Jan 30 22:53:00.570: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/30/23 22:53:00.57
    Jan 30 22:53:00.579: INFO: Observed Service test-service-jrrsj in namespace services-1964 with annotations: map[] & Conditions: {[]}
    Jan 30 22:53:00.586: INFO: Observed event: &Service{ObjectMeta:{test-service-jrrsj  services-1964  8fd3c105-24f4-4ab5-a986-df677100eb0f 24764 0 2023-01-30 22:53:00 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-30 22:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-30 22:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.255.99,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.255.99],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 30 22:53:00.588: INFO: Found Service test-service-jrrsj in namespace services-1964 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 22:53:00.588: INFO: Service test-service-jrrsj has service status updated
    STEP: patching the service 01/30/23 22:53:00.588
    STEP: watching for the Service to be patched 01/30/23 22:53:00.615
    Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
    Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
    Jan 30 22:53:00.622: INFO: observed Service test-service-jrrsj in namespace services-1964 with labels: map[test-service-static:true]
    Jan 30 22:53:00.624: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service:patched test-service-static:true]
    Jan 30 22:53:00.624: INFO: Service test-service-jrrsj patched
    STEP: deleting the service 01/30/23 22:53:00.624
    STEP: watching for the Service to be deleted 01/30/23 22:53:00.719
    Jan 30 22:53:00.728: INFO: Observed event: ADDED
    Jan 30 22:53:00.728: INFO: Observed event: MODIFIED
    Jan 30 22:53:00.729: INFO: Observed event: MODIFIED
    Jan 30 22:53:00.729: INFO: Observed event: MODIFIED
    Jan 30 22:53:00.729: INFO: Found Service test-service-jrrsj in namespace services-1964 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 30 22:53:00.729: INFO: Service test-service-jrrsj deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:00.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1964" for this suite. 01/30/23 22:53:00.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:00.787
Jan 30 22:53:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 22:53:00.79
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:00.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:00.881
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 30 22:53:00.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: creating the pod 01/30/23 22:53:00.901
STEP: submitting the pod to kubernetes 01/30/23 22:53:00.901
Jan 30 22:53:00.932: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7" in namespace "pods-7863" to be "running and ready"
Jan 30 22:53:00.968: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.491569ms
Jan 30 22:53:00.968: INFO: The phase of Pod pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:53:02.988: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.054753492s
Jan 30 22:53:02.988: INFO: The phase of Pod pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7 is Running (Ready = true)
Jan 30 22:53:02.989: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7863" for this suite. 01/30/23 22:53:03.277
------------------------------
• [2.522 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:00.787
    Jan 30 22:53:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 22:53:00.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:00.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:00.881
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 30 22:53:00.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: creating the pod 01/30/23 22:53:00.901
    STEP: submitting the pod to kubernetes 01/30/23 22:53:00.901
    Jan 30 22:53:00.932: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7" in namespace "pods-7863" to be "running and ready"
    Jan 30 22:53:00.968: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.491569ms
    Jan 30 22:53:00.968: INFO: The phase of Pod pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:53:02.988: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.054753492s
    Jan 30 22:53:02.988: INFO: The phase of Pod pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7 is Running (Ready = true)
    Jan 30 22:53:02.989: INFO: Pod "pod-exec-websocket-9b5d9c90-b7d5-4000-b0e4-8b14f3f511e7" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7863" for this suite. 01/30/23 22:53:03.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:03.313
Jan 30 22:53:03.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 22:53:03.315
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:03.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:03.407
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 30 22:53:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:04.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5478" for this suite. 01/30/23 22:53:04.266
------------------------------
• [0.983 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:03.313
    Jan 30 22:53:03.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 22:53:03.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:03.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:03.407
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 30 22:53:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:04.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5478" for this suite. 01/30/23 22:53:04.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:04.305
Jan 30 22:53:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context 01/30/23 22:53:04.308
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:04.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:04.396
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 22:53:04.413
Jan 30 22:53:04.449: INFO: Waiting up to 5m0s for pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb" in namespace "security-context-8242" to be "Succeeded or Failed"
Jan 30 22:53:04.469: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.811156ms
Jan 30 22:53:06.512: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063158675s
Jan 30 22:53:08.490: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040712914s
Jan 30 22:53:10.492: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043449803s
STEP: Saw pod success 01/30/23 22:53:10.493
Jan 30 22:53:10.493: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb" satisfied condition "Succeeded or Failed"
Jan 30 22:53:10.513: INFO: Trying to get logs from node 10.15.28.237 pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb container test-container: <nil>
STEP: delete the pod 01/30/23 22:53:10.569
Jan 30 22:53:10.618: INFO: Waiting for pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb to disappear
Jan 30 22:53:10.639: INFO: Pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:10.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8242" for this suite. 01/30/23 22:53:10.658
------------------------------
• [SLOW TEST] [6.392 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:04.305
    Jan 30 22:53:04.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context 01/30/23 22:53:04.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:04.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:04.396
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/30/23 22:53:04.413
    Jan 30 22:53:04.449: INFO: Waiting up to 5m0s for pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb" in namespace "security-context-8242" to be "Succeeded or Failed"
    Jan 30 22:53:04.469: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.811156ms
    Jan 30 22:53:06.512: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063158675s
    Jan 30 22:53:08.490: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040712914s
    Jan 30 22:53:10.492: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043449803s
    STEP: Saw pod success 01/30/23 22:53:10.493
    Jan 30 22:53:10.493: INFO: Pod "security-context-b097a323-65be-47f2-8b31-44dfe5088fcb" satisfied condition "Succeeded or Failed"
    Jan 30 22:53:10.513: INFO: Trying to get logs from node 10.15.28.237 pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb container test-container: <nil>
    STEP: delete the pod 01/30/23 22:53:10.569
    Jan 30 22:53:10.618: INFO: Waiting for pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb to disappear
    Jan 30 22:53:10.639: INFO: Pod security-context-b097a323-65be-47f2-8b31-44dfe5088fcb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:10.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8242" for this suite. 01/30/23 22:53:10.658
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:10.698
Jan 30 22:53:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/30/23 22:53:10.701
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:10.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:10.803
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 30 22:53:10.879: INFO: Waiting up to 5m0s for pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8" in namespace "svcaccounts-2904" to be "running"
Jan 30 22:53:10.899: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.829372ms
Jan 30 22:53:12.918: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039159046s
Jan 30 22:53:14.921: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Running", Reason="", readiness=true. Elapsed: 4.041441127s
Jan 30 22:53:14.921: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8" satisfied condition "running"
STEP: reading a file in the container 01/30/23 22:53:14.921
Jan 30 22:53:14.921: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/30/23 22:53:15.313
Jan 30 22:53:15.313: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/30/23 22:53:15.718
Jan 30 22:53:15.718: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 30 22:53:16.140: INFO: Got root ca configmap in namespace "svcaccounts-2904"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:16.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2904" for this suite. 01/30/23 22:53:16.172
------------------------------
• [SLOW TEST] [5.511 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:10.698
    Jan 30 22:53:10.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 22:53:10.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:10.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:10.803
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 30 22:53:10.879: INFO: Waiting up to 5m0s for pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8" in namespace "svcaccounts-2904" to be "running"
    Jan 30 22:53:10.899: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.829372ms
    Jan 30 22:53:12.918: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039159046s
    Jan 30 22:53:14.921: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8": Phase="Running", Reason="", readiness=true. Elapsed: 4.041441127s
    Jan 30 22:53:14.921: INFO: Pod "pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8" satisfied condition "running"
    STEP: reading a file in the container 01/30/23 22:53:14.921
    Jan 30 22:53:14.921: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/30/23 22:53:15.313
    Jan 30 22:53:15.313: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/30/23 22:53:15.718
    Jan 30 22:53:15.718: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2904 pod-service-account-1171c118-b6b7-4a91-b7d9-9f36ea647ed8 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 30 22:53:16.140: INFO: Got root ca configmap in namespace "svcaccounts-2904"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:16.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2904" for this suite. 01/30/23 22:53:16.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:16.234
Jan 30 22:53:16.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:53:16.235
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:16.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:16.327
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/30/23 22:53:16.343
Jan 30 22:53:16.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8425 api-versions'
Jan 30 22:53:16.516: INFO: stderr: ""
Jan 30 22:53:16.516: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:16.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8425" for this suite. 01/30/23 22:53:16.549
------------------------------
• [0.348 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:16.234
    Jan 30 22:53:16.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:53:16.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:16.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:16.327
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/30/23 22:53:16.343
    Jan 30 22:53:16.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8425 api-versions'
    Jan 30 22:53:16.516: INFO: stderr: ""
    Jan 30 22:53:16.516: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:16.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8425" for this suite. 01/30/23 22:53:16.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:16.593
Jan 30 22:53:16.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename podtemplate 01/30/23 22:53:16.596
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:16.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:16.682
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/30/23 22:53:16.7
Jan 30 22:53:16.744: INFO: created test-podtemplate-1
Jan 30 22:53:16.769: INFO: created test-podtemplate-2
Jan 30 22:53:16.800: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/30/23 22:53:16.801
STEP: delete collection of pod templates 01/30/23 22:53:16.824
Jan 30 22:53:16.824: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/30/23 22:53:16.928
Jan 30 22:53:16.929: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:16.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8988" for this suite. 01/30/23 22:53:16.972
------------------------------
• [0.430 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:16.593
    Jan 30 22:53:16.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename podtemplate 01/30/23 22:53:16.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:16.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:16.682
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/30/23 22:53:16.7
    Jan 30 22:53:16.744: INFO: created test-podtemplate-1
    Jan 30 22:53:16.769: INFO: created test-podtemplate-2
    Jan 30 22:53:16.800: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/30/23 22:53:16.801
    STEP: delete collection of pod templates 01/30/23 22:53:16.824
    Jan 30 22:53:16.824: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/30/23 22:53:16.928
    Jan 30 22:53:16.929: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:16.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8988" for this suite. 01/30/23 22:53:16.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:17.027
Jan 30 22:53:17.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename endpointslice 01/30/23 22:53:17.029
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:17.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:17.12
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 30 22:53:17.196: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Jan 30 22:53:17.196: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8151" for this suite. 01/30/23 22:53:17.229
------------------------------
• [0.236 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:17.027
    Jan 30 22:53:17.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename endpointslice 01/30/23 22:53:17.029
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:17.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:17.12
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 30 22:53:17.196: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Jan 30 22:53:17.196: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:17.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8151" for this suite. 01/30/23 22:53:17.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:17.27
Jan 30 22:53:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 22:53:17.272
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:17.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:17.367
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/30/23 22:53:17.383
STEP: Ensuring active pods == parallelism 01/30/23 22:53:17.406
STEP: Orphaning one of the Job's Pods 01/30/23 22:53:21.461
Jan 30 22:53:22.063: INFO: Successfully updated pod "adopt-release-k9hj9"
STEP: Checking that the Job readopts the Pod 01/30/23 22:53:22.063
Jan 30 22:53:22.065: INFO: Waiting up to 15m0s for pod "adopt-release-k9hj9" in namespace "job-2720" to be "adopted"
Jan 30 22:53:22.089: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 24.030757ms
Jan 30 22:53:24.111: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.046156403s
Jan 30 22:53:24.111: INFO: Pod "adopt-release-k9hj9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/30/23 22:53:24.111
Jan 30 22:53:24.670: INFO: Successfully updated pod "adopt-release-k9hj9"
STEP: Checking that the Job releases the Pod 01/30/23 22:53:24.67
Jan 30 22:53:24.670: INFO: Waiting up to 15m0s for pod "adopt-release-k9hj9" in namespace "job-2720" to be "released"
Jan 30 22:53:24.689: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.356628ms
Jan 30 22:53:26.711: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.040830178s
Jan 30 22:53:26.711: INFO: Pod "adopt-release-k9hj9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:26.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2720" for this suite. 01/30/23 22:53:26.735
------------------------------
• [SLOW TEST] [9.501 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:17.27
    Jan 30 22:53:17.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 22:53:17.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:17.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:17.367
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/30/23 22:53:17.383
    STEP: Ensuring active pods == parallelism 01/30/23 22:53:17.406
    STEP: Orphaning one of the Job's Pods 01/30/23 22:53:21.461
    Jan 30 22:53:22.063: INFO: Successfully updated pod "adopt-release-k9hj9"
    STEP: Checking that the Job readopts the Pod 01/30/23 22:53:22.063
    Jan 30 22:53:22.065: INFO: Waiting up to 15m0s for pod "adopt-release-k9hj9" in namespace "job-2720" to be "adopted"
    Jan 30 22:53:22.089: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 24.030757ms
    Jan 30 22:53:24.111: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.046156403s
    Jan 30 22:53:24.111: INFO: Pod "adopt-release-k9hj9" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/30/23 22:53:24.111
    Jan 30 22:53:24.670: INFO: Successfully updated pod "adopt-release-k9hj9"
    STEP: Checking that the Job releases the Pod 01/30/23 22:53:24.67
    Jan 30 22:53:24.670: INFO: Waiting up to 15m0s for pod "adopt-release-k9hj9" in namespace "job-2720" to be "released"
    Jan 30 22:53:24.689: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.356628ms
    Jan 30 22:53:26.711: INFO: Pod "adopt-release-k9hj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.040830178s
    Jan 30 22:53:26.711: INFO: Pod "adopt-release-k9hj9" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:26.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2720" for this suite. 01/30/23 22:53:26.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:26.79
Jan 30 22:53:26.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 22:53:26.791
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:26.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:26.88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:53:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8694" for this suite. 01/30/23 22:53:27.131
------------------------------
• [0.382 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:26.79
    Jan 30 22:53:26.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 22:53:26.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:26.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:26.88
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:53:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8694" for this suite. 01/30/23 22:53:27.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:53:27.172
Jan 30 22:53:27.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename cronjob 01/30/23 22:53:27.175
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:27.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:27.299
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/30/23 22:53:27.316
STEP: Ensuring a job is scheduled 01/30/23 22:53:27.345
STEP: Ensuring exactly one is scheduled 01/30/23 22:54:01.366
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 22:54:01.388
STEP: Ensuring no more jobs are scheduled 01/30/23 22:54:01.407
STEP: Removing cronjob 01/30/23 22:59:01.454
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2713" for this suite. 01/30/23 22:59:01.511
------------------------------
• [SLOW TEST] [334.380 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:53:27.172
    Jan 30 22:53:27.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename cronjob 01/30/23 22:53:27.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:53:27.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:53:27.299
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/30/23 22:53:27.316
    STEP: Ensuring a job is scheduled 01/30/23 22:53:27.345
    STEP: Ensuring exactly one is scheduled 01/30/23 22:54:01.366
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 22:54:01.388
    STEP: Ensuring no more jobs are scheduled 01/30/23 22:54:01.407
    STEP: Removing cronjob 01/30/23 22:59:01.454
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2713" for this suite. 01/30/23 22:59:01.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:01.556
Jan 30 22:59:01.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 22:59:01.559
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:01.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:01.733
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/30/23 22:59:01.749
Jan 30 22:59:01.783: INFO: Waiting up to 5m0s for pod "downward-api-cd85a296-f210-4b76-913f-12186589d618" in namespace "downward-api-8837" to be "Succeeded or Failed"
Jan 30 22:59:01.804: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 21.133884ms
Jan 30 22:59:03.827: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044289174s
Jan 30 22:59:05.824: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041362767s
Jan 30 22:59:07.823: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040048909s
STEP: Saw pod success 01/30/23 22:59:07.823
Jan 30 22:59:07.824: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618" satisfied condition "Succeeded or Failed"
Jan 30 22:59:07.869: INFO: Trying to get logs from node 10.15.28.237 pod downward-api-cd85a296-f210-4b76-913f-12186589d618 container dapi-container: <nil>
STEP: delete the pod 01/30/23 22:59:08.007
Jan 30 22:59:08.066: INFO: Waiting for pod downward-api-cd85a296-f210-4b76-913f-12186589d618 to disappear
Jan 30 22:59:08.086: INFO: Pod downward-api-cd85a296-f210-4b76-913f-12186589d618 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:08.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8837" for this suite. 01/30/23 22:59:08.112
------------------------------
• [SLOW TEST] [6.615 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:01.556
    Jan 30 22:59:01.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 22:59:01.559
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:01.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:01.733
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/30/23 22:59:01.749
    Jan 30 22:59:01.783: INFO: Waiting up to 5m0s for pod "downward-api-cd85a296-f210-4b76-913f-12186589d618" in namespace "downward-api-8837" to be "Succeeded or Failed"
    Jan 30 22:59:01.804: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 21.133884ms
    Jan 30 22:59:03.827: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044289174s
    Jan 30 22:59:05.824: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041362767s
    Jan 30 22:59:07.823: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040048909s
    STEP: Saw pod success 01/30/23 22:59:07.823
    Jan 30 22:59:07.824: INFO: Pod "downward-api-cd85a296-f210-4b76-913f-12186589d618" satisfied condition "Succeeded or Failed"
    Jan 30 22:59:07.869: INFO: Trying to get logs from node 10.15.28.237 pod downward-api-cd85a296-f210-4b76-913f-12186589d618 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 22:59:08.007
    Jan 30 22:59:08.066: INFO: Waiting for pod downward-api-cd85a296-f210-4b76-913f-12186589d618 to disappear
    Jan 30 22:59:08.086: INFO: Pod downward-api-cd85a296-f210-4b76-913f-12186589d618 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:08.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8837" for this suite. 01/30/23 22:59:08.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:08.174
Jan 30 22:59:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:59:08.177
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:08.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:08.313
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 30 22:59:08.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 create -f -'
Jan 30 22:59:09.528: INFO: stderr: ""
Jan 30 22:59:09.528: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 30 22:59:09.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 create -f -'
Jan 30 22:59:09.921: INFO: stderr: ""
Jan 30 22:59:09.921: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 22:59:09.921
Jan 30 22:59:10.941: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 22:59:10.941: INFO: Found 0 / 1
Jan 30 22:59:11.963: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 22:59:11.963: INFO: Found 0 / 1
Jan 30 22:59:12.940: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 22:59:12.940: INFO: Found 1 / 1
Jan 30 22:59:12.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 22:59:12.959: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 22:59:12.959: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 22:59:12.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe pod agnhost-primary-gzr4s'
Jan 30 22:59:13.136: INFO: stderr: ""
Jan 30 22:59:13.136: INFO: stdout: "Name:             agnhost-primary-gzr4s\nNamespace:        kubectl-3706\nPriority:         0\nService Account:  default\nNode:             10.15.28.237/10.15.28.237\nStart Time:       Mon, 30 Jan 2023 22:59:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: eaa6e5e8510fb5d43c14f1aa33f4cb8fdfe0cc524f5811e38c9a1c744a04db99\n                  cni.projectcalico.org/podIP: 172.30.248.27/32\n                  cni.projectcalico.org/podIPs: 172.30.248.27/32\nStatus:           Running\nIP:               172.30.248.27\nIPs:\n  IP:           172.30.248.27\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://464eea068fe8b1c02fc4f18d8fb90b05fa24422fee73c794654ba5f94078acc5\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 Jan 2023 22:59:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-98pr7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-98pr7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3706/agnhost-primary-gzr4s to 10.15.28.237\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 30 22:59:13.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe rc agnhost-primary'
Jan 30 22:59:13.321: INFO: stderr: ""
Jan 30 22:59:13.321: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3706\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-gzr4s\n"
Jan 30 22:59:13.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe service agnhost-primary'
Jan 30 22:59:13.525: INFO: stderr: ""
Jan 30 22:59:13.525: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3706\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.124.184\nIPs:               172.21.124.184\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.248.27:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 30 22:59:13.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe node 10.15.28.225'
Jan 30 22:59:13.879: INFO: stderr: ""
Jan 30 22:59:13.879: INFO: stdout: "Name:               10.15.28.225\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=br-sao\n                    failure-domain.beta.kubernetes.io/zone=sao05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.109.73.247\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.15.28.225\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=br-sao\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfc28jfz0g89phhsur6g-kubee2epvgv-default-0000020c\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfc28jfz0g89phhsur6g-e5a096e\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.1_1520\n                    ibm-cloud.kubernetes.io/zone=sao05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.15.28.225\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=3083672\n                    publicVLAN=3083670\n                    topology.kubernetes.io/region=br-sao\n                    topology.kubernetes.io/zone=sao05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.15.28.225/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.237.128\nCreationTimestamp:  Mon, 30 Jan 2023 20:32:33 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.15.28.225\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 Jan 2023 22:59:03 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 30 Jan 2023 20:33:43 +0000   Mon, 30 Jan 2023 20:33:43 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:33:11 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.15.28.225\n  ExternalIP:  163.109.73.247\n  Hostname:    10.15.28.225\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102624184Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212384Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93927226085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f4fc3ff0e51644c6b8b7b4a46f5bb806\n  System UUID:                5A5E4BA4-95E0-F6D2-589A-1B39220C3136\n  Boot ID:                    babf711c-2291-400d-8f3c-4c6af4a76864\n  Kernel Version:             4.15.0-202-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.0-beta.2\n  Kubelet Version:            v1.26.1+IKS\n  Kube-Proxy Version:         v1.26.1+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cfc28jfz0g89phhsur6g/kube-cfc28jfz0g89phhsur6g-kubee2epvgv-default-0000020c\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         115m\n  kube-system                 calico-node-sgm4f                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         146m\n  kube-system                 calico-typha-5fcb7c495f-67gfv                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         154m\n  kube-system                 coredns-56697bd765-q4pqr                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     130m\n  kube-system                 ibm-keepalived-watcher-6j488                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         146m\n  kube-system                 ibm-master-proxy-static-10.15.28.225                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      145m\n  kube-system                 ibmcloud-block-storage-driver-rvhkt                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     146m\n  kube-system                 konnectivity-agent-76wnq                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     130m\n  kube-system                 metrics-server-5c45845f46-6mj46                            126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     96m\n  kube-system                 public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r         20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         119m\n  sonobuoy                    sonobuoy-e2e-job-1fdfddcee1544467                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                841m (21%)     866m (22%)\n  memory             723474Ki (5%)  2277664Ki (16%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Jan 30 22:59:13.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe namespace kubectl-3706'
Jan 30 22:59:14.157: INFO: stderr: ""
Jan 30 22:59:14.157: INFO: stdout: "Name:         kubectl-3706\nLabels:       e2e-framework=kubectl\n              e2e-run=55d643dd-b598-41d5-b9a7-a15abad2b29e\n              kubernetes.io/metadata.name=kubectl-3706\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3706" for this suite. 01/30/23 22:59:14.208
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:08.174
    Jan 30 22:59:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:59:08.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:08.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:08.313
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 30 22:59:08.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 create -f -'
    Jan 30 22:59:09.528: INFO: stderr: ""
    Jan 30 22:59:09.528: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 30 22:59:09.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 create -f -'
    Jan 30 22:59:09.921: INFO: stderr: ""
    Jan 30 22:59:09.921: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 22:59:09.921
    Jan 30 22:59:10.941: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 22:59:10.941: INFO: Found 0 / 1
    Jan 30 22:59:11.963: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 22:59:11.963: INFO: Found 0 / 1
    Jan 30 22:59:12.940: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 22:59:12.940: INFO: Found 1 / 1
    Jan 30 22:59:12.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 30 22:59:12.959: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 22:59:12.959: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 22:59:12.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe pod agnhost-primary-gzr4s'
    Jan 30 22:59:13.136: INFO: stderr: ""
    Jan 30 22:59:13.136: INFO: stdout: "Name:             agnhost-primary-gzr4s\nNamespace:        kubectl-3706\nPriority:         0\nService Account:  default\nNode:             10.15.28.237/10.15.28.237\nStart Time:       Mon, 30 Jan 2023 22:59:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: eaa6e5e8510fb5d43c14f1aa33f4cb8fdfe0cc524f5811e38c9a1c744a04db99\n                  cni.projectcalico.org/podIP: 172.30.248.27/32\n                  cni.projectcalico.org/podIPs: 172.30.248.27/32\nStatus:           Running\nIP:               172.30.248.27\nIPs:\n  IP:           172.30.248.27\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://464eea068fe8b1c02fc4f18d8fb90b05fa24422fee73c794654ba5f94078acc5\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 30 Jan 2023 22:59:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-98pr7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-98pr7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3706/agnhost-primary-gzr4s to 10.15.28.237\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 30 22:59:13.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe rc agnhost-primary'
    Jan 30 22:59:13.321: INFO: stderr: ""
    Jan 30 22:59:13.321: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3706\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-gzr4s\n"
    Jan 30 22:59:13.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe service agnhost-primary'
    Jan 30 22:59:13.525: INFO: stderr: ""
    Jan 30 22:59:13.525: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3706\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.124.184\nIPs:               172.21.124.184\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.248.27:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 30 22:59:13.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe node 10.15.28.225'
    Jan 30 22:59:13.879: INFO: stderr: ""
    Jan 30 22:59:13.879: INFO: stdout: "Name:               10.15.28.225\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=br-sao\n                    failure-domain.beta.kubernetes.io/zone=sao05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.109.73.247\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.15.28.225\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=br-sao\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfc28jfz0g89phhsur6g-kubee2epvgv-default-0000020c\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfc28jfz0g89phhsur6g-e5a096e\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.26.1_1520\n                    ibm-cloud.kubernetes.io/zone=sao05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.15.28.225\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=3083672\n                    publicVLAN=3083670\n                    topology.kubernetes.io/region=br-sao\n                    topology.kubernetes.io/zone=sao05\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.15.28.225/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.237.128\nCreationTimestamp:  Mon, 30 Jan 2023 20:32:33 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.15.28.225\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 30 Jan 2023 22:59:03 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 30 Jan 2023 20:33:43 +0000   Mon, 30 Jan 2023 20:33:43 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:32:33 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 30 Jan 2023 22:58:59 +0000   Mon, 30 Jan 2023 20:33:11 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.15.28.225\n  ExternalIP:  163.109.73.247\n  Hostname:    10.15.28.225\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102624184Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212384Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93927226085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 f4fc3ff0e51644c6b8b7b4a46f5bb806\n  System UUID:                5A5E4BA4-95E0-F6D2-589A-1B39220C3136\n  Boot ID:                    babf711c-2291-400d-8f3c-4c6af4a76864\n  Kernel Version:             4.15.0-202-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.0-beta.2\n  Kubelet Version:            v1.26.1+IKS\n  Kube-Proxy Version:         v1.26.1+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cfc28jfz0g89phhsur6g/kube-cfc28jfz0g89phhsur6g-kubee2epvgv-default-0000020c\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         115m\n  kube-system                 calico-node-sgm4f                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         146m\n  kube-system                 calico-typha-5fcb7c495f-67gfv                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         154m\n  kube-system                 coredns-56697bd765-q4pqr                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     130m\n  kube-system                 ibm-keepalived-watcher-6j488                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         146m\n  kube-system                 ibm-master-proxy-static-10.15.28.225                       25m (0%)      300m (7%)   32M (0%)         512M (3%)      145m\n  kube-system                 ibmcloud-block-storage-driver-rvhkt                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     146m\n  kube-system                 konnectivity-agent-76wnq                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     130m\n  kube-system                 metrics-server-5c45845f46-6mj46                            126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     96m\n  kube-system                 public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r         20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         119m\n  sonobuoy                    sonobuoy-e2e-job-1fdfddcee1544467                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                841m (21%)     866m (22%)\n  memory             723474Ki (5%)  2277664Ki (16%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
    Jan 30 22:59:13.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3706 describe namespace kubectl-3706'
    Jan 30 22:59:14.157: INFO: stderr: ""
    Jan 30 22:59:14.157: INFO: stdout: "Name:         kubectl-3706\nLabels:       e2e-framework=kubectl\n              e2e-run=55d643dd-b598-41d5-b9a7-a15abad2b29e\n              kubernetes.io/metadata.name=kubectl-3706\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3706" for this suite. 01/30/23 22:59:14.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:14.243
Jan 30 22:59:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 22:59:14.246
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:14.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:14.336
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 22:59:14.356
Jan 30 22:59:14.391: INFO: Waiting up to 5m0s for pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a" in namespace "emptydir-3698" to be "Succeeded or Failed"
Jan 30 22:59:14.412: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.864342ms
Jan 30 22:59:16.466: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074801273s
Jan 30 22:59:18.430: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039599692s
Jan 30 22:59:20.433: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041727592s
STEP: Saw pod success 01/30/23 22:59:20.433
Jan 30 22:59:20.433: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a" satisfied condition "Succeeded or Failed"
Jan 30 22:59:20.454: INFO: Trying to get logs from node 10.15.28.227 pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a container test-container: <nil>
STEP: delete the pod 01/30/23 22:59:20.574
Jan 30 22:59:20.639: INFO: Waiting for pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a to disappear
Jan 30 22:59:20.666: INFO: Pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:20.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3698" for this suite. 01/30/23 22:59:20.687
------------------------------
• [SLOW TEST] [6.482 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:14.243
    Jan 30 22:59:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 22:59:14.246
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:14.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:14.336
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 22:59:14.356
    Jan 30 22:59:14.391: INFO: Waiting up to 5m0s for pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a" in namespace "emptydir-3698" to be "Succeeded or Failed"
    Jan 30 22:59:14.412: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.864342ms
    Jan 30 22:59:16.466: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074801273s
    Jan 30 22:59:18.430: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039599692s
    Jan 30 22:59:20.433: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041727592s
    STEP: Saw pod success 01/30/23 22:59:20.433
    Jan 30 22:59:20.433: INFO: Pod "pod-34632755-33d9-48da-9ab3-d63c1247fb5a" satisfied condition "Succeeded or Failed"
    Jan 30 22:59:20.454: INFO: Trying to get logs from node 10.15.28.227 pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a container test-container: <nil>
    STEP: delete the pod 01/30/23 22:59:20.574
    Jan 30 22:59:20.639: INFO: Waiting for pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a to disappear
    Jan 30 22:59:20.666: INFO: Pod pod-34632755-33d9-48da-9ab3-d63c1247fb5a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:20.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3698" for this suite. 01/30/23 22:59:20.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:20.743
Jan 30 22:59:20.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename init-container 01/30/23 22:59:20.745
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:20.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:20.834
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/30/23 22:59:20.851
Jan 30 22:59:20.852: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:26.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5603" for this suite. 01/30/23 22:59:26.877
------------------------------
• [SLOW TEST] [6.174 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:20.743
    Jan 30 22:59:20.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename init-container 01/30/23 22:59:20.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:20.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:20.834
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/30/23 22:59:20.851
    Jan 30 22:59:20.852: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:26.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5603" for this suite. 01/30/23 22:59:26.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:26.928
Jan 30 22:59:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 22:59:26.931
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:27.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:27.026
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/30/23 22:59:27.049
STEP: Getting a ResourceQuota 01/30/23 22:59:27.101
STEP: Listing all ResourceQuotas with LabelSelector 01/30/23 22:59:27.122
STEP: Patching the ResourceQuota 01/30/23 22:59:27.145
STEP: Deleting a Collection of ResourceQuotas 01/30/23 22:59:27.183
STEP: Verifying the deleted ResourceQuota 01/30/23 22:59:27.23
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:27.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5286" for this suite. 01/30/23 22:59:27.308
------------------------------
• [0.417 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:26.928
    Jan 30 22:59:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 22:59:26.931
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:27.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:27.026
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/30/23 22:59:27.049
    STEP: Getting a ResourceQuota 01/30/23 22:59:27.101
    STEP: Listing all ResourceQuotas with LabelSelector 01/30/23 22:59:27.122
    STEP: Patching the ResourceQuota 01/30/23 22:59:27.145
    STEP: Deleting a Collection of ResourceQuotas 01/30/23 22:59:27.183
    STEP: Verifying the deleted ResourceQuota 01/30/23 22:59:27.23
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:27.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5286" for this suite. 01/30/23 22:59:27.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:27.347
Jan 30 22:59:27.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 22:59:27.35
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:27.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:27.491
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b8da83dc-b7a0-4989-b0a6-1deffbaa1d6e 01/30/23 22:59:27.509
STEP: Creating a pod to test consume secrets 01/30/23 22:59:27.579
Jan 30 22:59:27.612: INFO: Waiting up to 5m0s for pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226" in namespace "secrets-4144" to be "Succeeded or Failed"
Jan 30 22:59:27.635: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 22.699292ms
Jan 30 22:59:29.655: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042494423s
Jan 30 22:59:31.655: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042189661s
Jan 30 22:59:33.657: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045006994s
STEP: Saw pod success 01/30/23 22:59:33.658
Jan 30 22:59:33.658: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226" satisfied condition "Succeeded or Failed"
Jan 30 22:59:33.678: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 22:59:33.715
Jan 30 22:59:33.785: INFO: Waiting for pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 to disappear
Jan 30 22:59:33.802: INFO: Pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4144" for this suite. 01/30/23 22:59:33.844
------------------------------
• [SLOW TEST] [6.534 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:27.347
    Jan 30 22:59:27.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 22:59:27.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:27.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:27.491
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b8da83dc-b7a0-4989-b0a6-1deffbaa1d6e 01/30/23 22:59:27.509
    STEP: Creating a pod to test consume secrets 01/30/23 22:59:27.579
    Jan 30 22:59:27.612: INFO: Waiting up to 5m0s for pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226" in namespace "secrets-4144" to be "Succeeded or Failed"
    Jan 30 22:59:27.635: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 22.699292ms
    Jan 30 22:59:29.655: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042494423s
    Jan 30 22:59:31.655: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042189661s
    Jan 30 22:59:33.657: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045006994s
    STEP: Saw pod success 01/30/23 22:59:33.658
    Jan 30 22:59:33.658: INFO: Pod "pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226" satisfied condition "Succeeded or Failed"
    Jan 30 22:59:33.678: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 22:59:33.715
    Jan 30 22:59:33.785: INFO: Waiting for pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 to disappear
    Jan 30 22:59:33.802: INFO: Pod pod-secrets-615ab58f-9b34-4421-a6d7-2c38c9d19226 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4144" for this suite. 01/30/23 22:59:33.844
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:33.881
Jan 30 22:59:33.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 22:59:33.882
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:33.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:33.978
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 22:59:34.064
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:59:34.6
STEP: Deploying the webhook pod 01/30/23 22:59:34.647
STEP: Wait for the deployment to be ready 01/30/23 22:59:34.732
Jan 30 22:59:34.769: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 30 22:59:36.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 22:59:38.855
STEP: Verifying the service has paired with the endpoint 01/30/23 22:59:38.954
Jan 30 22:59:39.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/30/23 22:59:39.971
STEP: create a configmap that should be updated by the webhook 01/30/23 22:59:40.079
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:40.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2697" for this suite. 01/30/23 22:59:40.426
STEP: Destroying namespace "webhook-2697-markers" for this suite. 01/30/23 22:59:40.463
------------------------------
• [SLOW TEST] [6.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:33.881
    Jan 30 22:59:33.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 22:59:33.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:33.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:33.978
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 22:59:34.064
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 22:59:34.6
    STEP: Deploying the webhook pod 01/30/23 22:59:34.647
    STEP: Wait for the deployment to be ready 01/30/23 22:59:34.732
    Jan 30 22:59:34.769: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 30 22:59:36.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 22, 59, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 22:59:38.855
    STEP: Verifying the service has paired with the endpoint 01/30/23 22:59:38.954
    Jan 30 22:59:39.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/30/23 22:59:39.971
    STEP: create a configmap that should be updated by the webhook 01/30/23 22:59:40.079
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:40.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2697" for this suite. 01/30/23 22:59:40.426
    STEP: Destroying namespace "webhook-2697-markers" for this suite. 01/30/23 22:59:40.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:40.497
Jan 30 22:59:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 22:59:40.501
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:40.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:40.599
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/30/23 22:59:40.617
Jan 30 22:59:40.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca" in namespace "downward-api-5139" to be "Succeeded or Failed"
Jan 30 22:59:40.671: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 19.455165ms
Jan 30 22:59:42.693: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041394671s
Jan 30 22:59:44.693: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041522415s
Jan 30 22:59:46.691: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040084823s
STEP: Saw pod success 01/30/23 22:59:46.691
Jan 30 22:59:46.692: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca" satisfied condition "Succeeded or Failed"
Jan 30 22:59:46.714: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca container client-container: <nil>
STEP: delete the pod 01/30/23 22:59:46.75
Jan 30 22:59:46.813: INFO: Waiting for pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca to disappear
Jan 30 22:59:46.832: INFO: Pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:46.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5139" for this suite. 01/30/23 22:59:46.866
------------------------------
• [SLOW TEST] [6.401 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:40.497
    Jan 30 22:59:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 22:59:40.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:40.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:40.599
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/30/23 22:59:40.617
    Jan 30 22:59:40.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca" in namespace "downward-api-5139" to be "Succeeded or Failed"
    Jan 30 22:59:40.671: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 19.455165ms
    Jan 30 22:59:42.693: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041394671s
    Jan 30 22:59:44.693: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041522415s
    Jan 30 22:59:46.691: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040084823s
    STEP: Saw pod success 01/30/23 22:59:46.691
    Jan 30 22:59:46.692: INFO: Pod "downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca" satisfied condition "Succeeded or Failed"
    Jan 30 22:59:46.714: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca container client-container: <nil>
    STEP: delete the pod 01/30/23 22:59:46.75
    Jan 30 22:59:46.813: INFO: Waiting for pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca to disappear
    Jan 30 22:59:46.832: INFO: Pod downwardapi-volume-9f407751-5bb1-4e60-86ad-3d6c0b1a7fca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:46.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5139" for this suite. 01/30/23 22:59:46.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:46.909
Jan 30 22:59:46.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-runtime 01/30/23 22:59:46.911
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:46.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:47.013
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/30/23 22:59:47.029
STEP: wait for the container to reach Succeeded 01/30/23 22:59:47.063
STEP: get the container status 01/30/23 22:59:51.16
STEP: the container should be terminated 01/30/23 22:59:51.179
STEP: the termination message should be set 01/30/23 22:59:51.18
Jan 30 22:59:51.180: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/30/23 22:59:51.18
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:51.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6365" for this suite. 01/30/23 22:59:51.285
------------------------------
• [4.409 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:46.909
    Jan 30 22:59:46.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-runtime 01/30/23 22:59:46.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:46.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:47.013
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/30/23 22:59:47.029
    STEP: wait for the container to reach Succeeded 01/30/23 22:59:47.063
    STEP: get the container status 01/30/23 22:59:51.16
    STEP: the container should be terminated 01/30/23 22:59:51.179
    STEP: the termination message should be set 01/30/23 22:59:51.18
    Jan 30 22:59:51.180: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/30/23 22:59:51.18
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:51.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6365" for this suite. 01/30/23 22:59:51.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:51.33
Jan 30 22:59:51.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 22:59:51.332
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:51.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:51.423
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-81cabad1-60e0-495d-96e3-ebe907acdd79 01/30/23 22:59:51.472
STEP: Creating configMap with name cm-test-opt-upd-2cbba045-ee91-44b0-8547-48094828cbd7 01/30/23 22:59:51.497
STEP: Creating the pod 01/30/23 22:59:51.518
Jan 30 22:59:51.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356" in namespace "configmap-4598" to be "running and ready"
Jan 30 22:59:51.584: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Pending", Reason="", readiness=false. Elapsed: 30.294608ms
Jan 30 22:59:51.584: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:59:53.604: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050459912s
Jan 30 22:59:53.604: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 22:59:55.610: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Running", Reason="", readiness=true. Elapsed: 4.056509887s
Jan 30 22:59:55.610: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Running (Ready = true)
Jan 30 22:59:55.610: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-81cabad1-60e0-495d-96e3-ebe907acdd79 01/30/23 22:59:55.778
STEP: Updating configmap cm-test-opt-upd-2cbba045-ee91-44b0-8547-48094828cbd7 01/30/23 22:59:55.82
STEP: Creating configMap with name cm-test-opt-create-6f74c7c7-00cc-48e8-bf4b-135a763d36f7 01/30/23 22:59:55.843
STEP: waiting to observe update in volume 01/30/23 22:59:55.876
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:58.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4598" for this suite. 01/30/23 22:59:58.063
------------------------------
• [SLOW TEST] [6.769 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:51.33
    Jan 30 22:59:51.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 22:59:51.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:51.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:51.423
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-81cabad1-60e0-495d-96e3-ebe907acdd79 01/30/23 22:59:51.472
    STEP: Creating configMap with name cm-test-opt-upd-2cbba045-ee91-44b0-8547-48094828cbd7 01/30/23 22:59:51.497
    STEP: Creating the pod 01/30/23 22:59:51.518
    Jan 30 22:59:51.554: INFO: Waiting up to 5m0s for pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356" in namespace "configmap-4598" to be "running and ready"
    Jan 30 22:59:51.584: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Pending", Reason="", readiness=false. Elapsed: 30.294608ms
    Jan 30 22:59:51.584: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:59:53.604: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050459912s
    Jan 30 22:59:53.604: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 22:59:55.610: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356": Phase="Running", Reason="", readiness=true. Elapsed: 4.056509887s
    Jan 30 22:59:55.610: INFO: The phase of Pod pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356 is Running (Ready = true)
    Jan 30 22:59:55.610: INFO: Pod "pod-configmaps-6dfef74b-4a28-4ac1-9ceb-9c166393c356" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-81cabad1-60e0-495d-96e3-ebe907acdd79 01/30/23 22:59:55.778
    STEP: Updating configmap cm-test-opt-upd-2cbba045-ee91-44b0-8547-48094828cbd7 01/30/23 22:59:55.82
    STEP: Creating configMap with name cm-test-opt-create-6f74c7c7-00cc-48e8-bf4b-135a763d36f7 01/30/23 22:59:55.843
    STEP: waiting to observe update in volume 01/30/23 22:59:55.876
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:58.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4598" for this suite. 01/30/23 22:59:58.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:58.124
Jan 30 22:59:58.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 22:59:58.125
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.214
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/30/23 22:59:58.234
Jan 30 22:59:58.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9884 cluster-info'
Jan 30 22:59:58.375: INFO: stderr: ""
Jan 30 22:59:58.375: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:58.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9884" for this suite. 01/30/23 22:59:58.396
------------------------------
• [0.305 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:58.124
    Jan 30 22:59:58.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 22:59:58.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.214
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/30/23 22:59:58.234
    Jan 30 22:59:58.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9884 cluster-info'
    Jan 30 22:59:58.375: INFO: stderr: ""
    Jan 30 22:59:58.375: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:58.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9884" for this suite. 01/30/23 22:59:58.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:58.441
Jan 30 22:59:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sysctl 01/30/23 22:59:58.442
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.529
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/30/23 22:59:58.545
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 22:59:58.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2528" for this suite. 01/30/23 22:59:58.588
------------------------------
• [0.199 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:58.441
    Jan 30 22:59:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sysctl 01/30/23 22:59:58.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.529
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/30/23 22:59:58.545
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 22:59:58.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2528" for this suite. 01/30/23 22:59:58.588
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 22:59:58.641
Jan 30 22:59:58.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 22:59:58.646
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.748
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/30/23 22:59:58.77
Jan 30 22:59:58.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/30/23 23:00:08.199
Jan 30 23:00:08.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 23:00:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:00:21.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7831" for this suite. 01/30/23 23:00:21.17
------------------------------
• [SLOW TEST] [22.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 22:59:58.641
    Jan 30 22:59:58.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 22:59:58.646
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 22:59:58.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 22:59:58.748
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/30/23 22:59:58.77
    Jan 30 22:59:58.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/30/23 23:00:08.199
    Jan 30 23:00:08.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 23:00:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:00:21.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7831" for this suite. 01/30/23 23:00:21.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:00:21.212
Jan 30 23:00:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:00:21.215
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:00:21.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:00:21.292
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/30/23 23:00:21.308
Jan 30 23:00:21.346: INFO: Waiting up to 5m0s for pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094" in namespace "projected-1429" to be "running and ready"
Jan 30 23:00:21.366: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 19.35609ms
Jan 30 23:00:21.366: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:00:23.386: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039620347s
Jan 30 23:00:23.386: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:00:25.385: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Running", Reason="", readiness=true. Elapsed: 4.03823635s
Jan 30 23:00:25.385: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Running (Ready = true)
Jan 30 23:00:25.385: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094" satisfied condition "running and ready"
Jan 30 23:00:26.046: INFO: Successfully updated pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:00:28.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1429" for this suite. 01/30/23 23:00:28.143
------------------------------
• [SLOW TEST] [6.961 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:00:21.212
    Jan 30 23:00:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:00:21.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:00:21.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:00:21.292
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/30/23 23:00:21.308
    Jan 30 23:00:21.346: INFO: Waiting up to 5m0s for pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094" in namespace "projected-1429" to be "running and ready"
    Jan 30 23:00:21.366: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 19.35609ms
    Jan 30 23:00:21.366: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:00:23.386: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039620347s
    Jan 30 23:00:23.386: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:00:25.385: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094": Phase="Running", Reason="", readiness=true. Elapsed: 4.03823635s
    Jan 30 23:00:25.385: INFO: The phase of Pod labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094 is Running (Ready = true)
    Jan 30 23:00:25.385: INFO: Pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094" satisfied condition "running and ready"
    Jan 30 23:00:26.046: INFO: Successfully updated pod "labelsupdate56ce4ebd-6d01-4a65-9228-853ca2c1b094"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:00:28.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1429" for this suite. 01/30/23 23:00:28.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:00:28.178
Jan 30 23:00:28.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:00:28.18
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:00:28.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:00:28.248
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-ca9d9ff4-3a17-46af-befe-1d8c5fa14bd3 01/30/23 23:00:28.288
STEP: Creating configMap with name cm-test-opt-upd-d6983422-ac89-4f08-8c0d-f71f68438621 01/30/23 23:00:28.31
STEP: Creating the pod 01/30/23 23:00:28.33
Jan 30 23:00:28.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252" in namespace "projected-9945" to be "running and ready"
Jan 30 23:00:28.416: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Pending", Reason="", readiness=false. Elapsed: 20.355914ms
Jan 30 23:00:28.416: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:00:30.435: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039949791s
Jan 30 23:00:30.435: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:00:32.436: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Running", Reason="", readiness=true. Elapsed: 4.041034567s
Jan 30 23:00:32.436: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Running (Ready = true)
Jan 30 23:00:32.436: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ca9d9ff4-3a17-46af-befe-1d8c5fa14bd3 01/30/23 23:00:32.647
STEP: Updating configmap cm-test-opt-upd-d6983422-ac89-4f08-8c0d-f71f68438621 01/30/23 23:00:32.679
STEP: Creating configMap with name cm-test-opt-create-56d4a54d-e1d7-4954-92de-c2324705fcf9 01/30/23 23:00:32.701
STEP: waiting to observe update in volume 01/30/23 23:00:32.723
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:01:46.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9945" for this suite. 01/30/23 23:01:46.7
------------------------------
• [SLOW TEST] [78.626 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:00:28.178
    Jan 30 23:00:28.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:00:28.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:00:28.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:00:28.248
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-ca9d9ff4-3a17-46af-befe-1d8c5fa14bd3 01/30/23 23:00:28.288
    STEP: Creating configMap with name cm-test-opt-upd-d6983422-ac89-4f08-8c0d-f71f68438621 01/30/23 23:00:28.31
    STEP: Creating the pod 01/30/23 23:00:28.33
    Jan 30 23:00:28.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252" in namespace "projected-9945" to be "running and ready"
    Jan 30 23:00:28.416: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Pending", Reason="", readiness=false. Elapsed: 20.355914ms
    Jan 30 23:00:28.416: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:00:30.435: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039949791s
    Jan 30 23:00:30.435: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:00:32.436: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252": Phase="Running", Reason="", readiness=true. Elapsed: 4.041034567s
    Jan 30 23:00:32.436: INFO: The phase of Pod pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252 is Running (Ready = true)
    Jan 30 23:00:32.436: INFO: Pod "pod-projected-configmaps-0bcb9799-1eb1-4087-bab8-a998d55b3252" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ca9d9ff4-3a17-46af-befe-1d8c5fa14bd3 01/30/23 23:00:32.647
    STEP: Updating configmap cm-test-opt-upd-d6983422-ac89-4f08-8c0d-f71f68438621 01/30/23 23:00:32.679
    STEP: Creating configMap with name cm-test-opt-create-56d4a54d-e1d7-4954-92de-c2324705fcf9 01/30/23 23:00:32.701
    STEP: waiting to observe update in volume 01/30/23 23:00:32.723
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:01:46.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9945" for this suite. 01/30/23 23:01:46.7
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:01:46.809
Jan 30 23:01:46.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:01:46.812
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:01:46.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:01:46.902
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4556 01/30/23 23:01:46.919
STEP: creating service affinity-nodeport in namespace services-4556 01/30/23 23:01:46.919
STEP: creating replication controller affinity-nodeport in namespace services-4556 01/30/23 23:01:46.986
I0130 23:01:47.038782      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4556, replica count: 3
I0130 23:01:50.091356      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:01:50.148: INFO: Creating new exec pod
Jan 30 23:01:50.207: INFO: Waiting up to 5m0s for pod "execpod-affinity9jbq5" in namespace "services-4556" to be "running"
Jan 30 23:01:50.225: INFO: Pod "execpod-affinity9jbq5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.160996ms
Jan 30 23:01:52.245: INFO: Pod "execpod-affinity9jbq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037813978s
Jan 30 23:01:54.244: INFO: Pod "execpod-affinity9jbq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.03770094s
Jan 30 23:01:54.245: INFO: Pod "execpod-affinity9jbq5" satisfied condition "running"
Jan 30 23:01:55.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 30 23:01:55.735: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 30 23:01:55.735: INFO: stdout: ""
Jan 30 23:01:55.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 172.21.173.17 80'
Jan 30 23:01:56.187: INFO: stderr: "+ nc -v -z -w 2 172.21.173.17 80\nConnection to 172.21.173.17 80 port [tcp/http] succeeded!\n"
Jan 30 23:01:56.187: INFO: stdout: ""
Jan 30 23:01:56.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30994'
Jan 30 23:01:56.592: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30994\nConnection to 10.15.28.237 30994 port [tcp/*] succeeded!\n"
Jan 30 23:01:56.592: INFO: stdout: ""
Jan 30 23:01:56.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 30994'
Jan 30 23:01:56.985: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 30994\nConnection to 10.15.28.225 30994 port [tcp/*] succeeded!\n"
Jan 30 23:01:56.985: INFO: stdout: ""
Jan 30 23:01:56.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30994/ ; done'
Jan 30 23:01:57.525: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n"
Jan 30 23:01:57.526: INFO: stdout: "\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5"
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
Jan 30 23:01:57.526: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4556, will wait for the garbage collector to delete the pods 01/30/23 23:01:57.605
Jan 30 23:01:57.704: INFO: Deleting ReplicationController affinity-nodeport took: 30.681849ms
Jan 30 23:01:57.905: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.692ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:00.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4556" for this suite. 01/30/23 23:02:00.925
------------------------------
• [SLOW TEST] [14.149 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:01:46.809
    Jan 30 23:01:46.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:01:46.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:01:46.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:01:46.902
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4556 01/30/23 23:01:46.919
    STEP: creating service affinity-nodeport in namespace services-4556 01/30/23 23:01:46.919
    STEP: creating replication controller affinity-nodeport in namespace services-4556 01/30/23 23:01:46.986
    I0130 23:01:47.038782      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4556, replica count: 3
    I0130 23:01:50.091356      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:01:50.148: INFO: Creating new exec pod
    Jan 30 23:01:50.207: INFO: Waiting up to 5m0s for pod "execpod-affinity9jbq5" in namespace "services-4556" to be "running"
    Jan 30 23:01:50.225: INFO: Pod "execpod-affinity9jbq5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.160996ms
    Jan 30 23:01:52.245: INFO: Pod "execpod-affinity9jbq5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037813978s
    Jan 30 23:01:54.244: INFO: Pod "execpod-affinity9jbq5": Phase="Running", Reason="", readiness=true. Elapsed: 4.03770094s
    Jan 30 23:01:54.245: INFO: Pod "execpod-affinity9jbq5" satisfied condition "running"
    Jan 30 23:01:55.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 30 23:01:55.735: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 30 23:01:55.735: INFO: stdout: ""
    Jan 30 23:01:55.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 172.21.173.17 80'
    Jan 30 23:01:56.187: INFO: stderr: "+ nc -v -z -w 2 172.21.173.17 80\nConnection to 172.21.173.17 80 port [tcp/http] succeeded!\n"
    Jan 30 23:01:56.187: INFO: stdout: ""
    Jan 30 23:01:56.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30994'
    Jan 30 23:01:56.592: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30994\nConnection to 10.15.28.237 30994 port [tcp/*] succeeded!\n"
    Jan 30 23:01:56.592: INFO: stdout: ""
    Jan 30 23:01:56.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 30994'
    Jan 30 23:01:56.985: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 30994\nConnection to 10.15.28.225 30994 port [tcp/*] succeeded!\n"
    Jan 30 23:01:56.985: INFO: stdout: ""
    Jan 30 23:01:56.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4556 exec execpod-affinity9jbq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30994/ ; done'
    Jan 30 23:01:57.525: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30994/\n"
    Jan 30 23:01:57.526: INFO: stdout: "\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5\naffinity-nodeport-gtgs5"
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Received response from host: affinity-nodeport-gtgs5
    Jan 30 23:01:57.526: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4556, will wait for the garbage collector to delete the pods 01/30/23 23:01:57.605
    Jan 30 23:01:57.704: INFO: Deleting ReplicationController affinity-nodeport took: 30.681849ms
    Jan 30 23:01:57.905: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.692ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:00.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4556" for this suite. 01/30/23 23:02:00.925
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:00.962
Jan 30 23:02:00.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:02:00.965
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:01.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:01.061
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-2695/secret-test-51902be0-b14b-4cc5-99f6-2691253f69bd 01/30/23 23:02:01.078
STEP: Creating a pod to test consume secrets 01/30/23 23:02:01.101
Jan 30 23:02:01.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715" in namespace "secrets-2695" to be "Succeeded or Failed"
Jan 30 23:02:01.173: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 34.555683ms
Jan 30 23:02:03.195: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056504035s
Jan 30 23:02:05.193: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054848298s
Jan 30 23:02:07.190: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05240491s
STEP: Saw pod success 01/30/23 23:02:07.191
Jan 30 23:02:07.191: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715" satisfied condition "Succeeded or Failed"
Jan 30 23:02:07.207: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 container env-test: <nil>
STEP: delete the pod 01/30/23 23:02:07.257
Jan 30 23:02:07.301: INFO: Waiting for pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 to disappear
Jan 30 23:02:07.318: INFO: Pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:07.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2695" for this suite. 01/30/23 23:02:07.342
------------------------------
• [SLOW TEST] [6.442 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:00.962
    Jan 30 23:02:00.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:02:00.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:01.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:01.061
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-2695/secret-test-51902be0-b14b-4cc5-99f6-2691253f69bd 01/30/23 23:02:01.078
    STEP: Creating a pod to test consume secrets 01/30/23 23:02:01.101
    Jan 30 23:02:01.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715" in namespace "secrets-2695" to be "Succeeded or Failed"
    Jan 30 23:02:01.173: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 34.555683ms
    Jan 30 23:02:03.195: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056504035s
    Jan 30 23:02:05.193: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054848298s
    Jan 30 23:02:07.190: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05240491s
    STEP: Saw pod success 01/30/23 23:02:07.191
    Jan 30 23:02:07.191: INFO: Pod "pod-configmaps-9514c3ae-198b-4974-9a35-236115601715" satisfied condition "Succeeded or Failed"
    Jan 30 23:02:07.207: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 container env-test: <nil>
    STEP: delete the pod 01/30/23 23:02:07.257
    Jan 30 23:02:07.301: INFO: Waiting for pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 to disappear
    Jan 30 23:02:07.318: INFO: Pod pod-configmaps-9514c3ae-198b-4974-9a35-236115601715 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:07.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2695" for this suite. 01/30/23 23:02:07.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:07.406
Jan 30 23:02:07.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 23:02:07.409
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:07.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:07.48
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/30/23 23:02:07.493
STEP: Creating a ResourceQuota 01/30/23 23:02:12.522
STEP: Ensuring resource quota status is calculated 01/30/23 23:02:12.556
STEP: Creating a ReplicationController 01/30/23 23:02:14.578
STEP: Ensuring resource quota status captures replication controller creation 01/30/23 23:02:14.633
STEP: Deleting a ReplicationController 01/30/23 23:02:16.656
STEP: Ensuring resource quota status released usage 01/30/23 23:02:16.686
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:18.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1552" for this suite. 01/30/23 23:02:18.747
------------------------------
• [SLOW TEST] [11.366 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:07.406
    Jan 30 23:02:07.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 23:02:07.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:07.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:07.48
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/30/23 23:02:07.493
    STEP: Creating a ResourceQuota 01/30/23 23:02:12.522
    STEP: Ensuring resource quota status is calculated 01/30/23 23:02:12.556
    STEP: Creating a ReplicationController 01/30/23 23:02:14.578
    STEP: Ensuring resource quota status captures replication controller creation 01/30/23 23:02:14.633
    STEP: Deleting a ReplicationController 01/30/23 23:02:16.656
    STEP: Ensuring resource quota status released usage 01/30/23 23:02:16.686
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:18.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1552" for this suite. 01/30/23 23:02:18.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:18.777
Jan 30 23:02:18.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename watch 01/30/23 23:02:18.779
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:18.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:18.899
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/30/23 23:02:18.915
STEP: creating a watch on configmaps with label B 01/30/23 23:02:18.922
STEP: creating a watch on configmaps with label A or B 01/30/23 23:02:18.928
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/30/23 23:02:18.935
Jan 30 23:02:18.963: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26499 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:18.963: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26499 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/30/23 23:02:18.963
Jan 30 23:02:19.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26500 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:19.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26500 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/30/23 23:02:19.01
Jan 30 23:02:19.051: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26501 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:19.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26501 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/30/23 23:02:19.053
Jan 30 23:02:19.083: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26502 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:19.083: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26502 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/30/23 23:02:19.083
Jan 30 23:02:19.105: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26503 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:19.106: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26503 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/30/23 23:02:29.107
Jan 30 23:02:29.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26525 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:02:29.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26525 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2013" for this suite. 01/30/23 23:02:39.222
------------------------------
• [SLOW TEST] [20.487 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:18.777
    Jan 30 23:02:18.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename watch 01/30/23 23:02:18.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:18.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:18.899
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/30/23 23:02:18.915
    STEP: creating a watch on configmaps with label B 01/30/23 23:02:18.922
    STEP: creating a watch on configmaps with label A or B 01/30/23 23:02:18.928
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/30/23 23:02:18.935
    Jan 30 23:02:18.963: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26499 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:18.963: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26499 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/30/23 23:02:18.963
    Jan 30 23:02:19.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26500 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:19.010: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26500 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/30/23 23:02:19.01
    Jan 30 23:02:19.051: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26501 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:19.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26501 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/30/23 23:02:19.053
    Jan 30 23:02:19.083: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26502 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:19.083: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2013  78971123-0819-4ad4-9b9a-6f9509e456bf 26502 0 2023-01-30 23:02:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/30/23 23:02:19.083
    Jan 30 23:02:19.105: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26503 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:19.106: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26503 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/30/23 23:02:29.107
    Jan 30 23:02:29.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26525 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:02:29.148: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2013  5bcb13b2-d9b2-4689-b3ea-44f7bac4a00b 26525 0 2023-01-30 23:02:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-30 23:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:39.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2013" for this suite. 01/30/23 23:02:39.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:39.266
Jan 30 23:02:39.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:02:39.268
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:39.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:39.333
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/30/23 23:02:39.352
Jan 30 23:02:39.352: INFO: Creating e2e-svc-a-vg5sj
Jan 30 23:02:39.403: INFO: Creating e2e-svc-b-lxlxh
Jan 30 23:02:39.465: INFO: Creating e2e-svc-c-tfk8w
STEP: deleting service collection 01/30/23 23:02:39.584
Jan 30 23:02:39.751: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:39.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2013" for this suite. 01/30/23 23:02:39.775
------------------------------
• [0.540 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:39.266
    Jan 30 23:02:39.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:02:39.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:39.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:39.333
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/30/23 23:02:39.352
    Jan 30 23:02:39.352: INFO: Creating e2e-svc-a-vg5sj
    Jan 30 23:02:39.403: INFO: Creating e2e-svc-b-lxlxh
    Jan 30 23:02:39.465: INFO: Creating e2e-svc-c-tfk8w
    STEP: deleting service collection 01/30/23 23:02:39.584
    Jan 30 23:02:39.751: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:39.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2013" for this suite. 01/30/23 23:02:39.775
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:39.808
Jan 30 23:02:39.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption 01/30/23 23:02:39.811
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:39.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:39.881
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/30/23 23:02:39.918
STEP: Waiting for all pods to be running 01/30/23 23:02:42.046
Jan 30 23:02:42.091: INFO: running pods: 0 < 3
Jan 30 23:02:44.118: INFO: running pods: 0 < 3
Jan 30 23:02:46.113: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:48.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6264" for this suite. 01/30/23 23:02:48.192
------------------------------
• [SLOW TEST] [8.414 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:39.808
    Jan 30 23:02:39.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption 01/30/23 23:02:39.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:39.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:39.881
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/30/23 23:02:39.918
    STEP: Waiting for all pods to be running 01/30/23 23:02:42.046
    Jan 30 23:02:42.091: INFO: running pods: 0 < 3
    Jan 30 23:02:44.118: INFO: running pods: 0 < 3
    Jan 30 23:02:46.113: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:48.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6264" for this suite. 01/30/23 23:02:48.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:48.224
Jan 30 23:02:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:02:48.225
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:48.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:48.286
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/30/23 23:02:48.301
Jan 30 23:02:48.335: INFO: Waiting up to 5m0s for pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f" in namespace "downward-api-8039" to be "Succeeded or Failed"
Jan 30 23:02:48.353: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.939657ms
Jan 30 23:02:50.372: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036484762s
Jan 30 23:02:52.376: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040624424s
Jan 30 23:02:54.374: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038931165s
STEP: Saw pod success 01/30/23 23:02:54.374
Jan 30 23:02:54.375: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f" satisfied condition "Succeeded or Failed"
Jan 30 23:02:54.399: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f container dapi-container: <nil>
STEP: delete the pod 01/30/23 23:02:54.44
Jan 30 23:02:54.531: INFO: Waiting for pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f to disappear
Jan 30 23:02:54.548: INFO: Pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 23:02:54.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8039" for this suite. 01/30/23 23:02:54.634
------------------------------
• [SLOW TEST] [6.440 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:48.224
    Jan 30 23:02:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:02:48.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:48.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:48.286
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/30/23 23:02:48.301
    Jan 30 23:02:48.335: INFO: Waiting up to 5m0s for pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f" in namespace "downward-api-8039" to be "Succeeded or Failed"
    Jan 30 23:02:48.353: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.939657ms
    Jan 30 23:02:50.372: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036484762s
    Jan 30 23:02:52.376: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040624424s
    Jan 30 23:02:54.374: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038931165s
    STEP: Saw pod success 01/30/23 23:02:54.374
    Jan 30 23:02:54.375: INFO: Pod "downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f" satisfied condition "Succeeded or Failed"
    Jan 30 23:02:54.399: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f container dapi-container: <nil>
    STEP: delete the pod 01/30/23 23:02:54.44
    Jan 30 23:02:54.531: INFO: Waiting for pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f to disappear
    Jan 30 23:02:54.548: INFO: Pod downward-api-da9d58b8-07df-44d1-86ab-a6c990f0463f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:02:54.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8039" for this suite. 01/30/23 23:02:54.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:02:54.666
Jan 30 23:02:54.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:02:54.67
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:54.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:54.767
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:02:54.78
Jan 30 23:02:54.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f" in namespace "projected-7963" to be "Succeeded or Failed"
Jan 30 23:02:54.836: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.2763ms
Jan 30 23:02:56.856: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034628071s
Jan 30 23:02:58.860: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038818316s
Jan 30 23:03:00.876: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055046427s
STEP: Saw pod success 01/30/23 23:03:00.876
Jan 30 23:03:00.877: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f" satisfied condition "Succeeded or Failed"
Jan 30 23:03:00.899: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f container client-container: <nil>
STEP: delete the pod 01/30/23 23:03:00.967
Jan 30 23:03:01.018: INFO: Waiting for pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f to disappear
Jan 30 23:03:01.034: INFO: Pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:01.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7963" for this suite. 01/30/23 23:03:01.076
------------------------------
• [SLOW TEST] [6.438 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:02:54.666
    Jan 30 23:02:54.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:02:54.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:02:54.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:02:54.767
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:02:54.78
    Jan 30 23:02:54.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f" in namespace "projected-7963" to be "Succeeded or Failed"
    Jan 30 23:02:54.836: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.2763ms
    Jan 30 23:02:56.856: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034628071s
    Jan 30 23:02:58.860: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038818316s
    Jan 30 23:03:00.876: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055046427s
    STEP: Saw pod success 01/30/23 23:03:00.876
    Jan 30 23:03:00.877: INFO: Pod "downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f" satisfied condition "Succeeded or Failed"
    Jan 30 23:03:00.899: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f container client-container: <nil>
    STEP: delete the pod 01/30/23 23:03:00.967
    Jan 30 23:03:01.018: INFO: Waiting for pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f to disappear
    Jan 30 23:03:01.034: INFO: Pod downwardapi-volume-0a637d9f-5172-480f-adb1-c3745dc1ba8f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:01.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7963" for this suite. 01/30/23 23:03:01.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:01.11
Jan 30 23:03:01.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context-test 01/30/23 23:03:01.112
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:01.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:01.213
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 30 23:03:01.331: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242" in namespace "security-context-test-6847" to be "Succeeded or Failed"
Jan 30 23:03:01.357: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 25.773796ms
Jan 30 23:03:03.377: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04592414s
Jan 30 23:03:05.381: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04960394s
Jan 30 23:03:07.373: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042143189s
Jan 30 23:03:07.373: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:07.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6847" for this suite. 01/30/23 23:03:07.425
------------------------------
• [SLOW TEST] [6.344 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:01.11
    Jan 30 23:03:01.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context-test 01/30/23 23:03:01.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:01.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:01.213
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 30 23:03:01.331: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242" in namespace "security-context-test-6847" to be "Succeeded or Failed"
    Jan 30 23:03:01.357: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 25.773796ms
    Jan 30 23:03:03.377: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04592414s
    Jan 30 23:03:05.381: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04960394s
    Jan 30 23:03:07.373: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042143189s
    Jan 30 23:03:07.373: INFO: Pod "busybox-user-65534-c48b7979-b5cd-4813-af1b-b2afdbee9242" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:07.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6847" for this suite. 01/30/23 23:03:07.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:07.453
Jan 30 23:03:07.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:03:07.455
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:07.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:07.594
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/30/23 23:03:07.612
STEP: fetching the ConfigMap 01/30/23 23:03:07.635
STEP: patching the ConfigMap 01/30/23 23:03:07.672
STEP: listing all ConfigMaps in all namespaces with a label selector 01/30/23 23:03:07.724
STEP: deleting the ConfigMap by collection with a label selector 01/30/23 23:03:07.745
STEP: listing all ConfigMaps in test namespace 01/30/23 23:03:07.793
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:07.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6343" for this suite. 01/30/23 23:03:07.835
------------------------------
• [0.431 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:07.453
    Jan 30 23:03:07.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:03:07.455
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:07.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:07.594
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/30/23 23:03:07.612
    STEP: fetching the ConfigMap 01/30/23 23:03:07.635
    STEP: patching the ConfigMap 01/30/23 23:03:07.672
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/30/23 23:03:07.724
    STEP: deleting the ConfigMap by collection with a label selector 01/30/23 23:03:07.745
    STEP: listing all ConfigMaps in test namespace 01/30/23 23:03:07.793
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:07.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6343" for this suite. 01/30/23 23:03:07.835
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:07.886
Jan 30 23:03:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 23:03:07.889
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:07.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:07.974
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/30/23 23:03:07.99
STEP: Ensuring ResourceQuota status is calculated 01/30/23 23:03:08.016
STEP: Creating a ResourceQuota with not best effort scope 01/30/23 23:03:10.034
STEP: Ensuring ResourceQuota status is calculated 01/30/23 23:03:10.056
STEP: Creating a best-effort pod 01/30/23 23:03:12.077
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/30/23 23:03:12.139
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/30/23 23:03:14.159
STEP: Deleting the pod 01/30/23 23:03:16.177
STEP: Ensuring resource quota status released the pod usage 01/30/23 23:03:16.222
STEP: Creating a not best-effort pod 01/30/23 23:03:18.241
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/30/23 23:03:18.296
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/30/23 23:03:20.317
STEP: Deleting the pod 01/30/23 23:03:22.337
STEP: Ensuring resource quota status released the pod usage 01/30/23 23:03:22.408
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:24.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9863" for this suite. 01/30/23 23:03:24.487
------------------------------
• [SLOW TEST] [16.634 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:07.886
    Jan 30 23:03:07.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 23:03:07.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:07.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:07.974
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/30/23 23:03:07.99
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 23:03:08.016
    STEP: Creating a ResourceQuota with not best effort scope 01/30/23 23:03:10.034
    STEP: Ensuring ResourceQuota status is calculated 01/30/23 23:03:10.056
    STEP: Creating a best-effort pod 01/30/23 23:03:12.077
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/30/23 23:03:12.139
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/30/23 23:03:14.159
    STEP: Deleting the pod 01/30/23 23:03:16.177
    STEP: Ensuring resource quota status released the pod usage 01/30/23 23:03:16.222
    STEP: Creating a not best-effort pod 01/30/23 23:03:18.241
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/30/23 23:03:18.296
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/30/23 23:03:20.317
    STEP: Deleting the pod 01/30/23 23:03:22.337
    STEP: Ensuring resource quota status released the pod usage 01/30/23 23:03:22.408
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:24.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9863" for this suite. 01/30/23 23:03:24.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:24.525
Jan 30 23:03:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:03:24.527
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:24.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:24.604
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:28.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2410" for this suite. 01/30/23 23:03:28.801
------------------------------
• [4.305 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:24.525
    Jan 30 23:03:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:03:24.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:24.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:24.604
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:28.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2410" for this suite. 01/30/23 23:03:28.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:28.833
Jan 30 23:03:28.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:03:28.835
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:28.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:28.963
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-3eb1a155-617b-4c84-9056-fcbf59973f27 01/30/23 23:03:28.981
STEP: Creating a pod to test consume configMaps 01/30/23 23:03:29.003
Jan 30 23:03:29.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49" in namespace "configmap-9965" to be "Succeeded or Failed"
Jan 30 23:03:29.058: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 21.297214ms
Jan 30 23:03:31.081: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044395523s
Jan 30 23:03:33.078: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041563715s
Jan 30 23:03:35.083: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046592251s
STEP: Saw pod success 01/30/23 23:03:35.083
Jan 30 23:03:35.084: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49" satisfied condition "Succeeded or Failed"
Jan 30 23:03:35.105: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:03:35.219
Jan 30 23:03:35.273: INFO: Waiting for pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 to disappear
Jan 30 23:03:35.291: INFO: Pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:35.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9965" for this suite. 01/30/23 23:03:35.348
------------------------------
• [SLOW TEST] [6.546 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:28.833
    Jan 30 23:03:28.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:03:28.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:28.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:28.963
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-3eb1a155-617b-4c84-9056-fcbf59973f27 01/30/23 23:03:28.981
    STEP: Creating a pod to test consume configMaps 01/30/23 23:03:29.003
    Jan 30 23:03:29.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49" in namespace "configmap-9965" to be "Succeeded or Failed"
    Jan 30 23:03:29.058: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 21.297214ms
    Jan 30 23:03:31.081: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044395523s
    Jan 30 23:03:33.078: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041563715s
    Jan 30 23:03:35.083: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046592251s
    STEP: Saw pod success 01/30/23 23:03:35.083
    Jan 30 23:03:35.084: INFO: Pod "pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49" satisfied condition "Succeeded or Failed"
    Jan 30 23:03:35.105: INFO: Trying to get logs from node 10.15.28.237 pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:03:35.219
    Jan 30 23:03:35.273: INFO: Waiting for pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 to disappear
    Jan 30 23:03:35.291: INFO: Pod pod-configmaps-80a89eb3-6115-432f-8c6d-247d9f756e49 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:35.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9965" for this suite. 01/30/23 23:03:35.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:35.381
Jan 30 23:03:35.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:03:35.382
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:35.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:35.469
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-15108fa8-b12c-45e3-b0d7-cf6cfdc4d73b 01/30/23 23:03:35.484
STEP: Creating a pod to test consume secrets 01/30/23 23:03:35.503
Jan 30 23:03:35.565: INFO: Waiting up to 5m0s for pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0" in namespace "secrets-838" to be "Succeeded or Failed"
Jan 30 23:03:35.583: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.479044ms
Jan 30 23:03:37.602: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.03709903s
Jan 30 23:03:39.601: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.036458639s
Jan 30 23:03:41.600: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035009253s
STEP: Saw pod success 01/30/23 23:03:41.6
Jan 30 23:03:41.600: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0" satisfied condition "Succeeded or Failed"
Jan 30 23:03:41.618: INFO: Trying to get logs from node 10.15.28.237 pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:03:41.657
Jan 30 23:03:41.707: INFO: Waiting for pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 to disappear
Jan 30 23:03:41.724: INFO: Pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:41.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-838" for this suite. 01/30/23 23:03:41.75
------------------------------
• [SLOW TEST] [6.398 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:35.381
    Jan 30 23:03:35.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:03:35.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:35.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:35.469
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-15108fa8-b12c-45e3-b0d7-cf6cfdc4d73b 01/30/23 23:03:35.484
    STEP: Creating a pod to test consume secrets 01/30/23 23:03:35.503
    Jan 30 23:03:35.565: INFO: Waiting up to 5m0s for pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0" in namespace "secrets-838" to be "Succeeded or Failed"
    Jan 30 23:03:35.583: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.479044ms
    Jan 30 23:03:37.602: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.03709903s
    Jan 30 23:03:39.601: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.036458639s
    Jan 30 23:03:41.600: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035009253s
    STEP: Saw pod success 01/30/23 23:03:41.6
    Jan 30 23:03:41.600: INFO: Pod "pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0" satisfied condition "Succeeded or Failed"
    Jan 30 23:03:41.618: INFO: Trying to get logs from node 10.15.28.237 pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:03:41.657
    Jan 30 23:03:41.707: INFO: Waiting for pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 to disappear
    Jan 30 23:03:41.724: INFO: Pod pod-secrets-2f68cffb-4e0d-423f-9bd2-c84c6c8bf0f0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:41.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-838" for this suite. 01/30/23 23:03:41.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:41.789
Jan 30 23:03:41.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/30/23 23:03:41.791
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:41.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:41.865
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/30/23 23:03:41.879
Jan 30 23:03:41.922: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6818" to be "running and ready"
Jan 30 23:03:41.941: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385553ms
Jan 30 23:03:41.941: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:03:43.963: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041638016s
Jan 30 23:03:43.963: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:03:45.965: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.042927027s
Jan 30 23:03:45.965: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 30 23:03:45.965: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/30/23 23:03:45.983
STEP: Then the orphan pod is adopted 01/30/23 23:03:46.007
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6818" for this suite. 01/30/23 23:03:47.139
------------------------------
• [SLOW TEST] [5.421 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:41.789
    Jan 30 23:03:41.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/30/23 23:03:41.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:41.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:41.865
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/30/23 23:03:41.879
    Jan 30 23:03:41.922: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6818" to be "running and ready"
    Jan 30 23:03:41.941: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385553ms
    Jan 30 23:03:41.941: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:03:43.963: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041638016s
    Jan 30 23:03:43.963: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:03:45.965: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.042927027s
    Jan 30 23:03:45.965: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 30 23:03:45.965: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/30/23 23:03:45.983
    STEP: Then the orphan pod is adopted 01/30/23 23:03:46.007
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6818" for this suite. 01/30/23 23:03:47.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:47.228
Jan 30 23:03:47.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 23:03:47.229
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:47.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:47.323
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 23:03:47.481
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:03:47.504
Jan 30 23:03:47.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:03:47.549: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:03:48.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:03:48.606: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:03:49.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:03:49.596: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
Jan 30 23:03:50.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:03:50.619: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/30/23 23:03:50.641
Jan 30 23:03:50.658: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/30/23 23:03:50.659
Jan 30 23:03:50.701: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/30/23 23:03:50.702
Jan 30 23:03:50.712: INFO: Observed &DaemonSet event: ADDED
Jan 30 23:03:50.712: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.713: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.714: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.715: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.715: INFO: Found daemon set daemon-set in namespace daemonsets-7405 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 23:03:50.715: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/30/23 23:03:50.716
STEP: watching for the daemon set status to be patched 01/30/23 23:03:50.755
Jan 30 23:03:50.762: INFO: Observed &DaemonSet event: ADDED
Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.763: INFO: Observed daemon set daemon-set in namespace daemonsets-7405 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 23:03:50.764: INFO: Observed &DaemonSet event: MODIFIED
Jan 30 23:03:50.764: INFO: Found daemon set daemon-set in namespace daemonsets-7405 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 30 23:03:50.764: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:03:50.779
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7405, will wait for the garbage collector to delete the pods 01/30/23 23:03:50.78
Jan 30 23:03:50.883: INFO: Deleting DaemonSet.extensions daemon-set took: 38.254079ms
Jan 30 23:03:50.984: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.330802ms
Jan 30 23:03:53.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:03:53.905: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 23:03:53.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27092"},"items":null}

Jan 30 23:03:53.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27092"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:03:54.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7405" for this suite. 01/30/23 23:03:54.064
------------------------------
• [SLOW TEST] [6.868 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:47.228
    Jan 30 23:03:47.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 23:03:47.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:47.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:47.323
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 23:03:47.481
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:03:47.504
    Jan 30 23:03:47.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:03:47.549: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:03:48.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:03:48.606: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:03:49.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:03:49.596: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
    Jan 30 23:03:50.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:03:50.619: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/30/23 23:03:50.641
    Jan 30 23:03:50.658: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/30/23 23:03:50.659
    Jan 30 23:03:50.701: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/30/23 23:03:50.702
    Jan 30 23:03:50.712: INFO: Observed &DaemonSet event: ADDED
    Jan 30 23:03:50.712: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.713: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.714: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.715: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.715: INFO: Found daemon set daemon-set in namespace daemonsets-7405 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 23:03:50.715: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/30/23 23:03:50.716
    STEP: watching for the daemon set status to be patched 01/30/23 23:03:50.755
    Jan 30 23:03:50.762: INFO: Observed &DaemonSet event: ADDED
    Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.763: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.763: INFO: Observed daemon set daemon-set in namespace daemonsets-7405 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 23:03:50.764: INFO: Observed &DaemonSet event: MODIFIED
    Jan 30 23:03:50.764: INFO: Found daemon set daemon-set in namespace daemonsets-7405 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 30 23:03:50.764: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:03:50.779
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7405, will wait for the garbage collector to delete the pods 01/30/23 23:03:50.78
    Jan 30 23:03:50.883: INFO: Deleting DaemonSet.extensions daemon-set took: 38.254079ms
    Jan 30 23:03:50.984: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.330802ms
    Jan 30 23:03:53.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:03:53.905: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 23:03:53.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27092"},"items":null}

    Jan 30 23:03:53.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27092"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:03:54.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7405" for this suite. 01/30/23 23:03:54.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:03:54.104
Jan 30 23:03:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:03:54.107
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:54.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:54.183
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:03:54.2
Jan 30 23:03:54.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15" in namespace "projected-2605" to be "Succeeded or Failed"
Jan 30 23:03:54.255: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 18.391809ms
Jan 30 23:03:56.276: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038654217s
Jan 30 23:03:58.273: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035931263s
Jan 30 23:04:00.275: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037617289s
STEP: Saw pod success 01/30/23 23:04:00.275
Jan 30 23:04:00.275: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15" satisfied condition "Succeeded or Failed"
Jan 30 23:04:00.293: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 container client-container: <nil>
STEP: delete the pod 01/30/23 23:04:00.332
Jan 30 23:04:00.387: INFO: Waiting for pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 to disappear
Jan 30 23:04:00.406: INFO: Pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:04:00.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2605" for this suite. 01/30/23 23:04:00.434
------------------------------
• [SLOW TEST] [6.372 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:03:54.104
    Jan 30 23:03:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:03:54.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:03:54.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:03:54.183
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:03:54.2
    Jan 30 23:03:54.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15" in namespace "projected-2605" to be "Succeeded or Failed"
    Jan 30 23:03:54.255: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 18.391809ms
    Jan 30 23:03:56.276: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038654217s
    Jan 30 23:03:58.273: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035931263s
    Jan 30 23:04:00.275: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037617289s
    STEP: Saw pod success 01/30/23 23:04:00.275
    Jan 30 23:04:00.275: INFO: Pod "downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15" satisfied condition "Succeeded or Failed"
    Jan 30 23:04:00.293: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:04:00.332
    Jan 30 23:04:00.387: INFO: Waiting for pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 to disappear
    Jan 30 23:04:00.406: INFO: Pod downwardapi-volume-ff442e5d-07ac-4862-88e5-6437e83d7a15 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:04:00.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2605" for this suite. 01/30/23 23:04:00.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:04:00.48
Jan 30 23:04:00.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:04:00.484
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:00.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:00.579
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-1ba2b05b-16e9-47e9-87c1-124c764c5eda 01/30/23 23:04:00.617
STEP: Creating the pod 01/30/23 23:04:00.658
Jan 30 23:04:00.693: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d" in namespace "configmap-4990" to be "running and ready"
Jan 30 23:04:00.723: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d": Phase="Pending", Reason="", readiness=false. Elapsed: 29.087265ms
Jan 30 23:04:00.723: INFO: The phase of Pod pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:04:02.762: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.068316009s
Jan 30 23:04:02.762: INFO: The phase of Pod pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d is Running (Ready = true)
Jan 30 23:04:02.762: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-1ba2b05b-16e9-47e9-87c1-124c764c5eda 01/30/23 23:04:02.817
STEP: waiting to observe update in volume 01/30/23 23:04:02.84
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:04:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4990" for this suite. 01/30/23 23:04:04.945
------------------------------
• [4.495 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:04:00.48
    Jan 30 23:04:00.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:04:00.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:00.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:00.579
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-1ba2b05b-16e9-47e9-87c1-124c764c5eda 01/30/23 23:04:00.617
    STEP: Creating the pod 01/30/23 23:04:00.658
    Jan 30 23:04:00.693: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d" in namespace "configmap-4990" to be "running and ready"
    Jan 30 23:04:00.723: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d": Phase="Pending", Reason="", readiness=false. Elapsed: 29.087265ms
    Jan 30 23:04:00.723: INFO: The phase of Pod pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:04:02.762: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d": Phase="Running", Reason="", readiness=true. Elapsed: 2.068316009s
    Jan 30 23:04:02.762: INFO: The phase of Pod pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d is Running (Ready = true)
    Jan 30 23:04:02.762: INFO: Pod "pod-configmaps-a3fd4692-58b0-4913-aa4a-a97e7938d98d" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-1ba2b05b-16e9-47e9-87c1-124c764c5eda 01/30/23 23:04:02.817
    STEP: waiting to observe update in volume 01/30/23 23:04:02.84
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:04:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4990" for this suite. 01/30/23 23:04:04.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:04:04.978
Jan 30 23:04:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 23:04:04.981
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:05.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:05.062
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/30/23 23:04:05.08
STEP: Ensuring active pods == parallelism 01/30/23 23:04:05.1
STEP: delete a job 01/30/23 23:04:09.125
STEP: deleting Job.batch foo in namespace job-3254, will wait for the garbage collector to delete the pods 01/30/23 23:04:09.125
Jan 30 23:04:09.228: INFO: Deleting Job.batch foo took: 33.224048ms
Jan 30 23:04:09.428: INFO: Terminating Job.batch foo pods took: 200.81941ms
STEP: Ensuring job was deleted 01/30/23 23:04:41.13
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 23:04:41.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3254" for this suite. 01/30/23 23:04:41.176
------------------------------
• [SLOW TEST] [36.229 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:04:04.978
    Jan 30 23:04:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 23:04:04.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:05.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:05.062
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/30/23 23:04:05.08
    STEP: Ensuring active pods == parallelism 01/30/23 23:04:05.1
    STEP: delete a job 01/30/23 23:04:09.125
    STEP: deleting Job.batch foo in namespace job-3254, will wait for the garbage collector to delete the pods 01/30/23 23:04:09.125
    Jan 30 23:04:09.228: INFO: Deleting Job.batch foo took: 33.224048ms
    Jan 30 23:04:09.428: INFO: Terminating Job.batch foo pods took: 200.81941ms
    STEP: Ensuring job was deleted 01/30/23 23:04:41.13
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:04:41.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3254" for this suite. 01/30/23 23:04:41.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:04:41.214
Jan 30 23:04:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:04:41.217
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:41.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:41.285
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:04:41.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8281" for this suite. 01/30/23 23:04:41.401
------------------------------
• [0.286 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:04:41.214
    Jan 30 23:04:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:04:41.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:41.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:41.285
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:04:41.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8281" for this suite. 01/30/23 23:04:41.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:04:41.506
Jan 30 23:04:41.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/30/23 23:04:41.509
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:41.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:41.628
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5119 01/30/23 23:04:41.644
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/30/23 23:04:41.665
Jan 30 23:04:41.757: INFO: Found 0 stateful pods, waiting for 3
Jan 30 23:04:51.783: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:04:51.783: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:04:51.783: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/30/23 23:04:51.836
Jan 30 23:04:51.911: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/30/23 23:04:51.911
STEP: Not applying an update when the partition is greater than the number of replicas 01/30/23 23:05:02.111
STEP: Performing a canary update 01/30/23 23:05:02.111
Jan 30 23:05:02.172: INFO: Updating stateful set ss2
Jan 30 23:05:02.264: INFO: Waiting for Pod statefulset-5119/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/30/23 23:05:12.323
Jan 30 23:05:12.592: INFO: Found 2 stateful pods, waiting for 3
Jan 30 23:05:22.614: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:05:22.615: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:05:22.615: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/30/23 23:05:22.653
Jan 30 23:05:22.712: INFO: Updating stateful set ss2
Jan 30 23:05:22.828: INFO: Waiting for Pod statefulset-5119/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 30 23:05:32.938: INFO: Updating stateful set ss2
Jan 30 23:05:32.975: INFO: Waiting for StatefulSet statefulset-5119/ss2 to complete update
Jan 30 23:05:32.975: INFO: Waiting for Pod statefulset-5119/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 23:05:43.016: INFO: Deleting all statefulset in ns statefulset-5119
Jan 30 23:05:43.031: INFO: Scaling statefulset ss2 to 0
Jan 30 23:05:53.113: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 23:05:53.131: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:05:53.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5119" for this suite. 01/30/23 23:05:53.221
------------------------------
• [SLOW TEST] [71.767 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:04:41.506
    Jan 30 23:04:41.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/30/23 23:04:41.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:04:41.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:04:41.628
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5119 01/30/23 23:04:41.644
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/30/23 23:04:41.665
    Jan 30 23:04:41.757: INFO: Found 0 stateful pods, waiting for 3
    Jan 30 23:04:51.783: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:04:51.783: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:04:51.783: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/30/23 23:04:51.836
    Jan 30 23:04:51.911: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/30/23 23:04:51.911
    STEP: Not applying an update when the partition is greater than the number of replicas 01/30/23 23:05:02.111
    STEP: Performing a canary update 01/30/23 23:05:02.111
    Jan 30 23:05:02.172: INFO: Updating stateful set ss2
    Jan 30 23:05:02.264: INFO: Waiting for Pod statefulset-5119/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/30/23 23:05:12.323
    Jan 30 23:05:12.592: INFO: Found 2 stateful pods, waiting for 3
    Jan 30 23:05:22.614: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:05:22.615: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:05:22.615: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/30/23 23:05:22.653
    Jan 30 23:05:22.712: INFO: Updating stateful set ss2
    Jan 30 23:05:22.828: INFO: Waiting for Pod statefulset-5119/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 30 23:05:32.938: INFO: Updating stateful set ss2
    Jan 30 23:05:32.975: INFO: Waiting for StatefulSet statefulset-5119/ss2 to complete update
    Jan 30 23:05:32.975: INFO: Waiting for Pod statefulset-5119/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 23:05:43.016: INFO: Deleting all statefulset in ns statefulset-5119
    Jan 30 23:05:43.031: INFO: Scaling statefulset ss2 to 0
    Jan 30 23:05:53.113: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 23:05:53.131: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:05:53.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5119" for this suite. 01/30/23 23:05:53.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:05:53.276
Jan 30 23:05:53.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename subpath 01/30/23 23:05:53.278
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:05:53.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:05:53.343
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 23:05:53.398
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-tzjq 01/30/23 23:05:53.467
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:05:53.467
Jan 30 23:05:53.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tzjq" in namespace "subpath-6184" to be "Succeeded or Failed"
Jan 30 23:05:53.540: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.886399ms
Jan 30 23:05:55.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038435282s
Jan 30 23:05:57.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 4.038619386s
Jan 30 23:05:59.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 6.037527231s
Jan 30 23:06:01.581: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 8.058920023s
Jan 30 23:06:03.561: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 10.038892926s
Jan 30 23:06:05.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 12.037261981s
Jan 30 23:06:07.558: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 14.0360403s
Jan 30 23:06:09.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 16.038326892s
Jan 30 23:06:11.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 18.038294586s
Jan 30 23:06:13.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 20.03758669s
Jan 30 23:06:15.561: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 22.038931347s
Jan 30 23:06:17.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=false. Elapsed: 24.038461136s
Jan 30 23:06:19.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.038535101s
STEP: Saw pod success 01/30/23 23:06:19.56
Jan 30 23:06:19.561: INFO: Pod "pod-subpath-test-configmap-tzjq" satisfied condition "Succeeded or Failed"
Jan 30 23:06:19.580: INFO: Trying to get logs from node 10.15.28.237 pod pod-subpath-test-configmap-tzjq container test-container-subpath-configmap-tzjq: <nil>
STEP: delete the pod 01/30/23 23:06:19.676
Jan 30 23:06:19.720: INFO: Waiting for pod pod-subpath-test-configmap-tzjq to disappear
Jan 30 23:06:19.736: INFO: Pod pod-subpath-test-configmap-tzjq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tzjq 01/30/23 23:06:19.736
Jan 30 23:06:19.736: INFO: Deleting pod "pod-subpath-test-configmap-tzjq" in namespace "subpath-6184"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 23:06:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6184" for this suite. 01/30/23 23:06:19.778
------------------------------
• [SLOW TEST] [26.530 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:05:53.276
    Jan 30 23:05:53.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename subpath 01/30/23 23:05:53.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:05:53.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:05:53.343
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 23:05:53.398
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-tzjq 01/30/23 23:05:53.467
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:05:53.467
    Jan 30 23:05:53.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tzjq" in namespace "subpath-6184" to be "Succeeded or Failed"
    Jan 30 23:05:53.540: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.886399ms
    Jan 30 23:05:55.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038435282s
    Jan 30 23:05:57.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 4.038619386s
    Jan 30 23:05:59.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 6.037527231s
    Jan 30 23:06:01.581: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 8.058920023s
    Jan 30 23:06:03.561: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 10.038892926s
    Jan 30 23:06:05.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 12.037261981s
    Jan 30 23:06:07.558: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 14.0360403s
    Jan 30 23:06:09.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 16.038326892s
    Jan 30 23:06:11.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 18.038294586s
    Jan 30 23:06:13.559: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 20.03758669s
    Jan 30 23:06:15.561: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=true. Elapsed: 22.038931347s
    Jan 30 23:06:17.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Running", Reason="", readiness=false. Elapsed: 24.038461136s
    Jan 30 23:06:19.560: INFO: Pod "pod-subpath-test-configmap-tzjq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.038535101s
    STEP: Saw pod success 01/30/23 23:06:19.56
    Jan 30 23:06:19.561: INFO: Pod "pod-subpath-test-configmap-tzjq" satisfied condition "Succeeded or Failed"
    Jan 30 23:06:19.580: INFO: Trying to get logs from node 10.15.28.237 pod pod-subpath-test-configmap-tzjq container test-container-subpath-configmap-tzjq: <nil>
    STEP: delete the pod 01/30/23 23:06:19.676
    Jan 30 23:06:19.720: INFO: Waiting for pod pod-subpath-test-configmap-tzjq to disappear
    Jan 30 23:06:19.736: INFO: Pod pod-subpath-test-configmap-tzjq no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-tzjq 01/30/23 23:06:19.736
    Jan 30 23:06:19.736: INFO: Deleting pod "pod-subpath-test-configmap-tzjq" in namespace "subpath-6184"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:06:19.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6184" for this suite. 01/30/23 23:06:19.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:06:19.812
Jan 30 23:06:19.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 23:06:19.815
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:19.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:19.898
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 23:06:19.915
Jan 30 23:06:19.951: INFO: Waiting up to 5m0s for pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca" in namespace "emptydir-2384" to be "Succeeded or Failed"
Jan 30 23:06:19.970: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 18.54812ms
Jan 30 23:06:21.992: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040413588s
Jan 30 23:06:23.990: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038290949s
Jan 30 23:06:25.992: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040432943s
STEP: Saw pod success 01/30/23 23:06:25.992
Jan 30 23:06:25.993: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca" satisfied condition "Succeeded or Failed"
Jan 30 23:06:26.010: INFO: Trying to get logs from node 10.15.28.237 pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca container test-container: <nil>
STEP: delete the pod 01/30/23 23:06:26.046
Jan 30 23:06:26.101: INFO: Waiting for pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca to disappear
Jan 30 23:06:26.117: INFO: Pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:06:26.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2384" for this suite. 01/30/23 23:06:26.147
------------------------------
• [SLOW TEST] [6.369 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:06:19.812
    Jan 30 23:06:19.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 23:06:19.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:19.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:19.898
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/30/23 23:06:19.915
    Jan 30 23:06:19.951: INFO: Waiting up to 5m0s for pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca" in namespace "emptydir-2384" to be "Succeeded or Failed"
    Jan 30 23:06:19.970: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 18.54812ms
    Jan 30 23:06:21.992: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040413588s
    Jan 30 23:06:23.990: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038290949s
    Jan 30 23:06:25.992: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040432943s
    STEP: Saw pod success 01/30/23 23:06:25.992
    Jan 30 23:06:25.993: INFO: Pod "pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca" satisfied condition "Succeeded or Failed"
    Jan 30 23:06:26.010: INFO: Trying to get logs from node 10.15.28.237 pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca container test-container: <nil>
    STEP: delete the pod 01/30/23 23:06:26.046
    Jan 30 23:06:26.101: INFO: Waiting for pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca to disappear
    Jan 30 23:06:26.117: INFO: Pod pod-b4a7dd88-a05b-4930-9fc5-25675b1184ca no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:06:26.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2384" for this suite. 01/30/23 23:06:26.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:06:26.192
Jan 30 23:06:26.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/30/23 23:06:26.194
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:26.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:26.263
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 30 23:06:26.321: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 30 23:06:31.350: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 23:06:31.351
Jan 30 23:06:31.351: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/30/23 23:06:31.396
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 23:06:31.455: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4278  97b5e782-0b33-4d34-89a9-aff324c542f7 27925 1 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 30 23:06:31.497: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4278  13f79a9d-21e9-477c-ab52-f57f2cb3075c 27927 1 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 97b5e782-0b33-4d34-89a9-aff324c542f7 0xc003cfe377 0xc003cfe378}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97b5e782-0b33-4d34-89a9-aff324c542f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfe408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:06:31.497: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 30 23:06:31.498: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4278  9a148778-b689-4c5c-b288-9871ccb4bac4 27926 1 2023-01-30 23:06:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 97b5e782-0b33-4d34-89a9-aff324c542f7 0xc003cfe237 0xc003cfe238}] [] [{e2e.test Update apps/v1 2023-01-30 23:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:06:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"97b5e782-0b33-4d34-89a9-aff324c542f7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003cfe308 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:06:31.534: INFO: Pod "test-cleanup-controller-r8wl7" is available:
&Pod{ObjectMeta:{test-cleanup-controller-r8wl7 test-cleanup-controller- deployment-4278  7a382573-ec9d-4d33-bd3a-b657bf299f8c 27916 0 2023-01-30 23:06:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:441c91b3c00ad3becc3264a8b86794ff43741136d4cba91cfeb14c2093cd3ddd cni.projectcalico.org/podIP:172.30.199.63/32 cni.projectcalico.org/podIPs:172.30.199.63/32] [{apps/v1 ReplicaSet test-cleanup-controller 9a148778-b689-4c5c-b288-9871ccb4bac4 0xc003cfe8b7 0xc003cfe8b8}] [] [{kube-controller-manager Update v1 2023-01-30 23:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a148778-b689-4c5c-b288-9871ccb4bac4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:06:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:06:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsmhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsmhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.63,StartTime:2023-01-30 23:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:06:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://af6b83152a15dc182c6463f02bc790d32f5a2c9e1e2bdc73b957a910b7a9b6a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 23:06:31.535: INFO: Pod "test-cleanup-deployment-7698ff6f6b-klrnq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-klrnq test-cleanup-deployment-7698ff6f6b- deployment-4278  fe60d587-0319-475c-acb3-38040abc9351 27931 0 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 13f79a9d-21e9-477c-ab52-f57f2cb3075c 0xc003cfead7 0xc003cfead8}] [] [{kube-controller-manager Update v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13f79a9d-21e9-477c-ab52-f57f2cb3075c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9brl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9brl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 23:06:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4278" for this suite. 01/30/23 23:06:31.579
------------------------------
• [SLOW TEST] [5.416 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:06:26.192
    Jan 30 23:06:26.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/30/23 23:06:26.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:26.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:26.263
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 30 23:06:26.321: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 30 23:06:31.350: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 23:06:31.351
    Jan 30 23:06:31.351: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/30/23 23:06:31.396
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 23:06:31.455: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4278  97b5e782-0b33-4d34-89a9-aff324c542f7 27925 1 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00404fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 30 23:06:31.497: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-4278  13f79a9d-21e9-477c-ab52-f57f2cb3075c 27927 1 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 97b5e782-0b33-4d34-89a9-aff324c542f7 0xc003cfe377 0xc003cfe378}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97b5e782-0b33-4d34-89a9-aff324c542f7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfe408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:06:31.497: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 30 23:06:31.498: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4278  9a148778-b689-4c5c-b288-9871ccb4bac4 27926 1 2023-01-30 23:06:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 97b5e782-0b33-4d34-89a9-aff324c542f7 0xc003cfe237 0xc003cfe238}] [] [{e2e.test Update apps/v1 2023-01-30 23:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:06:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"97b5e782-0b33-4d34-89a9-aff324c542f7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003cfe308 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:06:31.534: INFO: Pod "test-cleanup-controller-r8wl7" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-r8wl7 test-cleanup-controller- deployment-4278  7a382573-ec9d-4d33-bd3a-b657bf299f8c 27916 0 2023-01-30 23:06:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:441c91b3c00ad3becc3264a8b86794ff43741136d4cba91cfeb14c2093cd3ddd cni.projectcalico.org/podIP:172.30.199.63/32 cni.projectcalico.org/podIPs:172.30.199.63/32] [{apps/v1 ReplicaSet test-cleanup-controller 9a148778-b689-4c5c-b288-9871ccb4bac4 0xc003cfe8b7 0xc003cfe8b8}] [] [{kube-controller-manager Update v1 2023-01-30 23:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a148778-b689-4c5c-b288-9871ccb4bac4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:06:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:06:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsmhg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsmhg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.63,StartTime:2023-01-30 23:06:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:06:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://af6b83152a15dc182c6463f02bc790d32f5a2c9e1e2bdc73b957a910b7a9b6a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 23:06:31.535: INFO: Pod "test-cleanup-deployment-7698ff6f6b-klrnq" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-klrnq test-cleanup-deployment-7698ff6f6b- deployment-4278  fe60d587-0319-475c-acb3-38040abc9351 27931 0 2023-01-30 23:06:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 13f79a9d-21e9-477c-ab52-f57f2cb3075c 0xc003cfead7 0xc003cfead8}] [] [{kube-controller-manager Update v1 2023-01-30 23:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13f79a9d-21e9-477c-ab52-f57f2cb3075c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9brl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9brl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:06:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:06:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4278" for this suite. 01/30/23 23:06:31.579
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:06:31.611
Jan 30 23:06:31.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:06:31.614
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:31.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:31.681
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 30 23:06:31.853: INFO: created pod pod-service-account-defaultsa
Jan 30 23:06:31.853: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 30 23:06:31.876: INFO: created pod pod-service-account-mountsa
Jan 30 23:06:31.876: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 30 23:06:31.898: INFO: created pod pod-service-account-nomountsa
Jan 30 23:06:31.898: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 30 23:06:31.925: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 30 23:06:31.925: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 30 23:06:31.951: INFO: created pod pod-service-account-mountsa-mountspec
Jan 30 23:06:31.951: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 30 23:06:31.978: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 30 23:06:31.978: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 30 23:06:31.997: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 30 23:06:31.997: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 30 23:06:32.017: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 30 23:06:32.017: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 30 23:06:32.048: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 30 23:06:32.048: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 23:06:32.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-850" for this suite. 01/30/23 23:06:32.087
------------------------------
• [0.520 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:06:31.611
    Jan 30 23:06:31.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:06:31.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:31.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:31.681
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 30 23:06:31.853: INFO: created pod pod-service-account-defaultsa
    Jan 30 23:06:31.853: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 30 23:06:31.876: INFO: created pod pod-service-account-mountsa
    Jan 30 23:06:31.876: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 30 23:06:31.898: INFO: created pod pod-service-account-nomountsa
    Jan 30 23:06:31.898: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 30 23:06:31.925: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 30 23:06:31.925: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 30 23:06:31.951: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 30 23:06:31.951: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 30 23:06:31.978: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 30 23:06:31.978: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 30 23:06:31.997: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 30 23:06:31.997: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 30 23:06:32.017: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 30 23:06:32.017: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 30 23:06:32.048: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 30 23:06:32.048: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:06:32.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-850" for this suite. 01/30/23 23:06:32.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:06:32.134
Jan 30 23:06:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:06:32.136
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:32.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:32.233
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 23:06:32.306: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:07:32.476: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:07:32.496
Jan 30 23:07:32.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 23:07:32.498
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:32.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:32.572
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jan 30 23:07:32.663: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 30 23:07:32.679: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 30 23:07:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:07:32.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1849" for this suite. 01/30/23 23:07:33.078
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7062" for this suite. 01/30/23 23:07:33.116
------------------------------
• [SLOW TEST] [61.009 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:06:32.134
    Jan 30 23:06:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:06:32.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:06:32.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:06:32.233
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 23:06:32.306: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:07:32.476: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:07:32.496
    Jan 30 23:07:32.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 23:07:32.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:32.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:32.572
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jan 30 23:07:32.663: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 30 23:07:32.679: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:07:32.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:07:32.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1849" for this suite. 01/30/23 23:07:33.078
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7062" for this suite. 01/30/23 23:07:33.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:07:33.148
Jan 30 23:07:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename init-container 01/30/23 23:07:33.148
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:33.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:33.241
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/30/23 23:07:33.257
Jan 30 23:07:33.258: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:07:39.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1642" for this suite. 01/30/23 23:07:39.596
------------------------------
• [SLOW TEST] [6.479 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:07:33.148
    Jan 30 23:07:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename init-container 01/30/23 23:07:33.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:33.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:33.241
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/30/23 23:07:33.257
    Jan 30 23:07:33.258: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:07:39.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1642" for this suite. 01/30/23 23:07:39.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:07:39.632
Jan 30 23:07:39.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename watch 01/30/23 23:07:39.637
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:39.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:39.701
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/30/23 23:07:39.716
STEP: starting a background goroutine to produce watch events 01/30/23 23:07:39.763
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/30/23 23:07:39.763
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 23:07:42.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5152" for this suite. 01/30/23 23:07:42.526
------------------------------
• [2.935 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:07:39.632
    Jan 30 23:07:39.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename watch 01/30/23 23:07:39.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:39.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:39.701
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/30/23 23:07:39.716
    STEP: starting a background goroutine to produce watch events 01/30/23 23:07:39.763
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/30/23 23:07:39.763
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:07:42.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5152" for this suite. 01/30/23 23:07:42.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:07:42.57
Jan 30 23:07:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:07:42.573
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:42.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:42.673
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-8e5533c3-fe36-4523-b3ef-34108a8271f0 01/30/23 23:07:42.759
STEP: Creating secret with name s-test-opt-upd-9d8dc72b-4870-4d9e-8d4a-8304464b21c8 01/30/23 23:07:42.781
STEP: Creating the pod 01/30/23 23:07:42.8
Jan 30 23:07:42.880: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8" in namespace "projected-9385" to be "running and ready"
Jan 30 23:07:42.899: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.660299ms
Jan 30 23:07:42.899: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:07:44.948: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068087893s
Jan 30 23:07:44.948: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:07:46.919: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.038976476s
Jan 30 23:07:46.919: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Running (Ready = true)
Jan 30 23:07:46.919: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8e5533c3-fe36-4523-b3ef-34108a8271f0 01/30/23 23:07:47.14
STEP: Updating secret s-test-opt-upd-9d8dc72b-4870-4d9e-8d4a-8304464b21c8 01/30/23 23:07:47.168
STEP: Creating secret with name s-test-opt-create-1dbff964-4370-47a2-b72e-271749bb1575 01/30/23 23:07:47.196
STEP: waiting to observe update in volume 01/30/23 23:07:47.214
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 23:08:56.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9385" for this suite. 01/30/23 23:08:56.919
------------------------------
• [SLOW TEST] [74.376 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:07:42.57
    Jan 30 23:07:42.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:07:42.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:07:42.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:07:42.673
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-8e5533c3-fe36-4523-b3ef-34108a8271f0 01/30/23 23:07:42.759
    STEP: Creating secret with name s-test-opt-upd-9d8dc72b-4870-4d9e-8d4a-8304464b21c8 01/30/23 23:07:42.781
    STEP: Creating the pod 01/30/23 23:07:42.8
    Jan 30 23:07:42.880: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8" in namespace "projected-9385" to be "running and ready"
    Jan 30 23:07:42.899: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.660299ms
    Jan 30 23:07:42.899: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:07:44.948: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068087893s
    Jan 30 23:07:44.948: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:07:46.919: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8": Phase="Running", Reason="", readiness=true. Elapsed: 4.038976476s
    Jan 30 23:07:46.919: INFO: The phase of Pod pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8 is Running (Ready = true)
    Jan 30 23:07:46.919: INFO: Pod "pod-projected-secrets-c7978a02-7ea9-40fc-a518-f91b0e3002d8" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8e5533c3-fe36-4523-b3ef-34108a8271f0 01/30/23 23:07:47.14
    STEP: Updating secret s-test-opt-upd-9d8dc72b-4870-4d9e-8d4a-8304464b21c8 01/30/23 23:07:47.168
    STEP: Creating secret with name s-test-opt-create-1dbff964-4370-47a2-b72e-271749bb1575 01/30/23 23:07:47.196
    STEP: waiting to observe update in volume 01/30/23 23:07:47.214
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:08:56.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9385" for this suite. 01/30/23 23:08:56.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:08:56.951
Jan 30 23:08:56.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 23:08:56.953
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:08:57.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:08:57.017
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/30/23 23:08:57.035
STEP: Ensure pods equal to parallelism count is attached to the job 01/30/23 23:08:57.058
STEP: patching /status 01/30/23 23:09:01.077
STEP: updating /status 01/30/23 23:09:01.1
STEP: get /status 01/30/23 23:09:01.181
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9164" for this suite. 01/30/23 23:09:01.227
------------------------------
• [4.309 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:08:56.951
    Jan 30 23:08:56.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 23:08:56.953
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:08:57.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:08:57.017
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/30/23 23:08:57.035
    STEP: Ensure pods equal to parallelism count is attached to the job 01/30/23 23:08:57.058
    STEP: patching /status 01/30/23 23:09:01.077
    STEP: updating /status 01/30/23 23:09:01.1
    STEP: get /status 01/30/23 23:09:01.181
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:01.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9164" for this suite. 01/30/23 23:09:01.227
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:01.264
Jan 30 23:09:01.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/30/23 23:09:01.266
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:01.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:01.34
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/30/23 23:09:01.356
STEP: When the matched label of one of its pods change 01/30/23 23:09:01.384
Jan 30 23:09:01.408: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 30 23:09:06.432: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/30/23 23:09:06.478
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-885" for this suite. 01/30/23 23:09:07.548
------------------------------
• [SLOW TEST] [6.315 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:01.264
    Jan 30 23:09:01.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/30/23 23:09:01.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:01.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:01.34
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/30/23 23:09:01.356
    STEP: When the matched label of one of its pods change 01/30/23 23:09:01.384
    Jan 30 23:09:01.408: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 30 23:09:06.432: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/30/23 23:09:06.478
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-885" for this suite. 01/30/23 23:09:07.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:07.592
Jan 30 23:09:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:09:07.594
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:07.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:07.682
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 23:09:07.699
Jan 30 23:09:07.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 30 23:09:07.898: INFO: stderr: ""
Jan 30 23:09:07.898: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/30/23 23:09:07.898
Jan 30 23:09:07.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 30 23:09:08.255: INFO: stderr: ""
Jan 30 23:09:08.255: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 23:09:08.255
Jan 30 23:09:08.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 delete pods e2e-test-httpd-pod'
Jan 30 23:09:12.097: INFO: stderr: ""
Jan 30 23:09:12.097: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:12.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4540" for this suite. 01/30/23 23:09:12.122
------------------------------
• [4.558 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:07.592
    Jan 30 23:09:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:09:07.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:07.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:07.682
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 23:09:07.699
    Jan 30 23:09:07.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 30 23:09:07.898: INFO: stderr: ""
    Jan 30 23:09:07.898: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/30/23 23:09:07.898
    Jan 30 23:09:07.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 30 23:09:08.255: INFO: stderr: ""
    Jan 30 23:09:08.255: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/30/23 23:09:08.255
    Jan 30 23:09:08.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-4540 delete pods e2e-test-httpd-pod'
    Jan 30 23:09:12.097: INFO: stderr: ""
    Jan 30 23:09:12.097: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:12.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4540" for this suite. 01/30/23 23:09:12.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:12.156
Jan 30 23:09:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:09:12.159
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:12.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:12.227
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-991 01/30/23 23:09:12.249
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[] 01/30/23 23:09:12.327
Jan 30 23:09:12.367: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-991 01/30/23 23:09:12.367
Jan 30 23:09:12.421: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-991" to be "running and ready"
Jan 30 23:09:12.441: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.698093ms
Jan 30 23:09:12.441: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:09:14.462: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040448409s
Jan 30 23:09:14.462: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:09:16.462: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.040400156s
Jan 30 23:09:16.462: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 23:09:16.462: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod1:[80]] 01/30/23 23:09:16.479
Jan 30 23:09:16.542: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/30/23 23:09:16.542
Jan 30 23:09:16.543: INFO: Creating new exec pod
Jan 30 23:09:16.566: INFO: Waiting up to 5m0s for pod "execpodv8j4r" in namespace "services-991" to be "running"
Jan 30 23:09:16.587: INFO: Pod "execpodv8j4r": Phase="Pending", Reason="", readiness=false. Elapsed: 20.688286ms
Jan 30 23:09:18.609: INFO: Pod "execpodv8j4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042653728s
Jan 30 23:09:20.613: INFO: Pod "execpodv8j4r": Phase="Running", Reason="", readiness=true. Elapsed: 4.047051232s
Jan 30 23:09:20.613: INFO: Pod "execpodv8j4r" satisfied condition "running"
Jan 30 23:09:21.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 23:09:22.098: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:22.098: INFO: stdout: ""
Jan 30 23:09:22.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
Jan 30 23:09:22.532: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:22.532: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-991 01/30/23 23:09:22.532
Jan 30 23:09:22.557: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-991" to be "running and ready"
Jan 30 23:09:22.579: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.915394ms
Jan 30 23:09:22.579: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:09:24.613: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055602428s
Jan 30 23:09:24.613: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:09:26.601: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.043297133s
Jan 30 23:09:26.601: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 23:09:26.601: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod1:[80] pod2:[80]] 01/30/23 23:09:26.619
Jan 30 23:09:26.718: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/30/23 23:09:26.718
Jan 30 23:09:27.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 23:09:28.122: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:28.122: INFO: stdout: ""
Jan 30 23:09:28.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
Jan 30 23:09:28.515: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:28.515: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-991 01/30/23 23:09:28.515
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod2:[80]] 01/30/23 23:09:28.587
Jan 30 23:09:28.658: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/30/23 23:09:28.658
Jan 30 23:09:29.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 30 23:09:30.046: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:30.046: INFO: stdout: ""
Jan 30 23:09:30.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
Jan 30 23:09:30.469: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
Jan 30 23:09:30.469: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-991 01/30/23 23:09:30.469
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[] 01/30/23 23:09:30.535
Jan 30 23:09:30.581: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:30.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-991" for this suite. 01/30/23 23:09:30.712
------------------------------
• [SLOW TEST] [18.585 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:12.156
    Jan 30 23:09:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:09:12.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:12.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:12.227
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-991 01/30/23 23:09:12.249
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[] 01/30/23 23:09:12.327
    Jan 30 23:09:12.367: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-991 01/30/23 23:09:12.367
    Jan 30 23:09:12.421: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-991" to be "running and ready"
    Jan 30 23:09:12.441: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.698093ms
    Jan 30 23:09:12.441: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:09:14.462: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040448409s
    Jan 30 23:09:14.462: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:09:16.462: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.040400156s
    Jan 30 23:09:16.462: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 23:09:16.462: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod1:[80]] 01/30/23 23:09:16.479
    Jan 30 23:09:16.542: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/30/23 23:09:16.542
    Jan 30 23:09:16.543: INFO: Creating new exec pod
    Jan 30 23:09:16.566: INFO: Waiting up to 5m0s for pod "execpodv8j4r" in namespace "services-991" to be "running"
    Jan 30 23:09:16.587: INFO: Pod "execpodv8j4r": Phase="Pending", Reason="", readiness=false. Elapsed: 20.688286ms
    Jan 30 23:09:18.609: INFO: Pod "execpodv8j4r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042653728s
    Jan 30 23:09:20.613: INFO: Pod "execpodv8j4r": Phase="Running", Reason="", readiness=true. Elapsed: 4.047051232s
    Jan 30 23:09:20.613: INFO: Pod "execpodv8j4r" satisfied condition "running"
    Jan 30 23:09:21.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 23:09:22.098: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:22.098: INFO: stdout: ""
    Jan 30 23:09:22.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
    Jan 30 23:09:22.532: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:22.532: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-991 01/30/23 23:09:22.532
    Jan 30 23:09:22.557: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-991" to be "running and ready"
    Jan 30 23:09:22.579: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.915394ms
    Jan 30 23:09:22.579: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:09:24.613: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055602428s
    Jan 30 23:09:24.613: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:09:26.601: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.043297133s
    Jan 30 23:09:26.601: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 23:09:26.601: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod1:[80] pod2:[80]] 01/30/23 23:09:26.619
    Jan 30 23:09:26.718: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/30/23 23:09:26.718
    Jan 30 23:09:27.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 23:09:28.122: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:28.122: INFO: stdout: ""
    Jan 30 23:09:28.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
    Jan 30 23:09:28.515: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:28.515: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-991 01/30/23 23:09:28.515
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[pod2:[80]] 01/30/23 23:09:28.587
    Jan 30 23:09:28.658: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/30/23 23:09:28.658
    Jan 30 23:09:29.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 30 23:09:30.046: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:30.046: INFO: stdout: ""
    Jan 30 23:09:30.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-991 exec execpodv8j4r -- /bin/sh -x -c nc -v -z -w 2 172.21.81.254 80'
    Jan 30 23:09:30.469: INFO: stderr: "+ nc -v -z -w 2 172.21.81.254 80\nConnection to 172.21.81.254 80 port [tcp/http] succeeded!\n"
    Jan 30 23:09:30.469: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-991 01/30/23 23:09:30.469
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-991 to expose endpoints map[] 01/30/23 23:09:30.535
    Jan 30 23:09:30.581: INFO: successfully validated that service endpoint-test2 in namespace services-991 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:30.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-991" for this suite. 01/30/23 23:09:30.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:30.743
Jan 30 23:09:30.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename lease-test 01/30/23 23:09:30.746
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:30.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:30.825
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:31.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-8462" for this suite. 01/30/23 23:09:31.161
------------------------------
• [0.445 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:30.743
    Jan 30 23:09:30.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename lease-test 01/30/23 23:09:30.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:30.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:30.825
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:31.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-8462" for this suite. 01/30/23 23:09:31.161
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:31.189
Jan 30 23:09:31.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:09:31.192
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:31.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:31.282
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/30/23 23:09:31.299
Jan 30 23:09:31.299: INFO: namespace kubectl-9876
Jan 30 23:09:31.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 create -f -'
Jan 30 23:09:31.669: INFO: stderr: ""
Jan 30 23:09:31.669: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 23:09:31.669
Jan 30 23:09:32.688: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:09:32.688: INFO: Found 0 / 1
Jan 30 23:09:33.691: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:09:33.691: INFO: Found 0 / 1
Jan 30 23:09:34.698: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:09:34.698: INFO: Found 1 / 1
Jan 30 23:09:34.698: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 30 23:09:34.718: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:09:34.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 23:09:34.718: INFO: wait on agnhost-primary startup in kubectl-9876 
Jan 30 23:09:34.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 logs agnhost-primary-b4g9p agnhost-primary'
Jan 30 23:09:35.004: INFO: stderr: ""
Jan 30 23:09:35.004: INFO: stdout: "Paused\n"
STEP: exposing RC 01/30/23 23:09:35.004
Jan 30 23:09:35.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 30 23:09:35.231: INFO: stderr: ""
Jan 30 23:09:35.231: INFO: stdout: "service/rm2 exposed\n"
Jan 30 23:09:35.246: INFO: Service rm2 in namespace kubectl-9876 found.
STEP: exposing service 01/30/23 23:09:37.296
Jan 30 23:09:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 30 23:09:37.508: INFO: stderr: ""
Jan 30 23:09:37.508: INFO: stdout: "service/rm3 exposed\n"
Jan 30 23:09:37.530: INFO: Service rm3 in namespace kubectl-9876 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:39.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9876" for this suite. 01/30/23 23:09:39.608
------------------------------
• [SLOW TEST] [8.446 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:31.189
    Jan 30 23:09:31.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:09:31.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:31.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:31.282
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/30/23 23:09:31.299
    Jan 30 23:09:31.299: INFO: namespace kubectl-9876
    Jan 30 23:09:31.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 create -f -'
    Jan 30 23:09:31.669: INFO: stderr: ""
    Jan 30 23:09:31.669: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 23:09:31.669
    Jan 30 23:09:32.688: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:09:32.688: INFO: Found 0 / 1
    Jan 30 23:09:33.691: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:09:33.691: INFO: Found 0 / 1
    Jan 30 23:09:34.698: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:09:34.698: INFO: Found 1 / 1
    Jan 30 23:09:34.698: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 30 23:09:34.718: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:09:34.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 23:09:34.718: INFO: wait on agnhost-primary startup in kubectl-9876 
    Jan 30 23:09:34.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 logs agnhost-primary-b4g9p agnhost-primary'
    Jan 30 23:09:35.004: INFO: stderr: ""
    Jan 30 23:09:35.004: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/30/23 23:09:35.004
    Jan 30 23:09:35.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 30 23:09:35.231: INFO: stderr: ""
    Jan 30 23:09:35.231: INFO: stdout: "service/rm2 exposed\n"
    Jan 30 23:09:35.246: INFO: Service rm2 in namespace kubectl-9876 found.
    STEP: exposing service 01/30/23 23:09:37.296
    Jan 30 23:09:37.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-9876 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 30 23:09:37.508: INFO: stderr: ""
    Jan 30 23:09:37.508: INFO: stdout: "service/rm3 exposed\n"
    Jan 30 23:09:37.530: INFO: Service rm3 in namespace kubectl-9876 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:39.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9876" for this suite. 01/30/23 23:09:39.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:39.641
Jan 30 23:09:39.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-pred 01/30/23 23:09:39.643
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:39.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:39.756
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 23:09:39.771: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 23:09:39.825: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 23:09:39.843: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.225 before test
Jan 30 23:09:39.881: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:09:39.881: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:09:39.881: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:09:39.881: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:09:39.881: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:09:39.881: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:09:39.881: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:09:39.881: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:09:39.881: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:09:39.881: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
Jan 30 23:09:39.881: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:09:39.881: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:09:39.882: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:09:39.882: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.882: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:09:39.882: INFO: agnhost-primary-b4g9p from kubectl-9876 started at 2023-01-30 23:09:31 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.882: INFO: 	Container agnhost-primary ready: true, restart count 0
Jan 30 23:09:39.882: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.882: INFO: 	Container e2e ready: true, restart count 0
Jan 30 23:09:39.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:09:39.882: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:09:39.882: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:09:39.882: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.227 before test
Jan 30 23:09:39.921: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-pd8h4 from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.921: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:09:39.921: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.921: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:09:39.921: INFO: calico-typha-5fcb7c495f-k54xr from kube-system started at 2023-01-30 20:40:10 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.922: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:09:39.922: INFO: coredns-56697bd765-8dkc9 from kube-system started at 2023-01-30 20:48:51 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.922: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:09:39.922: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.922: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:09:39.922: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.923: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:09:39.923: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:09:39.923: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.923: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:09:39.923: INFO: ingress-cluster-healthcheck-bbddc799d-b2ks6 from kube-system started at 2023-01-30 20:53:30 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.923: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 30 23:09:39.923: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.923: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:09:39.923: INFO: metrics-server-5c45845f46-9cskr from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
Jan 30 23:09:39.923: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:09:39.923: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:09:39.924: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:09:39.924: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.924: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 23:09:39.924: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:09:39.925: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:09:39.925: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-30 20:41:56 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.925: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 30 23:09:39.925: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.237 before test
Jan 30 23:09:39.962: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 23:09:39.962: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:09:39.962: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:09:39.962: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:09:39.962: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container autoscaler ready: true, restart count 0
Jan 30 23:09:39.962: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:09:39.962: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:09:39.962: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 30 23:09:39.962: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:09:39.962: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 30 23:09:39.962: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:09:39.962: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:09:39.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:09:39.962: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/30/23 23:09:39.962
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173f38edbafbbd74], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/30/23 23:09:40.12
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:41.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4203" for this suite. 01/30/23 23:09:41.147
------------------------------
• [1.557 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:39.641
    Jan 30 23:09:39.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-pred 01/30/23 23:09:39.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:39.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:39.756
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 23:09:39.771: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 23:09:39.825: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 23:09:39.843: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.225 before test
    Jan 30 23:09:39.881: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
    Jan 30 23:09:39.881: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:09:39.881: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.882: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: agnhost-primary-b4g9p from kubectl-9876 started at 2023-01-30 23:09:31 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.882: INFO: 	Container agnhost-primary ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.882: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:09:39.882: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.227 before test
    Jan 30 23:09:39.921: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-pd8h4 from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.921: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:09:39.921: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.921: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:09:39.921: INFO: calico-typha-5fcb7c495f-k54xr from kube-system started at 2023-01-30 20:40:10 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.922: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:09:39.922: INFO: coredns-56697bd765-8dkc9 from kube-system started at 2023-01-30 20:48:51 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.922: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:09:39.922: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.922: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:09:39.922: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.923: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.923: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: ingress-cluster-healthcheck-bbddc799d-b2ks6 from kube-system started at 2023-01-30 20:53:30 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.923: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.923: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: metrics-server-5c45845f46-9cskr from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
    Jan 30 23:09:39.923: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:09:39.923: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:09:39.924: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:09:39.924: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.924: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 23:09:39.924: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:09:39.925: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:09:39.925: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-30 20:41:56 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.925: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jan 30 23:09:39.925: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.237 before test
    Jan 30 23:09:39.962: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:09:39.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:09:39.962: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/30/23 23:09:39.962
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173f38edbafbbd74], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/30/23 23:09:40.12
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:41.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4203" for this suite. 01/30/23 23:09:41.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:41.2
Jan 30 23:09:41.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 23:09:41.203
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:41.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:41.276
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 23:09:41.295
Jan 30 23:09:41.330: INFO: Waiting up to 5m0s for pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87" in namespace "emptydir-900" to be "Succeeded or Failed"
Jan 30 23:09:41.349: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 19.181195ms
Jan 30 23:09:43.369: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03887076s
Jan 30 23:09:45.368: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03858303s
Jan 30 23:09:47.384: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054716513s
STEP: Saw pod success 01/30/23 23:09:47.385
Jan 30 23:09:47.385: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87" satisfied condition "Succeeded or Failed"
Jan 30 23:09:47.401: INFO: Trying to get logs from node 10.15.28.237 pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 container test-container: <nil>
STEP: delete the pod 01/30/23 23:09:47.485
Jan 30 23:09:47.557: INFO: Waiting for pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 to disappear
Jan 30 23:09:47.592: INFO: Pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:09:47.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-900" for this suite. 01/30/23 23:09:47.619
------------------------------
• [SLOW TEST] [6.448 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:41.2
    Jan 30 23:09:41.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 23:09:41.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:41.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:41.276
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/30/23 23:09:41.295
    Jan 30 23:09:41.330: INFO: Waiting up to 5m0s for pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87" in namespace "emptydir-900" to be "Succeeded or Failed"
    Jan 30 23:09:41.349: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 19.181195ms
    Jan 30 23:09:43.369: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03887076s
    Jan 30 23:09:45.368: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03858303s
    Jan 30 23:09:47.384: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054716513s
    STEP: Saw pod success 01/30/23 23:09:47.385
    Jan 30 23:09:47.385: INFO: Pod "pod-f336a90a-7dc7-4cf5-8941-2801c3420a87" satisfied condition "Succeeded or Failed"
    Jan 30 23:09:47.401: INFO: Trying to get logs from node 10.15.28.237 pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 container test-container: <nil>
    STEP: delete the pod 01/30/23 23:09:47.485
    Jan 30 23:09:47.557: INFO: Waiting for pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 to disappear
    Jan 30 23:09:47.592: INFO: Pod pod-f336a90a-7dc7-4cf5-8941-2801c3420a87 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:09:47.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-900" for this suite. 01/30/23 23:09:47.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:09:47.652
Jan 30 23:09:47.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/30/23 23:09:47.655
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:47.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:47.732
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4147 01/30/23 23:09:47.751
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 30 23:09:47.811: INFO: Found 0 stateful pods, waiting for 1
Jan 30 23:09:57.851: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/30/23 23:09:57.888
W0130 23:09:57.942601      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 30 23:09:57.980: INFO: Found 1 stateful pods, waiting for 2
Jan 30 23:10:08.002: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:10:08.002: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/30/23 23:10:08.039
STEP: Delete all of the StatefulSets 01/30/23 23:10:08.057
STEP: Verify that StatefulSets have been deleted 01/30/23 23:10:08.096
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 23:10:08.119: INFO: Deleting all statefulset in ns statefulset-4147
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:10:08.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4147" for this suite. 01/30/23 23:10:08.225
------------------------------
• [SLOW TEST] [20.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:09:47.652
    Jan 30 23:09:47.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/30/23 23:09:47.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:09:47.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:09:47.732
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4147 01/30/23 23:09:47.751
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 30 23:09:47.811: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 23:09:57.851: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/30/23 23:09:57.888
    W0130 23:09:57.942601      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 30 23:09:57.980: INFO: Found 1 stateful pods, waiting for 2
    Jan 30 23:10:08.002: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:10:08.002: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/30/23 23:10:08.039
    STEP: Delete all of the StatefulSets 01/30/23 23:10:08.057
    STEP: Verify that StatefulSets have been deleted 01/30/23 23:10:08.096
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 23:10:08.119: INFO: Deleting all statefulset in ns statefulset-4147
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:10:08.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4147" for this suite. 01/30/23 23:10:08.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:10:08.292
Jan 30 23:10:08.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:10:08.295
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:08.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:08.398
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9895-delete-me 01/30/23 23:10:08.434
STEP: Waiting for the RuntimeClass to disappear 01/30/23 23:10:08.492
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 23:10:08.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9895" for this suite. 01/30/23 23:10:08.573
------------------------------
• [0.328 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:10:08.292
    Jan 30 23:10:08.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:10:08.295
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:08.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:08.398
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9895-delete-me 01/30/23 23:10:08.434
    STEP: Waiting for the RuntimeClass to disappear 01/30/23 23:10:08.492
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:10:08.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9895" for this suite. 01/30/23 23:10:08.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:10:08.636
Jan 30 23:10:08.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 23:10:08.638
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:08.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:08.722
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/30/23 23:10:08.745
STEP: Creating a ResourceQuota 01/30/23 23:10:13.768
STEP: Ensuring resource quota status is calculated 01/30/23 23:10:13.79
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 23:10:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7456" for this suite. 01/30/23 23:10:15.87
------------------------------
• [SLOW TEST] [7.266 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:10:08.636
    Jan 30 23:10:08.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 23:10:08.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:08.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:08.722
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/30/23 23:10:08.745
    STEP: Creating a ResourceQuota 01/30/23 23:10:13.768
    STEP: Ensuring resource quota status is calculated 01/30/23 23:10:13.79
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:10:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7456" for this suite. 01/30/23 23:10:15.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:10:15.925
Jan 30 23:10:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:10:15.927
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:16.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:16.041
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/30/23 23:10:16.056
Jan 30 23:10:16.104: INFO: Waiting up to 5m0s for pod "pod-rkmft" in namespace "pods-7099" to be "running"
Jan 30 23:10:16.128: INFO: Pod "pod-rkmft": Phase="Pending", Reason="", readiness=false. Elapsed: 24.281881ms
Jan 30 23:10:18.150: INFO: Pod "pod-rkmft": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045715342s
Jan 30 23:10:20.147: INFO: Pod "pod-rkmft": Phase="Running", Reason="", readiness=true. Elapsed: 4.042793988s
Jan 30 23:10:20.147: INFO: Pod "pod-rkmft" satisfied condition "running"
STEP: patching /status 01/30/23 23:10:20.147
Jan 30 23:10:20.172: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:10:20.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7099" for this suite. 01/30/23 23:10:20.2
------------------------------
• [4.304 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:10:15.925
    Jan 30 23:10:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:10:15.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:16.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:16.041
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/30/23 23:10:16.056
    Jan 30 23:10:16.104: INFO: Waiting up to 5m0s for pod "pod-rkmft" in namespace "pods-7099" to be "running"
    Jan 30 23:10:16.128: INFO: Pod "pod-rkmft": Phase="Pending", Reason="", readiness=false. Elapsed: 24.281881ms
    Jan 30 23:10:18.150: INFO: Pod "pod-rkmft": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045715342s
    Jan 30 23:10:20.147: INFO: Pod "pod-rkmft": Phase="Running", Reason="", readiness=true. Elapsed: 4.042793988s
    Jan 30 23:10:20.147: INFO: Pod "pod-rkmft" satisfied condition "running"
    STEP: patching /status 01/30/23 23:10:20.147
    Jan 30 23:10:20.172: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:10:20.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7099" for this suite. 01/30/23 23:10:20.2
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:10:20.233
Jan 30 23:10:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:10:20.236
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:20.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:20.31
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3114 01/30/23 23:10:20.331
STEP: changing the ExternalName service to type=ClusterIP 01/30/23 23:10:20.355
STEP: creating replication controller externalname-service in namespace services-3114 01/30/23 23:10:20.448
I0130 23:10:20.475417      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3114, replica count: 2
I0130 23:10:23.526769      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:10:23.526: INFO: Creating new exec pod
Jan 30 23:10:23.559: INFO: Waiting up to 5m0s for pod "execpodhtjhd" in namespace "services-3114" to be "running"
Jan 30 23:10:23.602: INFO: Pod "execpodhtjhd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.259356ms
Jan 30 23:10:25.631: INFO: Pod "execpodhtjhd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071367731s
Jan 30 23:10:27.621: INFO: Pod "execpodhtjhd": Phase="Running", Reason="", readiness=true. Elapsed: 4.061715729s
Jan 30 23:10:27.622: INFO: Pod "execpodhtjhd" satisfied condition "running"
Jan 30 23:10:28.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3114 exec execpodhtjhd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 30 23:10:29.085: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 30 23:10:29.085: INFO: stdout: ""
Jan 30 23:10:29.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3114 exec execpodhtjhd -- /bin/sh -x -c nc -v -z -w 2 172.21.90.131 80'
Jan 30 23:10:29.497: INFO: stderr: "+ nc -v -z -w 2 172.21.90.131 80\nConnection to 172.21.90.131 80 port [tcp/http] succeeded!\n"
Jan 30 23:10:29.497: INFO: stdout: ""
Jan 30 23:10:29.498: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:10:29.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3114" for this suite. 01/30/23 23:10:29.624
------------------------------
• [SLOW TEST] [9.421 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:10:20.233
    Jan 30 23:10:20.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:10:20.236
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:20.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:20.31
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3114 01/30/23 23:10:20.331
    STEP: changing the ExternalName service to type=ClusterIP 01/30/23 23:10:20.355
    STEP: creating replication controller externalname-service in namespace services-3114 01/30/23 23:10:20.448
    I0130 23:10:20.475417      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3114, replica count: 2
    I0130 23:10:23.526769      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:10:23.526: INFO: Creating new exec pod
    Jan 30 23:10:23.559: INFO: Waiting up to 5m0s for pod "execpodhtjhd" in namespace "services-3114" to be "running"
    Jan 30 23:10:23.602: INFO: Pod "execpodhtjhd": Phase="Pending", Reason="", readiness=false. Elapsed: 42.259356ms
    Jan 30 23:10:25.631: INFO: Pod "execpodhtjhd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071367731s
    Jan 30 23:10:27.621: INFO: Pod "execpodhtjhd": Phase="Running", Reason="", readiness=true. Elapsed: 4.061715729s
    Jan 30 23:10:27.622: INFO: Pod "execpodhtjhd" satisfied condition "running"
    Jan 30 23:10:28.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3114 exec execpodhtjhd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 30 23:10:29.085: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 30 23:10:29.085: INFO: stdout: ""
    Jan 30 23:10:29.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3114 exec execpodhtjhd -- /bin/sh -x -c nc -v -z -w 2 172.21.90.131 80'
    Jan 30 23:10:29.497: INFO: stderr: "+ nc -v -z -w 2 172.21.90.131 80\nConnection to 172.21.90.131 80 port [tcp/http] succeeded!\n"
    Jan 30 23:10:29.497: INFO: stdout: ""
    Jan 30 23:10:29.498: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:10:29.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3114" for this suite. 01/30/23 23:10:29.624
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:10:29.658
Jan 30 23:10:29.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:10:29.659
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:29.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:29.727
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-60b61450-ef9b-45a1-97aa-475abbb3d589 01/30/23 23:10:29.767
STEP: Creating secret with name s-test-opt-upd-8cb0d42a-d0bd-43fa-9f0b-0db060e8f959 01/30/23 23:10:29.787
STEP: Creating the pod 01/30/23 23:10:29.806
Jan 30 23:10:29.844: INFO: Waiting up to 5m0s for pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0" in namespace "secrets-8325" to be "running and ready"
Jan 30 23:10:29.862: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.65766ms
Jan 30 23:10:29.862: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:10:31.894: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049521733s
Jan 30 23:10:31.894: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:10:33.883: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038750604s
Jan 30 23:10:33.883: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Running (Ready = true)
Jan 30 23:10:33.883: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-60b61450-ef9b-45a1-97aa-475abbb3d589 01/30/23 23:10:34.13
STEP: Updating secret s-test-opt-upd-8cb0d42a-d0bd-43fa-9f0b-0db060e8f959 01/30/23 23:10:34.158
STEP: Creating secret with name s-test-opt-create-f97daa57-f2dc-4f7a-a5be-2fd4de6fadc5 01/30/23 23:10:34.178
STEP: waiting to observe update in volume 01/30/23 23:10:34.2
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:11:50.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8325" for this suite. 01/30/23 23:11:50.111
------------------------------
• [SLOW TEST] [80.487 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:10:29.658
    Jan 30 23:10:29.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:10:29.659
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:10:29.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:10:29.727
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-60b61450-ef9b-45a1-97aa-475abbb3d589 01/30/23 23:10:29.767
    STEP: Creating secret with name s-test-opt-upd-8cb0d42a-d0bd-43fa-9f0b-0db060e8f959 01/30/23 23:10:29.787
    STEP: Creating the pod 01/30/23 23:10:29.806
    Jan 30 23:10:29.844: INFO: Waiting up to 5m0s for pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0" in namespace "secrets-8325" to be "running and ready"
    Jan 30 23:10:29.862: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.65766ms
    Jan 30 23:10:29.862: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:10:31.894: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049521733s
    Jan 30 23:10:31.894: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:10:33.883: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038750604s
    Jan 30 23:10:33.883: INFO: The phase of Pod pod-secrets-765245a8-029f-4f70-987b-783c203331c0 is Running (Ready = true)
    Jan 30 23:10:33.883: INFO: Pod "pod-secrets-765245a8-029f-4f70-987b-783c203331c0" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-60b61450-ef9b-45a1-97aa-475abbb3d589 01/30/23 23:10:34.13
    STEP: Updating secret s-test-opt-upd-8cb0d42a-d0bd-43fa-9f0b-0db060e8f959 01/30/23 23:10:34.158
    STEP: Creating secret with name s-test-opt-create-f97daa57-f2dc-4f7a-a5be-2fd4de6fadc5 01/30/23 23:10:34.178
    STEP: waiting to observe update in volume 01/30/23 23:10:34.2
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:11:50.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8325" for this suite. 01/30/23 23:11:50.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:11:50.146
Jan 30 23:11:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/30/23 23:11:50.149
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:11:50.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:11:50.233
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6067 01/30/23 23:11:50.252
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/30/23 23:11:50.273
STEP: Creating stateful set ss in namespace statefulset-6067 01/30/23 23:11:50.291
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6067 01/30/23 23:11:50.314
Jan 30 23:11:50.331: INFO: Found 0 stateful pods, waiting for 1
Jan 30 23:12:00.358: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/30/23 23:12:00.359
Jan 30 23:12:00.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 23:12:00.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 23:12:00.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 23:12:00.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 23:12:00.770: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 30 23:12:10.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 23:12:10.790: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 23:12:10.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998066s
Jan 30 23:12:11.909: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977092421s
Jan 30 23:12:12.926: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.939859253s
Jan 30 23:12:13.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.923401045s
Jan 30 23:12:14.980: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.892168389s
Jan 30 23:12:16.000: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.869001668s
Jan 30 23:12:17.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.849708602s
Jan 30 23:12:18.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.828133503s
Jan 30 23:12:19.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.807431975s
Jan 30 23:12:20.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 787.420299ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6067 01/30/23 23:12:21.082
Jan 30 23:12:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 23:12:21.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 23:12:21.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 23:12:21.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 23:12:21.539: INFO: Found 1 stateful pods, waiting for 3
Jan 30 23:12:31.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:12:31.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 30 23:12:31.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/30/23 23:12:31.56
STEP: Scale down will halt with unhealthy stateful pod 01/30/23 23:12:31.561
Jan 30 23:12:31.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 23:12:31.996: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 23:12:31.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 23:12:31.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 23:12:31.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 23:12:32.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 23:12:32.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 23:12:32.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 23:12:32.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 30 23:12:32.863: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 30 23:12:32.863: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 30 23:12:32.863: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 30 23:12:32.863: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 23:12:32.880: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 30 23:12:42.920: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 23:12:42.920: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 23:12:42.920: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 30 23:12:42.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999995929s
Jan 30 23:12:44.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980022012s
Jan 30 23:12:45.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962921118s
Jan 30 23:12:46.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942790104s
Jan 30 23:12:47.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.921272468s
Jan 30 23:12:48.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.901110026s
Jan 30 23:12:49.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875093626s
Jan 30 23:12:50.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.85483409s
Jan 30 23:12:51.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.832146949s
Jan 30 23:12:52.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 810.644629ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6067 01/30/23 23:12:53.186
Jan 30 23:12:53.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 23:12:53.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 23:12:53.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 23:12:53.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 23:12:53.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 23:12:54.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 23:12:54.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 23:12:54.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 23:12:54.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 30 23:12:54.494: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 30 23:12:54.494: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 30 23:12:54.494: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 30 23:12:54.494: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/30/23 23:13:04.58
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 30 23:13:04.581: INFO: Deleting all statefulset in ns statefulset-6067
Jan 30 23:13:04.600: INFO: Scaling statefulset ss to 0
Jan 30 23:13:04.657: INFO: Waiting for statefulset status.replicas updated to 0
Jan 30 23:13:04.679: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:13:04.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6067" for this suite. 01/30/23 23:13:04.772
------------------------------
• [SLOW TEST] [74.654 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:11:50.146
    Jan 30 23:11:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/30/23 23:11:50.149
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:11:50.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:11:50.233
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6067 01/30/23 23:11:50.252
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/30/23 23:11:50.273
    STEP: Creating stateful set ss in namespace statefulset-6067 01/30/23 23:11:50.291
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6067 01/30/23 23:11:50.314
    Jan 30 23:11:50.331: INFO: Found 0 stateful pods, waiting for 1
    Jan 30 23:12:00.358: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/30/23 23:12:00.359
    Jan 30 23:12:00.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 23:12:00.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 23:12:00.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 23:12:00.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 23:12:00.770: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 30 23:12:10.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 23:12:10.790: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 23:12:10.872: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998066s
    Jan 30 23:12:11.909: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977092421s
    Jan 30 23:12:12.926: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.939859253s
    Jan 30 23:12:13.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.923401045s
    Jan 30 23:12:14.980: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.892168389s
    Jan 30 23:12:16.000: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.869001668s
    Jan 30 23:12:17.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.849708602s
    Jan 30 23:12:18.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.828133503s
    Jan 30 23:12:19.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.807431975s
    Jan 30 23:12:20.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 787.420299ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6067 01/30/23 23:12:21.082
    Jan 30 23:12:21.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 23:12:21.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 23:12:21.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 23:12:21.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 23:12:21.539: INFO: Found 1 stateful pods, waiting for 3
    Jan 30 23:12:31.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:12:31.560: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 30 23:12:31.560: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/30/23 23:12:31.56
    STEP: Scale down will halt with unhealthy stateful pod 01/30/23 23:12:31.561
    Jan 30 23:12:31.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 23:12:31.996: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 23:12:31.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 23:12:31.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 23:12:31.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 23:12:32.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 23:12:32.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 23:12:32.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 23:12:32.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 30 23:12:32.863: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 30 23:12:32.863: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 30 23:12:32.863: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 30 23:12:32.863: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 23:12:32.880: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Jan 30 23:12:42.920: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 23:12:42.920: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 23:12:42.920: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 30 23:12:42.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999995929s
    Jan 30 23:12:44.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980022012s
    Jan 30 23:12:45.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962921118s
    Jan 30 23:12:46.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942790104s
    Jan 30 23:12:47.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.921272468s
    Jan 30 23:12:48.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.901110026s
    Jan 30 23:12:49.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875093626s
    Jan 30 23:12:50.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.85483409s
    Jan 30 23:12:51.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.832146949s
    Jan 30 23:12:52.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 810.644629ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6067 01/30/23 23:12:53.186
    Jan 30 23:12:53.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 23:12:53.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 23:12:53.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 23:12:53.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 23:12:53.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 23:12:54.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 23:12:54.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 23:12:54.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 23:12:54.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-6067 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 30 23:12:54.494: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 30 23:12:54.494: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 30 23:12:54.494: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 30 23:12:54.494: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/30/23 23:13:04.58
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 30 23:13:04.581: INFO: Deleting all statefulset in ns statefulset-6067
    Jan 30 23:13:04.600: INFO: Scaling statefulset ss to 0
    Jan 30 23:13:04.657: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 30 23:13:04.679: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:13:04.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6067" for this suite. 01/30/23 23:13:04.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:13:04.803
Jan 30 23:13:04.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename events 01/30/23 23:13:04.806
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:04.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:04.88
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/30/23 23:13:04.897
Jan 30 23:13:04.920: INFO: created test-event-1
Jan 30 23:13:04.942: INFO: created test-event-2
Jan 30 23:13:04.963: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/30/23 23:13:04.964
STEP: delete collection of events 01/30/23 23:13:04.988
Jan 30 23:13:04.989: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/30/23 23:13:05.112
Jan 30 23:13:05.112: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 30 23:13:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8320" for this suite. 01/30/23 23:13:05.161
------------------------------
• [0.386 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:13:04.803
    Jan 30 23:13:04.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename events 01/30/23 23:13:04.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:04.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:04.88
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/30/23 23:13:04.897
    Jan 30 23:13:04.920: INFO: created test-event-1
    Jan 30 23:13:04.942: INFO: created test-event-2
    Jan 30 23:13:04.963: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/30/23 23:13:04.964
    STEP: delete collection of events 01/30/23 23:13:04.988
    Jan 30 23:13:04.989: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/30/23 23:13:05.112
    Jan 30 23:13:05.112: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:13:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8320" for this suite. 01/30/23 23:13:05.161
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:13:05.197
Jan 30 23:13:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/30/23 23:13:05.199
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:05.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:05.327
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/30/23 23:13:05.349
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:05.371
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:05.371
STEP: creating a pod to probe DNS 01/30/23 23:13:05.372
STEP: submitting the pod to kubernetes 01/30/23 23:13:05.373
Jan 30 23:13:05.413: INFO: Waiting up to 15m0s for pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4" in namespace "dns-4135" to be "running"
Jan 30 23:13:05.443: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006736ms
Jan 30 23:13:07.461: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048501843s
Jan 30 23:13:09.463: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050691949s
Jan 30 23:13:11.460: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046779632s
Jan 30 23:13:13.463: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050397624s
Jan 30 23:13:15.464: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Running", Reason="", readiness=true. Elapsed: 10.051201536s
Jan 30 23:13:15.464: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:13:15.464
STEP: looking for the results for each expected name from probers 01/30/23 23:13:15.485
Jan 30 23:13:15.580: INFO: DNS probes using dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4 succeeded

STEP: deleting the pod 01/30/23 23:13:15.58
STEP: changing the externalName to bar.example.com 01/30/23 23:13:15.649
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:15.683
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:15.683
STEP: creating a second pod to probe DNS 01/30/23 23:13:15.684
STEP: submitting the pod to kubernetes 01/30/23 23:13:15.684
Jan 30 23:13:15.705: INFO: Waiting up to 15m0s for pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a" in namespace "dns-4135" to be "running"
Jan 30 23:13:15.722: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.786134ms
Jan 30 23:13:17.739: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033585518s
Jan 30 23:13:19.741: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035333533s
Jan 30 23:13:21.739: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03429619s
Jan 30 23:13:23.745: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040277144s
Jan 30 23:13:25.745: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Running", Reason="", readiness=true. Elapsed: 10.040261425s
Jan 30 23:13:25.746: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:13:25.746
STEP: looking for the results for each expected name from probers 01/30/23 23:13:25.765
Jan 30 23:13:25.857: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:25.884: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:25.884: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

Jan 30 23:13:30.917: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:30.941: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:30.941: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

Jan 30 23:13:35.911: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:35.936: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 30 23:13:35.936: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

Jan 30 23:13:40.940: INFO: DNS probes using dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a succeeded

STEP: deleting the pod 01/30/23 23:13:40.94
STEP: changing the service to type=ClusterIP 01/30/23 23:13:41.003
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:41.083
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
 01/30/23 23:13:41.084
STEP: creating a third pod to probe DNS 01/30/23 23:13:41.086
STEP: submitting the pod to kubernetes 01/30/23 23:13:41.105
Jan 30 23:13:41.148: INFO: Waiting up to 15m0s for pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0" in namespace "dns-4135" to be "running"
Jan 30 23:13:41.178: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.728485ms
Jan 30 23:13:43.199: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05116718s
Jan 30 23:13:45.195: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Running", Reason="", readiness=true. Elapsed: 4.047363686s
Jan 30 23:13:45.195: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:13:45.195
STEP: looking for the results for each expected name from probers 01/30/23 23:13:45.212
Jan 30 23:13:45.278: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 contains '' instead of '172.21.224.73'
Jan 30 23:13:45.304: INFO: Lookups using dns-4135/dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local]

Jan 30 23:13:50.362: INFO: DNS probes using dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 succeeded

STEP: deleting the pod 01/30/23 23:13:50.362
STEP: deleting the test externalName service 01/30/23 23:13:50.411
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 23:13:50.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4135" for this suite. 01/30/23 23:13:50.588
------------------------------
• [SLOW TEST] [45.427 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:13:05.197
    Jan 30 23:13:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/30/23 23:13:05.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:05.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:05.327
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/30/23 23:13:05.349
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:05.371
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:05.371
    STEP: creating a pod to probe DNS 01/30/23 23:13:05.372
    STEP: submitting the pod to kubernetes 01/30/23 23:13:05.373
    Jan 30 23:13:05.413: INFO: Waiting up to 15m0s for pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4" in namespace "dns-4135" to be "running"
    Jan 30 23:13:05.443: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006736ms
    Jan 30 23:13:07.461: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048501843s
    Jan 30 23:13:09.463: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050691949s
    Jan 30 23:13:11.460: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046779632s
    Jan 30 23:13:13.463: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050397624s
    Jan 30 23:13:15.464: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4": Phase="Running", Reason="", readiness=true. Elapsed: 10.051201536s
    Jan 30 23:13:15.464: INFO: Pod "dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:13:15.464
    STEP: looking for the results for each expected name from probers 01/30/23 23:13:15.485
    Jan 30 23:13:15.580: INFO: DNS probes using dns-test-9e7b8f03-7380-4185-9901-913e0e0aa5e4 succeeded

    STEP: deleting the pod 01/30/23 23:13:15.58
    STEP: changing the externalName to bar.example.com 01/30/23 23:13:15.649
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:15.683
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:15.683
    STEP: creating a second pod to probe DNS 01/30/23 23:13:15.684
    STEP: submitting the pod to kubernetes 01/30/23 23:13:15.684
    Jan 30 23:13:15.705: INFO: Waiting up to 15m0s for pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a" in namespace "dns-4135" to be "running"
    Jan 30 23:13:15.722: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.786134ms
    Jan 30 23:13:17.739: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033585518s
    Jan 30 23:13:19.741: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035333533s
    Jan 30 23:13:21.739: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03429619s
    Jan 30 23:13:23.745: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040277144s
    Jan 30 23:13:25.745: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a": Phase="Running", Reason="", readiness=true. Elapsed: 10.040261425s
    Jan 30 23:13:25.746: INFO: Pod "dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:13:25.746
    STEP: looking for the results for each expected name from probers 01/30/23 23:13:25.765
    Jan 30 23:13:25.857: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:25.884: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:25.884: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

    Jan 30 23:13:30.917: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:30.941: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:30.941: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

    Jan 30 23:13:35.911: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:35.936: INFO: File jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 30 23:13:35.936: INFO: Lookups using dns-4135/dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local]

    Jan 30 23:13:40.940: INFO: DNS probes using dns-test-4c04c6b1-6880-4bb6-a73e-87bbfaf0e33a succeeded

    STEP: deleting the pod 01/30/23 23:13:40.94
    STEP: changing the service to type=ClusterIP 01/30/23 23:13:41.003
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:41.083
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4135.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4135.svc.cluster.local; sleep 1; done
     01/30/23 23:13:41.084
    STEP: creating a third pod to probe DNS 01/30/23 23:13:41.086
    STEP: submitting the pod to kubernetes 01/30/23 23:13:41.105
    Jan 30 23:13:41.148: INFO: Waiting up to 15m0s for pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0" in namespace "dns-4135" to be "running"
    Jan 30 23:13:41.178: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 29.728485ms
    Jan 30 23:13:43.199: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05116718s
    Jan 30 23:13:45.195: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0": Phase="Running", Reason="", readiness=true. Elapsed: 4.047363686s
    Jan 30 23:13:45.195: INFO: Pod "dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:13:45.195
    STEP: looking for the results for each expected name from probers 01/30/23 23:13:45.212
    Jan 30 23:13:45.278: INFO: File wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local from pod  dns-4135/dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 contains '' instead of '172.21.224.73'
    Jan 30 23:13:45.304: INFO: Lookups using dns-4135/dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 failed for: [wheezy_udp@dns-test-service-3.dns-4135.svc.cluster.local]

    Jan 30 23:13:50.362: INFO: DNS probes using dns-test-ff339797-8b4b-42cb-b3d9-29b1e5166cc0 succeeded

    STEP: deleting the pod 01/30/23 23:13:50.362
    STEP: deleting the test externalName service 01/30/23 23:13:50.411
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:13:50.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4135" for this suite. 01/30/23 23:13:50.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:13:50.626
Jan 30 23:13:50.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:13:50.627
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:50.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:50.763
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:13:50.781
Jan 30 23:13:50.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6" in namespace "downward-api-1391" to be "Succeeded or Failed"
Jan 30 23:13:50.840: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700575ms
Jan 30 23:13:52.859: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037589237s
Jan 30 23:13:54.873: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051498526s
Jan 30 23:13:56.871: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049899165s
STEP: Saw pod success 01/30/23 23:13:56.871
Jan 30 23:13:56.872: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6" satisfied condition "Succeeded or Failed"
Jan 30 23:13:56.892: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 container client-container: <nil>
STEP: delete the pod 01/30/23 23:13:57.003
Jan 30 23:13:57.049: INFO: Waiting for pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 to disappear
Jan 30 23:13:57.068: INFO: Pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:13:57.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1391" for this suite. 01/30/23 23:13:57.132
------------------------------
• [SLOW TEST] [6.535 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:13:50.626
    Jan 30 23:13:50.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:13:50.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:50.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:50.763
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:13:50.781
    Jan 30 23:13:50.821: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6" in namespace "downward-api-1391" to be "Succeeded or Failed"
    Jan 30 23:13:50.840: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.700575ms
    Jan 30 23:13:52.859: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037589237s
    Jan 30 23:13:54.873: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051498526s
    Jan 30 23:13:56.871: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049899165s
    STEP: Saw pod success 01/30/23 23:13:56.871
    Jan 30 23:13:56.872: INFO: Pod "downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6" satisfied condition "Succeeded or Failed"
    Jan 30 23:13:56.892: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:13:57.003
    Jan 30 23:13:57.049: INFO: Waiting for pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 to disappear
    Jan 30 23:13:57.068: INFO: Pod downwardapi-volume-6d1efa43-b5b1-4344-a7be-7dfaf97a2ea6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:13:57.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1391" for this suite. 01/30/23 23:13:57.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:13:57.166
Jan 30 23:13:57.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:13:57.168
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:57.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:57.253
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 30 23:13:57.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 23:13:59.821
Jan 30 23:13:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 create -f -'
Jan 30 23:14:00.824: INFO: stderr: ""
Jan 30 23:14:00.824: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 30 23:14:00.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 delete e2e-test-crd-publish-openapi-1369-crds test-cr'
Jan 30 23:14:01.015: INFO: stderr: ""
Jan 30 23:14:01.015: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 30 23:14:01.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 apply -f -'
Jan 30 23:14:01.917: INFO: stderr: ""
Jan 30 23:14:01.917: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 30 23:14:01.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 delete e2e-test-crd-publish-openapi-1369-crds test-cr'
Jan 30 23:14:02.092: INFO: stderr: ""
Jan 30 23:14:02.092: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/30/23 23:14:02.092
Jan 30 23:14:02.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 explain e2e-test-crd-publish-openapi-1369-crds'
Jan 30 23:14:02.889: INFO: stderr: ""
Jan 30 23:14:02.889: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1369-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:14:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1692" for this suite. 01/30/23 23:14:05.565
------------------------------
• [SLOW TEST] [8.438 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:13:57.166
    Jan 30 23:13:57.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:13:57.168
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:13:57.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:13:57.253
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 30 23:13:57.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/30/23 23:13:59.821
    Jan 30 23:13:59.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 create -f -'
    Jan 30 23:14:00.824: INFO: stderr: ""
    Jan 30 23:14:00.824: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 30 23:14:00.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 delete e2e-test-crd-publish-openapi-1369-crds test-cr'
    Jan 30 23:14:01.015: INFO: stderr: ""
    Jan 30 23:14:01.015: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 30 23:14:01.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 apply -f -'
    Jan 30 23:14:01.917: INFO: stderr: ""
    Jan 30 23:14:01.917: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 30 23:14:01.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 --namespace=crd-publish-openapi-1692 delete e2e-test-crd-publish-openapi-1369-crds test-cr'
    Jan 30 23:14:02.092: INFO: stderr: ""
    Jan 30 23:14:02.092: INFO: stdout: "e2e-test-crd-publish-openapi-1369-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/30/23 23:14:02.092
    Jan 30 23:14:02.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1692 explain e2e-test-crd-publish-openapi-1369-crds'
    Jan 30 23:14:02.889: INFO: stderr: ""
    Jan 30 23:14:02.889: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1369-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:14:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1692" for this suite. 01/30/23 23:14:05.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:14:05.607
Jan 30 23:14:05.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename limitrange 01/30/23 23:14:05.609
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:05.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:05.706
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/30/23 23:14:05.72
STEP: Setting up watch 01/30/23 23:14:05.721
STEP: Submitting a LimitRange 01/30/23 23:14:05.841
STEP: Verifying LimitRange creation was observed 01/30/23 23:14:05.863
STEP: Fetching the LimitRange to ensure it has proper values 01/30/23 23:14:05.863
Jan 30 23:14:05.884: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 30 23:14:05.884: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/30/23 23:14:05.884
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/30/23 23:14:05.908
Jan 30 23:14:05.926: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 30 23:14:05.926: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/30/23 23:14:05.926
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/30/23 23:14:05.961
Jan 30 23:14:05.988: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 30 23:14:05.988: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/30/23 23:14:05.988
STEP: Failing to create a Pod with more than max resources 01/30/23 23:14:05.998
STEP: Updating a LimitRange 01/30/23 23:14:06.008
STEP: Verifying LimitRange updating is effective 01/30/23 23:14:06.032
STEP: Creating a Pod with less than former min resources 01/30/23 23:14:08.064
STEP: Failing to create a Pod with more than max resources 01/30/23 23:14:08.094
STEP: Deleting a LimitRange 01/30/23 23:14:08.104
STEP: Verifying the LimitRange was deleted 01/30/23 23:14:08.16
Jan 30 23:14:13.188: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/30/23 23:14:13.188
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 30 23:14:13.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-843" for this suite. 01/30/23 23:14:13.296
------------------------------
• [SLOW TEST] [7.716 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:14:05.607
    Jan 30 23:14:05.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename limitrange 01/30/23 23:14:05.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:05.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:05.706
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/30/23 23:14:05.72
    STEP: Setting up watch 01/30/23 23:14:05.721
    STEP: Submitting a LimitRange 01/30/23 23:14:05.841
    STEP: Verifying LimitRange creation was observed 01/30/23 23:14:05.863
    STEP: Fetching the LimitRange to ensure it has proper values 01/30/23 23:14:05.863
    Jan 30 23:14:05.884: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 30 23:14:05.884: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/30/23 23:14:05.884
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/30/23 23:14:05.908
    Jan 30 23:14:05.926: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 30 23:14:05.926: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/30/23 23:14:05.926
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/30/23 23:14:05.961
    Jan 30 23:14:05.988: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 30 23:14:05.988: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/30/23 23:14:05.988
    STEP: Failing to create a Pod with more than max resources 01/30/23 23:14:05.998
    STEP: Updating a LimitRange 01/30/23 23:14:06.008
    STEP: Verifying LimitRange updating is effective 01/30/23 23:14:06.032
    STEP: Creating a Pod with less than former min resources 01/30/23 23:14:08.064
    STEP: Failing to create a Pod with more than max resources 01/30/23 23:14:08.094
    STEP: Deleting a LimitRange 01/30/23 23:14:08.104
    STEP: Verifying the LimitRange was deleted 01/30/23 23:14:08.16
    Jan 30 23:14:13.188: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/30/23 23:14:13.188
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:14:13.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-843" for this suite. 01/30/23 23:14:13.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:14:13.329
Jan 30 23:14:13.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:14:13.331
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:13.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:13.397
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:14:13.41
Jan 30 23:14:13.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727" in namespace "projected-2770" to be "Succeeded or Failed"
Jan 30 23:14:13.462: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 14.929083ms
Jan 30 23:14:15.498: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05125844s
Jan 30 23:14:17.486: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039075302s
Jan 30 23:14:19.484: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036934196s
STEP: Saw pod success 01/30/23 23:14:19.484
Jan 30 23:14:19.485: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727" satisfied condition "Succeeded or Failed"
Jan 30 23:14:19.504: INFO: Trying to get logs from node 10.15.28.237 pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 container client-container: <nil>
STEP: delete the pod 01/30/23 23:14:19.624
Jan 30 23:14:19.672: INFO: Waiting for pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 to disappear
Jan 30 23:14:19.717: INFO: Pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:14:19.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2770" for this suite. 01/30/23 23:14:19.741
------------------------------
• [SLOW TEST] [6.453 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:14:13.329
    Jan 30 23:14:13.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:14:13.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:13.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:13.397
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:14:13.41
    Jan 30 23:14:13.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727" in namespace "projected-2770" to be "Succeeded or Failed"
    Jan 30 23:14:13.462: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 14.929083ms
    Jan 30 23:14:15.498: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05125844s
    Jan 30 23:14:17.486: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039075302s
    Jan 30 23:14:19.484: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036934196s
    STEP: Saw pod success 01/30/23 23:14:19.484
    Jan 30 23:14:19.485: INFO: Pod "downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727" satisfied condition "Succeeded or Failed"
    Jan 30 23:14:19.504: INFO: Trying to get logs from node 10.15.28.237 pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:14:19.624
    Jan 30 23:14:19.672: INFO: Waiting for pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 to disappear
    Jan 30 23:14:19.717: INFO: Pod downwardapi-volume-e040f6c7-148f-419c-973e-b60215318727 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:14:19.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2770" for this suite. 01/30/23 23:14:19.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:14:19.787
Jan 30 23:14:19.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename taint-multiple-pods 01/30/23 23:14:19.789
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:19.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:19.876
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 30 23:14:19.886: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:15:20.004: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 30 23:15:20.021: INFO: Starting informer...
STEP: Starting pods... 01/30/23 23:15:20.022
Jan 30 23:15:20.316: INFO: Pod1 is running on 10.15.28.227. Tainting Node
Jan 30 23:15:20.557: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9614" to be "running"
Jan 30 23:15:20.578: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.239278ms
Jan 30 23:15:22.598: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.041233438s
Jan 30 23:15:22.598: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 30 23:15:22.599: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9614" to be "running"
Jan 30 23:15:22.615: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.395616ms
Jan 30 23:15:24.631: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.032138071s
Jan 30 23:15:24.631: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 30 23:15:24.631: INFO: Pod2 is running on 10.15.28.227. Tainting Node
STEP: Trying to apply a taint on the Node 01/30/23 23:15:24.631
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:15:24.684
STEP: Waiting for Pod1 and Pod2 to be deleted 01/30/23 23:15:24.708
Jan 30 23:15:31.148: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 30 23:15:51.241: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:15:51.294
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:15:51.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9614" for this suite. 01/30/23 23:15:51.341
------------------------------
• [SLOW TEST] [91.581 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:14:19.787
    Jan 30 23:14:19.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename taint-multiple-pods 01/30/23 23:14:19.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:14:19.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:14:19.876
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 30 23:14:19.886: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:15:20.004: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 30 23:15:20.021: INFO: Starting informer...
    STEP: Starting pods... 01/30/23 23:15:20.022
    Jan 30 23:15:20.316: INFO: Pod1 is running on 10.15.28.227. Tainting Node
    Jan 30 23:15:20.557: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9614" to be "running"
    Jan 30 23:15:20.578: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.239278ms
    Jan 30 23:15:22.598: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.041233438s
    Jan 30 23:15:22.598: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 30 23:15:22.599: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9614" to be "running"
    Jan 30 23:15:22.615: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.395616ms
    Jan 30 23:15:24.631: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.032138071s
    Jan 30 23:15:24.631: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 30 23:15:24.631: INFO: Pod2 is running on 10.15.28.227. Tainting Node
    STEP: Trying to apply a taint on the Node 01/30/23 23:15:24.631
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:15:24.684
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/30/23 23:15:24.708
    Jan 30 23:15:31.148: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 30 23:15:51.241: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:15:51.294
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:15:51.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9614" for this suite. 01/30/23 23:15:51.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:15:51.384
Jan 30 23:15:51.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename prestop 01/30/23 23:15:51.385
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:15:51.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:15:51.463
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5154 01/30/23 23:15:51.48
STEP: Waiting for pods to come up. 01/30/23 23:15:51.521
Jan 30 23:15:51.522: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5154" to be "running"
Jan 30 23:15:51.536: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 14.309488ms
Jan 30 23:15:53.556: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033749259s
Jan 30 23:15:55.557: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.035150733s
Jan 30 23:15:55.557: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5154 01/30/23 23:15:55.597
Jan 30 23:15:55.628: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5154" to be "running"
Jan 30 23:15:55.655: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 26.820661ms
Jan 30 23:15:57.673: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044981206s
Jan 30 23:15:59.673: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.04494582s
Jan 30 23:15:59.673: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/30/23 23:15:59.674
Jan 30 23:16:04.801: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/30/23 23:16:04.801
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:04.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5154" for this suite. 01/30/23 23:16:04.878
------------------------------
• [SLOW TEST] [13.525 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:15:51.384
    Jan 30 23:15:51.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename prestop 01/30/23 23:15:51.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:15:51.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:15:51.463
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5154 01/30/23 23:15:51.48
    STEP: Waiting for pods to come up. 01/30/23 23:15:51.521
    Jan 30 23:15:51.522: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5154" to be "running"
    Jan 30 23:15:51.536: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 14.309488ms
    Jan 30 23:15:53.556: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033749259s
    Jan 30 23:15:55.557: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.035150733s
    Jan 30 23:15:55.557: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5154 01/30/23 23:15:55.597
    Jan 30 23:15:55.628: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5154" to be "running"
    Jan 30 23:15:55.655: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 26.820661ms
    Jan 30 23:15:57.673: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044981206s
    Jan 30 23:15:59.673: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.04494582s
    Jan 30 23:15:59.673: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/30/23 23:15:59.674
    Jan 30 23:16:04.801: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/30/23 23:16:04.801
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:04.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5154" for this suite. 01/30/23 23:16:04.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:04.919
Jan 30 23:16:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/30/23 23:16:04.922
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:04.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:04.993
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/30/23 23:16:05.014
Jan 30 23:16:05.030: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/30/23 23:16:05.03
Jan 30 23:16:05.055: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/30/23 23:16:05.056
Jan 30 23:16:05.095: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:05.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6715" for this suite. 01/30/23 23:16:05.128
------------------------------
• [0.240 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:04.919
    Jan 30 23:16:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/30/23 23:16:04.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:04.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:04.993
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/30/23 23:16:05.014
    Jan 30 23:16:05.030: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/30/23 23:16:05.03
    Jan 30 23:16:05.055: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/30/23 23:16:05.056
    Jan 30 23:16:05.095: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:05.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6715" for this suite. 01/30/23 23:16:05.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:05.168
Jan 30 23:16:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 23:16:05.17
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:05.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:05.26
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/30/23 23:16:05.296
STEP: Verify that the required pods have come up. 01/30/23 23:16:05.321
Jan 30 23:16:05.338: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 23:16:10.355: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 23:16:10.355
STEP: Getting /status 01/30/23 23:16:10.355
Jan 30 23:16:10.378: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/30/23 23:16:10.378
Jan 30 23:16:10.423: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/30/23 23:16:10.424
Jan 30 23:16:10.432: INFO: Observed &ReplicaSet event: ADDED
Jan 30 23:16:10.433: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.433: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.434: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.434: INFO: Found replicaset test-rs in namespace replicaset-9591 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 30 23:16:10.434: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/30/23 23:16:10.434
Jan 30 23:16:10.434: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 30 23:16:10.468: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/30/23 23:16:10.468
Jan 30 23:16:10.476: INFO: Observed &ReplicaSet event: ADDED
Jan 30 23:16:10.477: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.478: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.478: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.478: INFO: Observed replicaset test-rs in namespace replicaset-9591 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 30 23:16:10.479: INFO: Observed &ReplicaSet event: MODIFIED
Jan 30 23:16:10.479: INFO: Found replicaset test-rs in namespace replicaset-9591 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 30 23:16:10.480: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:10.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9591" for this suite. 01/30/23 23:16:10.505
------------------------------
• [SLOW TEST] [5.363 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:05.168
    Jan 30 23:16:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 23:16:05.17
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:05.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:05.26
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/30/23 23:16:05.296
    STEP: Verify that the required pods have come up. 01/30/23 23:16:05.321
    Jan 30 23:16:05.338: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 23:16:10.355: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 23:16:10.355
    STEP: Getting /status 01/30/23 23:16:10.355
    Jan 30 23:16:10.378: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/30/23 23:16:10.378
    Jan 30 23:16:10.423: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/30/23 23:16:10.424
    Jan 30 23:16:10.432: INFO: Observed &ReplicaSet event: ADDED
    Jan 30 23:16:10.433: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.433: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.434: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.434: INFO: Found replicaset test-rs in namespace replicaset-9591 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 30 23:16:10.434: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/30/23 23:16:10.434
    Jan 30 23:16:10.434: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 30 23:16:10.468: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/30/23 23:16:10.468
    Jan 30 23:16:10.476: INFO: Observed &ReplicaSet event: ADDED
    Jan 30 23:16:10.477: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.478: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.478: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.478: INFO: Observed replicaset test-rs in namespace replicaset-9591 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 30 23:16:10.479: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 30 23:16:10.479: INFO: Found replicaset test-rs in namespace replicaset-9591 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 30 23:16:10.480: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:10.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9591" for this suite. 01/30/23 23:16:10.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:10.547
Jan 30 23:16:10.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:16:10.549
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:10.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:10.636
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-36b11748-ac97-42b7-bf8c-0647091a3c76 01/30/23 23:16:10.651
STEP: Creating a pod to test consume configMaps 01/30/23 23:16:10.672
Jan 30 23:16:10.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a" in namespace "configmap-8668" to be "Succeeded or Failed"
Jan 30 23:16:10.741: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.632482ms
Jan 30 23:16:12.758: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047114115s
Jan 30 23:16:14.782: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070765897s
Jan 30 23:16:16.759: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048497284s
STEP: Saw pod success 01/30/23 23:16:16.76
Jan 30 23:16:16.760: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a" satisfied condition "Succeeded or Failed"
Jan 30 23:16:16.778: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:16:16.885
Jan 30 23:16:16.935: INFO: Waiting for pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a to disappear
Jan 30 23:16:16.953: INFO: Pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8668" for this suite. 01/30/23 23:16:16.992
------------------------------
• [SLOW TEST] [6.482 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:10.547
    Jan 30 23:16:10.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:16:10.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:10.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:10.636
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-36b11748-ac97-42b7-bf8c-0647091a3c76 01/30/23 23:16:10.651
    STEP: Creating a pod to test consume configMaps 01/30/23 23:16:10.672
    Jan 30 23:16:10.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a" in namespace "configmap-8668" to be "Succeeded or Failed"
    Jan 30 23:16:10.741: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.632482ms
    Jan 30 23:16:12.758: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047114115s
    Jan 30 23:16:14.782: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070765897s
    Jan 30 23:16:16.759: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048497284s
    STEP: Saw pod success 01/30/23 23:16:16.76
    Jan 30 23:16:16.760: INFO: Pod "pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a" satisfied condition "Succeeded or Failed"
    Jan 30 23:16:16.778: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:16:16.885
    Jan 30 23:16:16.935: INFO: Waiting for pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a to disappear
    Jan 30 23:16:16.953: INFO: Pod pod-configmaps-c46c82a8-d612-4548-8198-2e2721be6f4a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8668" for this suite. 01/30/23 23:16:16.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:17.034
Jan 30 23:16:17.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:16:17.036
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:17.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:17.107
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/30/23 23:16:17.125
Jan 30 23:16:17.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 create -f -'
Jan 30 23:16:18.018: INFO: stderr: ""
Jan 30 23:16:18.018: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:16:18.018
Jan 30 23:16:18.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:16:18.152: INFO: stderr: ""
Jan 30 23:16:18.153: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
Jan 30 23:16:18.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:16:18.291: INFO: stderr: ""
Jan 30 23:16:18.291: INFO: stdout: ""
Jan 30 23:16:18.291: INFO: update-demo-nautilus-2qnkb is created but not running
Jan 30 23:16:23.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:16:23.425: INFO: stderr: ""
Jan 30 23:16:23.425: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
Jan 30 23:16:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:16:23.577: INFO: stderr: ""
Jan 30 23:16:23.577: INFO: stdout: ""
Jan 30 23:16:23.577: INFO: update-demo-nautilus-2qnkb is created but not running
Jan 30 23:16:28.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:16:28.743: INFO: stderr: ""
Jan 30 23:16:28.743: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
Jan 30 23:16:28.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:16:28.874: INFO: stderr: ""
Jan 30 23:16:28.874: INFO: stdout: "true"
Jan 30 23:16:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:16:29.027: INFO: stderr: ""
Jan 30 23:16:29.027: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:16:29.027: INFO: validating pod update-demo-nautilus-2qnkb
Jan 30 23:16:29.074: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:16:29.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:16:29.074: INFO: update-demo-nautilus-2qnkb is verified up and running
Jan 30 23:16:29.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-lczbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:16:29.212: INFO: stderr: ""
Jan 30 23:16:29.212: INFO: stdout: "true"
Jan 30 23:16:29.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-lczbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:16:29.356: INFO: stderr: ""
Jan 30 23:16:29.356: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:16:29.356: INFO: validating pod update-demo-nautilus-lczbf
Jan 30 23:16:29.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:16:29.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:16:29.405: INFO: update-demo-nautilus-lczbf is verified up and running
STEP: using delete to clean up resources 01/30/23 23:16:29.405
Jan 30 23:16:29.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 delete --grace-period=0 --force -f -'
Jan 30 23:16:29.610: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 23:16:29.610: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 23:16:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get rc,svc -l name=update-demo --no-headers'
Jan 30 23:16:29.802: INFO: stderr: "No resources found in kubectl-3668 namespace.\n"
Jan 30 23:16:29.802: INFO: stdout: ""
Jan 30 23:16:29.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 23:16:30.040: INFO: stderr: ""
Jan 30 23:16:30.040: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:30.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3668" for this suite. 01/30/23 23:16:30.064
------------------------------
• [SLOW TEST] [13.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:17.034
    Jan 30 23:16:17.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:16:17.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:17.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:17.107
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/30/23 23:16:17.125
    Jan 30 23:16:17.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 create -f -'
    Jan 30 23:16:18.018: INFO: stderr: ""
    Jan 30 23:16:18.018: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:16:18.018
    Jan 30 23:16:18.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:16:18.152: INFO: stderr: ""
    Jan 30 23:16:18.153: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
    Jan 30 23:16:18.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:16:18.291: INFO: stderr: ""
    Jan 30 23:16:18.291: INFO: stdout: ""
    Jan 30 23:16:18.291: INFO: update-demo-nautilus-2qnkb is created but not running
    Jan 30 23:16:23.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:16:23.425: INFO: stderr: ""
    Jan 30 23:16:23.425: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
    Jan 30 23:16:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:16:23.577: INFO: stderr: ""
    Jan 30 23:16:23.577: INFO: stdout: ""
    Jan 30 23:16:23.577: INFO: update-demo-nautilus-2qnkb is created but not running
    Jan 30 23:16:28.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:16:28.743: INFO: stderr: ""
    Jan 30 23:16:28.743: INFO: stdout: "update-demo-nautilus-2qnkb update-demo-nautilus-lczbf "
    Jan 30 23:16:28.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:16:28.874: INFO: stderr: ""
    Jan 30 23:16:28.874: INFO: stdout: "true"
    Jan 30 23:16:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-2qnkb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:16:29.027: INFO: stderr: ""
    Jan 30 23:16:29.027: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:16:29.027: INFO: validating pod update-demo-nautilus-2qnkb
    Jan 30 23:16:29.074: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:16:29.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:16:29.074: INFO: update-demo-nautilus-2qnkb is verified up and running
    Jan 30 23:16:29.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-lczbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:16:29.212: INFO: stderr: ""
    Jan 30 23:16:29.212: INFO: stdout: "true"
    Jan 30 23:16:29.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods update-demo-nautilus-lczbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:16:29.356: INFO: stderr: ""
    Jan 30 23:16:29.356: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:16:29.356: INFO: validating pod update-demo-nautilus-lczbf
    Jan 30 23:16:29.405: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:16:29.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:16:29.405: INFO: update-demo-nautilus-lczbf is verified up and running
    STEP: using delete to clean up resources 01/30/23 23:16:29.405
    Jan 30 23:16:29.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 delete --grace-period=0 --force -f -'
    Jan 30 23:16:29.610: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 23:16:29.610: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 30 23:16:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get rc,svc -l name=update-demo --no-headers'
    Jan 30 23:16:29.802: INFO: stderr: "No resources found in kubectl-3668 namespace.\n"
    Jan 30 23:16:29.802: INFO: stdout: ""
    Jan 30 23:16:29.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-3668 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 23:16:30.040: INFO: stderr: ""
    Jan 30 23:16:30.040: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:30.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3668" for this suite. 01/30/23 23:16:30.064
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:30.11
Jan 30 23:16:30.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename subpath 01/30/23 23:16:30.111
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:30.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:30.179
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 23:16:30.197
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-xv4z 01/30/23 23:16:30.241
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:16:30.241
Jan 30 23:16:30.291: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xv4z" in namespace "subpath-3584" to be "Succeeded or Failed"
Jan 30 23:16:30.312: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Pending", Reason="", readiness=false. Elapsed: 20.928709ms
Jan 30 23:16:32.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039414385s
Jan 30 23:16:34.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 4.039659443s
Jan 30 23:16:36.352: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 6.061307249s
Jan 30 23:16:38.333: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 8.041757453s
Jan 30 23:16:40.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.040078269s
Jan 30 23:16:42.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 12.039433771s
Jan 30 23:16:44.359: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 14.067494693s
Jan 30 23:16:46.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 16.040023843s
Jan 30 23:16:48.334: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 18.042476987s
Jan 30 23:16:50.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 20.039965289s
Jan 30 23:16:52.334: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 22.042739009s
Jan 30 23:16:54.341: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=false. Elapsed: 24.049871999s
Jan 30 23:16:56.330: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039159964s
STEP: Saw pod success 01/30/23 23:16:56.33
Jan 30 23:16:56.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z" satisfied condition "Succeeded or Failed"
Jan 30 23:16:56.349: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-downwardapi-xv4z container test-container-subpath-downwardapi-xv4z: <nil>
STEP: delete the pod 01/30/23 23:16:56.428
Jan 30 23:16:56.493: INFO: Waiting for pod pod-subpath-test-downwardapi-xv4z to disappear
Jan 30 23:16:56.511: INFO: Pod pod-subpath-test-downwardapi-xv4z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xv4z 01/30/23 23:16:56.511
Jan 30 23:16:56.511: INFO: Deleting pod "pod-subpath-test-downwardapi-xv4z" in namespace "subpath-3584"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 23:16:56.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3584" for this suite. 01/30/23 23:16:56.557
------------------------------
• [SLOW TEST] [26.477 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:30.11
    Jan 30 23:16:30.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename subpath 01/30/23 23:16:30.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:30.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:30.179
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 23:16:30.197
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-xv4z 01/30/23 23:16:30.241
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:16:30.241
    Jan 30 23:16:30.291: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xv4z" in namespace "subpath-3584" to be "Succeeded or Failed"
    Jan 30 23:16:30.312: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Pending", Reason="", readiness=false. Elapsed: 20.928709ms
    Jan 30 23:16:32.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039414385s
    Jan 30 23:16:34.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 4.039659443s
    Jan 30 23:16:36.352: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 6.061307249s
    Jan 30 23:16:38.333: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 8.041757453s
    Jan 30 23:16:40.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 10.040078269s
    Jan 30 23:16:42.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 12.039433771s
    Jan 30 23:16:44.359: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 14.067494693s
    Jan 30 23:16:46.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 16.040023843s
    Jan 30 23:16:48.334: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 18.042476987s
    Jan 30 23:16:50.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 20.039965289s
    Jan 30 23:16:52.334: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=true. Elapsed: 22.042739009s
    Jan 30 23:16:54.341: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Running", Reason="", readiness=false. Elapsed: 24.049871999s
    Jan 30 23:16:56.330: INFO: Pod "pod-subpath-test-downwardapi-xv4z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039159964s
    STEP: Saw pod success 01/30/23 23:16:56.33
    Jan 30 23:16:56.331: INFO: Pod "pod-subpath-test-downwardapi-xv4z" satisfied condition "Succeeded or Failed"
    Jan 30 23:16:56.349: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-downwardapi-xv4z container test-container-subpath-downwardapi-xv4z: <nil>
    STEP: delete the pod 01/30/23 23:16:56.428
    Jan 30 23:16:56.493: INFO: Waiting for pod pod-subpath-test-downwardapi-xv4z to disappear
    Jan 30 23:16:56.511: INFO: Pod pod-subpath-test-downwardapi-xv4z no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-xv4z 01/30/23 23:16:56.511
    Jan 30 23:16:56.511: INFO: Deleting pod "pod-subpath-test-downwardapi-xv4z" in namespace "subpath-3584"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:16:56.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3584" for this suite. 01/30/23 23:16:56.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:16:56.588
Jan 30 23:16:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:16:56.592
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:56.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:56.654
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 in namespace container-probe-9092 01/30/23 23:16:56.672
Jan 30 23:16:56.715: INFO: Waiting up to 5m0s for pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322" in namespace "container-probe-9092" to be "not pending"
Jan 30 23:16:56.736: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322": Phase="Pending", Reason="", readiness=false. Elapsed: 20.805598ms
Jan 30 23:16:58.754: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322": Phase="Running", Reason="", readiness=true. Elapsed: 2.039080858s
Jan 30 23:16:58.754: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322" satisfied condition "not pending"
Jan 30 23:16:58.754: INFO: Started pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 in namespace container-probe-9092
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:16:58.754
Jan 30 23:16:58.776: INFO: Initial restart count of pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is 0
Jan 30 23:17:19.048: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 1 (20.27200702s elapsed)
Jan 30 23:17:39.260: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 2 (40.48371319s elapsed)
Jan 30 23:17:59.456: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 3 (1m0.679087493s elapsed)
Jan 30 23:18:19.667: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 4 (1m20.890760748s elapsed)
Jan 30 23:19:22.303: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 5 (2m23.526746556s elapsed)
STEP: deleting the pod 01/30/23 23:19:22.303
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 23:19:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9092" for this suite. 01/30/23 23:19:22.377
------------------------------
• [SLOW TEST] [145.818 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:16:56.588
    Jan 30 23:16:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:16:56.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:16:56.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:16:56.654
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 in namespace container-probe-9092 01/30/23 23:16:56.672
    Jan 30 23:16:56.715: INFO: Waiting up to 5m0s for pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322" in namespace "container-probe-9092" to be "not pending"
    Jan 30 23:16:56.736: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322": Phase="Pending", Reason="", readiness=false. Elapsed: 20.805598ms
    Jan 30 23:16:58.754: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322": Phase="Running", Reason="", readiness=true. Elapsed: 2.039080858s
    Jan 30 23:16:58.754: INFO: Pod "liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322" satisfied condition "not pending"
    Jan 30 23:16:58.754: INFO: Started pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 in namespace container-probe-9092
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:16:58.754
    Jan 30 23:16:58.776: INFO: Initial restart count of pod liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is 0
    Jan 30 23:17:19.048: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 1 (20.27200702s elapsed)
    Jan 30 23:17:39.260: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 2 (40.48371319s elapsed)
    Jan 30 23:17:59.456: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 3 (1m0.679087493s elapsed)
    Jan 30 23:18:19.667: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 4 (1m20.890760748s elapsed)
    Jan 30 23:19:22.303: INFO: Restart count of pod container-probe-9092/liveness-d6567aad-77ab-4424-8c60-e4ba4d7a5322 is now 5 (2m23.526746556s elapsed)
    STEP: deleting the pod 01/30/23 23:19:22.303
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:19:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9092" for this suite. 01/30/23 23:19:22.377
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:19:22.408
Jan 30 23:19:22.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename cronjob 01/30/23 23:19:22.41
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:19:22.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:19:22.49
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/30/23 23:19:22.506
STEP: Ensuring more than one job is running at a time 01/30/23 23:19:22.529
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/30/23 23:21:00.551
STEP: Removing cronjob 01/30/23 23:21:00.595
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-61" for this suite. 01/30/23 23:21:00.662
------------------------------
• [SLOW TEST] [98.283 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:19:22.408
    Jan 30 23:19:22.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename cronjob 01/30/23 23:19:22.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:19:22.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:19:22.49
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/30/23 23:19:22.506
    STEP: Ensuring more than one job is running at a time 01/30/23 23:19:22.529
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/30/23 23:21:00.551
    STEP: Removing cronjob 01/30/23 23:21:00.595
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-61" for this suite. 01/30/23 23:21:00.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:00.695
Jan 30 23:21:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:21:00.698
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:00.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:00.81
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/30/23 23:21:00.83
Jan 30 23:21:00.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 create -f -'
Jan 30 23:21:01.187: INFO: stderr: ""
Jan 30 23:21:01.187: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:01.187
Jan 30 23:21:01.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:01.358: INFO: stderr: ""
Jan 30 23:21:01.358: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
Jan 30 23:21:01.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:01.506: INFO: stderr: ""
Jan 30 23:21:01.506: INFO: stdout: ""
Jan 30 23:21:01.506: INFO: update-demo-nautilus-88l7l is created but not running
Jan 30 23:21:06.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:06.673: INFO: stderr: ""
Jan 30 23:21:06.673: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
Jan 30 23:21:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:06.809: INFO: stderr: ""
Jan 30 23:21:06.809: INFO: stdout: "true"
Jan 30 23:21:06.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:06.961: INFO: stderr: ""
Jan 30 23:21:06.961: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:06.961: INFO: validating pod update-demo-nautilus-88l7l
Jan 30 23:21:07.036: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:07.036: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:07.036: INFO: update-demo-nautilus-88l7l is verified up and running
Jan 30 23:21:07.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-gg2rb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:07.171: INFO: stderr: ""
Jan 30 23:21:07.171: INFO: stdout: "true"
Jan 30 23:21:07.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-gg2rb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:07.305: INFO: stderr: ""
Jan 30 23:21:07.305: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:07.305: INFO: validating pod update-demo-nautilus-gg2rb
Jan 30 23:21:07.353: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:07.353: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:07.353: INFO: update-demo-nautilus-gg2rb is verified up and running
STEP: scaling down the replication controller 01/30/23 23:21:07.353
Jan 30 23:21:07.357: INFO: scanned /root for discovery docs: <nil>
Jan 30 23:21:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 30 23:21:08.564: INFO: stderr: ""
Jan 30 23:21:08.564: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:08.564
Jan 30 23:21:08.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:08.701: INFO: stderr: ""
Jan 30 23:21:08.701: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/30/23 23:21:08.701
Jan 30 23:21:13.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:13.857: INFO: stderr: ""
Jan 30 23:21:13.857: INFO: stdout: "update-demo-nautilus-88l7l "
Jan 30 23:21:13.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:13.982: INFO: stderr: ""
Jan 30 23:21:13.982: INFO: stdout: "true"
Jan 30 23:21:13.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:14.104: INFO: stderr: ""
Jan 30 23:21:14.105: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:14.105: INFO: validating pod update-demo-nautilus-88l7l
Jan 30 23:21:14.138: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:14.138: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:14.138: INFO: update-demo-nautilus-88l7l is verified up and running
STEP: scaling up the replication controller 01/30/23 23:21:14.138
Jan 30 23:21:14.141: INFO: scanned /root for discovery docs: <nil>
Jan 30 23:21:14.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 30 23:21:15.329: INFO: stderr: ""
Jan 30 23:21:15.329: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:15.329
Jan 30 23:21:15.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:15.469: INFO: stderr: ""
Jan 30 23:21:15.469: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-nc27v "
Jan 30 23:21:15.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:15.599: INFO: stderr: ""
Jan 30 23:21:15.599: INFO: stdout: "true"
Jan 30 23:21:15.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:15.737: INFO: stderr: ""
Jan 30 23:21:15.737: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:15.737: INFO: validating pod update-demo-nautilus-88l7l
Jan 30 23:21:15.764: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:15.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:15.764: INFO: update-demo-nautilus-88l7l is verified up and running
Jan 30 23:21:15.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:15.896: INFO: stderr: ""
Jan 30 23:21:15.896: INFO: stdout: ""
Jan 30 23:21:15.896: INFO: update-demo-nautilus-nc27v is created but not running
Jan 30 23:21:20.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 30 23:21:21.027: INFO: stderr: ""
Jan 30 23:21:21.027: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-nc27v "
Jan 30 23:21:21.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:21.175: INFO: stderr: ""
Jan 30 23:21:21.175: INFO: stdout: "true"
Jan 30 23:21:21.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:21.308: INFO: stderr: ""
Jan 30 23:21:21.308: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:21.308: INFO: validating pod update-demo-nautilus-88l7l
Jan 30 23:21:21.331: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:21.332: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:21.332: INFO: update-demo-nautilus-88l7l is verified up and running
Jan 30 23:21:21.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 30 23:21:21.469: INFO: stderr: ""
Jan 30 23:21:21.469: INFO: stdout: "true"
Jan 30 23:21:21.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 30 23:21:21.606: INFO: stderr: ""
Jan 30 23:21:21.606: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 30 23:21:21.606: INFO: validating pod update-demo-nautilus-nc27v
Jan 30 23:21:21.702: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 30 23:21:21.702: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 30 23:21:21.702: INFO: update-demo-nautilus-nc27v is verified up and running
STEP: using delete to clean up resources 01/30/23 23:21:21.702
Jan 30 23:21:21.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 delete --grace-period=0 --force -f -'
Jan 30 23:21:21.859: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 30 23:21:21.859: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 30 23:21:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get rc,svc -l name=update-demo --no-headers'
Jan 30 23:21:22.007: INFO: stderr: "No resources found in kubectl-7859 namespace.\n"
Jan 30 23:21:22.007: INFO: stdout: ""
Jan 30 23:21:22.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 30 23:21:22.167: INFO: stderr: ""
Jan 30 23:21:22.167: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7859" for this suite. 01/30/23 23:21:22.194
------------------------------
• [SLOW TEST] [21.525 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:00.695
    Jan 30 23:21:00.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:21:00.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:00.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:00.81
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/30/23 23:21:00.83
    Jan 30 23:21:00.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 create -f -'
    Jan 30 23:21:01.187: INFO: stderr: ""
    Jan 30 23:21:01.187: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:01.187
    Jan 30 23:21:01.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:01.358: INFO: stderr: ""
    Jan 30 23:21:01.358: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
    Jan 30 23:21:01.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:01.506: INFO: stderr: ""
    Jan 30 23:21:01.506: INFO: stdout: ""
    Jan 30 23:21:01.506: INFO: update-demo-nautilus-88l7l is created but not running
    Jan 30 23:21:06.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:06.673: INFO: stderr: ""
    Jan 30 23:21:06.673: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
    Jan 30 23:21:06.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:06.809: INFO: stderr: ""
    Jan 30 23:21:06.809: INFO: stdout: "true"
    Jan 30 23:21:06.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:06.961: INFO: stderr: ""
    Jan 30 23:21:06.961: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:06.961: INFO: validating pod update-demo-nautilus-88l7l
    Jan 30 23:21:07.036: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:07.036: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:07.036: INFO: update-demo-nautilus-88l7l is verified up and running
    Jan 30 23:21:07.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-gg2rb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:07.171: INFO: stderr: ""
    Jan 30 23:21:07.171: INFO: stdout: "true"
    Jan 30 23:21:07.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-gg2rb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:07.305: INFO: stderr: ""
    Jan 30 23:21:07.305: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:07.305: INFO: validating pod update-demo-nautilus-gg2rb
    Jan 30 23:21:07.353: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:07.353: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:07.353: INFO: update-demo-nautilus-gg2rb is verified up and running
    STEP: scaling down the replication controller 01/30/23 23:21:07.353
    Jan 30 23:21:07.357: INFO: scanned /root for discovery docs: <nil>
    Jan 30 23:21:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 30 23:21:08.564: INFO: stderr: ""
    Jan 30 23:21:08.564: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:08.564
    Jan 30 23:21:08.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:08.701: INFO: stderr: ""
    Jan 30 23:21:08.701: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-gg2rb "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/30/23 23:21:08.701
    Jan 30 23:21:13.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:13.857: INFO: stderr: ""
    Jan 30 23:21:13.857: INFO: stdout: "update-demo-nautilus-88l7l "
    Jan 30 23:21:13.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:13.982: INFO: stderr: ""
    Jan 30 23:21:13.982: INFO: stdout: "true"
    Jan 30 23:21:13.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:14.104: INFO: stderr: ""
    Jan 30 23:21:14.105: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:14.105: INFO: validating pod update-demo-nautilus-88l7l
    Jan 30 23:21:14.138: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:14.138: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:14.138: INFO: update-demo-nautilus-88l7l is verified up and running
    STEP: scaling up the replication controller 01/30/23 23:21:14.138
    Jan 30 23:21:14.141: INFO: scanned /root for discovery docs: <nil>
    Jan 30 23:21:14.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 30 23:21:15.329: INFO: stderr: ""
    Jan 30 23:21:15.329: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/30/23 23:21:15.329
    Jan 30 23:21:15.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:15.469: INFO: stderr: ""
    Jan 30 23:21:15.469: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-nc27v "
    Jan 30 23:21:15.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:15.599: INFO: stderr: ""
    Jan 30 23:21:15.599: INFO: stdout: "true"
    Jan 30 23:21:15.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:15.737: INFO: stderr: ""
    Jan 30 23:21:15.737: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:15.737: INFO: validating pod update-demo-nautilus-88l7l
    Jan 30 23:21:15.764: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:15.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:15.764: INFO: update-demo-nautilus-88l7l is verified up and running
    Jan 30 23:21:15.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:15.896: INFO: stderr: ""
    Jan 30 23:21:15.896: INFO: stdout: ""
    Jan 30 23:21:15.896: INFO: update-demo-nautilus-nc27v is created but not running
    Jan 30 23:21:20.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 30 23:21:21.027: INFO: stderr: ""
    Jan 30 23:21:21.027: INFO: stdout: "update-demo-nautilus-88l7l update-demo-nautilus-nc27v "
    Jan 30 23:21:21.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:21.175: INFO: stderr: ""
    Jan 30 23:21:21.175: INFO: stdout: "true"
    Jan 30 23:21:21.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-88l7l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:21.308: INFO: stderr: ""
    Jan 30 23:21:21.308: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:21.308: INFO: validating pod update-demo-nautilus-88l7l
    Jan 30 23:21:21.331: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:21.332: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:21.332: INFO: update-demo-nautilus-88l7l is verified up and running
    Jan 30 23:21:21.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 30 23:21:21.469: INFO: stderr: ""
    Jan 30 23:21:21.469: INFO: stdout: "true"
    Jan 30 23:21:21.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods update-demo-nautilus-nc27v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 30 23:21:21.606: INFO: stderr: ""
    Jan 30 23:21:21.606: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 30 23:21:21.606: INFO: validating pod update-demo-nautilus-nc27v
    Jan 30 23:21:21.702: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 30 23:21:21.702: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 30 23:21:21.702: INFO: update-demo-nautilus-nc27v is verified up and running
    STEP: using delete to clean up resources 01/30/23 23:21:21.702
    Jan 30 23:21:21.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 delete --grace-period=0 --force -f -'
    Jan 30 23:21:21.859: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 30 23:21:21.859: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 30 23:21:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get rc,svc -l name=update-demo --no-headers'
    Jan 30 23:21:22.007: INFO: stderr: "No resources found in kubectl-7859 namespace.\n"
    Jan 30 23:21:22.007: INFO: stdout: ""
    Jan 30 23:21:22.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-7859 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 30 23:21:22.167: INFO: stderr: ""
    Jan 30 23:21:22.167: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:22.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7859" for this suite. 01/30/23 23:21:22.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:22.223
Jan 30 23:21:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:21:22.227
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:22.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:22.291
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:21:22.307
Jan 30 23:21:22.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a" in namespace "projected-2619" to be "Succeeded or Failed"
Jan 30 23:21:22.390: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 49.391563ms
Jan 30 23:21:24.413: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072176622s
Jan 30 23:21:26.412: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071670049s
Jan 30 23:21:28.434: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092886985s
STEP: Saw pod success 01/30/23 23:21:28.434
Jan 30 23:21:28.434: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a" satisfied condition "Succeeded or Failed"
Jan 30 23:21:28.452: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a container client-container: <nil>
STEP: delete the pod 01/30/23 23:21:28.608
Jan 30 23:21:28.673: INFO: Waiting for pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a to disappear
Jan 30 23:21:28.723: INFO: Pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2619" for this suite. 01/30/23 23:21:28.749
------------------------------
• [SLOW TEST] [6.557 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:22.223
    Jan 30 23:21:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:21:22.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:22.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:22.291
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:21:22.307
    Jan 30 23:21:22.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a" in namespace "projected-2619" to be "Succeeded or Failed"
    Jan 30 23:21:22.390: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 49.391563ms
    Jan 30 23:21:24.413: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072176622s
    Jan 30 23:21:26.412: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071670049s
    Jan 30 23:21:28.434: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092886985s
    STEP: Saw pod success 01/30/23 23:21:28.434
    Jan 30 23:21:28.434: INFO: Pod "downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a" satisfied condition "Succeeded or Failed"
    Jan 30 23:21:28.452: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a container client-container: <nil>
    STEP: delete the pod 01/30/23 23:21:28.608
    Jan 30 23:21:28.673: INFO: Waiting for pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a to disappear
    Jan 30 23:21:28.723: INFO: Pod downwardapi-volume-5024fbad-2403-41cc-a543-8589bf20ff8a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2619" for this suite. 01/30/23 23:21:28.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:28.787
Jan 30 23:21:28.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 23:21:28.789
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:28.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:28.862
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 23:21:28.948
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:21:29.602
STEP: Deploying the webhook pod 01/30/23 23:21:29.64
STEP: Wait for the deployment to be ready 01/30/23 23:21:29.685
Jan 30 23:21:29.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 30 23:21:31.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:21:33.797
STEP: Verifying the service has paired with the endpoint 01/30/23 23:21:33.863
Jan 30 23:21:34.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/30/23 23:21:34.882
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:34.993
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/30/23 23:21:35.063
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:35.107
STEP: Patching a validating webhook configuration's rules to include the create operation 01/30/23 23:21:35.162
STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:35.203
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:35.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9734" for this suite. 01/30/23 23:21:35.469
STEP: Destroying namespace "webhook-9734-markers" for this suite. 01/30/23 23:21:35.501
------------------------------
• [SLOW TEST] [6.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:28.787
    Jan 30 23:21:28.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 23:21:28.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:28.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:28.862
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 23:21:28.948
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:21:29.602
    STEP: Deploying the webhook pod 01/30/23 23:21:29.64
    STEP: Wait for the deployment to be ready 01/30/23 23:21:29.685
    Jan 30 23:21:29.724: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 30 23:21:31.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 21, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:21:33.797
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:21:33.863
    Jan 30 23:21:34.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/30/23 23:21:34.882
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:34.993
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/30/23 23:21:35.063
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:35.107
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/30/23 23:21:35.162
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/30/23 23:21:35.203
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:35.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9734" for this suite. 01/30/23 23:21:35.469
    STEP: Destroying namespace "webhook-9734-markers" for this suite. 01/30/23 23:21:35.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:35.542
Jan 30 23:21:35.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:21:35.544
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:35.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:35.614
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:21:35.633
Jan 30 23:21:35.670: INFO: Waiting up to 5m0s for pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535" in namespace "projected-1660" to be "Succeeded or Failed"
Jan 30 23:21:35.687: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 16.979925ms
Jan 30 23:21:37.706: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036297267s
Jan 30 23:21:39.705: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035410214s
Jan 30 23:21:41.709: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03956379s
STEP: Saw pod success 01/30/23 23:21:41.709
Jan 30 23:21:41.710: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535" satisfied condition "Succeeded or Failed"
Jan 30 23:21:41.728: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 container client-container: <nil>
STEP: delete the pod 01/30/23 23:21:41.769
Jan 30 23:21:41.834: INFO: Waiting for pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 to disappear
Jan 30 23:21:41.852: INFO: Pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:41.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1660" for this suite. 01/30/23 23:21:41.88
------------------------------
• [SLOW TEST] [6.371 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:35.542
    Jan 30 23:21:35.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:21:35.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:35.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:35.614
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:21:35.633
    Jan 30 23:21:35.670: INFO: Waiting up to 5m0s for pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535" in namespace "projected-1660" to be "Succeeded or Failed"
    Jan 30 23:21:35.687: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 16.979925ms
    Jan 30 23:21:37.706: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036297267s
    Jan 30 23:21:39.705: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035410214s
    Jan 30 23:21:41.709: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03956379s
    STEP: Saw pod success 01/30/23 23:21:41.709
    Jan 30 23:21:41.710: INFO: Pod "downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535" satisfied condition "Succeeded or Failed"
    Jan 30 23:21:41.728: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:21:41.769
    Jan 30 23:21:41.834: INFO: Waiting for pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 to disappear
    Jan 30 23:21:41.852: INFO: Pod downwardapi-volume-765968c1-f00e-4722-9141-fb0eb24d2535 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:41.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1660" for this suite. 01/30/23 23:21:41.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:41.918
Jan 30 23:21:41.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/30/23 23:21:41.92
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:41.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:41.991
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 30 23:21:42.141: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9d7a075b-c921-44c2-8e9b-4c12cba02e7f", Controller:(*bool)(0xc001b3aa9e), BlockOwnerDeletion:(*bool)(0xc001b3aa9f)}}
Jan 30 23:21:42.181: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9b81f50c-eab0-4e3a-9545-a2b3189414dd", Controller:(*bool)(0xc001b3b176), BlockOwnerDeletion:(*bool)(0xc001b3b177)}}
Jan 30 23:21:42.226: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bf095376-ba1d-458a-8b2d-8c71fa5ab3b1", Controller:(*bool)(0xc001b3b866), BlockOwnerDeletion:(*bool)(0xc001b3b867)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3323" for this suite. 01/30/23 23:21:47.304
------------------------------
• [SLOW TEST] [5.413 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:41.918
    Jan 30 23:21:41.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/30/23 23:21:41.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:41.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:41.991
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 30 23:21:42.141: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9d7a075b-c921-44c2-8e9b-4c12cba02e7f", Controller:(*bool)(0xc001b3aa9e), BlockOwnerDeletion:(*bool)(0xc001b3aa9f)}}
    Jan 30 23:21:42.181: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9b81f50c-eab0-4e3a-9545-a2b3189414dd", Controller:(*bool)(0xc001b3b176), BlockOwnerDeletion:(*bool)(0xc001b3b177)}}
    Jan 30 23:21:42.226: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bf095376-ba1d-458a-8b2d-8c71fa5ab3b1", Controller:(*bool)(0xc001b3b866), BlockOwnerDeletion:(*bool)(0xc001b3b867)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3323" for this suite. 01/30/23 23:21:47.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:47.336
Jan 30 23:21:47.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-pred 01/30/23 23:21:47.338
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:47.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:47.421
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 23:21:47.435: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 23:21:47.496: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 23:21:47.513: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.225 before test
Jan 30 23:21:47.551: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:21:47.551: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:21:47.551: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:21:47.551: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:21:47.551: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:21:47.551: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:21:47.551: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:21:47.551: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:21:47.551: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:21:47.551: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:21:47.551: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container e2e ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:21:47.551: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:21:47.551: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.227 before test
Jan 30 23:21:47.601: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.601: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:21:47.601: INFO: calico-typha-5fcb7c495f-9n9f2 from kube-system started at 2023-01-30 23:16:02 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.601: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:21:47.601: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.601: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:21:47.601: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.602: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:21:47.602: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:21:47.602: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.602: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:21:47.602: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.602: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:21:47.603: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.603: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 23:21:47.603: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:21:47.603: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:21:47.603: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.237 before test
Jan 30 23:21:47.655: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:21:47.656: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 23:21:47.656: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:21:47.656: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:21:47.656: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:21:47.656: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:21:47.656: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container autoscaler ready: true, restart count 0
Jan 30 23:21:47.656: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:21:47.656: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 30 23:21:47.656: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 30 23:21:47.656: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:21:47.656: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 30 23:21:47.656: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:21:47.656: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:21:47.656: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:21:47.656: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:21:47.656: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:21:47.656: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:21:47.656: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.15.28.225 01/30/23 23:21:47.763
STEP: verifying the node has the label node 10.15.28.227 01/30/23 23:21:47.835
STEP: verifying the node has the label node 10.15.28.237 01/30/23 23:21:47.893
Jan 30 23:21:47.957: INFO: Pod ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd requesting resource cpu=5m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z requesting resource cpu=5m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod calico-kube-controllers-5ddbd89486-xb4dr requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod calico-node-qbh88 requesting resource cpu=250m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod calico-node-sgm4f requesting resource cpu=250m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod calico-node-sqmvl requesting resource cpu=250m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-67gfv requesting resource cpu=250m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-7clq5 requesting resource cpu=250m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-9n9f2 requesting resource cpu=250m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-fzqcs requesting resource cpu=100m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-q4pqr requesting resource cpu=100m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-rnndc requesting resource cpu=100m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod coredns-autoscaler-57c58584b6-sn4w2 requesting resource cpu=1m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod dashboard-metrics-scraper-67f9957b6-q4kjj requesting resource cpu=1m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibm-file-plugin-855c994c98-xsp27 requesting resource cpu=50m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-5787n requesting resource cpu=5m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-6j488 requesting resource cpu=5m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-whvb4 requesting resource cpu=5m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.225 requesting resource cpu=25m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.227 requesting resource cpu=25m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.237 requesting resource cpu=25m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibm-storage-watcher-6b8f8bd5f7-jprpf requesting resource cpu=50m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-6kbjz requesting resource cpu=50m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-82h8n requesting resource cpu=50m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-rvhkt requesting resource cpu=50m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 requesting resource cpu=50m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod ingress-cluster-healthcheck-bbddc799d-w79wj requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-76wnq requesting resource cpu=10m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-k7vgh requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-vvjd9 requesting resource cpu=10m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod kubernetes-dashboard-58dffc9764-j7lxj requesting resource cpu=50m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod metrics-server-5c45845f46-6mj46 requesting resource cpu=126m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod metrics-server-5c45845f46-pmpxp requesting resource cpu=126m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r requesting resource cpu=20m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh requesting resource cpu=20m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-bkzdf requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-g657k requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-tlrsj requesting resource cpu=10m on Node 10.15.28.237
Jan 30 23:21:47.957: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod sonobuoy-e2e-job-1fdfddcee1544467 requesting resource cpu=0m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q requesting resource cpu=0m on Node 10.15.28.225
Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 requesting resource cpu=0m on Node 10.15.28.227
Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 requesting resource cpu=0m on Node 10.15.28.237
STEP: Starting Pods to consume most of the cluster CPU. 01/30/23 23:21:47.957
Jan 30 23:21:47.958: INFO: Creating a pod which consumes cpu=2148m on Node 10.15.28.225
Jan 30 23:21:47.996: INFO: Creating a pod which consumes cpu=2324m on Node 10.15.28.227
Jan 30 23:21:48.024: INFO: Creating a pod which consumes cpu=1901m on Node 10.15.28.237
Jan 30 23:21:48.066: INFO: Waiting up to 5m0s for pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d" in namespace "sched-pred-5655" to be "running"
Jan 30 23:21:48.086: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.717139ms
Jan 30 23:21:50.107: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040516917s
Jan 30 23:21:52.105: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Running", Reason="", readiness=true. Elapsed: 4.038041546s
Jan 30 23:21:52.105: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d" satisfied condition "running"
Jan 30 23:21:52.105: INFO: Waiting up to 5m0s for pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59" in namespace "sched-pred-5655" to be "running"
Jan 30 23:21:52.123: INFO: Pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59": Phase="Running", Reason="", readiness=true. Elapsed: 18.555174ms
Jan 30 23:21:52.123: INFO: Pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59" satisfied condition "running"
Jan 30 23:21:52.123: INFO: Waiting up to 5m0s for pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945" in namespace "sched-pred-5655" to be "running"
Jan 30 23:21:52.145: INFO: Pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945": Phase="Running", Reason="", readiness=true. Elapsed: 21.468094ms
Jan 30 23:21:52.145: INFO: Pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/30/23 23:21:52.145
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f39973a6058ce], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945 to 10.15.28.237] 01/30/23 23:21:52.171
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f39979041362b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.171
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f3997963612c9], Reason = [Created], Message = [Created container filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945] 01/30/23 23:21:52.171
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f3997a3d28435], Reason = [Started], Message = [Started container filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945] 01/30/23 23:21:52.171
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f399735d2f79f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-546e7a8e-8711-4518-96e5-dab87019859d to 10.15.28.225] 01/30/23 23:21:52.171
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39978c25d0fe], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39978ff9c206], Reason = [Created], Message = [Created container filler-pod-546e7a8e-8711-4518-96e5-dab87019859d] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39979e3cb5bd], Reason = [Started], Message = [Started container filler-pod-546e7a8e-8711-4518-96e5-dab87019859d] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f399738feae93], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59 to 10.15.28.227] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f39978e090c46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f399792cf8927], Reason = [Created], Message = [Created container filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f3997a36ae43c], Reason = [Started], Message = [Started container filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59] 01/30/23 23:21:52.172
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173f39982fb4f31e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/30/23 23:21:52.226
STEP: removing the label node off the node 10.15.28.227 01/30/23 23:21:53.226
STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.284
STEP: removing the label node off the node 10.15.28.237 01/30/23 23:21:53.309
STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.372
STEP: removing the label node off the node 10.15.28.225 01/30/23 23:21:53.391
STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.453
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5655" for this suite. 01/30/23 23:21:53.51
------------------------------
• [SLOW TEST] [6.201 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:47.336
    Jan 30 23:21:47.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-pred 01/30/23 23:21:47.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:47.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:47.421
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 23:21:47.435: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 23:21:47.496: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 23:21:47.513: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.225 before test
    Jan 30 23:21:47.551: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.551: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:21:47.551: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.227 before test
    Jan 30 23:21:47.601: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.601: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:21:47.601: INFO: calico-typha-5fcb7c495f-9n9f2 from kube-system started at 2023-01-30 23:16:02 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.601: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:21:47.601: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.601: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:21:47.601: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.602: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:21:47.602: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:21:47.602: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.602: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:21:47.602: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.602: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:21:47.603: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.603: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 23:21:47.603: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:21:47.603: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:21:47.603: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.237 before test
    Jan 30 23:21:47.655: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:21:47.656: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:21:47.656: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.15.28.225 01/30/23 23:21:47.763
    STEP: verifying the node has the label node 10.15.28.227 01/30/23 23:21:47.835
    STEP: verifying the node has the label node 10.15.28.237 01/30/23 23:21:47.893
    Jan 30 23:21:47.957: INFO: Pod ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd requesting resource cpu=5m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z requesting resource cpu=5m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod calico-kube-controllers-5ddbd89486-xb4dr requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod calico-node-qbh88 requesting resource cpu=250m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod calico-node-sgm4f requesting resource cpu=250m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod calico-node-sqmvl requesting resource cpu=250m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-67gfv requesting resource cpu=250m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-7clq5 requesting resource cpu=250m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod calico-typha-5fcb7c495f-9n9f2 requesting resource cpu=250m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-fzqcs requesting resource cpu=100m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-q4pqr requesting resource cpu=100m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod coredns-56697bd765-rnndc requesting resource cpu=100m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod coredns-autoscaler-57c58584b6-sn4w2 requesting resource cpu=1m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod dashboard-metrics-scraper-67f9957b6-q4kjj requesting resource cpu=1m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibm-file-plugin-855c994c98-xsp27 requesting resource cpu=50m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-5787n requesting resource cpu=5m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-6j488 requesting resource cpu=5m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod ibm-keepalived-watcher-whvb4 requesting resource cpu=5m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.225 requesting resource cpu=25m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.227 requesting resource cpu=25m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod ibm-master-proxy-static-10.15.28.237 requesting resource cpu=25m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibm-storage-watcher-6b8f8bd5f7-jprpf requesting resource cpu=50m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-6kbjz requesting resource cpu=50m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-82h8n requesting resource cpu=50m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-driver-rvhkt requesting resource cpu=50m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 requesting resource cpu=50m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod ingress-cluster-healthcheck-bbddc799d-w79wj requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-76wnq requesting resource cpu=10m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-k7vgh requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod konnectivity-agent-vvjd9 requesting resource cpu=10m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod kubernetes-dashboard-58dffc9764-j7lxj requesting resource cpu=50m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod metrics-server-5c45845f46-6mj46 requesting resource cpu=126m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod metrics-server-5c45845f46-pmpxp requesting resource cpu=126m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r requesting resource cpu=20m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh requesting resource cpu=20m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-bkzdf requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-g657k requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod snapshot-controller-6c8c86697-tlrsj requesting resource cpu=10m on Node 10.15.28.237
    Jan 30 23:21:47.957: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod sonobuoy-e2e-job-1fdfddcee1544467 requesting resource cpu=0m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q requesting resource cpu=0m on Node 10.15.28.225
    Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 requesting resource cpu=0m on Node 10.15.28.227
    Jan 30 23:21:47.957: INFO: Pod sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 requesting resource cpu=0m on Node 10.15.28.237
    STEP: Starting Pods to consume most of the cluster CPU. 01/30/23 23:21:47.957
    Jan 30 23:21:47.958: INFO: Creating a pod which consumes cpu=2148m on Node 10.15.28.225
    Jan 30 23:21:47.996: INFO: Creating a pod which consumes cpu=2324m on Node 10.15.28.227
    Jan 30 23:21:48.024: INFO: Creating a pod which consumes cpu=1901m on Node 10.15.28.237
    Jan 30 23:21:48.066: INFO: Waiting up to 5m0s for pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d" in namespace "sched-pred-5655" to be "running"
    Jan 30 23:21:48.086: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.717139ms
    Jan 30 23:21:50.107: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040516917s
    Jan 30 23:21:52.105: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d": Phase="Running", Reason="", readiness=true. Elapsed: 4.038041546s
    Jan 30 23:21:52.105: INFO: Pod "filler-pod-546e7a8e-8711-4518-96e5-dab87019859d" satisfied condition "running"
    Jan 30 23:21:52.105: INFO: Waiting up to 5m0s for pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59" in namespace "sched-pred-5655" to be "running"
    Jan 30 23:21:52.123: INFO: Pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59": Phase="Running", Reason="", readiness=true. Elapsed: 18.555174ms
    Jan 30 23:21:52.123: INFO: Pod "filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59" satisfied condition "running"
    Jan 30 23:21:52.123: INFO: Waiting up to 5m0s for pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945" in namespace "sched-pred-5655" to be "running"
    Jan 30 23:21:52.145: INFO: Pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945": Phase="Running", Reason="", readiness=true. Elapsed: 21.468094ms
    Jan 30 23:21:52.145: INFO: Pod "filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/30/23 23:21:52.145
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f39973a6058ce], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945 to 10.15.28.237] 01/30/23 23:21:52.171
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f39979041362b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.171
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f3997963612c9], Reason = [Created], Message = [Created container filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945] 01/30/23 23:21:52.171
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945.173f3997a3d28435], Reason = [Started], Message = [Started container filler-pod-4f077bf6-7f76-4a2a-b1b0-512a9eb5c945] 01/30/23 23:21:52.171
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f399735d2f79f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-546e7a8e-8711-4518-96e5-dab87019859d to 10.15.28.225] 01/30/23 23:21:52.171
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39978c25d0fe], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39978ff9c206], Reason = [Created], Message = [Created container filler-pod-546e7a8e-8711-4518-96e5-dab87019859d] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-546e7a8e-8711-4518-96e5-dab87019859d.173f39979e3cb5bd], Reason = [Started], Message = [Started container filler-pod-546e7a8e-8711-4518-96e5-dab87019859d] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f399738feae93], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5655/filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59 to 10.15.28.227] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f39978e090c46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f399792cf8927], Reason = [Created], Message = [Created container filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59.173f3997a36ae43c], Reason = [Started], Message = [Started container filler-pod-f4208787-7ce4-4435-b362-e0cbc607ab59] 01/30/23 23:21:52.172
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173f39982fb4f31e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/30/23 23:21:52.226
    STEP: removing the label node off the node 10.15.28.227 01/30/23 23:21:53.226
    STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.284
    STEP: removing the label node off the node 10.15.28.237 01/30/23 23:21:53.309
    STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.372
    STEP: removing the label node off the node 10.15.28.225 01/30/23 23:21:53.391
    STEP: verifying the node doesn't have the label node 01/30/23 23:21:53.453
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:53.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5655" for this suite. 01/30/23 23:21:53.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:53.546
Jan 30 23:21:53.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename server-version 01/30/23 23:21:53.548
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:53.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:53.633
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/30/23 23:21:53.647
STEP: Confirm major version 01/30/23 23:21:53.654
Jan 30 23:21:53.654: INFO: Major version: 1
STEP: Confirm minor version 01/30/23 23:21:53.654
Jan 30 23:21:53.654: INFO: cleanMinorVersion: 26
Jan 30 23:21:53.654: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:53.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3072" for this suite. 01/30/23 23:21:53.683
------------------------------
• [0.163 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:53.546
    Jan 30 23:21:53.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename server-version 01/30/23 23:21:53.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:53.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:53.633
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/30/23 23:21:53.647
    STEP: Confirm major version 01/30/23 23:21:53.654
    Jan 30 23:21:53.654: INFO: Major version: 1
    STEP: Confirm minor version 01/30/23 23:21:53.654
    Jan 30 23:21:53.654: INFO: cleanMinorVersion: 26
    Jan 30 23:21:53.654: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:53.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3072" for this suite. 01/30/23 23:21:53.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:53.71
Jan 30 23:21:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:21:53.711
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:53.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:53.788
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 30 23:21:53.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-5832 version'
Jan 30 23:21:53.922: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 30 23:21:53.922: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1+IKS\", GitCommit:\"85de84307cf885224045d6d24ae4c433e05efbea\", GitTreeState:\"clean\", BuildDate:\"2023-01-21T02:15:11Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:53.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5832" for this suite. 01/30/23 23:21:53.954
------------------------------
• [0.273 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:53.71
    Jan 30 23:21:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:21:53.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:53.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:53.788
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 30 23:21:53.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-5832 version'
    Jan 30 23:21:53.922: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 30 23:21:53.922: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1+IKS\", GitCommit:\"85de84307cf885224045d6d24ae4c433e05efbea\", GitTreeState:\"clean\", BuildDate:\"2023-01-21T02:15:11Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:53.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5832" for this suite. 01/30/23 23:21:53.954
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:53.985
Jan 30 23:21:53.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename conformance-tests 01/30/23 23:21:53.988
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:54.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:54.073
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/30/23 23:21:54.097
Jan 30 23:21:54.097: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 30 23:21:54.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8284" for this suite. 01/30/23 23:21:54.153
------------------------------
• [0.195 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:53.985
    Jan 30 23:21:53.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename conformance-tests 01/30/23 23:21:53.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:54.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:54.073
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/30/23 23:21:54.097
    Jan 30 23:21:54.097: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:21:54.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8284" for this suite. 01/30/23 23:21:54.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:21:54.185
Jan 30 23:21:54.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 23:21:54.187
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:54.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:54.251
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 30 23:21:54.328: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 23:21:59.351: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 23:21:59.351
STEP: Scaling up "test-rs" replicaset  01/30/23 23:21:59.352
Jan 30 23:21:59.398: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/30/23 23:21:59.398
W0130 23:21:59.430869      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 30 23:21:59.437: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 23:21:59.469: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 23:21:59.611: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 23:21:59.638: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
Jan 30 23:22:02.512: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 2, AvailableReplicas 2
Jan 30 23:22:02.723: INFO: observed Replicaset test-rs in namespace replicaset-1763 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:22:02.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1763" for this suite. 01/30/23 23:22:02.753
------------------------------
• [SLOW TEST] [8.613 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:21:54.185
    Jan 30 23:21:54.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 23:21:54.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:21:54.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:21:54.251
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 30 23:21:54.328: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 23:21:59.351: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 23:21:59.351
    STEP: Scaling up "test-rs" replicaset  01/30/23 23:21:59.352
    Jan 30 23:21:59.398: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/30/23 23:21:59.398
    W0130 23:21:59.430869      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 30 23:21:59.437: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 23:21:59.469: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 23:21:59.611: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 23:21:59.638: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 1, AvailableReplicas 1
    Jan 30 23:22:02.512: INFO: observed ReplicaSet test-rs in namespace replicaset-1763 with ReadyReplicas 2, AvailableReplicas 2
    Jan 30 23:22:02.723: INFO: observed Replicaset test-rs in namespace replicaset-1763 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:22:02.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1763" for this suite. 01/30/23 23:22:02.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:22:02.807
Jan 30 23:22:02.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption 01/30/23 23:22:02.81
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:02.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:02.89
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/30/23 23:22:02.927
STEP: Updating PodDisruptionBudget status 01/30/23 23:22:04.976
STEP: Waiting for all pods to be running 01/30/23 23:22:05.012
Jan 30 23:22:05.035: INFO: running pods: 0 < 1
Jan 30 23:22:07.057: INFO: running pods: 0 < 1
STEP: locating a running pod 01/30/23 23:22:09.055
STEP: Waiting for the pdb to be processed 01/30/23 23:22:09.127
STEP: Patching PodDisruptionBudget status 01/30/23 23:22:09.165
STEP: Waiting for the pdb to be processed 01/30/23 23:22:09.208
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:22:09.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2912" for this suite. 01/30/23 23:22:09.255
------------------------------
• [SLOW TEST] [6.478 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:22:02.807
    Jan 30 23:22:02.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption 01/30/23 23:22:02.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:02.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:02.89
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/30/23 23:22:02.927
    STEP: Updating PodDisruptionBudget status 01/30/23 23:22:04.976
    STEP: Waiting for all pods to be running 01/30/23 23:22:05.012
    Jan 30 23:22:05.035: INFO: running pods: 0 < 1
    Jan 30 23:22:07.057: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/30/23 23:22:09.055
    STEP: Waiting for the pdb to be processed 01/30/23 23:22:09.127
    STEP: Patching PodDisruptionBudget status 01/30/23 23:22:09.165
    STEP: Waiting for the pdb to be processed 01/30/23 23:22:09.208
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:22:09.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2912" for this suite. 01/30/23 23:22:09.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:22:09.319
Jan 30 23:22:09.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:22:09.321
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:09.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:09.393
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 30 23:22:09.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: creating the pod 01/30/23 23:22:09.411
STEP: submitting the pod to kubernetes 01/30/23 23:22:09.412
Jan 30 23:22:09.446: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57" in namespace "pods-6134" to be "running and ready"
Jan 30 23:22:09.463: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Pending", Reason="", readiness=false. Elapsed: 16.71508ms
Jan 30 23:22:09.463: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:22:11.481: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035112207s
Jan 30 23:22:11.481: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:22:13.525: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Running", Reason="", readiness=true. Elapsed: 4.078632792s
Jan 30 23:22:13.525: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Running (Ready = true)
Jan 30 23:22:13.525: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:22:13.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6134" for this suite. 01/30/23 23:22:13.691
------------------------------
• [4.399 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:22:09.319
    Jan 30 23:22:09.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:22:09.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:09.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:09.393
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 30 23:22:09.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: creating the pod 01/30/23 23:22:09.411
    STEP: submitting the pod to kubernetes 01/30/23 23:22:09.412
    Jan 30 23:22:09.446: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57" in namespace "pods-6134" to be "running and ready"
    Jan 30 23:22:09.463: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Pending", Reason="", readiness=false. Elapsed: 16.71508ms
    Jan 30 23:22:09.463: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:22:11.481: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035112207s
    Jan 30 23:22:11.481: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:22:13.525: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57": Phase="Running", Reason="", readiness=true. Elapsed: 4.078632792s
    Jan 30 23:22:13.525: INFO: The phase of Pod pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57 is Running (Ready = true)
    Jan 30 23:22:13.525: INFO: Pod "pod-logs-websocket-8aa9388b-a629-4ba9-889c-74c78ae3db57" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:22:13.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6134" for this suite. 01/30/23 23:22:13.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:22:13.72
Jan 30 23:22:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename certificates 01/30/23 23:22:13.723
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:13.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:13.81
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/30/23 23:22:14.348
STEP: getting /apis/certificates.k8s.io 01/30/23 23:22:14.368
STEP: getting /apis/certificates.k8s.io/v1 01/30/23 23:22:14.376
STEP: creating 01/30/23 23:22:14.382
STEP: getting 01/30/23 23:22:14.451
STEP: listing 01/30/23 23:22:14.468
STEP: watching 01/30/23 23:22:14.487
Jan 30 23:22:14.487: INFO: starting watch
STEP: patching 01/30/23 23:22:14.494
STEP: updating 01/30/23 23:22:14.546
Jan 30 23:22:14.570: INFO: waiting for watch events with expected annotations
Jan 30 23:22:14.570: INFO: saw patched and updated annotations
STEP: getting /approval 01/30/23 23:22:14.571
STEP: patching /approval 01/30/23 23:22:14.588
STEP: updating /approval 01/30/23 23:22:14.637
STEP: getting /status 01/30/23 23:22:14.685
STEP: patching /status 01/30/23 23:22:14.723
STEP: updating /status 01/30/23 23:22:14.748
STEP: deleting 01/30/23 23:22:14.77
STEP: deleting a collection 01/30/23 23:22:14.842
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:22:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-5399" for this suite. 01/30/23 23:22:14.948
------------------------------
• [1.256 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:22:13.72
    Jan 30 23:22:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename certificates 01/30/23 23:22:13.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:13.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:13.81
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/30/23 23:22:14.348
    STEP: getting /apis/certificates.k8s.io 01/30/23 23:22:14.368
    STEP: getting /apis/certificates.k8s.io/v1 01/30/23 23:22:14.376
    STEP: creating 01/30/23 23:22:14.382
    STEP: getting 01/30/23 23:22:14.451
    STEP: listing 01/30/23 23:22:14.468
    STEP: watching 01/30/23 23:22:14.487
    Jan 30 23:22:14.487: INFO: starting watch
    STEP: patching 01/30/23 23:22:14.494
    STEP: updating 01/30/23 23:22:14.546
    Jan 30 23:22:14.570: INFO: waiting for watch events with expected annotations
    Jan 30 23:22:14.570: INFO: saw patched and updated annotations
    STEP: getting /approval 01/30/23 23:22:14.571
    STEP: patching /approval 01/30/23 23:22:14.588
    STEP: updating /approval 01/30/23 23:22:14.637
    STEP: getting /status 01/30/23 23:22:14.685
    STEP: patching /status 01/30/23 23:22:14.723
    STEP: updating /status 01/30/23 23:22:14.748
    STEP: deleting 01/30/23 23:22:14.77
    STEP: deleting a collection 01/30/23 23:22:14.842
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:22:14.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-5399" for this suite. 01/30/23 23:22:14.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:22:14.981
Jan 30 23:22:14.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:22:14.983
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:15.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:15.058
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-3cf16d91-fb9e-43e8-8cac-0e9cd309654e 01/30/23 23:22:15.077
STEP: Creating a pod to test consume configMaps 01/30/23 23:22:15.097
Jan 30 23:22:15.135: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223" in namespace "projected-1198" to be "Succeeded or Failed"
Jan 30 23:22:15.153: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 17.595793ms
Jan 30 23:22:17.182: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046950966s
Jan 30 23:22:19.177: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041563511s
Jan 30 23:22:21.174: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038667304s
STEP: Saw pod success 01/30/23 23:22:21.174
Jan 30 23:22:21.174: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223" satisfied condition "Succeeded or Failed"
Jan 30 23:22:21.193: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:22:21.231
Jan 30 23:22:21.285: INFO: Waiting for pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 to disappear
Jan 30 23:22:21.301: INFO: Pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:22:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1198" for this suite. 01/30/23 23:22:21.329
------------------------------
• [SLOW TEST] [6.378 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:22:14.981
    Jan 30 23:22:14.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:22:14.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:15.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:15.058
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-3cf16d91-fb9e-43e8-8cac-0e9cd309654e 01/30/23 23:22:15.077
    STEP: Creating a pod to test consume configMaps 01/30/23 23:22:15.097
    Jan 30 23:22:15.135: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223" in namespace "projected-1198" to be "Succeeded or Failed"
    Jan 30 23:22:15.153: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 17.595793ms
    Jan 30 23:22:17.182: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046950966s
    Jan 30 23:22:19.177: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041563511s
    Jan 30 23:22:21.174: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038667304s
    STEP: Saw pod success 01/30/23 23:22:21.174
    Jan 30 23:22:21.174: INFO: Pod "pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223" satisfied condition "Succeeded or Failed"
    Jan 30 23:22:21.193: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:22:21.231
    Jan 30 23:22:21.285: INFO: Waiting for pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 to disappear
    Jan 30 23:22:21.301: INFO: Pod pod-projected-configmaps-0506875b-3374-467d-b32c-004dfcaaa223 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:22:21.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1198" for this suite. 01/30/23 23:22:21.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:22:21.361
Jan 30 23:22:21.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:22:21.362
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:21.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:21.432
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 23:22:21.527: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:23:21.685: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:23:21.704
Jan 30 23:23:21.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 23:23:21.707
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:21.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:21.794
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 01/30/23 23:23:21.811
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 23:23:21.811
Jan 30 23:23:21.848: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7135" to be "running"
Jan 30 23:23:21.896: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 47.72039ms
Jan 30 23:23:23.918: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069544383s
Jan 30 23:23:25.916: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.068016471s
Jan 30 23:23:25.916: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 23:23:25.932
Jan 30 23:23:25.977: INFO: found a healthy node: 10.15.28.227
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jan 30 23:23:38.382: INFO: pods created so far: [1 1 1]
Jan 30 23:23:38.382: INFO: length of pods created so far: 3
Jan 30 23:23:44.437: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 30 23:23:51.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:23:51.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7135" for this suite. 01/30/23 23:23:51.854
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-853" for this suite. 01/30/23 23:23:51.884
------------------------------
• [SLOW TEST] [90.551 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:22:21.361
    Jan 30 23:22:21.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:22:21.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:22:21.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:22:21.432
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 23:22:21.527: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:23:21.685: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:23:21.704
    Jan 30 23:23:21.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption-path 01/30/23 23:23:21.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:21.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:21.794
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 01/30/23 23:23:21.811
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 23:23:21.811
    Jan 30 23:23:21.848: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7135" to be "running"
    Jan 30 23:23:21.896: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 47.72039ms
    Jan 30 23:23:23.918: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069544383s
    Jan 30 23:23:25.916: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.068016471s
    Jan 30 23:23:25.916: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 23:23:25.932
    Jan 30 23:23:25.977: INFO: found a healthy node: 10.15.28.227
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jan 30 23:23:38.382: INFO: pods created so far: [1 1 1]
    Jan 30 23:23:38.382: INFO: length of pods created so far: 3
    Jan 30 23:23:44.437: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:23:51.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:23:51.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7135" for this suite. 01/30/23 23:23:51.854
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-853" for this suite. 01/30/23 23:23:51.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:23:51.918
Jan 30 23:23:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:23:51.919
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:52.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:52.023
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-285720de-e543-4925-b677-f3831d5b305a 01/30/23 23:23:52.039
STEP: Creating a pod to test consume secrets 01/30/23 23:23:52.062
Jan 30 23:23:52.125: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f" in namespace "projected-1318" to be "Succeeded or Failed"
Jan 30 23:23:52.146: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.845909ms
Jan 30 23:23:54.165: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039960602s
Jan 30 23:23:56.176: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051587812s
Jan 30 23:23:58.168: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042864179s
STEP: Saw pod success 01/30/23 23:23:58.168
Jan 30 23:23:58.169: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f" satisfied condition "Succeeded or Failed"
Jan 30 23:23:58.187: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:23:58.297
Jan 30 23:23:58.339: INFO: Waiting for pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f to disappear
Jan 30 23:23:58.361: INFO: Pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 23:23:58.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1318" for this suite. 01/30/23 23:23:58.421
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:23:51.918
    Jan 30 23:23:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:23:51.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:52.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:52.023
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-285720de-e543-4925-b677-f3831d5b305a 01/30/23 23:23:52.039
    STEP: Creating a pod to test consume secrets 01/30/23 23:23:52.062
    Jan 30 23:23:52.125: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f" in namespace "projected-1318" to be "Succeeded or Failed"
    Jan 30 23:23:52.146: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.845909ms
    Jan 30 23:23:54.165: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039960602s
    Jan 30 23:23:56.176: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051587812s
    Jan 30 23:23:58.168: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042864179s
    STEP: Saw pod success 01/30/23 23:23:58.168
    Jan 30 23:23:58.169: INFO: Pod "pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f" satisfied condition "Succeeded or Failed"
    Jan 30 23:23:58.187: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:23:58.297
    Jan 30 23:23:58.339: INFO: Waiting for pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f to disappear
    Jan 30 23:23:58.361: INFO: Pod pod-projected-secrets-00910385-f3c4-4598-89f0-4e19646b5a5f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:23:58.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1318" for this suite. 01/30/23 23:23:58.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:23:58.453
Jan 30 23:23:58.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 23:23:58.456
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:58.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:58.575
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/30/23 23:23:58.591
STEP: Counting existing ResourceQuota 01/30/23 23:24:03.607
STEP: Creating a ResourceQuota 01/30/23 23:24:08.63
STEP: Ensuring resource quota status is calculated 01/30/23 23:24:08.655
STEP: Creating a Secret 01/30/23 23:24:10.677
STEP: Ensuring resource quota status captures secret creation 01/30/23 23:24:10.74
STEP: Deleting a secret 01/30/23 23:24:12.769
STEP: Ensuring resource quota status released usage 01/30/23 23:24:12.798
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 23:24:14.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7788" for this suite. 01/30/23 23:24:14.845
------------------------------
• [SLOW TEST] [16.436 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:23:58.453
    Jan 30 23:23:58.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 23:23:58.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:23:58.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:23:58.575
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/30/23 23:23:58.591
    STEP: Counting existing ResourceQuota 01/30/23 23:24:03.607
    STEP: Creating a ResourceQuota 01/30/23 23:24:08.63
    STEP: Ensuring resource quota status is calculated 01/30/23 23:24:08.655
    STEP: Creating a Secret 01/30/23 23:24:10.677
    STEP: Ensuring resource quota status captures secret creation 01/30/23 23:24:10.74
    STEP: Deleting a secret 01/30/23 23:24:12.769
    STEP: Ensuring resource quota status released usage 01/30/23 23:24:12.798
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:24:14.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7788" for this suite. 01/30/23 23:24:14.845
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:24:14.89
Jan 30 23:24:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:24:14.894
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:14.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:14.98
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-8239127a-d9e7-445b-a682-4a545a6015e9 01/30/23 23:24:14.996
STEP: Creating a pod to test consume secrets 01/30/23 23:24:15.017
Jan 30 23:24:15.054: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6" in namespace "projected-3909" to be "Succeeded or Failed"
Jan 30 23:24:15.090: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016039ms
Jan 30 23:24:17.119: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Running", Reason="", readiness=true. Elapsed: 2.064778446s
Jan 30 23:24:19.113: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Running", Reason="", readiness=false. Elapsed: 4.05880895s
Jan 30 23:24:21.110: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055226322s
STEP: Saw pod success 01/30/23 23:24:21.11
Jan 30 23:24:21.110: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6" satisfied condition "Succeeded or Failed"
Jan 30 23:24:21.127: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:24:21.165
Jan 30 23:24:21.219: INFO: Waiting for pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 to disappear
Jan 30 23:24:21.236: INFO: Pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 23:24:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3909" for this suite. 01/30/23 23:24:21.269
------------------------------
• [SLOW TEST] [6.419 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:24:14.89
    Jan 30 23:24:14.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:24:14.894
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:14.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:14.98
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-8239127a-d9e7-445b-a682-4a545a6015e9 01/30/23 23:24:14.996
    STEP: Creating a pod to test consume secrets 01/30/23 23:24:15.017
    Jan 30 23:24:15.054: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6" in namespace "projected-3909" to be "Succeeded or Failed"
    Jan 30 23:24:15.090: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016039ms
    Jan 30 23:24:17.119: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Running", Reason="", readiness=true. Elapsed: 2.064778446s
    Jan 30 23:24:19.113: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Running", Reason="", readiness=false. Elapsed: 4.05880895s
    Jan 30 23:24:21.110: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055226322s
    STEP: Saw pod success 01/30/23 23:24:21.11
    Jan 30 23:24:21.110: INFO: Pod "pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6" satisfied condition "Succeeded or Failed"
    Jan 30 23:24:21.127: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:24:21.165
    Jan 30 23:24:21.219: INFO: Waiting for pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 to disappear
    Jan 30 23:24:21.236: INFO: Pod pod-projected-secrets-18af69af-a474-4511-9ae2-fcddf7c00ba6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:24:21.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3909" for this suite. 01/30/23 23:24:21.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:24:21.32
Jan 30 23:24:21.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/30/23 23:24:21.322
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:21.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:21.38
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/30/23 23:24:21.394
Jan 30 23:24:21.425: INFO: Waiting up to 5m0s for pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6" in namespace "var-expansion-3401" to be "Succeeded or Failed"
Jan 30 23:24:21.456: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 31.520605ms
Jan 30 23:24:23.481: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056012676s
Jan 30 23:24:25.475: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04999059s
Jan 30 23:24:27.476: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051458815s
STEP: Saw pod success 01/30/23 23:24:27.477
Jan 30 23:24:27.477: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6" satisfied condition "Succeeded or Failed"
Jan 30 23:24:27.502: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 container dapi-container: <nil>
STEP: delete the pod 01/30/23 23:24:27.539
Jan 30 23:24:27.587: INFO: Waiting for pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 to disappear
Jan 30 23:24:27.602: INFO: Pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 23:24:27.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3401" for this suite. 01/30/23 23:24:27.624
------------------------------
• [SLOW TEST] [6.332 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:24:21.32
    Jan 30 23:24:21.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/30/23 23:24:21.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:21.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:21.38
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/30/23 23:24:21.394
    Jan 30 23:24:21.425: INFO: Waiting up to 5m0s for pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6" in namespace "var-expansion-3401" to be "Succeeded or Failed"
    Jan 30 23:24:21.456: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 31.520605ms
    Jan 30 23:24:23.481: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056012676s
    Jan 30 23:24:25.475: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04999059s
    Jan 30 23:24:27.476: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051458815s
    STEP: Saw pod success 01/30/23 23:24:27.477
    Jan 30 23:24:27.477: INFO: Pod "var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6" satisfied condition "Succeeded or Failed"
    Jan 30 23:24:27.502: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 23:24:27.539
    Jan 30 23:24:27.587: INFO: Waiting for pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 to disappear
    Jan 30 23:24:27.602: INFO: Pod var-expansion-ee422156-7b14-4c84-aaf4-306bb5b6d5d6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:24:27.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3401" for this suite. 01/30/23 23:24:27.624
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:24:27.656
Jan 30 23:24:27.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context-test 01/30/23 23:24:27.658
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:27.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:27.729
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 30 23:24:27.785: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5" in namespace "security-context-test-5968" to be "Succeeded or Failed"
Jan 30 23:24:27.804: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.138792ms
Jan 30 23:24:29.821: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035917847s
Jan 30 23:24:31.824: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038155199s
Jan 30 23:24:33.820: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0345058s
Jan 30 23:24:33.820: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5" satisfied condition "Succeeded or Failed"
Jan 30 23:24:33.873: INFO: Got logs for pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 23:24:33.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5968" for this suite. 01/30/23 23:24:33.905
------------------------------
• [SLOW TEST] [6.283 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:24:27.656
    Jan 30 23:24:27.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context-test 01/30/23 23:24:27.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:27.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:27.729
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 30 23:24:27.785: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5" in namespace "security-context-test-5968" to be "Succeeded or Failed"
    Jan 30 23:24:27.804: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.138792ms
    Jan 30 23:24:29.821: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035917847s
    Jan 30 23:24:31.824: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038155199s
    Jan 30 23:24:33.820: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0345058s
    Jan 30 23:24:33.820: INFO: Pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5" satisfied condition "Succeeded or Failed"
    Jan 30 23:24:33.873: INFO: Got logs for pod "busybox-privileged-false-0899b500-a0b8-4780-afdc-b9f710596dc5": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:24:33.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5968" for this suite. 01/30/23 23:24:33.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:24:33.945
Jan 30 23:24:33.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/30/23 23:24:33.947
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:33.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:34.014
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/30/23 23:24:51.048
STEP: Creating a ResourceQuota 01/30/23 23:24:56.065
STEP: Ensuring resource quota status is calculated 01/30/23 23:24:56.087
STEP: Creating a ConfigMap 01/30/23 23:24:58.108
STEP: Ensuring resource quota status captures configMap creation 01/30/23 23:24:58.158
STEP: Deleting a ConfigMap 01/30/23 23:25:00.177
STEP: Ensuring resource quota status released usage 01/30/23 23:25:00.21
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 30 23:25:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1694" for this suite. 01/30/23 23:25:02.268
------------------------------
• [SLOW TEST] [28.353 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:24:33.945
    Jan 30 23:24:33.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/30/23 23:24:33.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:24:33.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:24:34.014
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/30/23 23:24:51.048
    STEP: Creating a ResourceQuota 01/30/23 23:24:56.065
    STEP: Ensuring resource quota status is calculated 01/30/23 23:24:56.087
    STEP: Creating a ConfigMap 01/30/23 23:24:58.108
    STEP: Ensuring resource quota status captures configMap creation 01/30/23 23:24:58.158
    STEP: Deleting a ConfigMap 01/30/23 23:25:00.177
    STEP: Ensuring resource quota status released usage 01/30/23 23:25:00.21
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:25:02.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1694" for this suite. 01/30/23 23:25:02.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:25:02.309
Jan 30 23:25:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:25:02.311
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:02.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:02.386
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/30/23 23:25:02.404
STEP: setting up watch 01/30/23 23:25:02.404
STEP: submitting the pod to kubernetes 01/30/23 23:25:02.523
STEP: verifying the pod is in kubernetes 01/30/23 23:25:02.566
STEP: verifying pod creation was observed 01/30/23 23:25:02.585
Jan 30 23:25:02.586: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3" in namespace "pods-1672" to be "running"
Jan 30 23:25:02.606: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.411585ms
Jan 30 23:25:04.624: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038287939s
Jan 30 23:25:06.625: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Running", Reason="", readiness=true. Elapsed: 4.039632196s
Jan 30 23:25:06.626: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3" satisfied condition "running"
STEP: deleting the pod gracefully 01/30/23 23:25:06.645
STEP: verifying pod deletion was observed 01/30/23 23:25:06.676
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:25:08.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1672" for this suite. 01/30/23 23:25:08.229
------------------------------
• [SLOW TEST] [5.945 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:25:02.309
    Jan 30 23:25:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:25:02.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:02.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:02.386
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/30/23 23:25:02.404
    STEP: setting up watch 01/30/23 23:25:02.404
    STEP: submitting the pod to kubernetes 01/30/23 23:25:02.523
    STEP: verifying the pod is in kubernetes 01/30/23 23:25:02.566
    STEP: verifying pod creation was observed 01/30/23 23:25:02.585
    Jan 30 23:25:02.586: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3" in namespace "pods-1672" to be "running"
    Jan 30 23:25:02.606: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.411585ms
    Jan 30 23:25:04.624: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038287939s
    Jan 30 23:25:06.625: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3": Phase="Running", Reason="", readiness=true. Elapsed: 4.039632196s
    Jan 30 23:25:06.626: INFO: Pod "pod-submit-remove-f2550f77-5891-463f-9306-1f8bd3d930a3" satisfied condition "running"
    STEP: deleting the pod gracefully 01/30/23 23:25:06.645
    STEP: verifying pod deletion was observed 01/30/23 23:25:06.676
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:25:08.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1672" for this suite. 01/30/23 23:25:08.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:25:08.259
Jan 30 23:25:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:25:08.261
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:08.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:08.345
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/30/23 23:25:08.36
Jan 30 23:25:08.402: INFO: Waiting up to 5m0s for pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798" in namespace "downward-api-2471" to be "Succeeded or Failed"
Jan 30 23:25:08.428: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 25.944996ms
Jan 30 23:25:10.461: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058495434s
Jan 30 23:25:12.448: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046212334s
Jan 30 23:25:14.453: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050988489s
STEP: Saw pod success 01/30/23 23:25:14.453
Jan 30 23:25:14.454: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798" satisfied condition "Succeeded or Failed"
Jan 30 23:25:14.486: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 container dapi-container: <nil>
STEP: delete the pod 01/30/23 23:25:14.529
Jan 30 23:25:14.585: INFO: Waiting for pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 to disappear
Jan 30 23:25:14.605: INFO: Pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 23:25:14.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2471" for this suite. 01/30/23 23:25:14.628
------------------------------
• [SLOW TEST] [6.397 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:25:08.259
    Jan 30 23:25:08.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:25:08.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:08.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:08.345
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/30/23 23:25:08.36
    Jan 30 23:25:08.402: INFO: Waiting up to 5m0s for pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798" in namespace "downward-api-2471" to be "Succeeded or Failed"
    Jan 30 23:25:08.428: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 25.944996ms
    Jan 30 23:25:10.461: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058495434s
    Jan 30 23:25:12.448: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046212334s
    Jan 30 23:25:14.453: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050988489s
    STEP: Saw pod success 01/30/23 23:25:14.453
    Jan 30 23:25:14.454: INFO: Pod "downward-api-796080be-68fd-421d-ab65-f08e3e1a7798" satisfied condition "Succeeded or Failed"
    Jan 30 23:25:14.486: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 23:25:14.529
    Jan 30 23:25:14.585: INFO: Waiting for pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 to disappear
    Jan 30 23:25:14.605: INFO: Pod downward-api-796080be-68fd-421d-ab65-f08e3e1a7798 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:25:14.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2471" for this suite. 01/30/23 23:25:14.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:25:14.673
Jan 30 23:25:14.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:25:14.674
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:14.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:14.746
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 in namespace container-probe-6284 01/30/23 23:25:14.763
Jan 30 23:25:14.801: INFO: Waiting up to 5m0s for pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932" in namespace "container-probe-6284" to be "not pending"
Jan 30 23:25:14.821: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Pending", Reason="", readiness=false. Elapsed: 19.525618ms
Jan 30 23:25:16.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041788632s
Jan 30 23:25:18.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Running", Reason="", readiness=true. Elapsed: 4.041326154s
Jan 30 23:25:18.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932" satisfied condition "not pending"
Jan 30 23:25:18.843: INFO: Started pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 in namespace container-probe-6284
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:25:18.843
Jan 30 23:25:18.865: INFO: Initial restart count of pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 is 0
STEP: deleting the pod 01/30/23 23:29:19.331
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 23:29:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6284" for this suite. 01/30/23 23:29:19.461
------------------------------
• [SLOW TEST] [244.831 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:25:14.673
    Jan 30 23:25:14.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:25:14.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:25:14.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:25:14.746
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 in namespace container-probe-6284 01/30/23 23:25:14.763
    Jan 30 23:25:14.801: INFO: Waiting up to 5m0s for pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932" in namespace "container-probe-6284" to be "not pending"
    Jan 30 23:25:14.821: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Pending", Reason="", readiness=false. Elapsed: 19.525618ms
    Jan 30 23:25:16.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041788632s
    Jan 30 23:25:18.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932": Phase="Running", Reason="", readiness=true. Elapsed: 4.041326154s
    Jan 30 23:25:18.843: INFO: Pod "busybox-f7628444-884e-4bb9-92f6-1c3f8db67932" satisfied condition "not pending"
    Jan 30 23:25:18.843: INFO: Started pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 in namespace container-probe-6284
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:25:18.843
    Jan 30 23:25:18.865: INFO: Initial restart count of pod busybox-f7628444-884e-4bb9-92f6-1c3f8db67932 is 0
    STEP: deleting the pod 01/30/23 23:29:19.331
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:29:19.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6284" for this suite. 01/30/23 23:29:19.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:29:19.512
Jan 30 23:29:19.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-pred 01/30/23 23:29:19.514
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:19.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:19.599
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 30 23:29:19.618: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 30 23:29:19.682: INFO: Waiting for terminating namespaces to be deleted...
Jan 30 23:29:19.719: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.225 before test
Jan 30 23:29:19.761: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:29:19.761: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:29:19.761: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:29:19.761: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:29:19.761: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:29:19.761: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:29:19.761: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:29:19.761: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:29:19.761: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:29:19.761: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
Jan 30 23:29:19.761: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:29:19.761: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:29:19.761: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:29:19.761: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.762: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:29:19.762: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.762: INFO: 	Container e2e ready: true, restart count 0
Jan 30 23:29:19.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:29:19.762: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:29:19.762: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:29:19.762: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.227 before test
Jan 30 23:29:19.794: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:29:19.794: INFO: calico-typha-5fcb7c495f-9n9f2 from kube-system started at 2023-01-30 23:16:02 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:29:19.794: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:29:19.794: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:29:19.794: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:29:19.794: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:29:19.794: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:29:19.794: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 30 23:29:19.794: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:29:19.794: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 30 23:29:19.794: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.237 before test
Jan 30 23:29:19.845: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 30 23:29:19.845: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 30 23:29:19.845: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container calico-node ready: true, restart count 0
Jan 30 23:29:19.845: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container calico-typha ready: true, restart count 0
Jan 30 23:29:19.845: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:29:19.845: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.845: INFO: 	Container coredns ready: true, restart count 0
Jan 30 23:29:19.846: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container autoscaler ready: true, restart count 0
Jan 30 23:29:19.846: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 30 23:29:19.846: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 30 23:29:19.846: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 30 23:29:19.846: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 30 23:29:19.846: INFO: 	Container pause ready: true, restart count 0
Jan 30 23:29:19.846: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.846: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 30 23:29:19.846: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 30 23:29:19.847: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 30 23:29:19.847: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 30 23:29:19.847: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 30 23:29:19.847: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 30 23:29:19.847: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container config-watcher ready: true, restart count 0
Jan 30 23:29:19.847: INFO: 	Container metrics-server ready: true, restart count 0
Jan 30 23:29:19.847: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 30 23:29:19.847: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 30 23:29:19.847: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 30 23:29:19.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 30 23:29:19.847: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 23:29:19.847
Jan 30 23:29:19.887: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-374" to be "running"
Jan 30 23:29:19.908: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.840819ms
Jan 30 23:29:21.925: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038035906s
Jan 30 23:29:23.925: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.03875061s
Jan 30 23:29:23.925: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 23:29:23.943
STEP: Trying to apply a random label on the found node. 01/30/23 23:29:23.993
STEP: verifying the node has the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e 42 01/30/23 23:29:24.041
STEP: Trying to relaunch the pod, now with labels. 01/30/23 23:29:24.102
Jan 30 23:29:24.128: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-374" to be "not pending"
Jan 30 23:29:24.146: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 17.991143ms
Jan 30 23:29:26.165: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036995464s
Jan 30 23:29:28.181: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.053056818s
Jan 30 23:29:28.181: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e off the node 10.15.28.227 01/30/23 23:29:28.198
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e 01/30/23 23:29:28.263
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:29:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-374" for this suite. 01/30/23 23:29:28.338
------------------------------
• [SLOW TEST] [8.854 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:29:19.512
    Jan 30 23:29:19.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-pred 01/30/23 23:29:19.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:19.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:19.599
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 30 23:29:19.618: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 30 23:29:19.682: INFO: Waiting for terminating namespaces to be deleted...
    Jan 30 23:29:19.719: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.225 before test
    Jan 30 23:29:19.761: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
    Jan 30 23:29:19.761: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:29:19.761: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.762: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:29:19.762: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.762: INFO: 	Container e2e ready: true, restart count 0
    Jan 30 23:29:19.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:29:19.762: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:29:19.762: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:29:19.762: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.227 before test
    Jan 30 23:29:19.794: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: calico-typha-5fcb7c495f-9n9f2 from kube-system started at 2023-01-30 23:16:02 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 30 23:29:19.794: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.237 before test
    Jan 30 23:29:19.845: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 30 23:29:19.845: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 30 23:29:19.845: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container calico-node ready: true, restart count 0
    Jan 30 23:29:19.845: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 30 23:29:19.845: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:29:19.845: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.845: INFO: 	Container coredns ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: 	Container pause ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.846: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 30 23:29:19.846: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 30 23:29:19.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 30 23:29:19.847: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/30/23 23:29:19.847
    Jan 30 23:29:19.887: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-374" to be "running"
    Jan 30 23:29:19.908: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.840819ms
    Jan 30 23:29:21.925: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038035906s
    Jan 30 23:29:23.925: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.03875061s
    Jan 30 23:29:23.925: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/30/23 23:29:23.943
    STEP: Trying to apply a random label on the found node. 01/30/23 23:29:23.993
    STEP: verifying the node has the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e 42 01/30/23 23:29:24.041
    STEP: Trying to relaunch the pod, now with labels. 01/30/23 23:29:24.102
    Jan 30 23:29:24.128: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-374" to be "not pending"
    Jan 30 23:29:24.146: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 17.991143ms
    Jan 30 23:29:26.165: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036995464s
    Jan 30 23:29:28.181: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.053056818s
    Jan 30 23:29:28.181: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e off the node 10.15.28.227 01/30/23 23:29:28.198
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c6ce7f81-60e0-4b86-9d17-42bfecdca66e 01/30/23 23:29:28.263
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:29:28.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-374" for this suite. 01/30/23 23:29:28.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:29:28.375
Jan 30 23:29:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/30/23 23:29:28.376
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:28.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:28.469
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 30 23:29:28.485: INFO: Creating simple deployment test-new-deployment
Jan 30 23:29:28.550: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 30 23:29:30.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/30/23 23:29:32.64
STEP: updating a scale subresource 01/30/23 23:29:32.659
STEP: verifying the deployment Spec.Replicas was modified 01/30/23 23:29:32.68
STEP: Patch a scale subresource 01/30/23 23:29:32.763
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 23:29:32.980: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6232  265d07ab-4885-4f28-9f32-747dc2730824 33074 3 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-30 23:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b3af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-30 23:29:31 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 23:29:32 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 23:29:33.058: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-6232  54be6170-9922-4fd1-88c8-d79a408eba30 33076 3 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 265d07ab-4885-4f28-9f32-747dc2730824 0xc0045fb877 0xc0045fb878}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"265d07ab-4885-4f28-9f32-747dc2730824\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fb908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:29:33.109: INFO: Pod "test-new-deployment-7f5969cbc7-97plx" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-97plx test-new-deployment-7f5969cbc7- deployment-6232  4b9aed5f-1c1b-4e17-b925-4722680545e8 33077 0 2023-01-30 23:29:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3ba17 0xc001b3ba18}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrpsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrpsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 23:29:33.110: INFO: Pod "test-new-deployment-7f5969cbc7-lbwlh" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lbwlh test-new-deployment-7f5969cbc7- deployment-6232  19844abc-1d31-4387-9cd4-05099cf9ea98 33075 0 2023-01-30 23:29:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bb57 0xc001b3bb58}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nkrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nkrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-30 23:29:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 23:29:33.110: INFO: Pod "test-new-deployment-7f5969cbc7-st2z8" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-st2z8 test-new-deployment-7f5969cbc7- deployment-6232  2ed5ae3f-a602-4b2d-87e4-0ef950f3dd5a 33057 0 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c56c100ba367acf62e3e4fd2b03bd462d375946e899868ca5474237ff9767763 cni.projectcalico.org/podIP:172.30.199.58/32 cni.projectcalico.org/podIPs:172.30.199.58/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bd47 0xc001b3bd48}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:29:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:29:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8gxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8gxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.58,StartTime:2023-01-30 23:29:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:29:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4ed38a8e43943d0a1a679396cf3b9db19a0ce722bf5aaf675d3fd9d050a4d0af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 30 23:29:33.111: INFO: Pod "test-new-deployment-7f5969cbc7-thmv6" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-thmv6 test-new-deployment-7f5969cbc7- deployment-6232  8c10dd8a-1b3e-41f7-8627-da9a414ab799 33080 0 2023-01-30 23:29:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bf57 0xc001b3bf58}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8z2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8z2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 23:29:33.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6232" for this suite. 01/30/23 23:29:33.167
------------------------------
• [4.832 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:29:28.375
    Jan 30 23:29:28.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/30/23 23:29:28.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:28.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:28.469
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 30 23:29:28.485: INFO: Creating simple deployment test-new-deployment
    Jan 30 23:29:28.550: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Jan 30 23:29:30.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/30/23 23:29:32.64
    STEP: updating a scale subresource 01/30/23 23:29:32.659
    STEP: verifying the deployment Spec.Replicas was modified 01/30/23 23:29:32.68
    STEP: Patch a scale subresource 01/30/23 23:29:32.763
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 23:29:32.980: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6232  265d07ab-4885-4f28-9f32-747dc2730824 33074 3 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-30 23:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b3af28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-30 23:29:31 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-30 23:29:32 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 23:29:33.058: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-6232  54be6170-9922-4fd1-88c8-d79a408eba30 33076 3 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 265d07ab-4885-4f28-9f32-747dc2730824 0xc0045fb877 0xc0045fb878}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"265d07ab-4885-4f28-9f32-747dc2730824\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fb908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:29:33.109: INFO: Pod "test-new-deployment-7f5969cbc7-97plx" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-97plx test-new-deployment-7f5969cbc7- deployment-6232  4b9aed5f-1c1b-4e17-b925-4722680545e8 33077 0 2023-01-30 23:29:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3ba17 0xc001b3ba18}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrpsc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrpsc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 23:29:33.110: INFO: Pod "test-new-deployment-7f5969cbc7-lbwlh" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-lbwlh test-new-deployment-7f5969cbc7- deployment-6232  19844abc-1d31-4387-9cd4-05099cf9ea98 33075 0 2023-01-30 23:29:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bb57 0xc001b3bb58}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-30 23:29:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nkrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nkrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-30 23:29:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 23:29:33.110: INFO: Pod "test-new-deployment-7f5969cbc7-st2z8" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-st2z8 test-new-deployment-7f5969cbc7- deployment-6232  2ed5ae3f-a602-4b2d-87e4-0ef950f3dd5a 33057 0 2023-01-30 23:29:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c56c100ba367acf62e3e4fd2b03bd462d375946e899868ca5474237ff9767763 cni.projectcalico.org/podIP:172.30.199.58/32 cni.projectcalico.org/podIPs:172.30.199.58/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bd47 0xc001b3bd48}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:29:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:29:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8gxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8gxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:29:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.58,StartTime:2023-01-30 23:29:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:29:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4ed38a8e43943d0a1a679396cf3b9db19a0ce722bf5aaf675d3fd9d050a4d0af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 30 23:29:33.111: INFO: Pod "test-new-deployment-7f5969cbc7-thmv6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-thmv6 test-new-deployment-7f5969cbc7- deployment-6232  8c10dd8a-1b3e-41f7-8627-da9a414ab799 33080 0 2023-01-30 23:29:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 54be6170-9922-4fd1-88c8-d79a408eba30 0xc001b3bf57 0xc001b3bf58}] [] [{kube-controller-manager Update v1 2023-01-30 23:29:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54be6170-9922-4fd1-88c8-d79a408eba30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t8z2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t8z2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:29:33.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6232" for this suite. 01/30/23 23:29:33.167
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:29:33.211
Jan 30 23:29:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename aggregator 01/30/23 23:29:33.214
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:33.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:33.306
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 30 23:29:33.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/30/23 23:29:33.326
Jan 30 23:29:33.888: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 30 23:29:36.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:38.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:40.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:42.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:44.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:46.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:48.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:50.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:52.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:54.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:56.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:29:58.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:30:00.467: INFO: Waited 220.638671ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/30/23 23:30:00.756
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/30/23 23:30:00.775
STEP: List APIServices 01/30/23 23:30:00.802
Jan 30 23:30:00.834: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:01.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-3923" for this suite. 01/30/23 23:30:01.473
------------------------------
• [SLOW TEST] [28.310 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:29:33.211
    Jan 30 23:29:33.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename aggregator 01/30/23 23:29:33.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:29:33.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:29:33.306
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 30 23:29:33.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/30/23 23:29:33.326
    Jan 30 23:29:33.888: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 30 23:29:36.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:38.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:40.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:42.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:44.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:46.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:48.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:50.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:52.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:54.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:56.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:29:58.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 29, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 29, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:30:00.467: INFO: Waited 220.638671ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/30/23 23:30:00.756
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/30/23 23:30:00.775
    STEP: List APIServices 01/30/23 23:30:00.802
    Jan 30 23:30:00.834: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:01.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-3923" for this suite. 01/30/23 23:30:01.473
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:01.524
Jan 30 23:30:01.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:30:01.526
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:01.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:01.603
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/30/23 23:30:01.62
Jan 30 23:30:01.684: INFO: Waiting up to 5m0s for pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a" in namespace "downward-api-1233" to be "Succeeded or Failed"
Jan 30 23:30:01.716: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.093734ms
Jan 30 23:30:03.735: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050536271s
Jan 30 23:30:05.734: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04862355s
Jan 30 23:30:07.736: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051025726s
STEP: Saw pod success 01/30/23 23:30:07.736
Jan 30 23:30:07.736: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a" satisfied condition "Succeeded or Failed"
Jan 30 23:30:07.757: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a container dapi-container: <nil>
STEP: delete the pod 01/30/23 23:30:07.883
Jan 30 23:30:07.940: INFO: Waiting for pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a to disappear
Jan 30 23:30:07.959: INFO: Pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:07.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1233" for this suite. 01/30/23 23:30:07.985
------------------------------
• [SLOW TEST] [6.488 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:01.524
    Jan 30 23:30:01.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:30:01.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:01.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:01.603
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/30/23 23:30:01.62
    Jan 30 23:30:01.684: INFO: Waiting up to 5m0s for pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a" in namespace "downward-api-1233" to be "Succeeded or Failed"
    Jan 30 23:30:01.716: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.093734ms
    Jan 30 23:30:03.735: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050536271s
    Jan 30 23:30:05.734: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04862355s
    Jan 30 23:30:07.736: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051025726s
    STEP: Saw pod success 01/30/23 23:30:07.736
    Jan 30 23:30:07.736: INFO: Pod "downward-api-9b998e3d-5061-4910-9411-cae19d28538a" satisfied condition "Succeeded or Failed"
    Jan 30 23:30:07.757: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a container dapi-container: <nil>
    STEP: delete the pod 01/30/23 23:30:07.883
    Jan 30 23:30:07.940: INFO: Waiting for pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a to disappear
    Jan 30 23:30:07.959: INFO: Pod downward-api-9b998e3d-5061-4910-9411-cae19d28538a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:07.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1233" for this suite. 01/30/23 23:30:07.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:08.017
Jan 30 23:30:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:30:08.02
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:08.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:08.089
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-801 01/30/23 23:30:08.105
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[] 01/30/23 23:30:08.157
Jan 30 23:30:08.202: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-801 01/30/23 23:30:08.202
Jan 30 23:30:08.236: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-801" to be "running and ready"
Jan 30 23:30:08.251: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.763751ms
Jan 30 23:30:08.251: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:10.266: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030515065s
Jan 30 23:30:10.266: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:12.269: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.032938373s
Jan 30 23:30:12.269: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 30 23:30:12.269: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100]] 01/30/23 23:30:12.285
Jan 30 23:30:12.344: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-801 01/30/23 23:30:12.344
Jan 30 23:30:12.366: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-801" to be "running and ready"
Jan 30 23:30:12.382: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.778537ms
Jan 30 23:30:12.383: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:14.413: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047238962s
Jan 30 23:30:14.413: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:16.401: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034893262s
Jan 30 23:30:16.401: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 30 23:30:16.401: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100] pod2:[101]] 01/30/23 23:30:16.418
Jan 30 23:30:16.500: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/30/23 23:30:16.5
Jan 30 23:30:16.501: INFO: Creating new exec pod
Jan 30 23:30:16.521: INFO: Waiting up to 5m0s for pod "execpodf6jtq" in namespace "services-801" to be "running"
Jan 30 23:30:16.538: INFO: Pod "execpodf6jtq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.945453ms
Jan 30 23:30:18.557: INFO: Pod "execpodf6jtq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035470714s
Jan 30 23:30:20.559: INFO: Pod "execpodf6jtq": Phase="Running", Reason="", readiness=true. Elapsed: 4.037382658s
Jan 30 23:30:20.559: INFO: Pod "execpodf6jtq" satisfied condition "running"
Jan 30 23:30:21.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 30 23:30:21.967: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 30 23:30:21.967: INFO: stdout: ""
Jan 30 23:30:21.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 172.21.48.77 80'
Jan 30 23:30:22.335: INFO: stderr: "+ nc -v -z -w 2 172.21.48.77 80\nConnection to 172.21.48.77 80 port [tcp/http] succeeded!\n"
Jan 30 23:30:22.336: INFO: stdout: ""
Jan 30 23:30:22.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 30 23:30:22.705: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 30 23:30:22.705: INFO: stdout: ""
Jan 30 23:30:22.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 172.21.48.77 81'
Jan 30 23:30:23.174: INFO: stderr: "+ nc -v -z -w 2 172.21.48.77 81\nConnection to 172.21.48.77 81 port [tcp/*] succeeded!\n"
Jan 30 23:30:23.174: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-801 01/30/23 23:30:23.174
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod2:[101]] 01/30/23 23:30:23.232
Jan 30 23:30:23.302: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-801 01/30/23 23:30:23.302
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[] 01/30/23 23:30:23.349
Jan 30 23:30:23.394: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-801" for this suite. 01/30/23 23:30:23.514
------------------------------
• [SLOW TEST] [15.523 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:08.017
    Jan 30 23:30:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:30:08.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:08.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:08.089
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-801 01/30/23 23:30:08.105
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[] 01/30/23 23:30:08.157
    Jan 30 23:30:08.202: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-801 01/30/23 23:30:08.202
    Jan 30 23:30:08.236: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-801" to be "running and ready"
    Jan 30 23:30:08.251: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.763751ms
    Jan 30 23:30:08.251: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:10.266: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030515065s
    Jan 30 23:30:10.266: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:12.269: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.032938373s
    Jan 30 23:30:12.269: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 30 23:30:12.269: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100]] 01/30/23 23:30:12.285
    Jan 30 23:30:12.344: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-801 01/30/23 23:30:12.344
    Jan 30 23:30:12.366: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-801" to be "running and ready"
    Jan 30 23:30:12.382: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.778537ms
    Jan 30 23:30:12.383: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:14.413: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047238962s
    Jan 30 23:30:14.413: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:16.401: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034893262s
    Jan 30 23:30:16.401: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 30 23:30:16.401: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod1:[100] pod2:[101]] 01/30/23 23:30:16.418
    Jan 30 23:30:16.500: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/30/23 23:30:16.5
    Jan 30 23:30:16.501: INFO: Creating new exec pod
    Jan 30 23:30:16.521: INFO: Waiting up to 5m0s for pod "execpodf6jtq" in namespace "services-801" to be "running"
    Jan 30 23:30:16.538: INFO: Pod "execpodf6jtq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.945453ms
    Jan 30 23:30:18.557: INFO: Pod "execpodf6jtq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035470714s
    Jan 30 23:30:20.559: INFO: Pod "execpodf6jtq": Phase="Running", Reason="", readiness=true. Elapsed: 4.037382658s
    Jan 30 23:30:20.559: INFO: Pod "execpodf6jtq" satisfied condition "running"
    Jan 30 23:30:21.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 30 23:30:21.967: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 30 23:30:21.967: INFO: stdout: ""
    Jan 30 23:30:21.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 172.21.48.77 80'
    Jan 30 23:30:22.335: INFO: stderr: "+ nc -v -z -w 2 172.21.48.77 80\nConnection to 172.21.48.77 80 port [tcp/http] succeeded!\n"
    Jan 30 23:30:22.336: INFO: stdout: ""
    Jan 30 23:30:22.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 30 23:30:22.705: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 30 23:30:22.705: INFO: stdout: ""
    Jan 30 23:30:22.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-801 exec execpodf6jtq -- /bin/sh -x -c nc -v -z -w 2 172.21.48.77 81'
    Jan 30 23:30:23.174: INFO: stderr: "+ nc -v -z -w 2 172.21.48.77 81\nConnection to 172.21.48.77 81 port [tcp/*] succeeded!\n"
    Jan 30 23:30:23.174: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-801 01/30/23 23:30:23.174
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[pod2:[101]] 01/30/23 23:30:23.232
    Jan 30 23:30:23.302: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-801 01/30/23 23:30:23.302
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-801 to expose endpoints map[] 01/30/23 23:30:23.349
    Jan 30 23:30:23.394: INFO: successfully validated that service multi-endpoint-test in namespace services-801 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-801" for this suite. 01/30/23 23:30:23.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:23.541
Jan 30 23:30:23.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 23:30:23.543
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:23.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:23.613
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/30/23 23:30:23.626
Jan 30 23:30:23.664: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5186" to be "running and ready"
Jan 30 23:30:23.685: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 20.443769ms
Jan 30 23:30:23.685: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:25.705: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.040783349s
Jan 30 23:30:25.705: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 30 23:30:25.706: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/30/23 23:30:25.724
STEP: Then the orphan pod is adopted 01/30/23 23:30:25.749
STEP: When the matched label of one of its pods change 01/30/23 23:30:26.791
Jan 30 23:30:26.810: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/30/23 23:30:26.852
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:27.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5186" for this suite. 01/30/23 23:30:27.917
------------------------------
• [4.405 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:23.541
    Jan 30 23:30:23.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 23:30:23.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:23.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:23.613
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/30/23 23:30:23.626
    Jan 30 23:30:23.664: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5186" to be "running and ready"
    Jan 30 23:30:23.685: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 20.443769ms
    Jan 30 23:30:23.685: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:25.705: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.040783349s
    Jan 30 23:30:25.705: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 30 23:30:25.706: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/30/23 23:30:25.724
    STEP: Then the orphan pod is adopted 01/30/23 23:30:25.749
    STEP: When the matched label of one of its pods change 01/30/23 23:30:26.791
    Jan 30 23:30:26.810: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/30/23 23:30:26.852
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:27.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5186" for this suite. 01/30/23 23:30:27.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:27.948
Jan 30 23:30:27.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/30/23 23:30:27.95
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:28.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:28.02
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/30/23 23:30:28.045
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:28.097
STEP: Creating a pod in the namespace 01/30/23 23:30:28.113
STEP: Waiting for the pod to have running status 01/30/23 23:30:28.149
Jan 30 23:30:28.149: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5132" to be "running"
Jan 30 23:30:28.166: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.14155ms
Jan 30 23:30:30.185: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035930761s
Jan 30 23:30:32.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.037007281s
Jan 30 23:30:32.186: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/30/23 23:30:32.186
STEP: Waiting for the namespace to be removed. 01/30/23 23:30:32.214
STEP: Recreating the namespace 01/30/23 23:30:45.233
STEP: Verifying there are no pods in the namespace 01/30/23 23:30:45.296
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:45.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2714" for this suite. 01/30/23 23:30:45.341
STEP: Destroying namespace "nsdeletetest-5132" for this suite. 01/30/23 23:30:45.374
Jan 30 23:30:45.390: INFO: Namespace nsdeletetest-5132 was already deleted
STEP: Destroying namespace "nsdeletetest-1986" for this suite. 01/30/23 23:30:45.39
------------------------------
• [SLOW TEST] [17.485 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:27.948
    Jan 30 23:30:27.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/30/23 23:30:27.95
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:28.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:28.02
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/30/23 23:30:28.045
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:28.097
    STEP: Creating a pod in the namespace 01/30/23 23:30:28.113
    STEP: Waiting for the pod to have running status 01/30/23 23:30:28.149
    Jan 30 23:30:28.149: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5132" to be "running"
    Jan 30 23:30:28.166: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.14155ms
    Jan 30 23:30:30.185: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035930761s
    Jan 30 23:30:32.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.037007281s
    Jan 30 23:30:32.186: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/30/23 23:30:32.186
    STEP: Waiting for the namespace to be removed. 01/30/23 23:30:32.214
    STEP: Recreating the namespace 01/30/23 23:30:45.233
    STEP: Verifying there are no pods in the namespace 01/30/23 23:30:45.296
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:45.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2714" for this suite. 01/30/23 23:30:45.341
    STEP: Destroying namespace "nsdeletetest-5132" for this suite. 01/30/23 23:30:45.374
    Jan 30 23:30:45.390: INFO: Namespace nsdeletetest-5132 was already deleted
    STEP: Destroying namespace "nsdeletetest-1986" for this suite. 01/30/23 23:30:45.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:45.441
Jan 30 23:30:45.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:30:45.444
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:45.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:45.518
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/30/23 23:30:45.534
Jan 30 23:30:45.580: INFO: Waiting up to 5m0s for pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3" in namespace "pods-5774" to be "running and ready"
Jan 30 23:30:45.606: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.668363ms
Jan 30 23:30:45.606: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:47.625: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045072381s
Jan 30 23:30:47.625: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:30:49.625: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.045271273s
Jan 30 23:30:49.625: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Running (Ready = true)
Jan 30 23:30:49.626: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3" satisfied condition "running and ready"
Jan 30 23:30:49.660: INFO: Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 has hostIP: 10.15.28.227
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:49.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5774" for this suite. 01/30/23 23:30:49.686
------------------------------
• [4.274 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:45.441
    Jan 30 23:30:45.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:30:45.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:45.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:45.518
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/30/23 23:30:45.534
    Jan 30 23:30:45.580: INFO: Waiting up to 5m0s for pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3" in namespace "pods-5774" to be "running and ready"
    Jan 30 23:30:45.606: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.668363ms
    Jan 30 23:30:45.606: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:47.625: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045072381s
    Jan 30 23:30:47.625: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:30:49.625: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.045271273s
    Jan 30 23:30:49.625: INFO: The phase of Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 is Running (Ready = true)
    Jan 30 23:30:49.626: INFO: Pod "pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3" satisfied condition "running and ready"
    Jan 30 23:30:49.660: INFO: Pod pod-hostip-ba357976-245f-4f23-815a-8e3d076c36c3 has hostIP: 10.15.28.227
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:49.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5774" for this suite. 01/30/23 23:30:49.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:49.732
Jan 30 23:30:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context-test 01/30/23 23:30:49.734
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:49.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:49.809
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 30 23:30:49.863: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5" in namespace "security-context-test-9808" to be "Succeeded or Failed"
Jan 30 23:30:49.881: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.378488ms
Jan 30 23:30:51.902: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038787133s
Jan 30 23:30:53.900: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Running", Reason="", readiness=true. Elapsed: 4.036935208s
Jan 30 23:30:55.898: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Running", Reason="", readiness=false. Elapsed: 6.035168893s
Jan 30 23:30:57.902: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038733178s
Jan 30 23:30:57.903: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 30 23:30:57.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9808" for this suite. 01/30/23 23:30:57.972
------------------------------
• [SLOW TEST] [8.270 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:49.732
    Jan 30 23:30:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context-test 01/30/23 23:30:49.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:49.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:49.809
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 30 23:30:49.863: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5" in namespace "security-context-test-9808" to be "Succeeded or Failed"
    Jan 30 23:30:49.881: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.378488ms
    Jan 30 23:30:51.902: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038787133s
    Jan 30 23:30:53.900: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Running", Reason="", readiness=true. Elapsed: 4.036935208s
    Jan 30 23:30:55.898: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Running", Reason="", readiness=false. Elapsed: 6.035168893s
    Jan 30 23:30:57.902: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.038733178s
    Jan 30 23:30:57.903: INFO: Pod "alpine-nnp-false-478b29fe-0b27-4b74-aa9f-b654ebe928b5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:30:57.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9808" for this suite. 01/30/23 23:30:57.972
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:30:58.003
Jan 30 23:30:58.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:30:58.006
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:58.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:58.078
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-3673 01/30/23 23:30:58.105
STEP: creating replication controller nodeport-test in namespace services-3673 01/30/23 23:30:58.177
I0130 23:30:58.204004      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3673, replica count: 2
I0130 23:31:01.256296      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:31:01.256: INFO: Creating new exec pod
Jan 30 23:31:01.276: INFO: Waiting up to 5m0s for pod "execpod96fj6" in namespace "services-3673" to be "running"
Jan 30 23:31:01.322: INFO: Pod "execpod96fj6": Phase="Pending", Reason="", readiness=false. Elapsed: 45.5277ms
Jan 30 23:31:03.342: INFO: Pod "execpod96fj6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065077537s
Jan 30 23:31:05.340: INFO: Pod "execpod96fj6": Phase="Running", Reason="", readiness=true. Elapsed: 4.06320289s
Jan 30 23:31:05.340: INFO: Pod "execpod96fj6" satisfied condition "running"
Jan 30 23:31:06.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 30 23:31:06.793: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 30 23:31:06.793: INFO: stdout: ""
Jan 30 23:31:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 172.21.148.70 80'
Jan 30 23:31:07.174: INFO: stderr: "+ nc -v -z -w 2 172.21.148.70 80\nConnection to 172.21.148.70 80 port [tcp/http] succeeded!\n"
Jan 30 23:31:07.174: INFO: stdout: ""
Jan 30 23:31:07.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 31246'
Jan 30 23:31:07.574: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 31246\nConnection to 10.15.28.225 31246 port [tcp/*] succeeded!\n"
Jan 30 23:31:07.574: INFO: stdout: ""
Jan 30 23:31:07.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 31246'
Jan 30 23:31:07.968: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 31246\nConnection to 10.15.28.237 31246 port [tcp/*] succeeded!\n"
Jan 30 23:31:07.968: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3673" for this suite. 01/30/23 23:31:07.997
------------------------------
• [SLOW TEST] [10.050 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:30:58.003
    Jan 30 23:30:58.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:30:58.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:30:58.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:30:58.078
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-3673 01/30/23 23:30:58.105
    STEP: creating replication controller nodeport-test in namespace services-3673 01/30/23 23:30:58.177
    I0130 23:30:58.204004      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3673, replica count: 2
    I0130 23:31:01.256296      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:31:01.256: INFO: Creating new exec pod
    Jan 30 23:31:01.276: INFO: Waiting up to 5m0s for pod "execpod96fj6" in namespace "services-3673" to be "running"
    Jan 30 23:31:01.322: INFO: Pod "execpod96fj6": Phase="Pending", Reason="", readiness=false. Elapsed: 45.5277ms
    Jan 30 23:31:03.342: INFO: Pod "execpod96fj6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065077537s
    Jan 30 23:31:05.340: INFO: Pod "execpod96fj6": Phase="Running", Reason="", readiness=true. Elapsed: 4.06320289s
    Jan 30 23:31:05.340: INFO: Pod "execpod96fj6" satisfied condition "running"
    Jan 30 23:31:06.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 30 23:31:06.793: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 30 23:31:06.793: INFO: stdout: ""
    Jan 30 23:31:06.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 172.21.148.70 80'
    Jan 30 23:31:07.174: INFO: stderr: "+ nc -v -z -w 2 172.21.148.70 80\nConnection to 172.21.148.70 80 port [tcp/http] succeeded!\n"
    Jan 30 23:31:07.174: INFO: stdout: ""
    Jan 30 23:31:07.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 31246'
    Jan 30 23:31:07.574: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 31246\nConnection to 10.15.28.225 31246 port [tcp/*] succeeded!\n"
    Jan 30 23:31:07.574: INFO: stdout: ""
    Jan 30 23:31:07.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3673 exec execpod96fj6 -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 31246'
    Jan 30 23:31:07.968: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 31246\nConnection to 10.15.28.237 31246 port [tcp/*] succeeded!\n"
    Jan 30 23:31:07.968: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3673" for this suite. 01/30/23 23:31:07.997
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:08.055
Jan 30 23:31:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename watch 01/30/23 23:31:08.057
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:08.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:08.164
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/30/23 23:31:08.178
STEP: creating a new configmap 01/30/23 23:31:08.186
STEP: modifying the configmap once 01/30/23 23:31:08.207
STEP: changing the label value of the configmap 01/30/23 23:31:08.245
STEP: Expecting to observe a delete notification for the watched object 01/30/23 23:31:08.286
Jan 30 23:31:08.287: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33714 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:31:08.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33715 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:31:08.288: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33716 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/30/23 23:31:08.288
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/30/23 23:31:08.324
STEP: changing the label value of the configmap back 01/30/23 23:31:18.325
STEP: modifying the configmap a third time 01/30/23 23:31:18.376
STEP: deleting the configmap 01/30/23 23:31:18.417
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/30/23 23:31:18.448
Jan 30 23:31:18.448: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33778 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:31:18.449: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33779 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 30 23:31:18.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33780 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5587" for this suite. 01/30/23 23:31:18.482
------------------------------
• [SLOW TEST] [10.461 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:08.055
    Jan 30 23:31:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename watch 01/30/23 23:31:08.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:08.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:08.164
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/30/23 23:31:08.178
    STEP: creating a new configmap 01/30/23 23:31:08.186
    STEP: modifying the configmap once 01/30/23 23:31:08.207
    STEP: changing the label value of the configmap 01/30/23 23:31:08.245
    STEP: Expecting to observe a delete notification for the watched object 01/30/23 23:31:08.286
    Jan 30 23:31:08.287: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33714 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:31:08.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33715 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:31:08.288: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33716 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/30/23 23:31:08.288
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/30/23 23:31:08.324
    STEP: changing the label value of the configmap back 01/30/23 23:31:18.325
    STEP: modifying the configmap a third time 01/30/23 23:31:18.376
    STEP: deleting the configmap 01/30/23 23:31:18.417
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/30/23 23:31:18.448
    Jan 30 23:31:18.448: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33778 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:31:18.449: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33779 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 30 23:31:18.450: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5587  3365ed5a-e05e-4fee-822b-75bfd9fe17d5 33780 0 2023-01-30 23:31:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-30 23:31:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:18.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5587" for this suite. 01/30/23 23:31:18.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:18.52
Jan 30 23:31:18.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 23:31:18.523
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:18.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:18.605
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 23:31:18.62
Jan 30 23:31:18.658: INFO: Waiting up to 5m0s for pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4" in namespace "emptydir-9610" to be "Succeeded or Failed"
Jan 30 23:31:18.684: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.319395ms
Jan 30 23:31:20.701: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042925873s
Jan 30 23:31:22.704: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0462735s
Jan 30 23:31:24.705: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046706256s
STEP: Saw pod success 01/30/23 23:31:24.705
Jan 30 23:31:24.706: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4" satisfied condition "Succeeded or Failed"
Jan 30 23:31:24.725: INFO: Trying to get logs from node 10.15.28.227 pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 container test-container: <nil>
STEP: delete the pod 01/30/23 23:31:24.762
Jan 30 23:31:24.816: INFO: Waiting for pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 to disappear
Jan 30 23:31:24.834: INFO: Pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:24.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9610" for this suite. 01/30/23 23:31:24.891
------------------------------
• [SLOW TEST] [6.401 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:18.52
    Jan 30 23:31:18.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 23:31:18.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:18.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:18.605
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 23:31:18.62
    Jan 30 23:31:18.658: INFO: Waiting up to 5m0s for pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4" in namespace "emptydir-9610" to be "Succeeded or Failed"
    Jan 30 23:31:18.684: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.319395ms
    Jan 30 23:31:20.701: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042925873s
    Jan 30 23:31:22.704: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0462735s
    Jan 30 23:31:24.705: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046706256s
    STEP: Saw pod success 01/30/23 23:31:24.705
    Jan 30 23:31:24.706: INFO: Pod "pod-c2ec4862-c170-4e93-9a62-d95f5debaca4" satisfied condition "Succeeded or Failed"
    Jan 30 23:31:24.725: INFO: Trying to get logs from node 10.15.28.227 pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 container test-container: <nil>
    STEP: delete the pod 01/30/23 23:31:24.762
    Jan 30 23:31:24.816: INFO: Waiting for pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 to disappear
    Jan 30 23:31:24.834: INFO: Pod pod-c2ec4862-c170-4e93-9a62-d95f5debaca4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:24.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9610" for this suite. 01/30/23 23:31:24.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:24.93
Jan 30 23:31:24.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/30/23 23:31:24.932
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:25.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:25.08
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-x4vc7" 01/30/23 23:31:25.099
Jan 30 23:31:25.137: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
Jan 30 23:31:26.156: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
Jan 30 23:31:26.181: INFO: Found 1 replicas for "e2e-rc-x4vc7" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-x4vc7" 01/30/23 23:31:26.181
STEP: Updating a scale subresource 01/30/23 23:31:26.204
STEP: Verifying replicas where modified for replication controller "e2e-rc-x4vc7" 01/30/23 23:31:26.23
Jan 30 23:31:26.231: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
Jan 30 23:31:27.250: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
Jan 30 23:31:27.268: INFO: Found 2 replicas for "e2e-rc-x4vc7" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:27.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9419" for this suite. 01/30/23 23:31:27.299
------------------------------
• [2.400 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:24.93
    Jan 30 23:31:24.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/30/23 23:31:24.932
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:25.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:25.08
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-x4vc7" 01/30/23 23:31:25.099
    Jan 30 23:31:25.137: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
    Jan 30 23:31:26.156: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
    Jan 30 23:31:26.181: INFO: Found 1 replicas for "e2e-rc-x4vc7" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-x4vc7" 01/30/23 23:31:26.181
    STEP: Updating a scale subresource 01/30/23 23:31:26.204
    STEP: Verifying replicas where modified for replication controller "e2e-rc-x4vc7" 01/30/23 23:31:26.23
    Jan 30 23:31:26.231: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
    Jan 30 23:31:27.250: INFO: Get Replication Controller "e2e-rc-x4vc7" to confirm replicas
    Jan 30 23:31:27.268: INFO: Found 2 replicas for "e2e-rc-x4vc7" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:27.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9419" for this suite. 01/30/23 23:31:27.299
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:27.335
Jan 30 23:31:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-runtime 01/30/23 23:31:27.338
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:27.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:27.406
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/30/23 23:31:27.422
STEP: wait for the container to reach Succeeded 01/30/23 23:31:27.466
STEP: get the container status 01/30/23 23:31:32.609
STEP: the container should be terminated 01/30/23 23:31:32.627
STEP: the termination message should be set 01/30/23 23:31:32.627
Jan 30 23:31:32.628: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/30/23 23:31:32.628
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:32.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-922" for this suite. 01/30/23 23:31:32.738
------------------------------
• [SLOW TEST] [5.430 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:27.335
    Jan 30 23:31:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-runtime 01/30/23 23:31:27.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:27.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:27.406
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/30/23 23:31:27.422
    STEP: wait for the container to reach Succeeded 01/30/23 23:31:27.466
    STEP: get the container status 01/30/23 23:31:32.609
    STEP: the container should be terminated 01/30/23 23:31:32.627
    STEP: the termination message should be set 01/30/23 23:31:32.627
    Jan 30 23:31:32.628: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/30/23 23:31:32.628
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:32.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-922" for this suite. 01/30/23 23:31:32.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:32.766
Jan 30 23:31:32.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-webhook 01/30/23 23:31:32.768
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:32.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:32.842
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/30/23 23:31:32.858
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 23:31:33.268
STEP: Deploying the custom resource conversion webhook pod 01/30/23 23:31:33.309
STEP: Wait for the deployment to be ready 01/30/23 23:31:33.358
Jan 30 23:31:33.402: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 30 23:31:35.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:31:37.487
STEP: Verifying the service has paired with the endpoint 01/30/23 23:31:37.548
Jan 30 23:31:38.548: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 30 23:31:38.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Creating a v1 custom resource 01/30/23 23:31:41.398
STEP: Create a v2 custom resource 01/30/23 23:31:41.483
STEP: List CRs in v1 01/30/23 23:31:41.629
STEP: List CRs in v2 01/30/23 23:31:41.67
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:42.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-2849" for this suite. 01/30/23 23:31:42.458
------------------------------
• [SLOW TEST] [9.726 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:32.766
    Jan 30 23:31:32.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-webhook 01/30/23 23:31:32.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:32.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:32.842
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/30/23 23:31:32.858
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 23:31:33.268
    STEP: Deploying the custom resource conversion webhook pod 01/30/23 23:31:33.309
    STEP: Wait for the deployment to be ready 01/30/23 23:31:33.358
    Jan 30 23:31:33.402: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 30 23:31:35.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 31, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:31:37.487
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:31:37.548
    Jan 30 23:31:38.548: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 30 23:31:38.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Creating a v1 custom resource 01/30/23 23:31:41.398
    STEP: Create a v2 custom resource 01/30/23 23:31:41.483
    STEP: List CRs in v1 01/30/23 23:31:41.629
    STEP: List CRs in v2 01/30/23 23:31:41.67
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:42.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-2849" for this suite. 01/30/23 23:31:42.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:42.494
Jan 30 23:31:42.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 23:31:42.496
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:42.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:42.573
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 23:31:42.669
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:31:43.197
STEP: Deploying the webhook pod 01/30/23 23:31:43.217
STEP: Wait for the deployment to be ready 01/30/23 23:31:43.262
Jan 30 23:31:43.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/30/23 23:31:45.361
STEP: Verifying the service has paired with the endpoint 01/30/23 23:31:45.418
Jan 30 23:31:46.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 23:31:46.436
STEP: create a pod that should be denied by the webhook 01/30/23 23:31:46.525
STEP: create a pod that causes the webhook to hang 01/30/23 23:31:46.606
STEP: create a configmap that should be denied by the webhook 01/30/23 23:31:56.64
STEP: create a configmap that should be admitted by the webhook 01/30/23 23:31:56.715
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 23:31:56.787
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 23:31:56.835
STEP: create a namespace that bypass the webhook 01/30/23 23:31:56.868
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/30/23 23:31:56.902
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:31:57.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3548" for this suite. 01/30/23 23:31:57.234
STEP: Destroying namespace "webhook-3548-markers" for this suite. 01/30/23 23:31:57.262
------------------------------
• [SLOW TEST] [14.799 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:42.494
    Jan 30 23:31:42.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 23:31:42.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:42.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:42.573
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 23:31:42.669
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:31:43.197
    STEP: Deploying the webhook pod 01/30/23 23:31:43.217
    STEP: Wait for the deployment to be ready 01/30/23 23:31:43.262
    Jan 30 23:31:43.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/30/23 23:31:45.361
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:31:45.418
    Jan 30 23:31:46.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/30/23 23:31:46.436
    STEP: create a pod that should be denied by the webhook 01/30/23 23:31:46.525
    STEP: create a pod that causes the webhook to hang 01/30/23 23:31:46.606
    STEP: create a configmap that should be denied by the webhook 01/30/23 23:31:56.64
    STEP: create a configmap that should be admitted by the webhook 01/30/23 23:31:56.715
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 23:31:56.787
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/30/23 23:31:56.835
    STEP: create a namespace that bypass the webhook 01/30/23 23:31:56.868
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/30/23 23:31:56.902
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:31:57.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3548" for this suite. 01/30/23 23:31:57.234
    STEP: Destroying namespace "webhook-3548-markers" for this suite. 01/30/23 23:31:57.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:31:57.301
Jan 30 23:31:57.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:31:57.303
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:57.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:57.387
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/30/23 23:31:57.401
Jan 30 23:31:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 30 23:31:59.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:10.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9370" for this suite. 01/30/23 23:32:10.292
------------------------------
• [SLOW TEST] [13.020 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:31:57.301
    Jan 30 23:31:57.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:31:57.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:31:57.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:31:57.387
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/30/23 23:31:57.401
    Jan 30 23:31:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 30 23:31:59.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:10.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9370" for this suite. 01/30/23 23:32:10.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:10.326
Jan 30 23:32:10.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:32:10.329
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:10.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:10.402
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a24b8452-071d-47fc-9b30-2fce09aabed8 01/30/23 23:32:10.558
STEP: Creating a pod to test consume secrets 01/30/23 23:32:10.578
Jan 30 23:32:10.614: INFO: Waiting up to 5m0s for pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e" in namespace "secrets-2694" to be "Succeeded or Failed"
Jan 30 23:32:10.631: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.02291ms
Jan 30 23:32:12.651: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036818617s
Jan 30 23:32:14.655: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041250477s
Jan 30 23:32:16.651: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037349033s
STEP: Saw pod success 01/30/23 23:32:16.651
Jan 30 23:32:16.652: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e" satisfied condition "Succeeded or Failed"
Jan 30 23:32:16.669: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:32:16.709
Jan 30 23:32:16.755: INFO: Waiting for pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e to disappear
Jan 30 23:32:16.773: INFO: Pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2694" for this suite. 01/30/23 23:32:16.794
STEP: Destroying namespace "secret-namespace-3459" for this suite. 01/30/23 23:32:16.823
------------------------------
• [SLOW TEST] [6.526 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:10.326
    Jan 30 23:32:10.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:32:10.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:10.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:10.402
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a24b8452-071d-47fc-9b30-2fce09aabed8 01/30/23 23:32:10.558
    STEP: Creating a pod to test consume secrets 01/30/23 23:32:10.578
    Jan 30 23:32:10.614: INFO: Waiting up to 5m0s for pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e" in namespace "secrets-2694" to be "Succeeded or Failed"
    Jan 30 23:32:10.631: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.02291ms
    Jan 30 23:32:12.651: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036818617s
    Jan 30 23:32:14.655: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041250477s
    Jan 30 23:32:16.651: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037349033s
    STEP: Saw pod success 01/30/23 23:32:16.651
    Jan 30 23:32:16.652: INFO: Pod "pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e" satisfied condition "Succeeded or Failed"
    Jan 30 23:32:16.669: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:32:16.709
    Jan 30 23:32:16.755: INFO: Waiting for pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e to disappear
    Jan 30 23:32:16.773: INFO: Pod pod-secrets-06d3c3b4-81a9-4fb0-bbd1-dea7d7a2ee3e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2694" for this suite. 01/30/23 23:32:16.794
    STEP: Destroying namespace "secret-namespace-3459" for this suite. 01/30/23 23:32:16.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:16.856
Jan 30 23:32:16.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename tables 01/30/23 23:32:16.859
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:16.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:16.952
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:16.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-8115" for this suite. 01/30/23 23:32:17.012
------------------------------
• [0.185 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:16.856
    Jan 30 23:32:16.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename tables 01/30/23 23:32:16.859
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:16.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:16.952
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:16.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-8115" for this suite. 01/30/23 23:32:17.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:17.047
Jan 30 23:32:17.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-webhook 01/30/23 23:32:17.049
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:17.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:17.188
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/30/23 23:32:17.203
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 23:32:17.465
STEP: Deploying the custom resource conversion webhook pod 01/30/23 23:32:17.528
STEP: Wait for the deployment to be ready 01/30/23 23:32:17.585
Jan 30 23:32:17.619: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 30 23:32:19.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:32:21.731
STEP: Verifying the service has paired with the endpoint 01/30/23 23:32:21.825
Jan 30 23:32:22.827: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 30 23:32:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Creating a v1 custom resource 01/30/23 23:32:25.736
STEP: v2 custom resource should be converted 01/30/23 23:32:25.758
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6726" for this suite. 01/30/23 23:32:26.672
------------------------------
• [SLOW TEST] [9.709 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:17.047
    Jan 30 23:32:17.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-webhook 01/30/23 23:32:17.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:17.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:17.188
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/30/23 23:32:17.203
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/30/23 23:32:17.465
    STEP: Deploying the custom resource conversion webhook pod 01/30/23 23:32:17.528
    STEP: Wait for the deployment to be ready 01/30/23 23:32:17.585
    Jan 30 23:32:17.619: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jan 30 23:32:19.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:32:21.731
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:32:21.825
    Jan 30 23:32:22.827: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 30 23:32:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Creating a v1 custom resource 01/30/23 23:32:25.736
    STEP: v2 custom resource should be converted 01/30/23 23:32:25.758
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6726" for this suite. 01/30/23 23:32:26.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:26.774
Jan 30 23:32:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/30/23 23:32:26.776
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:26.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:26.905
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-s4lpc" 01/30/23 23:32:26.923
Jan 30 23:32:27.005: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-s4lpc-3627" 01/30/23 23:32:27.005
Jan 30 23:32:27.041: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-s4lpc-3627" 01/30/23 23:32:27.041
Jan 30 23:32:27.082: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:27.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9471" for this suite. 01/30/23 23:32:27.104
STEP: Destroying namespace "e2e-ns-s4lpc-3627" for this suite. 01/30/23 23:32:27.133
------------------------------
• [0.387 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:26.774
    Jan 30 23:32:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/30/23 23:32:26.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:26.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:26.905
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-s4lpc" 01/30/23 23:32:26.923
    Jan 30 23:32:27.005: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-s4lpc-3627" 01/30/23 23:32:27.005
    Jan 30 23:32:27.041: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-s4lpc-3627" 01/30/23 23:32:27.041
    Jan 30 23:32:27.082: INFO: Namespace "e2e-ns-s4lpc-3627" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:27.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9471" for this suite. 01/30/23 23:32:27.104
    STEP: Destroying namespace "e2e-ns-s4lpc-3627" for this suite. 01/30/23 23:32:27.133
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:27.162
Jan 30 23:32:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:32:27.164
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:27.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:27.254
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-59aa9043-ffe7-4f29-90be-f7abb3a8b046 01/30/23 23:32:27.272
STEP: Creating a pod to test consume secrets 01/30/23 23:32:27.292
Jan 30 23:32:27.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b" in namespace "projected-8222" to be "Succeeded or Failed"
Jan 30 23:32:27.344: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.158662ms
Jan 30 23:32:29.372: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045253275s
Jan 30 23:32:31.367: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040096902s
Jan 30 23:32:33.374: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047480266s
STEP: Saw pod success 01/30/23 23:32:33.374
Jan 30 23:32:33.375: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b" satisfied condition "Succeeded or Failed"
Jan 30 23:32:33.393: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:32:33.459
Jan 30 23:32:33.525: INFO: Waiting for pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b to disappear
Jan 30 23:32:33.542: INFO: Pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:33.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8222" for this suite. 01/30/23 23:32:33.565
------------------------------
• [SLOW TEST] [6.427 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:27.162
    Jan 30 23:32:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:32:27.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:27.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:27.254
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-59aa9043-ffe7-4f29-90be-f7abb3a8b046 01/30/23 23:32:27.272
    STEP: Creating a pod to test consume secrets 01/30/23 23:32:27.292
    Jan 30 23:32:27.326: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b" in namespace "projected-8222" to be "Succeeded or Failed"
    Jan 30 23:32:27.344: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.158662ms
    Jan 30 23:32:29.372: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045253275s
    Jan 30 23:32:31.367: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040096902s
    Jan 30 23:32:33.374: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047480266s
    STEP: Saw pod success 01/30/23 23:32:33.374
    Jan 30 23:32:33.375: INFO: Pod "pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b" satisfied condition "Succeeded or Failed"
    Jan 30 23:32:33.393: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:32:33.459
    Jan 30 23:32:33.525: INFO: Waiting for pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b to disappear
    Jan 30 23:32:33.542: INFO: Pod pod-projected-secrets-6aa444b3-be6f-49c5-b84e-9af8a8d8fa9b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:33.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8222" for this suite. 01/30/23 23:32:33.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:33.595
Jan 30 23:32:33.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename events 01/30/23 23:32:33.598
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:33.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:33.702
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/30/23 23:32:33.716
STEP: get a list of Events with a label in the current namespace 01/30/23 23:32:33.802
STEP: delete a list of events 01/30/23 23:32:33.817
Jan 30 23:32:33.818: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/30/23 23:32:33.931
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 30 23:32:33.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1420" for this suite. 01/30/23 23:32:33.968
------------------------------
• [0.401 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:33.595
    Jan 30 23:32:33.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename events 01/30/23 23:32:33.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:33.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:33.702
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/30/23 23:32:33.716
    STEP: get a list of Events with a label in the current namespace 01/30/23 23:32:33.802
    STEP: delete a list of events 01/30/23 23:32:33.817
    Jan 30 23:32:33.818: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/30/23 23:32:33.931
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:32:33.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1420" for this suite. 01/30/23 23:32:33.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:32:34.01
Jan 30 23:32:34.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename init-container 01/30/23 23:32:34.012
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:34.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:34.103
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/30/23 23:32:34.121
Jan 30 23:32:34.121: INFO: PodSpec: initContainers in spec.initContainers
Jan 30 23:33:18.662: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-081171fc-2357-456d-843c-abfafa788d3c", GenerateName:"", Namespace:"init-container-475", SelfLink:"", UID:"b17b8d32-b42a-46ae-9495-e6b783fc7c5f", ResourceVersion:"34446", Generation:0, CreationTimestamp:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"121325237"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"33f1c7146dee453ba978f25bf3d116ea0bea69c4d2277444a6b45317a8b4425d", "cni.projectcalico.org/podIP":"172.30.199.17/32", "cni.projectcalico.org/podIPs":"172.30.199.17/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012596e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 32, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001259740), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 33, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012597a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8dlgg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004f41ce0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004223d28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.15.28.227", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004742850), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004223db0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004223dd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004223dd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004223ddc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004ebb470), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.15.28.227", PodIP:"172.30.199.17", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.199.17"}}, StartTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004742930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0047429a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://5cacc6b4b5d77f136a43216a5d97dda90783c0099209d8161e30019fda968aac", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f41d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f41d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004223e5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:18.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-475" for this suite. 01/30/23 23:33:18.686
------------------------------
• [SLOW TEST] [44.704 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:32:34.01
    Jan 30 23:32:34.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename init-container 01/30/23 23:32:34.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:32:34.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:32:34.103
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/30/23 23:32:34.121
    Jan 30 23:32:34.121: INFO: PodSpec: initContainers in spec.initContainers
    Jan 30 23:33:18.662: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-081171fc-2357-456d-843c-abfafa788d3c", GenerateName:"", Namespace:"init-container-475", SelfLink:"", UID:"b17b8d32-b42a-46ae-9495-e6b783fc7c5f", ResourceVersion:"34446", Generation:0, CreationTimestamp:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"121325237"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"33f1c7146dee453ba978f25bf3d116ea0bea69c4d2277444a6b45317a8b4425d", "cni.projectcalico.org/podIP":"172.30.199.17/32", "cni.projectcalico.org/podIPs":"172.30.199.17/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012596e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 32, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001259740), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 30, 23, 33, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012597a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8dlgg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004f41ce0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8dlgg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004223d28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.15.28.227", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004742850), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004223db0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004223dd0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004223dd8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004223ddc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004ebb470), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.15.28.227", PodIP:"172.30.199.17", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.199.17"}}, StartTime:time.Date(2023, time.January, 30, 23, 32, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004742930)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0047429a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://5cacc6b4b5d77f136a43216a5d97dda90783c0099209d8161e30019fda968aac", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f41d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f41d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004223e5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:18.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-475" for this suite. 01/30/23 23:33:18.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:18.718
Jan 30 23:33:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 23:33:18.721
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:18.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:18.844
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 23:33:18.86
Jan 30 23:33:18.907: INFO: Waiting up to 5m0s for pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c" in namespace "emptydir-5489" to be "Succeeded or Failed"
Jan 30 23:33:18.925: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.967485ms
Jan 30 23:33:20.945: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03707993s
Jan 30 23:33:22.941: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033721799s
Jan 30 23:33:24.976: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068083961s
STEP: Saw pod success 01/30/23 23:33:24.976
Jan 30 23:33:24.976: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c" satisfied condition "Succeeded or Failed"
Jan 30 23:33:25.000: INFO: Trying to get logs from node 10.15.28.227 pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c container test-container: <nil>
STEP: delete the pod 01/30/23 23:33:25.044
Jan 30 23:33:25.122: INFO: Waiting for pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c to disappear
Jan 30 23:33:25.142: INFO: Pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5489" for this suite. 01/30/23 23:33:25.169
------------------------------
• [SLOW TEST] [6.480 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:18.718
    Jan 30 23:33:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 23:33:18.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:18.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:18.844
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/30/23 23:33:18.86
    Jan 30 23:33:18.907: INFO: Waiting up to 5m0s for pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c" in namespace "emptydir-5489" to be "Succeeded or Failed"
    Jan 30 23:33:18.925: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.967485ms
    Jan 30 23:33:20.945: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03707993s
    Jan 30 23:33:22.941: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033721799s
    Jan 30 23:33:24.976: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068083961s
    STEP: Saw pod success 01/30/23 23:33:24.976
    Jan 30 23:33:24.976: INFO: Pod "pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c" satisfied condition "Succeeded or Failed"
    Jan 30 23:33:25.000: INFO: Trying to get logs from node 10.15.28.227 pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c container test-container: <nil>
    STEP: delete the pod 01/30/23 23:33:25.044
    Jan 30 23:33:25.122: INFO: Waiting for pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c to disappear
    Jan 30 23:33:25.142: INFO: Pod pod-2f308ab1-6fbb-43b4-b7bf-5cecbcf0b57c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5489" for this suite. 01/30/23 23:33:25.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:25.204
Jan 30 23:33:25.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 23:33:25.206
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:25.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:25.289
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 30 23:33:25.417: INFO: Waiting up to 5m0s for pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2" in namespace "emptydir-wrapper-1617" to be "running and ready"
Jan 30 23:33:25.436: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.805595ms
Jan 30 23:33:25.436: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:33:27.456: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038558487s
Jan 30 23:33:27.456: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:33:29.454: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037074311s
Jan 30 23:33:29.455: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Running (Ready = true)
Jan 30 23:33:29.455: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/30/23 23:33:29.473
STEP: Cleaning up the configmap 01/30/23 23:33:29.502
STEP: Cleaning up the pod 01/30/23 23:33:29.532
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:29.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1617" for this suite. 01/30/23 23:33:29.632
------------------------------
• [4.455 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:25.204
    Jan 30 23:33:25.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 23:33:25.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:25.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:25.289
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 30 23:33:25.417: INFO: Waiting up to 5m0s for pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2" in namespace "emptydir-wrapper-1617" to be "running and ready"
    Jan 30 23:33:25.436: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.805595ms
    Jan 30 23:33:25.436: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:33:27.456: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038558487s
    Jan 30 23:33:27.456: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:33:29.454: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037074311s
    Jan 30 23:33:29.455: INFO: The phase of Pod pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2 is Running (Ready = true)
    Jan 30 23:33:29.455: INFO: Pod "pod-secrets-5fdef172-75b5-4f21-bbac-3fc54f5eccd2" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/30/23 23:33:29.473
    STEP: Cleaning up the configmap 01/30/23 23:33:29.502
    STEP: Cleaning up the pod 01/30/23 23:33:29.532
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:29.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1617" for this suite. 01/30/23 23:33:29.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:29.663
Jan 30 23:33:29.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:33:29.665
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:29.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:29.767
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-34a30190-9a30-47a2-a168-165492804742 01/30/23 23:33:29.784
STEP: Creating a pod to test consume secrets 01/30/23 23:33:29.804
Jan 30 23:33:29.839: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46" in namespace "projected-4779" to be "Succeeded or Failed"
Jan 30 23:33:29.856: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045925ms
Jan 30 23:33:31.875: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Running", Reason="", readiness=true. Elapsed: 2.035251316s
Jan 30 23:33:33.874: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Running", Reason="", readiness=false. Elapsed: 4.03464665s
Jan 30 23:33:35.883: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04317446s
STEP: Saw pod success 01/30/23 23:33:35.883
Jan 30 23:33:35.883: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46" satisfied condition "Succeeded or Failed"
Jan 30 23:33:35.928: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:33:35.97
Jan 30 23:33:36.030: INFO: Waiting for pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 to disappear
Jan 30 23:33:36.050: INFO: Pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:36.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4779" for this suite. 01/30/23 23:33:36.073
------------------------------
• [SLOW TEST] [6.439 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:29.663
    Jan 30 23:33:29.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:33:29.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:29.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:29.767
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-34a30190-9a30-47a2-a168-165492804742 01/30/23 23:33:29.784
    STEP: Creating a pod to test consume secrets 01/30/23 23:33:29.804
    Jan 30 23:33:29.839: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46" in namespace "projected-4779" to be "Succeeded or Failed"
    Jan 30 23:33:29.856: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045925ms
    Jan 30 23:33:31.875: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Running", Reason="", readiness=true. Elapsed: 2.035251316s
    Jan 30 23:33:33.874: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Running", Reason="", readiness=false. Elapsed: 4.03464665s
    Jan 30 23:33:35.883: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04317446s
    STEP: Saw pod success 01/30/23 23:33:35.883
    Jan 30 23:33:35.883: INFO: Pod "pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46" satisfied condition "Succeeded or Failed"
    Jan 30 23:33:35.928: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:33:35.97
    Jan 30 23:33:36.030: INFO: Waiting for pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 to disappear
    Jan 30 23:33:36.050: INFO: Pod pod-projected-secrets-e2acfff0-484f-4297-ad46-b52cb7042b46 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:36.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4779" for this suite. 01/30/23 23:33:36.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:36.104
Jan 30 23:33:36.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/30/23 23:33:36.107
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:36.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:36.195
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/30/23 23:33:36.265
STEP: create the rc2 01/30/23 23:33:36.287
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/30/23 23:33:41.336
STEP: delete the rc simpletest-rc-to-be-deleted 01/30/23 23:33:43.143
STEP: wait for the rc to be deleted 01/30/23 23:33:43.176
STEP: Gathering metrics 01/30/23 23:33:48.246
W0130 23:33:48.289783      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 30 23:33:48.290: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 30 23:33:48.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-24crm" in namespace "gc-7782"
Jan 30 23:33:48.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gpb7" in namespace "gc-7782"
Jan 30 23:33:48.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nfcb" in namespace "gc-7782"
Jan 30 23:33:48.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pbnx" in namespace "gc-7782"
Jan 30 23:33:48.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-47gbr" in namespace "gc-7782"
Jan 30 23:33:48.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llj5" in namespace "gc-7782"
Jan 30 23:33:48.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lshp" in namespace "gc-7782"
Jan 30 23:33:48.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-5strp" in namespace "gc-7782"
Jan 30 23:33:49.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wt6g" in namespace "gc-7782"
Jan 30 23:33:49.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zlqj" in namespace "gc-7782"
Jan 30 23:33:49.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-6crlx" in namespace "gc-7782"
Jan 30 23:33:49.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmcx" in namespace "gc-7782"
Jan 30 23:33:49.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p95z" in namespace "gc-7782"
Jan 30 23:33:49.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s9ml" in namespace "gc-7782"
Jan 30 23:33:49.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vjgc" in namespace "gc-7782"
Jan 30 23:33:49.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-747t8" in namespace "gc-7782"
Jan 30 23:33:49.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-76dbd" in namespace "gc-7782"
Jan 30 23:33:49.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ggzt" in namespace "gc-7782"
Jan 30 23:33:49.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jjkx" in namespace "gc-7782"
Jan 30 23:33:49.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jwhj" in namespace "gc-7782"
Jan 30 23:33:49.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-88pwb" in namespace "gc-7782"
Jan 30 23:33:49.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gt5w" in namespace "gc-7782"
Jan 30 23:33:49.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t6bx" in namespace "gc-7782"
Jan 30 23:33:49.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7zk" in namespace "gc-7782"
Jan 30 23:33:50.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-97drp" in namespace "gc-7782"
Jan 30 23:33:50.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-989jg" in namespace "gc-7782"
Jan 30 23:33:50.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q8xr" in namespace "gc-7782"
Jan 30 23:33:50.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2vvk" in namespace "gc-7782"
Jan 30 23:33:50.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbmjd" in namespace "gc-7782"
Jan 30 23:33:50.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmnf7" in namespace "gc-7782"
Jan 30 23:33:50.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-br5jn" in namespace "gc-7782"
Jan 30 23:33:50.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgnf5" in namespace "gc-7782"
Jan 30 23:33:50.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjq2m" in namespace "gc-7782"
Jan 30 23:33:50.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqbgj" in namespace "gc-7782"
Jan 30 23:33:50.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-czhpx" in namespace "gc-7782"
Jan 30 23:33:50.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2jk8" in namespace "gc-7782"
Jan 30 23:33:50.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcfn5" in namespace "gc-7782"
Jan 30 23:33:50.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj8gb" in namespace "gc-7782"
Jan 30 23:33:51.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-djhhk" in namespace "gc-7782"
Jan 30 23:33:51.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-drw9j" in namespace "gc-7782"
Jan 30 23:33:51.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-dztfw" in namespace "gc-7782"
Jan 30 23:33:51.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-f64kl" in namespace "gc-7782"
Jan 30 23:33:51.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgrv2" in namespace "gc-7782"
Jan 30 23:33:51.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl9x5" in namespace "gc-7782"
Jan 30 23:33:51.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdds7" in namespace "gc-7782"
Jan 30 23:33:51.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfdhn" in namespace "gc-7782"
Jan 30 23:33:51.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnldk" in namespace "gc-7782"
Jan 30 23:33:51.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7rdq" in namespace "gc-7782"
Jan 30 23:33:51.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb8tp" in namespace "gc-7782"
Jan 30 23:33:51.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrhs9" in namespace "gc-7782"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7782" for this suite. 01/30/23 23:33:51.992
------------------------------
• [SLOW TEST] [15.916 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:36.104
    Jan 30 23:33:36.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/30/23 23:33:36.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:36.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:36.195
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/30/23 23:33:36.265
    STEP: create the rc2 01/30/23 23:33:36.287
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/30/23 23:33:41.336
    STEP: delete the rc simpletest-rc-to-be-deleted 01/30/23 23:33:43.143
    STEP: wait for the rc to be deleted 01/30/23 23:33:43.176
    STEP: Gathering metrics 01/30/23 23:33:48.246
    W0130 23:33:48.289783      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 30 23:33:48.290: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 30 23:33:48.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-24crm" in namespace "gc-7782"
    Jan 30 23:33:48.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gpb7" in namespace "gc-7782"
    Jan 30 23:33:48.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nfcb" in namespace "gc-7782"
    Jan 30 23:33:48.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pbnx" in namespace "gc-7782"
    Jan 30 23:33:48.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-47gbr" in namespace "gc-7782"
    Jan 30 23:33:48.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-4llj5" in namespace "gc-7782"
    Jan 30 23:33:48.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lshp" in namespace "gc-7782"
    Jan 30 23:33:48.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-5strp" in namespace "gc-7782"
    Jan 30 23:33:49.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wt6g" in namespace "gc-7782"
    Jan 30 23:33:49.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zlqj" in namespace "gc-7782"
    Jan 30 23:33:49.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-6crlx" in namespace "gc-7782"
    Jan 30 23:33:49.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gmcx" in namespace "gc-7782"
    Jan 30 23:33:49.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p95z" in namespace "gc-7782"
    Jan 30 23:33:49.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s9ml" in namespace "gc-7782"
    Jan 30 23:33:49.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vjgc" in namespace "gc-7782"
    Jan 30 23:33:49.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-747t8" in namespace "gc-7782"
    Jan 30 23:33:49.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-76dbd" in namespace "gc-7782"
    Jan 30 23:33:49.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ggzt" in namespace "gc-7782"
    Jan 30 23:33:49.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jjkx" in namespace "gc-7782"
    Jan 30 23:33:49.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jwhj" in namespace "gc-7782"
    Jan 30 23:33:49.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-88pwb" in namespace "gc-7782"
    Jan 30 23:33:49.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gt5w" in namespace "gc-7782"
    Jan 30 23:33:49.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t6bx" in namespace "gc-7782"
    Jan 30 23:33:49.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7zk" in namespace "gc-7782"
    Jan 30 23:33:50.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-97drp" in namespace "gc-7782"
    Jan 30 23:33:50.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-989jg" in namespace "gc-7782"
    Jan 30 23:33:50.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q8xr" in namespace "gc-7782"
    Jan 30 23:33:50.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2vvk" in namespace "gc-7782"
    Jan 30 23:33:50.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbmjd" in namespace "gc-7782"
    Jan 30 23:33:50.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmnf7" in namespace "gc-7782"
    Jan 30 23:33:50.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-br5jn" in namespace "gc-7782"
    Jan 30 23:33:50.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgnf5" in namespace "gc-7782"
    Jan 30 23:33:50.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjq2m" in namespace "gc-7782"
    Jan 30 23:33:50.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqbgj" in namespace "gc-7782"
    Jan 30 23:33:50.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-czhpx" in namespace "gc-7782"
    Jan 30 23:33:50.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2jk8" in namespace "gc-7782"
    Jan 30 23:33:50.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcfn5" in namespace "gc-7782"
    Jan 30 23:33:50.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-dj8gb" in namespace "gc-7782"
    Jan 30 23:33:51.029: INFO: Deleting pod "simpletest-rc-to-be-deleted-djhhk" in namespace "gc-7782"
    Jan 30 23:33:51.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-drw9j" in namespace "gc-7782"
    Jan 30 23:33:51.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-dztfw" in namespace "gc-7782"
    Jan 30 23:33:51.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-f64kl" in namespace "gc-7782"
    Jan 30 23:33:51.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgrv2" in namespace "gc-7782"
    Jan 30 23:33:51.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl9x5" in namespace "gc-7782"
    Jan 30 23:33:51.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdds7" in namespace "gc-7782"
    Jan 30 23:33:51.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfdhn" in namespace "gc-7782"
    Jan 30 23:33:51.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnldk" in namespace "gc-7782"
    Jan 30 23:33:51.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7rdq" in namespace "gc-7782"
    Jan 30 23:33:51.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb8tp" in namespace "gc-7782"
    Jan 30 23:33:51.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrhs9" in namespace "gc-7782"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7782" for this suite. 01/30/23 23:33:51.992
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:52.021
Jan 30 23:33:52.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption 01/30/23 23:33:52.023
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:52.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:52.11
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/30/23 23:33:52.123
STEP: Waiting for the pdb to be processed 01/30/23 23:33:52.158
STEP: updating the pdb 01/30/23 23:33:54.196
STEP: Waiting for the pdb to be processed 01/30/23 23:33:54.239
STEP: patching the pdb 01/30/23 23:33:54.263
STEP: Waiting for the pdb to be processed 01/30/23 23:33:54.314
STEP: Waiting for the pdb to be deleted 01/30/23 23:33:54.383
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:54.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1826" for this suite. 01/30/23 23:33:54.436
------------------------------
• [2.447 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:52.021
    Jan 30 23:33:52.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption 01/30/23 23:33:52.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:52.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:52.11
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/30/23 23:33:52.123
    STEP: Waiting for the pdb to be processed 01/30/23 23:33:52.158
    STEP: updating the pdb 01/30/23 23:33:54.196
    STEP: Waiting for the pdb to be processed 01/30/23 23:33:54.239
    STEP: patching the pdb 01/30/23 23:33:54.263
    STEP: Waiting for the pdb to be processed 01/30/23 23:33:54.314
    STEP: Waiting for the pdb to be deleted 01/30/23 23:33:54.383
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:54.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1826" for this suite. 01/30/23 23:33:54.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:54.473
Jan 30 23:33:54.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:33:54.476
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:54.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:54.546
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/30/23 23:33:54.562
STEP: submitting the pod to kubernetes 01/30/23 23:33:54.562
Jan 30 23:33:54.598: INFO: Waiting up to 5m0s for pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" in namespace "pods-1094" to be "running and ready"
Jan 30 23:33:54.614: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Pending", Reason="", readiness=false. Elapsed: 15.998644ms
Jan 30 23:33:54.614: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:33:56.634: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035432206s
Jan 30 23:33:56.634: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:33:58.679: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Running", Reason="", readiness=true. Elapsed: 4.080963701s
Jan 30 23:33:58.679: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Running (Ready = true)
Jan 30 23:33:58.679: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/30/23 23:33:58.699
STEP: updating the pod 01/30/23 23:33:58.717
Jan 30 23:33:59.267: INFO: Successfully updated pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48"
Jan 30 23:33:59.267: INFO: Waiting up to 5m0s for pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" in namespace "pods-1094" to be "running"
Jan 30 23:33:59.307: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Running", Reason="", readiness=true. Elapsed: 40.090447ms
Jan 30 23:33:59.308: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/30/23 23:33:59.308
Jan 30 23:33:59.350: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:33:59.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1094" for this suite. 01/30/23 23:33:59.376
------------------------------
• [4.930 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:54.473
    Jan 30 23:33:54.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:33:54.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:54.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:54.546
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/30/23 23:33:54.562
    STEP: submitting the pod to kubernetes 01/30/23 23:33:54.562
    Jan 30 23:33:54.598: INFO: Waiting up to 5m0s for pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" in namespace "pods-1094" to be "running and ready"
    Jan 30 23:33:54.614: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Pending", Reason="", readiness=false. Elapsed: 15.998644ms
    Jan 30 23:33:54.614: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:33:56.634: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035432206s
    Jan 30 23:33:56.634: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:33:58.679: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Running", Reason="", readiness=true. Elapsed: 4.080963701s
    Jan 30 23:33:58.679: INFO: The phase of Pod pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48 is Running (Ready = true)
    Jan 30 23:33:58.679: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/30/23 23:33:58.699
    STEP: updating the pod 01/30/23 23:33:58.717
    Jan 30 23:33:59.267: INFO: Successfully updated pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48"
    Jan 30 23:33:59.267: INFO: Waiting up to 5m0s for pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" in namespace "pods-1094" to be "running"
    Jan 30 23:33:59.307: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48": Phase="Running", Reason="", readiness=true. Elapsed: 40.090447ms
    Jan 30 23:33:59.308: INFO: Pod "pod-update-7d97a0c2-82e8-4b01-8245-25d9a10cac48" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/30/23 23:33:59.308
    Jan 30 23:33:59.350: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:33:59.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1094" for this suite. 01/30/23 23:33:59.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:33:59.406
Jan 30 23:33:59.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename endpointslice 01/30/23 23:33:59.41
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:59.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:59.486
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/30/23 23:34:04.835
STEP: referencing matching pods with named port 01/30/23 23:34:09.87
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/30/23 23:34:14.908
STEP: recreating EndpointSlices after they've been deleted 01/30/23 23:34:19.946
Jan 30 23:34:20.048: INFO: EndpointSlice for Service endpointslice-5583/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 23:34:30.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5583" for this suite. 01/30/23 23:34:30.115
------------------------------
• [SLOW TEST] [30.740 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:33:59.406
    Jan 30 23:33:59.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename endpointslice 01/30/23 23:33:59.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:33:59.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:33:59.486
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/30/23 23:34:04.835
    STEP: referencing matching pods with named port 01/30/23 23:34:09.87
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/30/23 23:34:14.908
    STEP: recreating EndpointSlices after they've been deleted 01/30/23 23:34:19.946
    Jan 30 23:34:20.048: INFO: EndpointSlice for Service endpointslice-5583/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:34:30.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5583" for this suite. 01/30/23 23:34:30.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:34:30.148
Jan 30 23:34:30.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:34:30.152
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:34:30.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:34:30.218
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/30/23 23:34:30.242
Jan 30 23:34:30.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 30 23:34:30.405: INFO: stderr: ""
Jan 30 23:34:30.405: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/30/23 23:34:30.405
Jan 30 23:34:30.405: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 30 23:34:30.405: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2510" to be "running and ready, or succeeded"
Jan 30 23:34:30.422: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.585688ms
Jan 30 23:34:30.422: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Jan 30 23:34:32.439: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.034001564s
Jan 30 23:34:32.439: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 30 23:34:32.439: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/30/23 23:34:32.439
Jan 30 23:34:32.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator'
Jan 30 23:34:32.680: INFO: stderr: ""
Jan 30 23:34:32.680: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\n"
Jan 30 23:34:34.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator'
Jan 30 23:34:34.868: INFO: stderr: ""
Jan 30 23:34:34.868: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\nI0130 23:34:32.775048       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gzj 261\nI0130 23:34:32.974560       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mbj 255\nI0130 23:34:33.175034       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nkc 233\nI0130 23:34:33.374495       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4gt 220\nI0130 23:34:33.574982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/skb 383\nI0130 23:34:33.775533       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/slmh 571\nI0130 23:34:33.975094       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9m9 420\nI0130 23:34:34.174579       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/tn8 206\nI0130 23:34:34.375101       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/hdk 257\nI0130 23:34:34.574510       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/6frf 442\nI0130 23:34:34.774920       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/gdg 305\n"
STEP: limiting log lines 01/30/23 23:34:34.868
Jan 30 23:34:34.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --tail=1'
Jan 30 23:34:35.111: INFO: stderr: ""
Jan 30 23:34:35.111: INFO: stdout: "I0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\n"
Jan 30 23:34:35.111: INFO: got output "I0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\n"
STEP: limiting log bytes 01/30/23 23:34:35.111
Jan 30 23:34:35.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --limit-bytes=1'
Jan 30 23:34:35.339: INFO: stderr: ""
Jan 30 23:34:35.339: INFO: stdout: "I"
Jan 30 23:34:35.339: INFO: got output "I"
STEP: exposing timestamps 01/30/23 23:34:35.339
Jan 30 23:34:35.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 30 23:34:35.525: INFO: stderr: ""
Jan 30 23:34:35.525: INFO: stdout: "2023-01-30T23:34:35.375051595Z I0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\n"
Jan 30 23:34:35.525: INFO: got output "2023-01-30T23:34:35.375051595Z I0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\n"
STEP: restricting to a time range 01/30/23 23:34:35.525
Jan 30 23:34:38.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --since=1s'
Jan 30 23:34:38.248: INFO: stderr: ""
Jan 30 23:34:38.248: INFO: stdout: "I0130 23:34:37.374631       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/86ld 323\nI0130 23:34:37.576006       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/f5b 436\nI0130 23:34:37.774606       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/xdj2 424\nI0130 23:34:37.975347       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p97 201\nI0130 23:34:38.175029       1 logs_generator.go:76] 30 POST /api/v1/namespaces/default/pods/2rb 280\n"
Jan 30 23:34:38.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --since=24h'
Jan 30 23:34:38.463: INFO: stderr: ""
Jan 30 23:34:38.463: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\nI0130 23:34:32.775048       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gzj 261\nI0130 23:34:32.974560       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mbj 255\nI0130 23:34:33.175034       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nkc 233\nI0130 23:34:33.374495       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4gt 220\nI0130 23:34:33.574982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/skb 383\nI0130 23:34:33.775533       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/slmh 571\nI0130 23:34:33.975094       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9m9 420\nI0130 23:34:34.174579       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/tn8 206\nI0130 23:34:34.375101       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/hdk 257\nI0130 23:34:34.574510       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/6frf 442\nI0130 23:34:34.774920       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/gdg 305\nI0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\nI0130 23:34:35.175160       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/wrl 345\nI0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\nI0130 23:34:35.575222       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/sg4r 557\nI0130 23:34:35.774598       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/cd7 540\nI0130 23:34:35.975217       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/6jks 556\nI0130 23:34:36.174819       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/stz 432\nI0130 23:34:36.375419       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/4s5 282\nI0130 23:34:36.574860       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/xf8l 578\nI0130 23:34:36.775314       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/cg9q 221\nI0130 23:34:36.975712       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2tp 322\nI0130 23:34:37.175214       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/f2t8 321\nI0130 23:34:37.374631       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/86ld 323\nI0130 23:34:37.576006       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/f5b 436\nI0130 23:34:37.774606       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/xdj2 424\nI0130 23:34:37.975347       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p97 201\nI0130 23:34:38.175029       1 logs_generator.go:76] 30 POST /api/v1/namespaces/default/pods/2rb 280\nI0130 23:34:38.375475       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/kctm 255\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 30 23:34:38.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 delete pod logs-generator'
Jan 30 23:34:40.501: INFO: stderr: ""
Jan 30 23:34:40.501: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:34:40.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2510" for this suite. 01/30/23 23:34:40.526
------------------------------
• [SLOW TEST] [10.407 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:34:30.148
    Jan 30 23:34:30.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:34:30.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:34:30.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:34:30.218
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/30/23 23:34:30.242
    Jan 30 23:34:30.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 30 23:34:30.405: INFO: stderr: ""
    Jan 30 23:34:30.405: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/30/23 23:34:30.405
    Jan 30 23:34:30.405: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 30 23:34:30.405: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2510" to be "running and ready, or succeeded"
    Jan 30 23:34:30.422: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.585688ms
    Jan 30 23:34:30.422: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Jan 30 23:34:32.439: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.034001564s
    Jan 30 23:34:32.439: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 30 23:34:32.439: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/30/23 23:34:32.439
    Jan 30 23:34:32.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator'
    Jan 30 23:34:32.680: INFO: stderr: ""
    Jan 30 23:34:32.680: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\n"
    Jan 30 23:34:34.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator'
    Jan 30 23:34:34.868: INFO: stderr: ""
    Jan 30 23:34:34.868: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\nI0130 23:34:32.775048       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gzj 261\nI0130 23:34:32.974560       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mbj 255\nI0130 23:34:33.175034       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nkc 233\nI0130 23:34:33.374495       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4gt 220\nI0130 23:34:33.574982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/skb 383\nI0130 23:34:33.775533       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/slmh 571\nI0130 23:34:33.975094       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9m9 420\nI0130 23:34:34.174579       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/tn8 206\nI0130 23:34:34.375101       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/hdk 257\nI0130 23:34:34.574510       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/6frf 442\nI0130 23:34:34.774920       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/gdg 305\n"
    STEP: limiting log lines 01/30/23 23:34:34.868
    Jan 30 23:34:34.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --tail=1'
    Jan 30 23:34:35.111: INFO: stderr: ""
    Jan 30 23:34:35.111: INFO: stdout: "I0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\n"
    Jan 30 23:34:35.111: INFO: got output "I0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\n"
    STEP: limiting log bytes 01/30/23 23:34:35.111
    Jan 30 23:34:35.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --limit-bytes=1'
    Jan 30 23:34:35.339: INFO: stderr: ""
    Jan 30 23:34:35.339: INFO: stdout: "I"
    Jan 30 23:34:35.339: INFO: got output "I"
    STEP: exposing timestamps 01/30/23 23:34:35.339
    Jan 30 23:34:35.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 30 23:34:35.525: INFO: stderr: ""
    Jan 30 23:34:35.525: INFO: stdout: "2023-01-30T23:34:35.375051595Z I0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\n"
    Jan 30 23:34:35.525: INFO: got output "2023-01-30T23:34:35.375051595Z I0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\n"
    STEP: restricting to a time range 01/30/23 23:34:35.525
    Jan 30 23:34:38.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --since=1s'
    Jan 30 23:34:38.248: INFO: stderr: ""
    Jan 30 23:34:38.248: INFO: stdout: "I0130 23:34:37.374631       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/86ld 323\nI0130 23:34:37.576006       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/f5b 436\nI0130 23:34:37.774606       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/xdj2 424\nI0130 23:34:37.975347       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p97 201\nI0130 23:34:38.175029       1 logs_generator.go:76] 30 POST /api/v1/namespaces/default/pods/2rb 280\n"
    Jan 30 23:34:38.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 logs logs-generator logs-generator --since=24h'
    Jan 30 23:34:38.463: INFO: stderr: ""
    Jan 30 23:34:38.463: INFO: stdout: "I0130 23:34:32.174371       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/6s9 521\nI0130 23:34:32.375083       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/k8v 240\nI0130 23:34:32.574515       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/drr4 487\nI0130 23:34:32.775048       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/gzj 261\nI0130 23:34:32.974560       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/mbj 255\nI0130 23:34:33.175034       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nkc 233\nI0130 23:34:33.374495       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/4gt 220\nI0130 23:34:33.574982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/skb 383\nI0130 23:34:33.775533       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/slmh 571\nI0130 23:34:33.975094       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/9m9 420\nI0130 23:34:34.174579       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/tn8 206\nI0130 23:34:34.375101       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/hdk 257\nI0130 23:34:34.574510       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/6frf 442\nI0130 23:34:34.774920       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/gdg 305\nI0130 23:34:34.975580       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/j6s 242\nI0130 23:34:35.175160       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/wrl 345\nI0130 23:34:35.374589       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/jn8 342\nI0130 23:34:35.575222       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/sg4r 557\nI0130 23:34:35.774598       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/cd7 540\nI0130 23:34:35.975217       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/6jks 556\nI0130 23:34:36.174819       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/stz 432\nI0130 23:34:36.375419       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/4s5 282\nI0130 23:34:36.574860       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/xf8l 578\nI0130 23:34:36.775314       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/cg9q 221\nI0130 23:34:36.975712       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2tp 322\nI0130 23:34:37.175214       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/f2t8 321\nI0130 23:34:37.374631       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/86ld 323\nI0130 23:34:37.576006       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/f5b 436\nI0130 23:34:37.774606       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/xdj2 424\nI0130 23:34:37.975347       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/p97 201\nI0130 23:34:38.175029       1 logs_generator.go:76] 30 POST /api/v1/namespaces/default/pods/2rb 280\nI0130 23:34:38.375475       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/ns/pods/kctm 255\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 30 23:34:38.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2510 delete pod logs-generator'
    Jan 30 23:34:40.501: INFO: stderr: ""
    Jan 30 23:34:40.501: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:34:40.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2510" for this suite. 01/30/23 23:34:40.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:34:40.558
Jan 30 23:34:40.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:34:40.561
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:34:40.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:34:40.631
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 23:34:40.706: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:35:40.868: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 01/30/23 23:35:40.887
Jan 30 23:35:41.007: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 30 23:35:41.029: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 30 23:35:41.111: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 30 23:35:41.135: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 30 23:35:41.252: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 30 23:35:41.286: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/30/23 23:35:41.286
Jan 30 23:35:41.286: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:41.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 47.898934ms
Jan 30 23:35:43.354: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068413731s
Jan 30 23:35:45.377: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.090986358s
Jan 30 23:35:45.377: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 30 23:35:45.377: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.449: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 71.425903ms
Jan 30 23:35:45.449: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:35:45.449: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.468: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.910114ms
Jan 30 23:35:45.468: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:35:45.468: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.486: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.94935ms
Jan 30 23:35:45.486: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:35:45.486: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.503: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.90182ms
Jan 30 23:35:45.503: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:35:45.503: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.529: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 26.651143ms
Jan 30 23:35:45.529: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/30/23 23:35:45.529
Jan 30 23:35:45.550: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9787" to be "running"
Jan 30 23:35:45.594: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 43.67576ms
Jan 30 23:35:47.612: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061485088s
Jan 30 23:35:49.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063381409s
Jan 30 23:35:51.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063728219s
Jan 30 23:35:53.612: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.062027318s
Jan 30 23:35:53.613: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:35:53.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9787" for this suite. 01/30/23 23:35:54.093
------------------------------
• [SLOW TEST] [73.563 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:34:40.558
    Jan 30 23:34:40.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:34:40.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:34:40.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:34:40.631
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 23:34:40.706: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:35:40.868: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 01/30/23 23:35:40.887
    Jan 30 23:35:41.007: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 30 23:35:41.029: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 30 23:35:41.111: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 30 23:35:41.135: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 30 23:35:41.252: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 30 23:35:41.286: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/30/23 23:35:41.286
    Jan 30 23:35:41.286: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:41.334: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 47.898934ms
    Jan 30 23:35:43.354: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068413731s
    Jan 30 23:35:45.377: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.090986358s
    Jan 30 23:35:45.377: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 30 23:35:45.377: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.449: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 71.425903ms
    Jan 30 23:35:45.449: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:35:45.449: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.468: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.910114ms
    Jan 30 23:35:45.468: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:35:45.468: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.486: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.94935ms
    Jan 30 23:35:45.486: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:35:45.486: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.503: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.90182ms
    Jan 30 23:35:45.503: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:35:45.503: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.529: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 26.651143ms
    Jan 30 23:35:45.529: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/30/23 23:35:45.529
    Jan 30 23:35:45.550: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9787" to be "running"
    Jan 30 23:35:45.594: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 43.67576ms
    Jan 30 23:35:47.612: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061485088s
    Jan 30 23:35:49.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063381409s
    Jan 30 23:35:51.614: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063728219s
    Jan 30 23:35:53.612: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.062027318s
    Jan 30 23:35:53.613: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:35:53.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9787" for this suite. 01/30/23 23:35:54.093
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:35:54.124
Jan 30 23:35:54.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/30/23 23:35:54.127
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:35:54.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:35:54.281
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/30/23 23:35:54.324
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local;sleep 1; done
 01/30/23 23:35:54.343
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local;sleep 1; done
 01/30/23 23:35:54.345
STEP: creating a pod to probe DNS 01/30/23 23:35:54.345
STEP: submitting the pod to kubernetes 01/30/23 23:35:54.345
Jan 30 23:35:54.396: INFO: Waiting up to 15m0s for pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37" in namespace "dns-8833" to be "running"
Jan 30 23:35:54.417: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Pending", Reason="", readiness=false. Elapsed: 21.47354ms
Jan 30 23:35:56.444: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048310142s
Jan 30 23:35:58.437: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Running", Reason="", readiness=true. Elapsed: 4.040962142s
Jan 30 23:35:58.437: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:35:58.437
STEP: looking for the results for each expected name from probers 01/30/23 23:35:58.456
Jan 30 23:35:58.522: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.545: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.568: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.596: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.619: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.644: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.668: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.690: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:35:58.691: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:03.718: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.741: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.766: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.790: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.812: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.857: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.891: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:03.891: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:08.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.788: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.811: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.851: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.902: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.928: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.957: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.987: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:08.987: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:13.718: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.744: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.772: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.817: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.841: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.864: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.887: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:13.909: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:18.716: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.745: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.769: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.793: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.843: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.865: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.890: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:18.890: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:23.715: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.764: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.863: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.887: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.911: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.934: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
Jan 30 23:36:23.934: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

Jan 30 23:36:29.024: INFO: DNS probes using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 succeeded

STEP: deleting the pod 01/30/23 23:36:29.024
STEP: deleting the test headless service 01/30/23 23:36:29.094
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 23:36:29.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8833" for this suite. 01/30/23 23:36:29.263
------------------------------
• [SLOW TEST] [35.197 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:35:54.124
    Jan 30 23:35:54.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/30/23 23:35:54.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:35:54.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:35:54.281
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/30/23 23:35:54.324
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local;sleep 1; done
     01/30/23 23:35:54.343
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8833.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local;sleep 1; done
     01/30/23 23:35:54.345
    STEP: creating a pod to probe DNS 01/30/23 23:35:54.345
    STEP: submitting the pod to kubernetes 01/30/23 23:35:54.345
    Jan 30 23:35:54.396: INFO: Waiting up to 15m0s for pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37" in namespace "dns-8833" to be "running"
    Jan 30 23:35:54.417: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Pending", Reason="", readiness=false. Elapsed: 21.47354ms
    Jan 30 23:35:56.444: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048310142s
    Jan 30 23:35:58.437: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37": Phase="Running", Reason="", readiness=true. Elapsed: 4.040962142s
    Jan 30 23:35:58.437: INFO: Pod "dns-test-02217aea-ccd4-43cf-aa5f-720394618e37" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:35:58.437
    STEP: looking for the results for each expected name from probers 01/30/23 23:35:58.456
    Jan 30 23:35:58.522: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.545: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.568: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.596: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.619: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.644: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.668: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.690: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:35:58.691: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:03.718: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.741: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.766: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.790: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.812: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.834: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.857: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.891: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:03.891: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:08.719: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.788: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.811: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.851: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.902: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.928: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.957: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.987: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:08.987: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:13.718: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.744: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.772: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.817: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.841: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.864: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.887: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:13.909: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:18.716: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.745: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.769: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.793: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.817: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.843: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.865: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.890: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:18.890: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:23.715: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.764: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.815: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.863: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.887: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.911: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.934: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local from pod dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37: the server could not find the requested resource (get pods dns-test-02217aea-ccd4-43cf-aa5f-720394618e37)
    Jan 30 23:36:23.934: INFO: Lookups using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8833.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8833.svc.cluster.local jessie_udp@dns-test-service-2.dns-8833.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8833.svc.cluster.local]

    Jan 30 23:36:29.024: INFO: DNS probes using dns-8833/dns-test-02217aea-ccd4-43cf-aa5f-720394618e37 succeeded

    STEP: deleting the pod 01/30/23 23:36:29.024
    STEP: deleting the test headless service 01/30/23 23:36:29.094
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:36:29.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8833" for this suite. 01/30/23 23:36:29.263
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:36:29.323
Jan 30 23:36:29.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-runtime 01/30/23 23:36:29.325
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:29.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:29.414
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/30/23 23:36:29.435
STEP: wait for the container to reach Failed 01/30/23 23:36:29.468
STEP: get the container status 01/30/23 23:36:34.596
STEP: the container should be terminated 01/30/23 23:36:34.614
STEP: the termination message should be set 01/30/23 23:36:34.614
Jan 30 23:36:34.615: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/30/23 23:36:34.615
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 23:36:34.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7599" for this suite. 01/30/23 23:36:34.856
------------------------------
• [SLOW TEST] [5.559 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:36:29.323
    Jan 30 23:36:29.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-runtime 01/30/23 23:36:29.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:29.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:29.414
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/30/23 23:36:29.435
    STEP: wait for the container to reach Failed 01/30/23 23:36:29.468
    STEP: get the container status 01/30/23 23:36:34.596
    STEP: the container should be terminated 01/30/23 23:36:34.614
    STEP: the termination message should be set 01/30/23 23:36:34.614
    Jan 30 23:36:34.615: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/30/23 23:36:34.615
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:36:34.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7599" for this suite. 01/30/23 23:36:34.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:36:34.895
Jan 30 23:36:34.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:36:34.897
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:34.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:34.967
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/30/23 23:36:34.986
Jan 30 23:36:34.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: rename a version 01/30/23 23:36:40.688
STEP: check the new version name is served 01/30/23 23:36:40.742
STEP: check the old version name is removed 01/30/23 23:36:43.151
STEP: check the other version is not changed 01/30/23 23:36:44.176
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:36:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5330" for this suite. 01/30/23 23:36:48.996
------------------------------
• [SLOW TEST] [14.134 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:36:34.895
    Jan 30 23:36:34.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:36:34.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:34.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:34.967
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/30/23 23:36:34.986
    Jan 30 23:36:34.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: rename a version 01/30/23 23:36:40.688
    STEP: check the new version name is served 01/30/23 23:36:40.742
    STEP: check the old version name is removed 01/30/23 23:36:43.151
    STEP: check the other version is not changed 01/30/23 23:36:44.176
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:36:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5330" for this suite. 01/30/23 23:36:48.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:36:49.036
Jan 30 23:36:49.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:36:49.038
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:49.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:49.137
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-e442674d-7141-430d-bf83-1ca98023eda2 01/30/23 23:36:49.145
STEP: Creating a pod to test consume configMaps 01/30/23 23:36:49.168
Jan 30 23:36:49.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2" in namespace "projected-5521" to be "Succeeded or Failed"
Jan 30 23:36:49.219: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.754236ms
Jan 30 23:36:51.236: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.035212s
Jan 30 23:36:53.238: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Running", Reason="", readiness=false. Elapsed: 4.03758865s
Jan 30 23:36:55.239: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038138093s
STEP: Saw pod success 01/30/23 23:36:55.239
Jan 30 23:36:55.240: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2" satisfied condition "Succeeded or Failed"
Jan 30 23:36:55.254: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:36:55.374
Jan 30 23:36:55.420: INFO: Waiting for pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 to disappear
Jan 30 23:36:55.437: INFO: Pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:36:55.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5521" for this suite. 01/30/23 23:36:55.454
------------------------------
• [SLOW TEST] [6.450 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:36:49.036
    Jan 30 23:36:49.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:36:49.038
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:49.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:49.137
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-e442674d-7141-430d-bf83-1ca98023eda2 01/30/23 23:36:49.145
    STEP: Creating a pod to test consume configMaps 01/30/23 23:36:49.168
    Jan 30 23:36:49.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2" in namespace "projected-5521" to be "Succeeded or Failed"
    Jan 30 23:36:49.219: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.754236ms
    Jan 30 23:36:51.236: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.035212s
    Jan 30 23:36:53.238: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Running", Reason="", readiness=false. Elapsed: 4.03758865s
    Jan 30 23:36:55.239: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038138093s
    STEP: Saw pod success 01/30/23 23:36:55.239
    Jan 30 23:36:55.240: INFO: Pod "pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2" satisfied condition "Succeeded or Failed"
    Jan 30 23:36:55.254: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:36:55.374
    Jan 30 23:36:55.420: INFO: Waiting for pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 to disappear
    Jan 30 23:36:55.437: INFO: Pod pod-projected-configmaps-7f375583-ac81-42b9-9ee5-ff2c6fed3bc2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:36:55.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5521" for this suite. 01/30/23 23:36:55.454
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:36:55.487
Jan 30 23:36:55.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 23:36:55.49
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:55.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:55.577
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/30/23 23:36:55.586
STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:36:56.761
Jan 30 23:36:56.803: INFO: Pod name wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee: Found 0 pods out of 5
Jan 30 23:37:01.851: INFO: Pod name wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 23:37:01.851
Jan 30 23:37:01.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:01.880: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq": Phase="Running", Reason="", readiness=true. Elapsed: 28.311144ms
Jan 30 23:37:01.880: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq" satisfied condition "running"
Jan 30 23:37:01.880: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:01.899: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn": Phase="Running", Reason="", readiness=true. Elapsed: 18.760302ms
Jan 30 23:37:01.899: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn" satisfied condition "running"
Jan 30 23:37:01.899: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:01.919: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn": Phase="Running", Reason="", readiness=true. Elapsed: 20.020596ms
Jan 30 23:37:01.919: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn" satisfied condition "running"
Jan 30 23:37:01.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:01.940: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k": Phase="Running", Reason="", readiness=true. Elapsed: 20.961241ms
Jan 30 23:37:01.940: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k" satisfied condition "running"
Jan 30 23:37:01.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:01.959: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb": Phase="Running", Reason="", readiness=true. Elapsed: 18.759495ms
Jan 30 23:37:01.959: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:01.959
Jan 30 23:37:02.042: INFO: Deleting ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee took: 19.301016ms
Jan 30 23:37:02.243: INFO: Terminating ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee pods took: 200.357114ms
STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:37:05.361
Jan 30 23:37:05.397: INFO: Pod name wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d: Found 0 pods out of 5
Jan 30 23:37:10.430: INFO: Pod name wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 23:37:10.43
Jan 30 23:37:10.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:10.457: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx": Phase="Running", Reason="", readiness=true. Elapsed: 26.497413ms
Jan 30 23:37:10.457: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx" satisfied condition "running"
Jan 30 23:37:10.457: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:10.473: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws": Phase="Running", Reason="", readiness=true. Elapsed: 15.917897ms
Jan 30 23:37:10.473: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws" satisfied condition "running"
Jan 30 23:37:10.473: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:10.488: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd": Phase="Running", Reason="", readiness=true. Elapsed: 15.078802ms
Jan 30 23:37:10.488: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd" satisfied condition "running"
Jan 30 23:37:10.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:10.504: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb": Phase="Running", Reason="", readiness=true. Elapsed: 15.813395ms
Jan 30 23:37:10.504: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb" satisfied condition "running"
Jan 30 23:37:10.504: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:10.526: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w": Phase="Running", Reason="", readiness=true. Elapsed: 21.48412ms
Jan 30 23:37:10.526: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:10.526
Jan 30 23:37:10.619: INFO: Deleting ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d took: 29.13375ms
Jan 30 23:37:10.820: INFO: Terminating ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d pods took: 201.051848ms
STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:37:13.84
Jan 30 23:37:13.880: INFO: Pod name wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710: Found 0 pods out of 5
Jan 30 23:37:18.913: INFO: Pod name wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/30/23 23:37:18.913
Jan 30 23:37:18.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:18.941: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p": Phase="Running", Reason="", readiness=true. Elapsed: 28.123479ms
Jan 30 23:37:18.942: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p" satisfied condition "running"
Jan 30 23:37:18.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:18.961: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj": Phase="Running", Reason="", readiness=true. Elapsed: 19.306002ms
Jan 30 23:37:18.961: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj" satisfied condition "running"
Jan 30 23:37:18.961: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:18.979: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252": Phase="Running", Reason="", readiness=true. Elapsed: 18.246492ms
Jan 30 23:37:18.979: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252" satisfied condition "running"
Jan 30 23:37:18.979: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:18.997: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl": Phase="Running", Reason="", readiness=true. Elapsed: 17.599842ms
Jan 30 23:37:18.997: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl" satisfied condition "running"
Jan 30 23:37:18.997: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8" in namespace "emptydir-wrapper-6663" to be "running"
Jan 30 23:37:19.025: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8": Phase="Running", Reason="", readiness=true. Elapsed: 27.032751ms
Jan 30 23:37:19.025: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:19.025
Jan 30 23:37:19.110: INFO: Deleting ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 took: 20.348897ms
Jan 30 23:37:19.310: INFO: Terminating ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 pods took: 200.502287ms
STEP: Cleaning up the configMaps 01/30/23 23:37:22.111
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6663" for this suite. 01/30/23 23:37:23.745
------------------------------
• [SLOW TEST] [28.306 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:36:55.487
    Jan 30 23:36:55.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir-wrapper 01/30/23 23:36:55.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:36:55.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:36:55.577
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/30/23 23:36:55.586
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:36:56.761
    Jan 30 23:36:56.803: INFO: Pod name wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee: Found 0 pods out of 5
    Jan 30 23:37:01.851: INFO: Pod name wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 23:37:01.851
    Jan 30 23:37:01.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:01.880: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq": Phase="Running", Reason="", readiness=true. Elapsed: 28.311144ms
    Jan 30 23:37:01.880: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-4zjnq" satisfied condition "running"
    Jan 30 23:37:01.880: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:01.899: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn": Phase="Running", Reason="", readiness=true. Elapsed: 18.760302ms
    Jan 30 23:37:01.899: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-7m7fn" satisfied condition "running"
    Jan 30 23:37:01.899: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:01.919: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn": Phase="Running", Reason="", readiness=true. Elapsed: 20.020596ms
    Jan 30 23:37:01.919: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-gxngn" satisfied condition "running"
    Jan 30 23:37:01.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:01.940: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k": Phase="Running", Reason="", readiness=true. Elapsed: 20.961241ms
    Jan 30 23:37:01.940: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wf42k" satisfied condition "running"
    Jan 30 23:37:01.940: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:01.959: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb": Phase="Running", Reason="", readiness=true. Elapsed: 18.759495ms
    Jan 30 23:37:01.959: INFO: Pod "wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee-wp2qb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:01.959
    Jan 30 23:37:02.042: INFO: Deleting ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee took: 19.301016ms
    Jan 30 23:37:02.243: INFO: Terminating ReplicationController wrapped-volume-race-6902c108-8846-4578-8921-5fcacc5f9aee pods took: 200.357114ms
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:37:05.361
    Jan 30 23:37:05.397: INFO: Pod name wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d: Found 0 pods out of 5
    Jan 30 23:37:10.430: INFO: Pod name wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 23:37:10.43
    Jan 30 23:37:10.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:10.457: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx": Phase="Running", Reason="", readiness=true. Elapsed: 26.497413ms
    Jan 30 23:37:10.457: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-56lzx" satisfied condition "running"
    Jan 30 23:37:10.457: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:10.473: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws": Phase="Running", Reason="", readiness=true. Elapsed: 15.917897ms
    Jan 30 23:37:10.473: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-jr8ws" satisfied condition "running"
    Jan 30 23:37:10.473: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:10.488: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd": Phase="Running", Reason="", readiness=true. Elapsed: 15.078802ms
    Jan 30 23:37:10.488: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-lf6nd" satisfied condition "running"
    Jan 30 23:37:10.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:10.504: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb": Phase="Running", Reason="", readiness=true. Elapsed: 15.813395ms
    Jan 30 23:37:10.504: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-nrnvb" satisfied condition "running"
    Jan 30 23:37:10.504: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:10.526: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w": Phase="Running", Reason="", readiness=true. Elapsed: 21.48412ms
    Jan 30 23:37:10.526: INFO: Pod "wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d-z4d9w" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:10.526
    Jan 30 23:37:10.619: INFO: Deleting ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d took: 29.13375ms
    Jan 30 23:37:10.820: INFO: Terminating ReplicationController wrapped-volume-race-a755b2f0-6dc1-4d37-9ce9-c8c927e5874d pods took: 201.051848ms
    STEP: Creating RC which spawns configmap-volume pods 01/30/23 23:37:13.84
    Jan 30 23:37:13.880: INFO: Pod name wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710: Found 0 pods out of 5
    Jan 30 23:37:18.913: INFO: Pod name wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/30/23 23:37:18.913
    Jan 30 23:37:18.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:18.941: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p": Phase="Running", Reason="", readiness=true. Elapsed: 28.123479ms
    Jan 30 23:37:18.942: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-4fl8p" satisfied condition "running"
    Jan 30 23:37:18.942: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:18.961: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj": Phase="Running", Reason="", readiness=true. Elapsed: 19.306002ms
    Jan 30 23:37:18.961: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-cvcbj" satisfied condition "running"
    Jan 30 23:37:18.961: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:18.979: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252": Phase="Running", Reason="", readiness=true. Elapsed: 18.246492ms
    Jan 30 23:37:18.979: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-lr252" satisfied condition "running"
    Jan 30 23:37:18.979: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:18.997: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl": Phase="Running", Reason="", readiness=true. Elapsed: 17.599842ms
    Jan 30 23:37:18.997: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-pbckl" satisfied condition "running"
    Jan 30 23:37:18.997: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8" in namespace "emptydir-wrapper-6663" to be "running"
    Jan 30 23:37:19.025: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8": Phase="Running", Reason="", readiness=true. Elapsed: 27.032751ms
    Jan 30 23:37:19.025: INFO: Pod "wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710-zvhq8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 in namespace emptydir-wrapper-6663, will wait for the garbage collector to delete the pods 01/30/23 23:37:19.025
    Jan 30 23:37:19.110: INFO: Deleting ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 took: 20.348897ms
    Jan 30 23:37:19.310: INFO: Terminating ReplicationController wrapped-volume-race-89d7f509-3329-4ce3-ac04-40e7c7592710 pods took: 200.502287ms
    STEP: Cleaning up the configMaps 01/30/23 23:37:22.111
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6663" for this suite. 01/30/23 23:37:23.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:23.796
Jan 30 23:37:23.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:37:23.799
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:23.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:23.869
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6852 01/30/23 23:37:23.885
STEP: changing the ExternalName service to type=NodePort 01/30/23 23:37:23.903
STEP: creating replication controller externalname-service in namespace services-6852 01/30/23 23:37:23.992
I0130 23:37:24.032064      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6852, replica count: 2
I0130 23:37:27.083589      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:37:27.083: INFO: Creating new exec pod
Jan 30 23:37:27.113: INFO: Waiting up to 5m0s for pod "execpodwbrzr" in namespace "services-6852" to be "running"
Jan 30 23:37:27.142: INFO: Pod "execpodwbrzr": Phase="Pending", Reason="", readiness=false. Elapsed: 28.822521ms
Jan 30 23:37:29.161: INFO: Pod "execpodwbrzr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047692262s
Jan 30 23:37:31.160: INFO: Pod "execpodwbrzr": Phase="Running", Reason="", readiness=true. Elapsed: 4.046891524s
Jan 30 23:37:31.161: INFO: Pod "execpodwbrzr" satisfied condition "running"
Jan 30 23:37:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 30 23:37:32.613: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 30 23:37:32.613: INFO: stdout: ""
Jan 30 23:37:32.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 172.21.164.210 80'
Jan 30 23:37:33.007: INFO: stderr: "+ nc -v -z -w 2 172.21.164.210 80\nConnection to 172.21.164.210 80 port [tcp/http] succeeded!\n"
Jan 30 23:37:33.007: INFO: stdout: ""
Jan 30 23:37:33.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 30943'
Jan 30 23:37:33.399: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 30943\nConnection to 10.15.28.225 30943 port [tcp/*] succeeded!\n"
Jan 30 23:37:33.399: INFO: stdout: ""
Jan 30 23:37:33.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30943'
Jan 30 23:37:33.791: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30943\nConnection to 10.15.28.237 30943 port [tcp/*] succeeded!\n"
Jan 30 23:37:33.791: INFO: stdout: ""
Jan 30 23:37:33.791: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:33.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6852" for this suite. 01/30/23 23:37:33.936
------------------------------
• [SLOW TEST] [10.167 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:23.796
    Jan 30 23:37:23.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:37:23.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:23.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:23.869
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6852 01/30/23 23:37:23.885
    STEP: changing the ExternalName service to type=NodePort 01/30/23 23:37:23.903
    STEP: creating replication controller externalname-service in namespace services-6852 01/30/23 23:37:23.992
    I0130 23:37:24.032064      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6852, replica count: 2
    I0130 23:37:27.083589      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:37:27.083: INFO: Creating new exec pod
    Jan 30 23:37:27.113: INFO: Waiting up to 5m0s for pod "execpodwbrzr" in namespace "services-6852" to be "running"
    Jan 30 23:37:27.142: INFO: Pod "execpodwbrzr": Phase="Pending", Reason="", readiness=false. Elapsed: 28.822521ms
    Jan 30 23:37:29.161: INFO: Pod "execpodwbrzr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047692262s
    Jan 30 23:37:31.160: INFO: Pod "execpodwbrzr": Phase="Running", Reason="", readiness=true. Elapsed: 4.046891524s
    Jan 30 23:37:31.161: INFO: Pod "execpodwbrzr" satisfied condition "running"
    Jan 30 23:37:32.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 30 23:37:32.613: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 30 23:37:32.613: INFO: stdout: ""
    Jan 30 23:37:32.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 172.21.164.210 80'
    Jan 30 23:37:33.007: INFO: stderr: "+ nc -v -z -w 2 172.21.164.210 80\nConnection to 172.21.164.210 80 port [tcp/http] succeeded!\n"
    Jan 30 23:37:33.007: INFO: stdout: ""
    Jan 30 23:37:33.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 10.15.28.225 30943'
    Jan 30 23:37:33.399: INFO: stderr: "+ nc -v -z -w 2 10.15.28.225 30943\nConnection to 10.15.28.225 30943 port [tcp/*] succeeded!\n"
    Jan 30 23:37:33.399: INFO: stdout: ""
    Jan 30 23:37:33.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-6852 exec execpodwbrzr -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30943'
    Jan 30 23:37:33.791: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30943\nConnection to 10.15.28.237 30943 port [tcp/*] succeeded!\n"
    Jan 30 23:37:33.791: INFO: stdout: ""
    Jan 30 23:37:33.791: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:33.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6852" for this suite. 01/30/23 23:37:33.936
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:33.963
Jan 30 23:37:33.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-runtime 01/30/23 23:37:33.964
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:34.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:34.033
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/30/23 23:37:34.05
STEP: wait for the container to reach Succeeded 01/30/23 23:37:34.093
STEP: get the container status 01/30/23 23:37:39.218
STEP: the container should be terminated 01/30/23 23:37:39.236
STEP: the termination message should be set 01/30/23 23:37:39.236
Jan 30 23:37:39.237: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/30/23 23:37:39.237
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:39.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1158" for this suite. 01/30/23 23:37:39.352
------------------------------
• [SLOW TEST] [5.416 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:33.963
    Jan 30 23:37:33.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-runtime 01/30/23 23:37:33.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:34.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:34.033
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/30/23 23:37:34.05
    STEP: wait for the container to reach Succeeded 01/30/23 23:37:34.093
    STEP: get the container status 01/30/23 23:37:39.218
    STEP: the container should be terminated 01/30/23 23:37:39.236
    STEP: the termination message should be set 01/30/23 23:37:39.236
    Jan 30 23:37:39.237: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/30/23 23:37:39.237
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:39.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1158" for this suite. 01/30/23 23:37:39.352
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:39.381
Jan 30 23:37:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replicaset 01/30/23 23:37:39.384
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:39.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:39.453
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/30/23 23:37:39.466
Jan 30 23:37:39.500: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 30 23:37:44.527: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 23:37:44.527
STEP: getting scale subresource 01/30/23 23:37:44.528
STEP: updating a scale subresource 01/30/23 23:37:44.544
STEP: verifying the replicaset Spec.Replicas was modified 01/30/23 23:37:44.572
STEP: Patch a scale subresource 01/30/23 23:37:44.601
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:44.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9976" for this suite. 01/30/23 23:37:44.7
------------------------------
• [SLOW TEST] [5.351 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:39.381
    Jan 30 23:37:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replicaset 01/30/23 23:37:39.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:39.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:39.453
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/30/23 23:37:39.466
    Jan 30 23:37:39.500: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 30 23:37:44.527: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 23:37:44.527
    STEP: getting scale subresource 01/30/23 23:37:44.528
    STEP: updating a scale subresource 01/30/23 23:37:44.544
    STEP: verifying the replicaset Spec.Replicas was modified 01/30/23 23:37:44.572
    STEP: Patch a scale subresource 01/30/23 23:37:44.601
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:44.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9976" for this suite. 01/30/23 23:37:44.7
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:44.732
Jan 30 23:37:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:37:44.734
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:44.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:44.804
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 30 23:37:44.877: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7684 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:44.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7684" for this suite. 01/30/23 23:37:44.963
------------------------------
• [0.260 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:44.732
    Jan 30 23:37:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:37:44.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:44.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:44.804
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 30 23:37:44.877: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7684 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:44.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7684" for this suite. 01/30/23 23:37:44.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:45.021
Jan 30 23:37:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption 01/30/23 23:37:45.023
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:45.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:45.102
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/30/23 23:37:45.12
STEP: Waiting for the pdb to be processed 01/30/23 23:37:45.141
STEP: First trying to evict a pod which shouldn't be evictable 01/30/23 23:37:47.208
STEP: Waiting for all pods to be running 01/30/23 23:37:47.209
Jan 30 23:37:47.226: INFO: pods: 0 < 3
Jan 30 23:37:49.247: INFO: running pods: 0 < 3
STEP: locating a running pod 01/30/23 23:37:51.247
STEP: Updating the pdb to allow a pod to be evicted 01/30/23 23:37:51.305
STEP: Waiting for the pdb to be processed 01/30/23 23:37:51.345
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 23:37:53.382
STEP: Waiting for all pods to be running 01/30/23 23:37:53.382
STEP: Waiting for the pdb to observed all healthy pods 01/30/23 23:37:53.402
STEP: Patching the pdb to disallow a pod to be evicted 01/30/23 23:37:53.519
STEP: Waiting for the pdb to be processed 01/30/23 23:37:53.585
STEP: Waiting for all pods to be running 01/30/23 23:37:53.604
Jan 30 23:37:53.622: INFO: running pods: 2 < 3
Jan 30 23:37:55.644: INFO: running pods: 2 < 3
STEP: locating a running pod 01/30/23 23:37:57.648
STEP: Deleting the pdb to allow a pod to be evicted 01/30/23 23:37:57.696
STEP: Waiting for the pdb to be deleted 01/30/23 23:37:57.727
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 23:37:57.745
STEP: Waiting for all pods to be running 01/30/23 23:37:57.745
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2082" for this suite. 01/30/23 23:37:57.865
------------------------------
• [SLOW TEST] [12.888 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:45.021
    Jan 30 23:37:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption 01/30/23 23:37:45.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:45.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:45.102
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/30/23 23:37:45.12
    STEP: Waiting for the pdb to be processed 01/30/23 23:37:45.141
    STEP: First trying to evict a pod which shouldn't be evictable 01/30/23 23:37:47.208
    STEP: Waiting for all pods to be running 01/30/23 23:37:47.209
    Jan 30 23:37:47.226: INFO: pods: 0 < 3
    Jan 30 23:37:49.247: INFO: running pods: 0 < 3
    STEP: locating a running pod 01/30/23 23:37:51.247
    STEP: Updating the pdb to allow a pod to be evicted 01/30/23 23:37:51.305
    STEP: Waiting for the pdb to be processed 01/30/23 23:37:51.345
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 23:37:53.382
    STEP: Waiting for all pods to be running 01/30/23 23:37:53.382
    STEP: Waiting for the pdb to observed all healthy pods 01/30/23 23:37:53.402
    STEP: Patching the pdb to disallow a pod to be evicted 01/30/23 23:37:53.519
    STEP: Waiting for the pdb to be processed 01/30/23 23:37:53.585
    STEP: Waiting for all pods to be running 01/30/23 23:37:53.604
    Jan 30 23:37:53.622: INFO: running pods: 2 < 3
    Jan 30 23:37:55.644: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/30/23 23:37:57.648
    STEP: Deleting the pdb to allow a pod to be evicted 01/30/23 23:37:57.696
    STEP: Waiting for the pdb to be deleted 01/30/23 23:37:57.727
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/30/23 23:37:57.745
    STEP: Waiting for all pods to be running 01/30/23 23:37:57.745
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2082" for this suite. 01/30/23 23:37:57.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:57.913
Jan 30 23:37:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:37:57.914
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:57.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:57.975
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/30/23 23:37:57.992
STEP: getting /apis/node.k8s.io 01/30/23 23:37:58.008
STEP: getting /apis/node.k8s.io/v1 01/30/23 23:37:58.015
STEP: creating 01/30/23 23:37:58.022
STEP: watching 01/30/23 23:37:58.099
Jan 30 23:37:58.099: INFO: starting watch
STEP: getting 01/30/23 23:37:58.126
STEP: listing 01/30/23 23:37:58.142
STEP: patching 01/30/23 23:37:58.16
STEP: updating 01/30/23 23:37:58.184
Jan 30 23:37:58.204: INFO: waiting for watch events with expected annotations
STEP: deleting 01/30/23 23:37:58.205
STEP: deleting a collection 01/30/23 23:37:58.267
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 23:37:58.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9660" for this suite. 01/30/23 23:37:58.376
------------------------------
• [0.494 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:57.913
    Jan 30 23:37:57.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:37:57.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:57.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:57.975
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/30/23 23:37:57.992
    STEP: getting /apis/node.k8s.io 01/30/23 23:37:58.008
    STEP: getting /apis/node.k8s.io/v1 01/30/23 23:37:58.015
    STEP: creating 01/30/23 23:37:58.022
    STEP: watching 01/30/23 23:37:58.099
    Jan 30 23:37:58.099: INFO: starting watch
    STEP: getting 01/30/23 23:37:58.126
    STEP: listing 01/30/23 23:37:58.142
    STEP: patching 01/30/23 23:37:58.16
    STEP: updating 01/30/23 23:37:58.184
    Jan 30 23:37:58.204: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/30/23 23:37:58.205
    STEP: deleting a collection 01/30/23 23:37:58.267
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:37:58.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9660" for this suite. 01/30/23 23:37:58.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:37:58.409
Jan 30 23:37:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 23:37:58.412
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:58.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:58.483
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 23:37:58.527
Jan 30 23:37:58.563: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4610" to be "running and ready"
Jan 30 23:37:58.598: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 35.053313ms
Jan 30 23:37:58.598: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:38:00.617: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054280528s
Jan 30 23:38:00.618: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:38:02.618: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.055356524s
Jan 30 23:38:02.619: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 23:38:02.619: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/30/23 23:38:02.639
Jan 30 23:38:02.661: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4610" to be "running and ready"
Jan 30 23:38:02.689: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 28.49272ms
Jan 30 23:38:02.690: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:38:04.708: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046795373s
Jan 30 23:38:04.708: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:38:06.713: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.052007569s
Jan 30 23:38:06.713: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 30 23:38:06.713: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/30/23 23:38:06.729
Jan 30 23:38:06.761: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 23:38:06.780: INFO: Pod pod-with-prestop-http-hook still exists
Jan 30 23:38:08.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 30 23:38:08.801: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/30/23 23:38:08.801
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 23:38:08.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4610" for this suite. 01/30/23 23:38:08.947
------------------------------
• [SLOW TEST] [10.567 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:37:58.409
    Jan 30 23:37:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 23:37:58.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:37:58.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:37:58.483
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 23:37:58.527
    Jan 30 23:37:58.563: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4610" to be "running and ready"
    Jan 30 23:37:58.598: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 35.053313ms
    Jan 30 23:37:58.598: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:38:00.617: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054280528s
    Jan 30 23:38:00.618: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:38:02.618: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.055356524s
    Jan 30 23:38:02.619: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 23:38:02.619: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/30/23 23:38:02.639
    Jan 30 23:38:02.661: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4610" to be "running and ready"
    Jan 30 23:38:02.689: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 28.49272ms
    Jan 30 23:38:02.690: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:38:04.708: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046795373s
    Jan 30 23:38:04.708: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:38:06.713: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.052007569s
    Jan 30 23:38:06.713: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 30 23:38:06.713: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/30/23 23:38:06.729
    Jan 30 23:38:06.761: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 30 23:38:06.780: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 30 23:38:08.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 30 23:38:08.801: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/30/23 23:38:08.801
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:38:08.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4610" for this suite. 01/30/23 23:38:08.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:38:08.979
Jan 30 23:38:08.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:38:08.981
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:09.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:09.068
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-74dbc0c6-3f0d-48d2-ad87-ec9c54b8c09f 01/30/23 23:38:09.084
STEP: Creating a pod to test consume secrets 01/30/23 23:38:09.134
Jan 30 23:38:09.172: INFO: Waiting up to 5m0s for pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9" in namespace "secrets-7142" to be "Succeeded or Failed"
Jan 30 23:38:09.191: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.57296ms
Jan 30 23:38:11.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038342378s
Jan 30 23:38:13.209: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037000303s
Jan 30 23:38:15.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037651501s
STEP: Saw pod success 01/30/23 23:38:15.21
Jan 30 23:38:15.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9" satisfied condition "Succeeded or Failed"
Jan 30 23:38:15.228: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:38:15.336
Jan 30 23:38:15.380: INFO: Waiting for pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 to disappear
Jan 30 23:38:15.397: INFO: Pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:38:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7142" for this suite. 01/30/23 23:38:15.422
------------------------------
• [SLOW TEST] [6.470 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:38:08.979
    Jan 30 23:38:08.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:38:08.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:09.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:09.068
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-74dbc0c6-3f0d-48d2-ad87-ec9c54b8c09f 01/30/23 23:38:09.084
    STEP: Creating a pod to test consume secrets 01/30/23 23:38:09.134
    Jan 30 23:38:09.172: INFO: Waiting up to 5m0s for pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9" in namespace "secrets-7142" to be "Succeeded or Failed"
    Jan 30 23:38:09.191: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.57296ms
    Jan 30 23:38:11.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038342378s
    Jan 30 23:38:13.209: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037000303s
    Jan 30 23:38:15.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037651501s
    STEP: Saw pod success 01/30/23 23:38:15.21
    Jan 30 23:38:15.210: INFO: Pod "pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9" satisfied condition "Succeeded or Failed"
    Jan 30 23:38:15.228: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:38:15.336
    Jan 30 23:38:15.380: INFO: Waiting for pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 to disappear
    Jan 30 23:38:15.397: INFO: Pod pod-secrets-5c5f91ae-ac45-4f98-924f-87a1412a98a9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:38:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7142" for this suite. 01/30/23 23:38:15.422
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:38:15.45
Jan 30 23:38:15.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:38:15.454
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:15.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:15.524
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-5fd1d407-ee72-4f62-aff7-9143d4debcb4 01/30/23 23:38:15.538
STEP: Creating a pod to test consume configMaps 01/30/23 23:38:15.557
Jan 30 23:38:15.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884" in namespace "configmap-5358" to be "Succeeded or Failed"
Jan 30 23:38:15.607: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Pending", Reason="", readiness=false. Elapsed: 16.895931ms
Jan 30 23:38:17.628: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0374306s
Jan 30 23:38:19.627: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036694191s
STEP: Saw pod success 01/30/23 23:38:19.627
Jan 30 23:38:19.628: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884" satisfied condition "Succeeded or Failed"
Jan 30 23:38:19.646: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:38:19.683
Jan 30 23:38:19.737: INFO: Waiting for pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 to disappear
Jan 30 23:38:19.755: INFO: Pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:38:19.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5358" for this suite. 01/30/23 23:38:19.781
------------------------------
• [4.360 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:38:15.45
    Jan 30 23:38:15.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:38:15.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:15.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:15.524
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-5fd1d407-ee72-4f62-aff7-9143d4debcb4 01/30/23 23:38:15.538
    STEP: Creating a pod to test consume configMaps 01/30/23 23:38:15.557
    Jan 30 23:38:15.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884" in namespace "configmap-5358" to be "Succeeded or Failed"
    Jan 30 23:38:15.607: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Pending", Reason="", readiness=false. Elapsed: 16.895931ms
    Jan 30 23:38:17.628: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0374306s
    Jan 30 23:38:19.627: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036694191s
    STEP: Saw pod success 01/30/23 23:38:19.627
    Jan 30 23:38:19.628: INFO: Pod "pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884" satisfied condition "Succeeded or Failed"
    Jan 30 23:38:19.646: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:38:19.683
    Jan 30 23:38:19.737: INFO: Waiting for pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 to disappear
    Jan 30 23:38:19.755: INFO: Pod pod-configmaps-a590f6d8-2d88-40fa-964e-143302f83884 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:38:19.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5358" for this suite. 01/30/23 23:38:19.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:38:19.83
Jan 30 23:38:19.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:38:19.831
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:19.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:19.911
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:38:19.926
Jan 30 23:38:19.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa" in namespace "downward-api-529" to be "Succeeded or Failed"
Jan 30 23:38:19.978: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.507016ms
Jan 30 23:38:21.999: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039455013s
Jan 30 23:38:23.997: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037194951s
Jan 30 23:38:26.000: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040677709s
STEP: Saw pod success 01/30/23 23:38:26
Jan 30 23:38:26.001: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa" satisfied condition "Succeeded or Failed"
Jan 30 23:38:26.019: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa container client-container: <nil>
STEP: delete the pod 01/30/23 23:38:26.061
Jan 30 23:38:26.105: INFO: Waiting for pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa to disappear
Jan 30 23:38:26.125: INFO: Pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:38:26.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-529" for this suite. 01/30/23 23:38:26.15
------------------------------
• [SLOW TEST] [6.351 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:38:19.83
    Jan 30 23:38:19.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:38:19.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:19.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:19.911
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:38:19.926
    Jan 30 23:38:19.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa" in namespace "downward-api-529" to be "Succeeded or Failed"
    Jan 30 23:38:19.978: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.507016ms
    Jan 30 23:38:21.999: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039455013s
    Jan 30 23:38:23.997: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037194951s
    Jan 30 23:38:26.000: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040677709s
    STEP: Saw pod success 01/30/23 23:38:26
    Jan 30 23:38:26.001: INFO: Pod "downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa" satisfied condition "Succeeded or Failed"
    Jan 30 23:38:26.019: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa container client-container: <nil>
    STEP: delete the pod 01/30/23 23:38:26.061
    Jan 30 23:38:26.105: INFO: Waiting for pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa to disappear
    Jan 30 23:38:26.125: INFO: Pod downwardapi-volume-d10404fe-d092-4b44-8ca8-a709700b6daa no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:38:26.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-529" for this suite. 01/30/23 23:38:26.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:38:26.183
Jan 30 23:38:26.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:38:26.185
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:26.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:26.256
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-f3df55e0-6877-4cb5-a808-ea5dda04adf0 01/30/23 23:38:26.273
STEP: Creating a pod to test consume configMaps 01/30/23 23:38:26.297
Jan 30 23:38:26.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143" in namespace "projected-9259" to be "Succeeded or Failed"
Jan 30 23:38:26.355: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 20.688905ms
Jan 30 23:38:28.375: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041006362s
Jan 30 23:38:30.376: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041901696s
Jan 30 23:38:32.376: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041612205s
STEP: Saw pod success 01/30/23 23:38:32.376
Jan 30 23:38:32.377: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143" satisfied condition "Succeeded or Failed"
Jan 30 23:38:32.396: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:38:32.433
Jan 30 23:38:32.497: INFO: Waiting for pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 to disappear
Jan 30 23:38:32.516: INFO: Pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:38:32.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9259" for this suite. 01/30/23 23:38:32.542
------------------------------
• [SLOW TEST] [6.389 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:38:26.183
    Jan 30 23:38:26.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:38:26.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:26.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:26.256
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-f3df55e0-6877-4cb5-a808-ea5dda04adf0 01/30/23 23:38:26.273
    STEP: Creating a pod to test consume configMaps 01/30/23 23:38:26.297
    Jan 30 23:38:26.334: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143" in namespace "projected-9259" to be "Succeeded or Failed"
    Jan 30 23:38:26.355: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 20.688905ms
    Jan 30 23:38:28.375: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041006362s
    Jan 30 23:38:30.376: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041901696s
    Jan 30 23:38:32.376: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041612205s
    STEP: Saw pod success 01/30/23 23:38:32.376
    Jan 30 23:38:32.377: INFO: Pod "pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143" satisfied condition "Succeeded or Failed"
    Jan 30 23:38:32.396: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:38:32.433
    Jan 30 23:38:32.497: INFO: Waiting for pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 to disappear
    Jan 30 23:38:32.516: INFO: Pod pod-projected-configmaps-db6689c1-7cac-4f4f-879d-715e2be1d143 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:38:32.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9259" for this suite. 01/30/23 23:38:32.542
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:38:32.574
Jan 30 23:38:32.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:38:32.577
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:32.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:32.646
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jan 30 23:38:32.728: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:39:32.891: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 01/30/23 23:39:32.911
Jan 30 23:39:32.990: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 30 23:39:33.016: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 30 23:39:33.081: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 30 23:39:33.107: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 30 23:39:33.170: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 30 23:39:33.193: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/30/23 23:39:33.193
Jan 30 23:39:33.193: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:33.227: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 34.24401ms
Jan 30 23:39:35.258: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06508582s
Jan 30 23:39:37.246: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.0528159s
Jan 30 23:39:37.246: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 30 23:39:37.246: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:37.272: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.496483ms
Jan 30 23:39:37.272: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:39:37.272: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:37.289: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.077693ms
Jan 30 23:39:37.289: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:39:37.289: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:37.306: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.781538ms
Jan 30 23:39:37.306: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:39:37.306: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:37.325: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.708097ms
Jan 30 23:39:37.325: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 30 23:39:37.325: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
Jan 30 23:39:37.343: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.727268ms
Jan 30 23:39:37.343: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/30/23 23:39:37.343
Jan 30 23:39:37.393: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 30 23:39:37.412: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.333942ms
Jan 30 23:39:39.432: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038395003s
Jan 30 23:39:41.442: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.047990287s
Jan 30 23:39:41.442: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:39:41.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4626" for this suite. 01/30/23 23:39:41.97
------------------------------
• [SLOW TEST] [69.425 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:38:32.574
    Jan 30 23:38:32.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-preemption 01/30/23 23:38:32.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:38:32.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:38:32.646
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jan 30 23:38:32.728: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:39:32.891: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 01/30/23 23:39:32.911
    Jan 30 23:39:32.990: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 30 23:39:33.016: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 30 23:39:33.081: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 30 23:39:33.107: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 30 23:39:33.170: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 30 23:39:33.193: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/30/23 23:39:33.193
    Jan 30 23:39:33.193: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:33.227: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 34.24401ms
    Jan 30 23:39:35.258: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06508582s
    Jan 30 23:39:37.246: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.0528159s
    Jan 30 23:39:37.246: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 30 23:39:37.246: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:37.272: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 25.496483ms
    Jan 30 23:39:37.272: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:39:37.272: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:37.289: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.077693ms
    Jan 30 23:39:37.289: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:39:37.289: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:37.306: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.781538ms
    Jan 30 23:39:37.306: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:39:37.306: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:37.325: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.708097ms
    Jan 30 23:39:37.325: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 30 23:39:37.325: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4626" to be "running"
    Jan 30 23:39:37.343: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.727268ms
    Jan 30 23:39:37.343: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/30/23 23:39:37.343
    Jan 30 23:39:37.393: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 30 23:39:37.412: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.333942ms
    Jan 30 23:39:39.432: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038395003s
    Jan 30 23:39:41.442: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.047990287s
    Jan 30 23:39:41.442: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:39:41.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4626" for this suite. 01/30/23 23:39:41.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:39:42.004
Jan 30 23:39:42.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:39:42.006
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:42.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:42.099
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 30 23:39:42.135: INFO: Got root ca configmap in namespace "svcaccounts-4068"
Jan 30 23:39:42.190: INFO: Deleted root ca configmap in namespace "svcaccounts-4068"
STEP: waiting for a new root ca configmap created 01/30/23 23:39:42.691
Jan 30 23:39:42.711: INFO: Recreated root ca configmap in namespace "svcaccounts-4068"
Jan 30 23:39:42.732: INFO: Updated root ca configmap in namespace "svcaccounts-4068"
STEP: waiting for the root ca configmap reconciled 01/30/23 23:39:43.233
Jan 30 23:39:43.252: INFO: Reconciled root ca configmap in namespace "svcaccounts-4068"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 23:39:43.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4068" for this suite. 01/30/23 23:39:43.277
------------------------------
• [1.301 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:39:42.004
    Jan 30 23:39:42.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:39:42.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:42.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:42.099
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 30 23:39:42.135: INFO: Got root ca configmap in namespace "svcaccounts-4068"
    Jan 30 23:39:42.190: INFO: Deleted root ca configmap in namespace "svcaccounts-4068"
    STEP: waiting for a new root ca configmap created 01/30/23 23:39:42.691
    Jan 30 23:39:42.711: INFO: Recreated root ca configmap in namespace "svcaccounts-4068"
    Jan 30 23:39:42.732: INFO: Updated root ca configmap in namespace "svcaccounts-4068"
    STEP: waiting for the root ca configmap reconciled 01/30/23 23:39:43.233
    Jan 30 23:39:43.252: INFO: Reconciled root ca configmap in namespace "svcaccounts-4068"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:39:43.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4068" for this suite. 01/30/23 23:39:43.277
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:39:43.308
Jan 30 23:39:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/30/23 23:39:43.31
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:43.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:43.402
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/30/23 23:39:43.465
STEP: delete the rc 01/30/23 23:39:48.522
STEP: wait for the rc to be deleted 01/30/23 23:39:48.575
Jan 30 23:39:49.693: INFO: 10 pods remaining
Jan 30 23:39:49.693: INFO: 0 pods has nil DeletionTimestamp
Jan 30 23:39:49.693: INFO: 
STEP: Gathering metrics 01/30/23 23:39:50.616
W0130 23:39:50.680151      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 30 23:39:50.680: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 30 23:39:50.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8382" for this suite. 01/30/23 23:39:50.714
------------------------------
• [SLOW TEST] [7.442 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:39:43.308
    Jan 30 23:39:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/30/23 23:39:43.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:43.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:43.402
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/30/23 23:39:43.465
    STEP: delete the rc 01/30/23 23:39:48.522
    STEP: wait for the rc to be deleted 01/30/23 23:39:48.575
    Jan 30 23:39:49.693: INFO: 10 pods remaining
    Jan 30 23:39:49.693: INFO: 0 pods has nil DeletionTimestamp
    Jan 30 23:39:49.693: INFO: 
    STEP: Gathering metrics 01/30/23 23:39:50.616
    W0130 23:39:50.680151      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 30 23:39:50.680: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:39:50.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8382" for this suite. 01/30/23 23:39:50.714
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:39:50.75
Jan 30 23:39:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename podtemplate 01/30/23 23:39:50.751
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:50.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:50.858
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/30/23 23:39:50.873
STEP: Replace a pod template 01/30/23 23:39:50.921
Jan 30 23:39:50.979: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 23:39:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3118" for this suite. 01/30/23 23:39:51.027
------------------------------
• [0.309 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:39:50.75
    Jan 30 23:39:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename podtemplate 01/30/23 23:39:50.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:50.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:50.858
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/30/23 23:39:50.873
    STEP: Replace a pod template 01/30/23 23:39:50.921
    Jan 30 23:39:50.979: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:39:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3118" for this suite. 01/30/23 23:39:51.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:39:51.061
Jan 30 23:39:51.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:39:51.062
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:51.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:51.133
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:39:51.149
Jan 30 23:39:51.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f" in namespace "downward-api-9600" to be "Succeeded or Failed"
Jan 30 23:39:51.231: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.198591ms
Jan 30 23:39:53.259: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076572295s
Jan 30 23:39:55.262: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079176091s
Jan 30 23:39:57.258: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07516728s
Jan 30 23:39:59.252: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069153359s
Jan 30 23:40:01.250: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.067610676s
STEP: Saw pod success 01/30/23 23:40:01.25
Jan 30 23:40:01.251: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f" satisfied condition "Succeeded or Failed"
Jan 30 23:40:01.275: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f container client-container: <nil>
STEP: delete the pod 01/30/23 23:40:01.33
Jan 30 23:40:01.402: INFO: Waiting for pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f to disappear
Jan 30 23:40:01.423: INFO: Pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:01.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9600" for this suite. 01/30/23 23:40:01.483
------------------------------
• [SLOW TEST] [10.450 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:39:51.061
    Jan 30 23:39:51.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:39:51.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:39:51.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:39:51.133
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:39:51.149
    Jan 30 23:39:51.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f" in namespace "downward-api-9600" to be "Succeeded or Failed"
    Jan 30 23:39:51.231: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 48.198591ms
    Jan 30 23:39:53.259: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076572295s
    Jan 30 23:39:55.262: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079176091s
    Jan 30 23:39:57.258: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07516728s
    Jan 30 23:39:59.252: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069153359s
    Jan 30 23:40:01.250: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.067610676s
    STEP: Saw pod success 01/30/23 23:40:01.25
    Jan 30 23:40:01.251: INFO: Pod "downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f" satisfied condition "Succeeded or Failed"
    Jan 30 23:40:01.275: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f container client-container: <nil>
    STEP: delete the pod 01/30/23 23:40:01.33
    Jan 30 23:40:01.402: INFO: Waiting for pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f to disappear
    Jan 30 23:40:01.423: INFO: Pod downwardapi-volume-c3fd9c70-9517-44c8-be5e-2a1d08aa899f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:01.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9600" for this suite. 01/30/23 23:40:01.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:01.513
Jan 30 23:40:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 23:40:01.514
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:01.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:01.59
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 30 23:40:01.812: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:40:01.837
Jan 30 23:40:01.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:40:01.932: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:40:02.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:40:02.983: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:40:03.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:40:03.981: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:40:04.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:40:04.980: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
Jan 30 23:40:05.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:40:05.982: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/30/23 23:40:06.048
STEP: Check that daemon pods images are updated. 01/30/23 23:40:06.115
Jan 30 23:40:06.134: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:06.134: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:07.189: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:07.189: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:08.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:08.188: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:09.187: INFO: Pod daemon-set-b4n5z is not available
Jan 30 23:40:09.187: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:09.187: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:10.187: INFO: Pod daemon-set-b4n5z is not available
Jan 30 23:40:10.187: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:10.187: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:11.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:12.193: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:13.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:13.188: INFO: Pod daemon-set-h6pv5 is not available
Jan 30 23:40:14.191: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:14.191: INFO: Pod daemon-set-h6pv5 is not available
Jan 30 23:40:15.189: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 30 23:40:15.189: INFO: Pod daemon-set-h6pv5 is not available
Jan 30 23:40:17.186: INFO: Pod daemon-set-7bvqd is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/30/23 23:40:17.212
Jan 30 23:40:17.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:40:17.257: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:40:18.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:40:18.305: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:40:19.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:40:19.304: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:40:20.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:40:20.328: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:40:20.417
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7636, will wait for the garbage collector to delete the pods 01/30/23 23:40:20.418
Jan 30 23:40:20.542: INFO: Deleting DaemonSet.extensions daemon-set took: 29.458656ms
Jan 30 23:40:20.743: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.843252ms
Jan 30 23:40:23.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:40:23.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 23:40:23.377: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40581"},"items":null}

Jan 30 23:40:23.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40581"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7636" for this suite. 01/30/23 23:40:23.514
------------------------------
• [SLOW TEST] [22.031 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:01.513
    Jan 30 23:40:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 23:40:01.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:01.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:01.59
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 30 23:40:01.812: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:40:01.837
    Jan 30 23:40:01.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:40:01.932: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:40:02.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:40:02.983: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:40:03.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:40:03.981: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:40:04.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:40:04.980: INFO: Node 10.15.28.227 is running 0 daemon pod, expected 1
    Jan 30 23:40:05.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:40:05.982: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/30/23 23:40:06.048
    STEP: Check that daemon pods images are updated. 01/30/23 23:40:06.115
    Jan 30 23:40:06.134: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:06.134: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:07.189: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:07.189: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:08.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:08.188: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:09.187: INFO: Pod daemon-set-b4n5z is not available
    Jan 30 23:40:09.187: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:09.187: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:10.187: INFO: Pod daemon-set-b4n5z is not available
    Jan 30 23:40:10.187: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:10.187: INFO: Wrong image for pod: daemon-set-vnzsq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:11.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:12.193: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:13.188: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:13.188: INFO: Pod daemon-set-h6pv5 is not available
    Jan 30 23:40:14.191: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:14.191: INFO: Pod daemon-set-h6pv5 is not available
    Jan 30 23:40:15.189: INFO: Wrong image for pod: daemon-set-bx7jn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 30 23:40:15.189: INFO: Pod daemon-set-h6pv5 is not available
    Jan 30 23:40:17.186: INFO: Pod daemon-set-7bvqd is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/30/23 23:40:17.212
    Jan 30 23:40:17.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:40:17.257: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:40:18.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:40:18.305: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:40:19.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:40:19.304: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:40:20.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:40:20.328: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:40:20.417
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7636, will wait for the garbage collector to delete the pods 01/30/23 23:40:20.418
    Jan 30 23:40:20.542: INFO: Deleting DaemonSet.extensions daemon-set took: 29.458656ms
    Jan 30 23:40:20.743: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.843252ms
    Jan 30 23:40:23.360: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:40:23.360: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 23:40:23.377: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40581"},"items":null}

    Jan 30 23:40:23.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40581"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7636" for this suite. 01/30/23 23:40:23.514
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:23.546
Jan 30 23:40:23.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/30/23 23:40:23.549
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:23.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:23.618
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/30/23 23:40:23.634
Jan 30 23:40:23.675: INFO: Waiting up to 5m0s for pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111" in namespace "var-expansion-7477" to be "Succeeded or Failed"
Jan 30 23:40:23.698: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Pending", Reason="", readiness=false. Elapsed: 22.991186ms
Jan 30 23:40:25.750: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Running", Reason="", readiness=true. Elapsed: 2.074843887s
Jan 30 23:40:27.715: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Running", Reason="", readiness=false. Elapsed: 4.039609872s
Jan 30 23:40:29.720: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044566865s
STEP: Saw pod success 01/30/23 23:40:29.72
Jan 30 23:40:29.721: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111" satisfied condition "Succeeded or Failed"
Jan 30 23:40:29.738: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 container dapi-container: <nil>
STEP: delete the pod 01/30/23 23:40:29.779
Jan 30 23:40:29.835: INFO: Waiting for pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 to disappear
Jan 30 23:40:29.853: INFO: Pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7477" for this suite. 01/30/23 23:40:29.901
------------------------------
• [SLOW TEST] [6.386 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:23.546
    Jan 30 23:40:23.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/30/23 23:40:23.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:23.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:23.618
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/30/23 23:40:23.634
    Jan 30 23:40:23.675: INFO: Waiting up to 5m0s for pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111" in namespace "var-expansion-7477" to be "Succeeded or Failed"
    Jan 30 23:40:23.698: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Pending", Reason="", readiness=false. Elapsed: 22.991186ms
    Jan 30 23:40:25.750: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Running", Reason="", readiness=true. Elapsed: 2.074843887s
    Jan 30 23:40:27.715: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Running", Reason="", readiness=false. Elapsed: 4.039609872s
    Jan 30 23:40:29.720: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044566865s
    STEP: Saw pod success 01/30/23 23:40:29.72
    Jan 30 23:40:29.721: INFO: Pod "var-expansion-a624fb11-b282-4211-8856-245ac5823111" satisfied condition "Succeeded or Failed"
    Jan 30 23:40:29.738: INFO: Trying to get logs from node 10.15.28.227 pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 container dapi-container: <nil>
    STEP: delete the pod 01/30/23 23:40:29.779
    Jan 30 23:40:29.835: INFO: Waiting for pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 to disappear
    Jan 30 23:40:29.853: INFO: Pod var-expansion-a624fb11-b282-4211-8856-245ac5823111 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:29.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7477" for this suite. 01/30/23 23:40:29.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:29.935
Jan 30 23:40:29.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename endpointslice 01/30/23 23:40:29.937
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:30.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:30.034
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/30/23 23:40:30.05
STEP: getting /apis/discovery.k8s.io 01/30/23 23:40:30.066
STEP: getting /apis/discovery.k8s.iov1 01/30/23 23:40:30.075
STEP: creating 01/30/23 23:40:30.083
STEP: getting 01/30/23 23:40:30.155
STEP: listing 01/30/23 23:40:30.171
STEP: watching 01/30/23 23:40:30.187
Jan 30 23:40:30.188: INFO: starting watch
STEP: cluster-wide listing 01/30/23 23:40:30.194
STEP: cluster-wide watching 01/30/23 23:40:30.215
Jan 30 23:40:30.216: INFO: starting watch
STEP: patching 01/30/23 23:40:30.223
STEP: updating 01/30/23 23:40:30.247
Jan 30 23:40:30.291: INFO: waiting for watch events with expected annotations
Jan 30 23:40:30.291: INFO: saw patched and updated annotations
STEP: deleting 01/30/23 23:40:30.292
STEP: deleting a collection 01/30/23 23:40:30.346
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:30.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-68" for this suite. 01/30/23 23:40:30.45
------------------------------
• [0.552 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:29.935
    Jan 30 23:40:29.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename endpointslice 01/30/23 23:40:29.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:30.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:30.034
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/30/23 23:40:30.05
    STEP: getting /apis/discovery.k8s.io 01/30/23 23:40:30.066
    STEP: getting /apis/discovery.k8s.iov1 01/30/23 23:40:30.075
    STEP: creating 01/30/23 23:40:30.083
    STEP: getting 01/30/23 23:40:30.155
    STEP: listing 01/30/23 23:40:30.171
    STEP: watching 01/30/23 23:40:30.187
    Jan 30 23:40:30.188: INFO: starting watch
    STEP: cluster-wide listing 01/30/23 23:40:30.194
    STEP: cluster-wide watching 01/30/23 23:40:30.215
    Jan 30 23:40:30.216: INFO: starting watch
    STEP: patching 01/30/23 23:40:30.223
    STEP: updating 01/30/23 23:40:30.247
    Jan 30 23:40:30.291: INFO: waiting for watch events with expected annotations
    Jan 30 23:40:30.291: INFO: saw patched and updated annotations
    STEP: deleting 01/30/23 23:40:30.292
    STEP: deleting a collection 01/30/23 23:40:30.346
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:30.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-68" for this suite. 01/30/23 23:40:30.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:30.495
Jan 30 23:40:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/30/23 23:40:30.497
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:30.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:30.57
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 30 23:40:30.642: INFO: Waiting up to 2m0s for pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" in namespace "var-expansion-5517" to be "container 0 failed with reason CreateContainerConfigError"
Jan 30 23:40:30.664: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef": Phase="Pending", Reason="", readiness=false. Elapsed: 21.972206ms
Jan 30 23:40:32.690: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047732911s
Jan 30 23:40:32.690: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 30 23:40:32.690: INFO: Deleting pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" in namespace "var-expansion-5517"
Jan 30 23:40:32.721: INFO: Wait up to 5m0s for pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5517" for this suite. 01/30/23 23:40:36.789
------------------------------
• [SLOW TEST] [6.330 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:30.495
    Jan 30 23:40:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/30/23 23:40:30.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:30.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:30.57
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 30 23:40:30.642: INFO: Waiting up to 2m0s for pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" in namespace "var-expansion-5517" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 30 23:40:30.664: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef": Phase="Pending", Reason="", readiness=false. Elapsed: 21.972206ms
    Jan 30 23:40:32.690: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047732911s
    Jan 30 23:40:32.690: INFO: Pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 30 23:40:32.690: INFO: Deleting pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" in namespace "var-expansion-5517"
    Jan 30 23:40:32.721: INFO: Wait up to 5m0s for pod "var-expansion-84de2238-60a9-4bb1-aaa6-1eb4ba106cef" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5517" for this suite. 01/30/23 23:40:36.789
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:36.826
Jan 30 23:40:36.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:40:36.83
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:36.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:36.905
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-755c9ecf-6cb8-4151-bfe2-d81f12d7945c 01/30/23 23:40:36.926
STEP: Creating a pod to test consume configMaps 01/30/23 23:40:36.951
Jan 30 23:40:36.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b" in namespace "configmap-2689" to be "Succeeded or Failed"
Jan 30 23:40:37.021: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.31802ms
Jan 30 23:40:39.039: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042327384s
Jan 30 23:40:41.038: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04128806s
Jan 30 23:40:43.040: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043299013s
STEP: Saw pod success 01/30/23 23:40:43.04
Jan 30 23:40:43.040: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b" satisfied condition "Succeeded or Failed"
Jan 30 23:40:43.059: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b container configmap-volume-test: <nil>
STEP: delete the pod 01/30/23 23:40:43.1
Jan 30 23:40:43.142: INFO: Waiting for pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b to disappear
Jan 30 23:40:43.163: INFO: Pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:43.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2689" for this suite. 01/30/23 23:40:43.188
------------------------------
• [SLOW TEST] [6.391 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:36.826
    Jan 30 23:40:36.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:40:36.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:36.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:36.905
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-755c9ecf-6cb8-4151-bfe2-d81f12d7945c 01/30/23 23:40:36.926
    STEP: Creating a pod to test consume configMaps 01/30/23 23:40:36.951
    Jan 30 23:40:36.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b" in namespace "configmap-2689" to be "Succeeded or Failed"
    Jan 30 23:40:37.021: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.31802ms
    Jan 30 23:40:39.039: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042327384s
    Jan 30 23:40:41.038: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04128806s
    Jan 30 23:40:43.040: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043299013s
    STEP: Saw pod success 01/30/23 23:40:43.04
    Jan 30 23:40:43.040: INFO: Pod "pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b" satisfied condition "Succeeded or Failed"
    Jan 30 23:40:43.059: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b container configmap-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:40:43.1
    Jan 30 23:40:43.142: INFO: Waiting for pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b to disappear
    Jan 30 23:40:43.163: INFO: Pod pod-configmaps-c09df93c-9aaa-48db-87fc-d98a5bb11b9b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:43.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2689" for this suite. 01/30/23 23:40:43.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:43.224
Jan 30 23:40:43.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sysctl 01/30/23 23:40:43.225
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:43.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:43.316
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/30/23 23:40:43.334
STEP: Watching for error events or started pod 01/30/23 23:40:43.376
STEP: Waiting for pod completion 01/30/23 23:40:45.398
Jan 30 23:40:45.398: INFO: Waiting up to 3m0s for pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec" in namespace "sysctl-7795" to be "completed"
Jan 30 23:40:45.423: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Pending", Reason="", readiness=false. Elapsed: 25.257545ms
Jan 30 23:40:47.450: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052413083s
Jan 30 23:40:49.452: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054464373s
Jan 30 23:40:49.452: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/30/23 23:40:49.478
STEP: Getting logs from the pod 01/30/23 23:40:49.479
STEP: Checking that the sysctl is actually updated 01/30/23 23:40:49.526
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:40:49.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7795" for this suite. 01/30/23 23:40:49.552
------------------------------
• [SLOW TEST] [6.357 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:43.224
    Jan 30 23:40:43.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sysctl 01/30/23 23:40:43.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:43.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:43.316
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/30/23 23:40:43.334
    STEP: Watching for error events or started pod 01/30/23 23:40:43.376
    STEP: Waiting for pod completion 01/30/23 23:40:45.398
    Jan 30 23:40:45.398: INFO: Waiting up to 3m0s for pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec" in namespace "sysctl-7795" to be "completed"
    Jan 30 23:40:45.423: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Pending", Reason="", readiness=false. Elapsed: 25.257545ms
    Jan 30 23:40:47.450: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052413083s
    Jan 30 23:40:49.452: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054464373s
    Jan 30 23:40:49.452: INFO: Pod "sysctl-0ddc85aa-c1ce-48cd-ad64-6c758cfabcec" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/30/23 23:40:49.478
    STEP: Getting logs from the pod 01/30/23 23:40:49.479
    STEP: Checking that the sysctl is actually updated 01/30/23 23:40:49.526
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:40:49.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7795" for this suite. 01/30/23 23:40:49.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:40:49.594
Jan 30 23:40:49.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:40:49.596
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:49.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:49.698
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 30 23:40:49.772: INFO: Waiting up to 5m0s for pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16" in namespace "container-probe-3595" to be "running and ready"
Jan 30 23:40:49.822: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Pending", Reason="", readiness=false. Elapsed: 49.14117ms
Jan 30 23:40:49.822: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:40:51.841: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 2.068763635s
Jan 30 23:40:51.841: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:40:53.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 4.070811082s
Jan 30 23:40:53.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:40:55.874: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 6.101481307s
Jan 30 23:40:55.874: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:40:57.845: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 8.072477439s
Jan 30 23:40:57.845: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:40:59.842: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 10.069935642s
Jan 30 23:40:59.842: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:01.839: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 12.066969928s
Jan 30 23:41:01.840: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:03.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 14.070220286s
Jan 30 23:41:03.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:05.846: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 16.073258421s
Jan 30 23:41:05.846: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:07.842: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 18.069465903s
Jan 30 23:41:07.842: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:09.844: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 20.071033778s
Jan 30 23:41:09.844: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
Jan 30 23:41:11.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=true. Elapsed: 22.070226417s
Jan 30 23:41:11.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = true)
Jan 30 23:41:11.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16" satisfied condition "running and ready"
Jan 30 23:41:11.867: INFO: Container started at 2023-01-30 23:40:51 +0000 UTC, pod became ready at 2023-01-30 23:41:10 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3595" for this suite. 01/30/23 23:41:11.894
------------------------------
• [SLOW TEST] [22.339 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:40:49.594
    Jan 30 23:40:49.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:40:49.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:40:49.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:40:49.698
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 30 23:40:49.772: INFO: Waiting up to 5m0s for pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16" in namespace "container-probe-3595" to be "running and ready"
    Jan 30 23:40:49.822: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Pending", Reason="", readiness=false. Elapsed: 49.14117ms
    Jan 30 23:40:49.822: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:40:51.841: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 2.068763635s
    Jan 30 23:40:51.841: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:40:53.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 4.070811082s
    Jan 30 23:40:53.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:40:55.874: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 6.101481307s
    Jan 30 23:40:55.874: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:40:57.845: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 8.072477439s
    Jan 30 23:40:57.845: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:40:59.842: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 10.069935642s
    Jan 30 23:40:59.842: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:01.839: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 12.066969928s
    Jan 30 23:41:01.840: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:03.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 14.070220286s
    Jan 30 23:41:03.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:05.846: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 16.073258421s
    Jan 30 23:41:05.846: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:07.842: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 18.069465903s
    Jan 30 23:41:07.842: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:09.844: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=false. Elapsed: 20.071033778s
    Jan 30 23:41:09.844: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = false)
    Jan 30 23:41:11.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16": Phase="Running", Reason="", readiness=true. Elapsed: 22.070226417s
    Jan 30 23:41:11.843: INFO: The phase of Pod test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16 is Running (Ready = true)
    Jan 30 23:41:11.843: INFO: Pod "test-webserver-31fcfcbb-6bf6-400a-83d3-69efdc126d16" satisfied condition "running and ready"
    Jan 30 23:41:11.867: INFO: Container started at 2023-01-30 23:40:51 +0000 UTC, pod became ready at 2023-01-30 23:41:10 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:11.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3595" for this suite. 01/30/23 23:41:11.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:11.941
Jan 30 23:41:11.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:41:11.943
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:12.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:12.035
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/30/23 23:41:12.051
Jan 30 23:41:12.096: INFO: Waiting up to 5m0s for pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c" in namespace "projected-1156" to be "running and ready"
Jan 30 23:41:12.112: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.217709ms
Jan 30 23:41:12.113: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:41:14.134: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038268808s
Jan 30 23:41:14.135: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:41:16.132: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Running", Reason="", readiness=true. Elapsed: 4.035567565s
Jan 30 23:41:16.132: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Running (Ready = true)
Jan 30 23:41:16.132: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c" satisfied condition "running and ready"
Jan 30 23:41:16.739: INFO: Successfully updated pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1156" for this suite. 01/30/23 23:41:18.875
------------------------------
• [SLOW TEST] [6.962 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:11.941
    Jan 30 23:41:11.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:41:11.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:12.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:12.035
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/30/23 23:41:12.051
    Jan 30 23:41:12.096: INFO: Waiting up to 5m0s for pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c" in namespace "projected-1156" to be "running and ready"
    Jan 30 23:41:12.112: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.217709ms
    Jan 30 23:41:12.113: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:41:14.134: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038268808s
    Jan 30 23:41:14.135: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:41:16.132: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c": Phase="Running", Reason="", readiness=true. Elapsed: 4.035567565s
    Jan 30 23:41:16.132: INFO: The phase of Pod annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c is Running (Ready = true)
    Jan 30 23:41:16.132: INFO: Pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c" satisfied condition "running and ready"
    Jan 30 23:41:16.739: INFO: Successfully updated pod "annotationupdateeaeaa800-8809-48d4-8ec7-f0c13adc200c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1156" for this suite. 01/30/23 23:41:18.875
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:18.908
Jan 30 23:41:18.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:41:18.914
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:18.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:19.016
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/30/23 23:41:19.032
STEP: submitting the pod to kubernetes 01/30/23 23:41:19.033
Jan 30 23:41:19.098: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" in namespace "pods-2186" to be "running and ready"
Jan 30 23:41:19.143: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.535431ms
Jan 30 23:41:19.143: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:41:21.165: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066977255s
Jan 30 23:41:21.165: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:41:23.162: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=true. Elapsed: 4.063882678s
Jan 30 23:41:23.162: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Running (Ready = true)
Jan 30 23:41:23.162: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/30/23 23:41:23.181
STEP: updating the pod 01/30/23 23:41:23.201
Jan 30 23:41:23.755: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f"
Jan 30 23:41:23.755: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" in namespace "pods-2186" to be "terminated with reason DeadlineExceeded"
Jan 30 23:41:23.783: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=true. Elapsed: 27.807176ms
Jan 30 23:41:25.804: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=false. Elapsed: 2.048980277s
Jan 30 23:41:27.803: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.047128282s
Jan 30 23:41:27.803: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:27.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2186" for this suite. 01/30/23 23:41:27.828
------------------------------
• [SLOW TEST] [8.949 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:18.908
    Jan 30 23:41:18.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:41:18.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:18.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:19.016
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/30/23 23:41:19.032
    STEP: submitting the pod to kubernetes 01/30/23 23:41:19.033
    Jan 30 23:41:19.098: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" in namespace "pods-2186" to be "running and ready"
    Jan 30 23:41:19.143: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.535431ms
    Jan 30 23:41:19.143: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:41:21.165: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066977255s
    Jan 30 23:41:21.165: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:41:23.162: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=true. Elapsed: 4.063882678s
    Jan 30 23:41:23.162: INFO: The phase of Pod pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f is Running (Ready = true)
    Jan 30 23:41:23.162: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/30/23 23:41:23.181
    STEP: updating the pod 01/30/23 23:41:23.201
    Jan 30 23:41:23.755: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f"
    Jan 30 23:41:23.755: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" in namespace "pods-2186" to be "terminated with reason DeadlineExceeded"
    Jan 30 23:41:23.783: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=true. Elapsed: 27.807176ms
    Jan 30 23:41:25.804: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Running", Reason="", readiness=false. Elapsed: 2.048980277s
    Jan 30 23:41:27.803: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.047128282s
    Jan 30 23:41:27.803: INFO: Pod "pod-update-activedeadlineseconds-9e628077-69de-4981-9ef2-ab0cf43ac27f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:27.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2186" for this suite. 01/30/23 23:41:27.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:27.865
Jan 30 23:41:27.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:41:27.867
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:27.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:27.937
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4247/configmap-test-a0f62bf2-517e-4992-a479-141f1bd682ab 01/30/23 23:41:27.954
STEP: Creating a pod to test consume configMaps 01/30/23 23:41:27.996
Jan 30 23:41:28.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2" in namespace "configmap-4247" to be "Succeeded or Failed"
Jan 30 23:41:28.056: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.089701ms
Jan 30 23:41:30.076: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039004283s
Jan 30 23:41:32.077: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039976996s
Jan 30 23:41:34.082: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044868531s
STEP: Saw pod success 01/30/23 23:41:34.082
Jan 30 23:41:34.082: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2" satisfied condition "Succeeded or Failed"
Jan 30 23:41:34.101: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 container env-test: <nil>
STEP: delete the pod 01/30/23 23:41:34.141
Jan 30 23:41:34.192: INFO: Waiting for pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 to disappear
Jan 30 23:41:34.212: INFO: Pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:34.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4247" for this suite. 01/30/23 23:41:34.236
------------------------------
• [SLOW TEST] [6.401 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:27.865
    Jan 30 23:41:27.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:41:27.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:27.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:27.937
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4247/configmap-test-a0f62bf2-517e-4992-a479-141f1bd682ab 01/30/23 23:41:27.954
    STEP: Creating a pod to test consume configMaps 01/30/23 23:41:27.996
    Jan 30 23:41:28.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2" in namespace "configmap-4247" to be "Succeeded or Failed"
    Jan 30 23:41:28.056: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.089701ms
    Jan 30 23:41:30.076: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039004283s
    Jan 30 23:41:32.077: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039976996s
    Jan 30 23:41:34.082: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044868531s
    STEP: Saw pod success 01/30/23 23:41:34.082
    Jan 30 23:41:34.082: INFO: Pod "pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2" satisfied condition "Succeeded or Failed"
    Jan 30 23:41:34.101: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 container env-test: <nil>
    STEP: delete the pod 01/30/23 23:41:34.141
    Jan 30 23:41:34.192: INFO: Waiting for pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 to disappear
    Jan 30 23:41:34.212: INFO: Pod pod-configmaps-2f242427-7ed9-46a3-9cc8-c061580371b2 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:34.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4247" for this suite. 01/30/23 23:41:34.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:34.277
Jan 30 23:41:34.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:41:34.28
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:34.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:34.376
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 30 23:41:34.483: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9059 to be scheduled
Jan 30 23:41:34.501: INFO: 1 pods are not scheduled: [runtimeclass-9059/test-runtimeclass-runtimeclass-9059-preconfigured-handler-t22qx(f0ec2328-4d6c-4cf7-8d8d-30af9419012e)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:36.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9059" for this suite. 01/30/23 23:41:36.574
------------------------------
• [2.325 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:34.277
    Jan 30 23:41:34.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:41:34.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:34.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:34.376
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 30 23:41:34.483: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9059 to be scheduled
    Jan 30 23:41:34.501: INFO: 1 pods are not scheduled: [runtimeclass-9059/test-runtimeclass-runtimeclass-9059-preconfigured-handler-t22qx(f0ec2328-4d6c-4cf7-8d8d-30af9419012e)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:36.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9059" for this suite. 01/30/23 23:41:36.574
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:36.603
Jan 30 23:41:36.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 23:41:36.605
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:36.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:36.705
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/30/23 23:41:36.821
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:41:36.849
Jan 30 23:41:36.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:41:36.920: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:41:38.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:41:38.003: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:41:39.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 30 23:41:39.003: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:41:39.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:41:39.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/30/23 23:41:39.976
Jan 30 23:41:40.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:40.077: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:41.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:41.142: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:42.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:42.124: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:43.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:43.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:44.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:44.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:45.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:41:45.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:41:46.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:41:46.120: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:41:46.142
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5398, will wait for the garbage collector to delete the pods 01/30/23 23:41:46.142
Jan 30 23:41:46.240: INFO: Deleting DaemonSet.extensions daemon-set took: 28.58224ms
Jan 30 23:41:46.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.392122ms
Jan 30 23:41:48.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:41:48.860: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 23:41:48.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41133"},"items":null}

Jan 30 23:41:48.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41133"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:41:48.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5398" for this suite. 01/30/23 23:41:49.041
------------------------------
• [SLOW TEST] [12.465 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:36.603
    Jan 30 23:41:36.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 23:41:36.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:36.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:36.705
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/30/23 23:41:36.821
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:41:36.849
    Jan 30 23:41:36.920: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:41:36.920: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:41:38.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:41:38.003: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:41:39.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 30 23:41:39.003: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:41:39.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:41:39.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/30/23 23:41:39.976
    Jan 30 23:41:40.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:40.077: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:41.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:41.142: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:42.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:42.124: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:43.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:43.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:44.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:44.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:45.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:41:45.123: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:41:46.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:41:46.120: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:41:46.142
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5398, will wait for the garbage collector to delete the pods 01/30/23 23:41:46.142
    Jan 30 23:41:46.240: INFO: Deleting DaemonSet.extensions daemon-set took: 28.58224ms
    Jan 30 23:41:46.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.392122ms
    Jan 30 23:41:48.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:41:48.860: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 23:41:48.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41133"},"items":null}

    Jan 30 23:41:48.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41133"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:41:48.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5398" for this suite. 01/30/23 23:41:49.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:41:49.069
Jan 30 23:41:49.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename taint-single-pod 01/30/23 23:41:49.071
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:49.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:49.152
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 30 23:41:49.177: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 30 23:42:49.308: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 30 23:42:49.329: INFO: Starting informer...
STEP: Starting pod... 01/30/23 23:42:49.329
Jan 30 23:42:49.595: INFO: Pod is running on 10.15.28.227. Tainting Node
STEP: Trying to apply a taint on the Node 01/30/23 23:42:49.596
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:42:49.663
STEP: Waiting short time to make sure Pod is queued for deletion 01/30/23 23:42:49.684
Jan 30 23:42:49.684: INFO: Pod wasn't evicted. Proceeding
Jan 30 23:42:49.684: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:42:49.741
STEP: Waiting some time to make sure that toleration time passed. 01/30/23 23:42:49.764
Jan 30 23:44:04.765: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:44:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-6692" for this suite. 01/30/23 23:44:04.791
------------------------------
• [SLOW TEST] [135.756 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:41:49.069
    Jan 30 23:41:49.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename taint-single-pod 01/30/23 23:41:49.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:41:49.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:41:49.152
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 30 23:41:49.177: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 30 23:42:49.308: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 30 23:42:49.329: INFO: Starting informer...
    STEP: Starting pod... 01/30/23 23:42:49.329
    Jan 30 23:42:49.595: INFO: Pod is running on 10.15.28.227. Tainting Node
    STEP: Trying to apply a taint on the Node 01/30/23 23:42:49.596
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:42:49.663
    STEP: Waiting short time to make sure Pod is queued for deletion 01/30/23 23:42:49.684
    Jan 30 23:42:49.684: INFO: Pod wasn't evicted. Proceeding
    Jan 30 23:42:49.684: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/30/23 23:42:49.741
    STEP: Waiting some time to make sure that toleration time passed. 01/30/23 23:42:49.764
    Jan 30 23:44:04.765: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:44:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-6692" for this suite. 01/30/23 23:44:04.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:44:04.832
Jan 30 23:44:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 23:44:04.833
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:44:04.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:44:04.907
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 23:44:04.977
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:44:05.526
STEP: Deploying the webhook pod 01/30/23 23:44:05.562
STEP: Wait for the deployment to be ready 01/30/23 23:44:05.606
Jan 30 23:44:05.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 30 23:44:07.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:44:09.723
STEP: Verifying the service has paired with the endpoint 01/30/23 23:44:09.771
Jan 30 23:44:10.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/30/23 23:44:10.813
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/30/23 23:44:10.821
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 23:44:10.821
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/30/23 23:44:10.821
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/30/23 23:44:10.829
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 23:44:10.829
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 23:44:10.844
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:44:10.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1257" for this suite. 01/30/23 23:44:11.082
STEP: Destroying namespace "webhook-1257-markers" for this suite. 01/30/23 23:44:11.143
------------------------------
• [SLOW TEST] [6.344 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:44:04.832
    Jan 30 23:44:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 23:44:04.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:44:04.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:44:04.907
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 23:44:04.977
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:44:05.526
    STEP: Deploying the webhook pod 01/30/23 23:44:05.562
    STEP: Wait for the deployment to be ready 01/30/23 23:44:05.606
    Jan 30 23:44:05.648: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 30 23:44:07.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 44, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:44:09.723
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:44:09.771
    Jan 30 23:44:10.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/30/23 23:44:10.813
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/30/23 23:44:10.821
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 23:44:10.821
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/30/23 23:44:10.821
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/30/23 23:44:10.829
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 23:44:10.829
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/30/23 23:44:10.844
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:44:10.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1257" for this suite. 01/30/23 23:44:11.082
    STEP: Destroying namespace "webhook-1257-markers" for this suite. 01/30/23 23:44:11.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:44:11.188
Jan 30 23:44:11.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename cronjob 01/30/23 23:44:11.19
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:44:11.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:44:11.269
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/30/23 23:44:11.286
STEP: Ensuring no jobs are scheduled 01/30/23 23:44:11.31
STEP: Ensuring no job exists by listing jobs explicitly 01/30/23 23:49:11.343
STEP: Removing cronjob 01/30/23 23:49:11.36
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 23:49:11.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1599" for this suite. 01/30/23 23:49:11.469
------------------------------
• [SLOW TEST] [300.318 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:44:11.188
    Jan 30 23:44:11.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename cronjob 01/30/23 23:44:11.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:44:11.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:44:11.269
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/30/23 23:44:11.286
    STEP: Ensuring no jobs are scheduled 01/30/23 23:44:11.31
    STEP: Ensuring no job exists by listing jobs explicitly 01/30/23 23:49:11.343
    STEP: Removing cronjob 01/30/23 23:49:11.36
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:49:11.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1599" for this suite. 01/30/23 23:49:11.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:49:11.524
Jan 30 23:49:11.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:49:11.526
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:11.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:11.641
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:49:11.662
Jan 30 23:49:11.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7" in namespace "projected-9016" to be "Succeeded or Failed"
Jan 30 23:49:11.716: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.587801ms
Jan 30 23:49:13.740: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043416684s
Jan 30 23:49:15.734: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037912268s
Jan 30 23:49:17.761: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065121336s
STEP: Saw pod success 01/30/23 23:49:17.761
Jan 30 23:49:17.762: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7" satisfied condition "Succeeded or Failed"
Jan 30 23:49:17.779: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 container client-container: <nil>
STEP: delete the pod 01/30/23 23:49:17.894
Jan 30 23:49:17.957: INFO: Waiting for pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 to disappear
Jan 30 23:49:17.976: INFO: Pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 30 23:49:17.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9016" for this suite. 01/30/23 23:49:18.02
------------------------------
• [SLOW TEST] [6.524 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:49:11.524
    Jan 30 23:49:11.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:49:11.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:11.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:11.641
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:49:11.662
    Jan 30 23:49:11.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7" in namespace "projected-9016" to be "Succeeded or Failed"
    Jan 30 23:49:11.716: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.587801ms
    Jan 30 23:49:13.740: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043416684s
    Jan 30 23:49:15.734: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037912268s
    Jan 30 23:49:17.761: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065121336s
    STEP: Saw pod success 01/30/23 23:49:17.761
    Jan 30 23:49:17.762: INFO: Pod "downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7" satisfied condition "Succeeded or Failed"
    Jan 30 23:49:17.779: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:49:17.894
    Jan 30 23:49:17.957: INFO: Waiting for pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 to disappear
    Jan 30 23:49:17.976: INFO: Pod downwardapi-volume-fe2d2d20-8055-4d2f-bc5f-83cd1a3031f7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:49:17.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9016" for this suite. 01/30/23 23:49:18.02
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:49:18.054
Jan 30 23:49:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/30/23 23:49:18.057
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:18.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:18.127
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-b2cb2dfc-da3c-4d0e-9b34-a31d538af1f5 01/30/23 23:49:18.146
STEP: Creating a pod to test consume configMaps 01/30/23 23:49:18.163
Jan 30 23:49:18.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc" in namespace "configmap-4851" to be "Succeeded or Failed"
Jan 30 23:49:18.221: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.658678ms
Jan 30 23:49:20.241: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039865005s
Jan 30 23:49:22.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038305217s
Jan 30 23:49:24.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038341427s
STEP: Saw pod success 01/30/23 23:49:24.24
Jan 30 23:49:24.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc" satisfied condition "Succeeded or Failed"
Jan 30 23:49:24.257: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:49:24.295
Jan 30 23:49:24.349: INFO: Waiting for pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc to disappear
Jan 30 23:49:24.365: INFO: Pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:49:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4851" for this suite. 01/30/23 23:49:24.407
------------------------------
• [SLOW TEST] [6.392 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:49:18.054
    Jan 30 23:49:18.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/30/23 23:49:18.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:18.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:18.127
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-b2cb2dfc-da3c-4d0e-9b34-a31d538af1f5 01/30/23 23:49:18.146
    STEP: Creating a pod to test consume configMaps 01/30/23 23:49:18.163
    Jan 30 23:49:18.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc" in namespace "configmap-4851" to be "Succeeded or Failed"
    Jan 30 23:49:18.221: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.658678ms
    Jan 30 23:49:20.241: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039865005s
    Jan 30 23:49:22.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038305217s
    Jan 30 23:49:24.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038341427s
    STEP: Saw pod success 01/30/23 23:49:24.24
    Jan 30 23:49:24.240: INFO: Pod "pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc" satisfied condition "Succeeded or Failed"
    Jan 30 23:49:24.257: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:49:24.295
    Jan 30 23:49:24.349: INFO: Waiting for pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc to disappear
    Jan 30 23:49:24.365: INFO: Pod pod-configmaps-1fdc78b5-22bb-48b2-9bf4-2d8a5ceea9fc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:49:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4851" for this suite. 01/30/23 23:49:24.407
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:49:24.449
Jan 30 23:49:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename subpath 01/30/23 23:49:24.451
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:24.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:24.518
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 23:49:24.531
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-8qs5 01/30/23 23:49:24.566
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:49:24.566
Jan 30 23:49:24.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8qs5" in namespace "subpath-3139" to be "Succeeded or Failed"
Jan 30 23:49:24.623: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.504169ms
Jan 30 23:49:26.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040871388s
Jan 30 23:49:28.666: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.063044481s
Jan 30 23:49:30.641: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.038473192s
Jan 30 23:49:32.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.041358921s
Jan 30 23:49:34.641: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.038790821s
Jan 30 23:49:36.642: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.039945449s
Jan 30 23:49:38.649: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.046054611s
Jan 30 23:49:40.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.040764186s
Jan 30 23:49:42.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.040153481s
Jan 30 23:49:44.642: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.039638556s
Jan 30 23:49:46.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 22.041371368s
Jan 30 23:49:48.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=false. Elapsed: 24.041955116s
Jan 30 23:49:50.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.040188909s
STEP: Saw pod success 01/30/23 23:49:50.643
Jan 30 23:49:50.644: INFO: Pod "pod-subpath-test-configmap-8qs5" satisfied condition "Succeeded or Failed"
Jan 30 23:49:50.658: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-configmap-8qs5 container test-container-subpath-configmap-8qs5: <nil>
STEP: delete the pod 01/30/23 23:49:50.693
Jan 30 23:49:50.751: INFO: Waiting for pod pod-subpath-test-configmap-8qs5 to disappear
Jan 30 23:49:50.768: INFO: Pod pod-subpath-test-configmap-8qs5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8qs5 01/30/23 23:49:50.768
Jan 30 23:49:50.768: INFO: Deleting pod "pod-subpath-test-configmap-8qs5" in namespace "subpath-3139"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 23:49:50.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3139" for this suite. 01/30/23 23:49:50.817
------------------------------
• [SLOW TEST] [26.400 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:49:24.449
    Jan 30 23:49:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename subpath 01/30/23 23:49:24.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:24.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:24.518
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 23:49:24.531
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-8qs5 01/30/23 23:49:24.566
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:49:24.566
    Jan 30 23:49:24.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8qs5" in namespace "subpath-3139" to be "Succeeded or Failed"
    Jan 30 23:49:24.623: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.504169ms
    Jan 30 23:49:26.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040871388s
    Jan 30 23:49:28.666: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 4.063044481s
    Jan 30 23:49:30.641: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 6.038473192s
    Jan 30 23:49:32.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 8.041358921s
    Jan 30 23:49:34.641: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 10.038790821s
    Jan 30 23:49:36.642: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 12.039945449s
    Jan 30 23:49:38.649: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 14.046054611s
    Jan 30 23:49:40.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 16.040764186s
    Jan 30 23:49:42.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 18.040153481s
    Jan 30 23:49:44.642: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 20.039638556s
    Jan 30 23:49:46.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=true. Elapsed: 22.041371368s
    Jan 30 23:49:48.644: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Running", Reason="", readiness=false. Elapsed: 24.041955116s
    Jan 30 23:49:50.643: INFO: Pod "pod-subpath-test-configmap-8qs5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.040188909s
    STEP: Saw pod success 01/30/23 23:49:50.643
    Jan 30 23:49:50.644: INFO: Pod "pod-subpath-test-configmap-8qs5" satisfied condition "Succeeded or Failed"
    Jan 30 23:49:50.658: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-configmap-8qs5 container test-container-subpath-configmap-8qs5: <nil>
    STEP: delete the pod 01/30/23 23:49:50.693
    Jan 30 23:49:50.751: INFO: Waiting for pod pod-subpath-test-configmap-8qs5 to disappear
    Jan 30 23:49:50.768: INFO: Pod pod-subpath-test-configmap-8qs5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8qs5 01/30/23 23:49:50.768
    Jan 30 23:49:50.768: INFO: Deleting pod "pod-subpath-test-configmap-8qs5" in namespace "subpath-3139"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:49:50.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3139" for this suite. 01/30/23 23:49:50.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:49:50.854
Jan 30 23:49:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:49:50.858
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:50.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:50.937
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 30 23:49:50.990: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a" in namespace "kubelet-test-8322" to be "running and ready"
Jan 30 23:49:51.005: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.688567ms
Jan 30 23:49:51.006: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:49:53.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033715023s
Jan 30 23:49:53.025: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:49:55.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Running", Reason="", readiness=true. Elapsed: 4.034357662s
Jan 30 23:49:55.025: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Running (Ready = true)
Jan 30 23:49:55.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 30 23:49:55.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8322" for this suite. 01/30/23 23:49:55.119
------------------------------
• [4.295 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:49:50.854
    Jan 30 23:49:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubelet-test 01/30/23 23:49:50.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:50.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:50.937
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 30 23:49:50.990: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a" in namespace "kubelet-test-8322" to be "running and ready"
    Jan 30 23:49:51.005: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.688567ms
    Jan 30 23:49:51.006: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:49:53.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033715023s
    Jan 30 23:49:53.025: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:49:55.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a": Phase="Running", Reason="", readiness=true. Elapsed: 4.034357662s
    Jan 30 23:49:55.025: INFO: The phase of Pod busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a is Running (Ready = true)
    Jan 30 23:49:55.025: INFO: Pod "busybox-readonly-fs52b13091-f45b-4b9e-baa8-482f2229562a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:49:55.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8322" for this suite. 01/30/23 23:49:55.119
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:49:55.15
Jan 30 23:49:55.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 23:49:55.152
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:55.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:55.267
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/30/23 23:49:55.319
Jan 30 23:49:55.356: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6746" to be "running and ready"
Jan 30 23:49:55.378: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.815572ms
Jan 30 23:49:55.379: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:49:57.402: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045005332s
Jan 30 23:49:57.402: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:49:59.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.039654848s
Jan 30 23:49:59.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 30 23:49:59.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/30/23 23:49:59.415
Jan 30 23:49:59.436: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6746" to be "running and ready"
Jan 30 23:49:59.453: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.56551ms
Jan 30 23:49:59.453: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:50:01.474: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037504088s
Jan 30 23:50:01.474: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 30 23:50:03.474: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.037688038s
Jan 30 23:50:03.474: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 30 23:50:03.474: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/30/23 23:50:03.493
STEP: delete the pod with lifecycle hook 01/30/23 23:50:03.532
Jan 30 23:50:03.565: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 23:50:03.591: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 30 23:50:05.592: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 30 23:50:05.611: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:05.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6746" for this suite. 01/30/23 23:50:05.641
------------------------------
• [SLOW TEST] [10.522 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:49:55.15
    Jan 30 23:49:55.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/30/23 23:49:55.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:49:55.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:49:55.267
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/30/23 23:49:55.319
    Jan 30 23:49:55.356: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6746" to be "running and ready"
    Jan 30 23:49:55.378: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 21.815572ms
    Jan 30 23:49:55.379: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:49:57.402: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045005332s
    Jan 30 23:49:57.402: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:49:59.396: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.039654848s
    Jan 30 23:49:59.396: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 30 23:49:59.396: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/30/23 23:49:59.415
    Jan 30 23:49:59.436: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6746" to be "running and ready"
    Jan 30 23:49:59.453: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.56551ms
    Jan 30 23:49:59.453: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:50:01.474: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037504088s
    Jan 30 23:50:01.474: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 30 23:50:03.474: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.037688038s
    Jan 30 23:50:03.474: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 30 23:50:03.474: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/30/23 23:50:03.493
    STEP: delete the pod with lifecycle hook 01/30/23 23:50:03.532
    Jan 30 23:50:03.565: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 30 23:50:03.591: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 30 23:50:05.592: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 30 23:50:05.611: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:05.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6746" for this suite. 01/30/23 23:50:05.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:05.677
Jan 30 23:50:05.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename containers 01/30/23 23:50:05.679
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:05.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:05.767
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 30 23:50:05.823: INFO: Waiting up to 5m0s for pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146" in namespace "containers-5924" to be "running"
Jan 30 23:50:05.844: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Pending", Reason="", readiness=false. Elapsed: 20.99861ms
Jan 30 23:50:07.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040056093s
Jan 30 23:50:09.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Running", Reason="", readiness=true. Elapsed: 4.039251522s
Jan 30 23:50:09.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5924" for this suite. 01/30/23 23:50:09.928
------------------------------
• [4.283 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:05.677
    Jan 30 23:50:05.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename containers 01/30/23 23:50:05.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:05.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:05.767
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 30 23:50:05.823: INFO: Waiting up to 5m0s for pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146" in namespace "containers-5924" to be "running"
    Jan 30 23:50:05.844: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Pending", Reason="", readiness=false. Elapsed: 20.99861ms
    Jan 30 23:50:07.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040056093s
    Jan 30 23:50:09.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146": Phase="Running", Reason="", readiness=true. Elapsed: 4.039251522s
    Jan 30 23:50:09.863: INFO: Pod "client-containers-af4ea062-151f-4ae9-a189-416368c93146" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5924" for this suite. 01/30/23 23:50:09.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:09.963
Jan 30 23:50:09.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/30/23 23:50:09.965
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:10.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:10.035
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/30/23 23:50:10.052
Jan 30 23:50:10.092: INFO: created test-pod-1
Jan 30 23:50:10.113: INFO: created test-pod-2
Jan 30 23:50:10.136: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/30/23 23:50:10.137
Jan 30 23:50:10.137: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9455' to be running and ready
Jan 30 23:50:10.189: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:10.189: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:10.189: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:10.190: INFO: 0 / 3 pods in namespace 'pods-9455' are running and ready (0 seconds elapsed)
Jan 30 23:50:10.190: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
Jan 30 23:50:10.190: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Jan 30 23:50:10.190: INFO: test-pod-1  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:10.190: INFO: test-pod-2  10.15.28.227  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:10.190: INFO: test-pod-3  10.15.28.225  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:10.190: INFO: 
Jan 30 23:50:12.250: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:12.250: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:12.251: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 30 23:50:12.251: INFO: 0 / 3 pods in namespace 'pods-9455' are running and ready (2 seconds elapsed)
Jan 30 23:50:12.251: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
Jan 30 23:50:12.252: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Jan 30 23:50:12.252: INFO: test-pod-1  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:12.252: INFO: test-pod-2  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:12.253: INFO: test-pod-3  10.15.28.225  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
Jan 30 23:50:12.253: INFO: 
Jan 30 23:50:14.250: INFO: 3 / 3 pods in namespace 'pods-9455' are running and ready (4 seconds elapsed)
Jan 30 23:50:14.250: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/30/23 23:50:14.346
Jan 30 23:50:14.365: INFO: Pod quantity 3 is different from expected quantity 0
Jan 30 23:50:15.386: INFO: Pod quantity 3 is different from expected quantity 0
Jan 30 23:50:16.389: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:17.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9455" for this suite. 01/30/23 23:50:17.412
------------------------------
• [SLOW TEST] [7.480 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:09.963
    Jan 30 23:50:09.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/30/23 23:50:09.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:10.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:10.035
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/30/23 23:50:10.052
    Jan 30 23:50:10.092: INFO: created test-pod-1
    Jan 30 23:50:10.113: INFO: created test-pod-2
    Jan 30 23:50:10.136: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/30/23 23:50:10.137
    Jan 30 23:50:10.137: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9455' to be running and ready
    Jan 30 23:50:10.189: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:10.189: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:10.189: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:10.190: INFO: 0 / 3 pods in namespace 'pods-9455' are running and ready (0 seconds elapsed)
    Jan 30 23:50:10.190: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
    Jan 30 23:50:10.190: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Jan 30 23:50:10.190: INFO: test-pod-1  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:10.190: INFO: test-pod-2  10.15.28.227  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:10.190: INFO: test-pod-3  10.15.28.225  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:10.190: INFO: 
    Jan 30 23:50:12.250: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:12.250: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:12.251: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 30 23:50:12.251: INFO: 0 / 3 pods in namespace 'pods-9455' are running and ready (2 seconds elapsed)
    Jan 30 23:50:12.251: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
    Jan 30 23:50:12.252: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Jan 30 23:50:12.252: INFO: test-pod-1  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:12.252: INFO: test-pod-2  10.15.28.227  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:12.253: INFO: test-pod-3  10.15.28.225  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-30 23:50:10 +0000 UTC  }]
    Jan 30 23:50:12.253: INFO: 
    Jan 30 23:50:14.250: INFO: 3 / 3 pods in namespace 'pods-9455' are running and ready (4 seconds elapsed)
    Jan 30 23:50:14.250: INFO: expected 0 pod replicas in namespace 'pods-9455', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/30/23 23:50:14.346
    Jan 30 23:50:14.365: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 30 23:50:15.386: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 30 23:50:16.389: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:17.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9455" for this suite. 01/30/23 23:50:17.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:17.45
Jan 30 23:50:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:50:17.452
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:17.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:17.53
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/30/23 23:50:17.544
Jan 30 23:50:17.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: mark a version not serverd 01/30/23 23:50:23.062
STEP: check the unserved version gets removed 01/30/23 23:50:23.138
STEP: check the other version is not changed 01/30/23 23:50:25.612
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2658" for this suite. 01/30/23 23:50:29.981
------------------------------
• [SLOW TEST] [12.569 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:17.45
    Jan 30 23:50:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/30/23 23:50:17.452
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:17.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:17.53
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/30/23 23:50:17.544
    Jan 30 23:50:17.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: mark a version not serverd 01/30/23 23:50:23.062
    STEP: check the unserved version gets removed 01/30/23 23:50:23.138
    STEP: check the other version is not changed 01/30/23 23:50:25.612
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2658" for this suite. 01/30/23 23:50:29.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:30.026
Jan 30 23:50:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:50:30.029
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:30.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:30.122
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:50:30.141
Jan 30 23:50:30.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3" in namespace "downward-api-8459" to be "Succeeded or Failed"
Jan 30 23:50:30.218: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.808547ms
Jan 30 23:50:32.234: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033374602s
Jan 30 23:50:34.239: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037870735s
Jan 30 23:50:36.240: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039290949s
STEP: Saw pod success 01/30/23 23:50:36.24
Jan 30 23:50:36.241: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3" satisfied condition "Succeeded or Failed"
Jan 30 23:50:36.261: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 container client-container: <nil>
STEP: delete the pod 01/30/23 23:50:36.395
Jan 30 23:50:36.437: INFO: Waiting for pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 to disappear
Jan 30 23:50:36.454: INFO: Pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8459" for this suite. 01/30/23 23:50:36.478
------------------------------
• [SLOW TEST] [6.477 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:30.026
    Jan 30 23:50:30.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:50:30.029
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:30.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:30.122
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:50:30.141
    Jan 30 23:50:30.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3" in namespace "downward-api-8459" to be "Succeeded or Failed"
    Jan 30 23:50:30.218: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.808547ms
    Jan 30 23:50:32.234: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033374602s
    Jan 30 23:50:34.239: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037870735s
    Jan 30 23:50:36.240: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039290949s
    STEP: Saw pod success 01/30/23 23:50:36.24
    Jan 30 23:50:36.241: INFO: Pod "downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3" satisfied condition "Succeeded or Failed"
    Jan 30 23:50:36.261: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:50:36.395
    Jan 30 23:50:36.437: INFO: Waiting for pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 to disappear
    Jan 30 23:50:36.454: INFO: Pod downwardapi-volume-c322e590-2d4f-46bf-98c0-f8325eca02b3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8459" for this suite. 01/30/23 23:50:36.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:36.508
Jan 30 23:50:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename containers 01/30/23 23:50:36.509
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:36.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:36.584
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/30/23 23:50:36.599
Jan 30 23:50:36.632: INFO: Waiting up to 5m0s for pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840" in namespace "containers-9326" to be "Succeeded or Failed"
Jan 30 23:50:36.649: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 16.585682ms
Jan 30 23:50:38.668: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03561256s
Jan 30 23:50:40.667: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035031245s
Jan 30 23:50:42.666: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033747055s
STEP: Saw pod success 01/30/23 23:50:42.666
Jan 30 23:50:42.666: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840" satisfied condition "Succeeded or Failed"
Jan 30 23:50:42.682: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:50:42.747
Jan 30 23:50:42.791: INFO: Waiting for pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 to disappear
Jan 30 23:50:42.808: INFO: Pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 30 23:50:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9326" for this suite. 01/30/23 23:50:42.832
------------------------------
• [SLOW TEST] [6.373 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:36.508
    Jan 30 23:50:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename containers 01/30/23 23:50:36.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:36.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:36.584
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/30/23 23:50:36.599
    Jan 30 23:50:36.632: INFO: Waiting up to 5m0s for pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840" in namespace "containers-9326" to be "Succeeded or Failed"
    Jan 30 23:50:36.649: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 16.585682ms
    Jan 30 23:50:38.668: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03561256s
    Jan 30 23:50:40.667: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035031245s
    Jan 30 23:50:42.666: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033747055s
    STEP: Saw pod success 01/30/23 23:50:42.666
    Jan 30 23:50:42.666: INFO: Pod "client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840" satisfied condition "Succeeded or Failed"
    Jan 30 23:50:42.682: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:50:42.747
    Jan 30 23:50:42.791: INFO: Waiting for pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 to disappear
    Jan 30 23:50:42.808: INFO: Pod client-containers-14785ecd-cb95-4f4c-bc5f-68bac8aa2840 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:50:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9326" for this suite. 01/30/23 23:50:42.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:50:42.898
Jan 30 23:50:42.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:50:42.9
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:42.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:42.984
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4239 01/30/23 23:50:43.004
STEP: creating service affinity-nodeport-transition in namespace services-4239 01/30/23 23:50:43.005
STEP: creating replication controller affinity-nodeport-transition in namespace services-4239 01/30/23 23:50:43.085
I0130 23:50:43.113170      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4239, replica count: 3
I0130 23:50:46.165139      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:50:46.218: INFO: Creating new exec pod
Jan 30 23:50:46.252: INFO: Waiting up to 5m0s for pod "execpod-affinity4zg5v" in namespace "services-4239" to be "running"
Jan 30 23:50:46.272: INFO: Pod "execpod-affinity4zg5v": Phase="Pending", Reason="", readiness=false. Elapsed: 18.688012ms
Jan 30 23:50:48.290: INFO: Pod "execpod-affinity4zg5v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036855113s
Jan 30 23:50:50.287: INFO: Pod "execpod-affinity4zg5v": Phase="Running", Reason="", readiness=true. Elapsed: 4.034603749s
Jan 30 23:50:50.288: INFO: Pod "execpod-affinity4zg5v" satisfied condition "running"
Jan 30 23:50:51.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 30 23:50:51.785: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 30 23:50:51.785: INFO: stdout: ""
Jan 30 23:50:51.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 172.21.159.200 80'
Jan 30 23:50:52.227: INFO: stderr: "+ nc -v -z -w 2 172.21.159.200 80\nConnection to 172.21.159.200 80 port [tcp/http] succeeded!\n"
Jan 30 23:50:52.227: INFO: stdout: ""
Jan 30 23:50:52.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 10.15.28.227 30904'
Jan 30 23:50:52.598: INFO: stderr: "+ nc -v -z -w 2 10.15.28.227 30904\nConnection to 10.15.28.227 30904 port [tcp/*] succeeded!\n"
Jan 30 23:50:52.598: INFO: stdout: ""
Jan 30 23:50:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30904'
Jan 30 23:50:52.998: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30904\nConnection to 10.15.28.237 30904 port [tcp/*] succeeded!\n"
Jan 30 23:50:52.998: INFO: stdout: ""
Jan 30 23:50:53.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
Jan 30 23:50:53.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
Jan 30 23:50:53.602: INFO: stdout: "\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2"
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:53.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
Jan 30 23:50:54.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
Jan 30 23:50:54.267: INFO: stdout: "\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf"
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
Jan 30 23:51:24.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
Jan 30 23:51:24.852: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
Jan 30 23:51:24.852: INFO: stdout: "\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb"
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
Jan 30 23:51:24.853: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4239, will wait for the garbage collector to delete the pods 01/30/23 23:51:24.89
Jan 30 23:51:24.995: INFO: Deleting ReplicationController affinity-nodeport-transition took: 32.301966ms
Jan 30 23:51:25.195: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.980464ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:51:28.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4239" for this suite. 01/30/23 23:51:28.361
------------------------------
• [SLOW TEST] [45.490 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:50:42.898
    Jan 30 23:50:42.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:50:42.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:50:42.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:50:42.984
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4239 01/30/23 23:50:43.004
    STEP: creating service affinity-nodeport-transition in namespace services-4239 01/30/23 23:50:43.005
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4239 01/30/23 23:50:43.085
    I0130 23:50:43.113170      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4239, replica count: 3
    I0130 23:50:46.165139      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:50:46.218: INFO: Creating new exec pod
    Jan 30 23:50:46.252: INFO: Waiting up to 5m0s for pod "execpod-affinity4zg5v" in namespace "services-4239" to be "running"
    Jan 30 23:50:46.272: INFO: Pod "execpod-affinity4zg5v": Phase="Pending", Reason="", readiness=false. Elapsed: 18.688012ms
    Jan 30 23:50:48.290: INFO: Pod "execpod-affinity4zg5v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036855113s
    Jan 30 23:50:50.287: INFO: Pod "execpod-affinity4zg5v": Phase="Running", Reason="", readiness=true. Elapsed: 4.034603749s
    Jan 30 23:50:50.288: INFO: Pod "execpod-affinity4zg5v" satisfied condition "running"
    Jan 30 23:50:51.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 30 23:50:51.785: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 30 23:50:51.785: INFO: stdout: ""
    Jan 30 23:50:51.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 172.21.159.200 80'
    Jan 30 23:50:52.227: INFO: stderr: "+ nc -v -z -w 2 172.21.159.200 80\nConnection to 172.21.159.200 80 port [tcp/http] succeeded!\n"
    Jan 30 23:50:52.227: INFO: stdout: ""
    Jan 30 23:50:52.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 10.15.28.227 30904'
    Jan 30 23:50:52.598: INFO: stderr: "+ nc -v -z -w 2 10.15.28.227 30904\nConnection to 10.15.28.227 30904 port [tcp/*] succeeded!\n"
    Jan 30 23:50:52.598: INFO: stdout: ""
    Jan 30 23:50:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c nc -v -z -w 2 10.15.28.237 30904'
    Jan 30 23:50:52.998: INFO: stderr: "+ nc -v -z -w 2 10.15.28.237 30904\nConnection to 10.15.28.237 30904 port [tcp/*] succeeded!\n"
    Jan 30 23:50:52.998: INFO: stdout: ""
    Jan 30 23:50:53.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
    Jan 30 23:50:53.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
    Jan 30 23:50:53.602: INFO: stdout: "\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2"
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:53.602: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:53.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
    Jan 30 23:50:54.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
    Jan 30 23:50:54.267: INFO: stdout: "\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-bbds2\naffinity-nodeport-transition-9qxwf"
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-bbds2
    Jan 30 23:50:54.267: INFO: Received response from host: affinity-nodeport-transition-9qxwf
    Jan 30 23:51:24.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-4239 exec execpod-affinity4zg5v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.15.28.225:30904/ ; done'
    Jan 30 23:51:24.852: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.15.28.225:30904/\n"
    Jan 30 23:51:24.852: INFO: stdout: "\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb\naffinity-nodeport-transition-dznsb"
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.852: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Received response from host: affinity-nodeport-transition-dznsb
    Jan 30 23:51:24.853: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4239, will wait for the garbage collector to delete the pods 01/30/23 23:51:24.89
    Jan 30 23:51:24.995: INFO: Deleting ReplicationController affinity-nodeport-transition took: 32.301966ms
    Jan 30 23:51:25.195: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.980464ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:51:28.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4239" for this suite. 01/30/23 23:51:28.361
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:51:28.39
Jan 30 23:51:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 23:51:28.395
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:28.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:28.479
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 23:51:28.541
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:51:28.862
STEP: Deploying the webhook pod 01/30/23 23:51:28.893
STEP: Wait for the deployment to be ready 01/30/23 23:51:28.952
Jan 30 23:51:28.985: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 30 23:51:31.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:51:33.052
STEP: Verifying the service has paired with the endpoint 01/30/23 23:51:33.104
Jan 30 23:51:34.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/30/23 23:51:34.122
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/30/23 23:51:34.275
STEP: Creating a configMap that should not be mutated 01/30/23 23:51:34.294
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/30/23 23:51:34.339
STEP: Creating a configMap that should be mutated 01/30/23 23:51:34.364
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:51:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4115" for this suite. 01/30/23 23:51:34.753
STEP: Destroying namespace "webhook-4115-markers" for this suite. 01/30/23 23:51:34.806
------------------------------
• [SLOW TEST] [6.446 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:51:28.39
    Jan 30 23:51:28.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 23:51:28.395
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:28.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:28.479
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 23:51:28.541
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:51:28.862
    STEP: Deploying the webhook pod 01/30/23 23:51:28.893
    STEP: Wait for the deployment to be ready 01/30/23 23:51:28.952
    Jan 30 23:51:28.985: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 30 23:51:31.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 51, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:51:33.052
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:51:33.104
    Jan 30 23:51:34.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/30/23 23:51:34.122
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/30/23 23:51:34.275
    STEP: Creating a configMap that should not be mutated 01/30/23 23:51:34.294
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/30/23 23:51:34.339
    STEP: Creating a configMap that should be mutated 01/30/23 23:51:34.364
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:51:34.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4115" for this suite. 01/30/23 23:51:34.753
    STEP: Destroying namespace "webhook-4115-markers" for this suite. 01/30/23 23:51:34.806
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:51:34.847
Jan 30 23:51:34.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption 01/30/23 23:51:34.85
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:34.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:35.007
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:51:35.022
Jan 30 23:51:35.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename disruption-2 01/30/23 23:51:35.025
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:35.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:35.108
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/30/23 23:51:35.146
STEP: Waiting for the pdb to be processed 01/30/23 23:51:37.233
STEP: Waiting for the pdb to be processed 01/30/23 23:51:37.273
STEP: listing a collection of PDBs across all namespaces 01/30/23 23:51:37.291
STEP: listing a collection of PDBs in namespace disruption-1569 01/30/23 23:51:37.312
STEP: deleting a collection of PDBs 01/30/23 23:51:37.337
STEP: Waiting for the PDB collection to be deleted 01/30/23 23:51:37.411
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 30 23:51:37.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 30 23:51:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-6830" for this suite. 01/30/23 23:51:37.463
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1569" for this suite. 01/30/23 23:51:37.492
------------------------------
• [2.671 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:51:34.847
    Jan 30 23:51:34.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption 01/30/23 23:51:34.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:34.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:35.007
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:51:35.022
    Jan 30 23:51:35.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename disruption-2 01/30/23 23:51:35.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:35.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:35.108
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/30/23 23:51:35.146
    STEP: Waiting for the pdb to be processed 01/30/23 23:51:37.233
    STEP: Waiting for the pdb to be processed 01/30/23 23:51:37.273
    STEP: listing a collection of PDBs across all namespaces 01/30/23 23:51:37.291
    STEP: listing a collection of PDBs in namespace disruption-1569 01/30/23 23:51:37.312
    STEP: deleting a collection of PDBs 01/30/23 23:51:37.337
    STEP: Waiting for the PDB collection to be deleted 01/30/23 23:51:37.411
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:51:37.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:51:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-6830" for this suite. 01/30/23 23:51:37.463
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1569" for this suite. 01/30/23 23:51:37.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:51:37.532
Jan 30 23:51:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-watch 01/30/23 23:51:37.533
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:37.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:37.615
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 30 23:51:37.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Creating first CR  01/30/23 23:51:40.402
Jan 30 23:51:40.418: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:51:40Z]] name:name1 resourceVersion:42669 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/30/23 23:51:50.419
Jan 30 23:51:50.444: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:51:50Z]] name:name2 resourceVersion:42702 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/30/23 23:52:00.447
Jan 30 23:52:00.511: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:00Z]] name:name1 resourceVersion:42715 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/30/23 23:52:10.514
Jan 30 23:52:10.538: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:10Z]] name:name2 resourceVersion:42727 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/30/23 23:52:20.539
Jan 30 23:52:20.572: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:00Z]] name:name1 resourceVersion:42741 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/30/23 23:52:30.573
Jan 30 23:52:30.603: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:10Z]] name:name2 resourceVersion:42754 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:52:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1590" for this suite. 01/30/23 23:52:41.18
------------------------------
• [SLOW TEST] [63.674 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:51:37.532
    Jan 30 23:51:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-watch 01/30/23 23:51:37.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:51:37.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:51:37.615
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 30 23:51:37.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Creating first CR  01/30/23 23:51:40.402
    Jan 30 23:51:40.418: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:51:40Z]] name:name1 resourceVersion:42669 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/30/23 23:51:50.419
    Jan 30 23:51:50.444: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:51:50Z]] name:name2 resourceVersion:42702 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/30/23 23:52:00.447
    Jan 30 23:52:00.511: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:00Z]] name:name1 resourceVersion:42715 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/30/23 23:52:10.514
    Jan 30 23:52:10.538: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:10Z]] name:name2 resourceVersion:42727 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/30/23 23:52:20.539
    Jan 30 23:52:20.572: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:00Z]] name:name1 resourceVersion:42741 uid:b33cc2dc-5db1-470a-b737-fdb5fba77048] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/30/23 23:52:30.573
    Jan 30 23:52:30.603: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-30T23:51:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-30T23:52:10Z]] name:name2 resourceVersion:42754 uid:17ace38f-8653-4ff8-8cf2-68199a0a9350] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:52:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1590" for this suite. 01/30/23 23:52:41.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:52:41.215
Jan 30 23:52:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/30/23 23:52:41.218
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:41.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:41.327
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/30/23 23:52:41.37
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/30/23 23:52:41.402
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/30/23 23:52:41.402
STEP: creating a pod to probe DNS 01/30/23 23:52:41.403
STEP: submitting the pod to kubernetes 01/30/23 23:52:41.403
Jan 30 23:52:41.435: INFO: Waiting up to 15m0s for pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de" in namespace "dns-5277" to be "running"
Jan 30 23:52:41.450: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Pending", Reason="", readiness=false. Elapsed: 14.639211ms
Jan 30 23:52:43.466: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030105111s
Jan 30 23:52:45.466: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Running", Reason="", readiness=true. Elapsed: 4.030780106s
Jan 30 23:52:45.467: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:52:45.467
STEP: looking for the results for each expected name from probers 01/30/23 23:52:45.485
Jan 30 23:52:45.674: INFO: DNS probes using dns-5277/dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de succeeded

STEP: deleting the pod 01/30/23 23:52:45.674
STEP: deleting the test headless service 01/30/23 23:52:45.738
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 23:52:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5277" for this suite. 01/30/23 23:52:45.804
------------------------------
• [4.612 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:52:41.215
    Jan 30 23:52:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/30/23 23:52:41.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:41.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:41.327
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/30/23 23:52:41.37
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/30/23 23:52:41.402
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5277.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/30/23 23:52:41.402
    STEP: creating a pod to probe DNS 01/30/23 23:52:41.403
    STEP: submitting the pod to kubernetes 01/30/23 23:52:41.403
    Jan 30 23:52:41.435: INFO: Waiting up to 15m0s for pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de" in namespace "dns-5277" to be "running"
    Jan 30 23:52:41.450: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Pending", Reason="", readiness=false. Elapsed: 14.639211ms
    Jan 30 23:52:43.466: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030105111s
    Jan 30 23:52:45.466: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de": Phase="Running", Reason="", readiness=true. Elapsed: 4.030780106s
    Jan 30 23:52:45.467: INFO: Pod "dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:52:45.467
    STEP: looking for the results for each expected name from probers 01/30/23 23:52:45.485
    Jan 30 23:52:45.674: INFO: DNS probes using dns-5277/dns-test-39ccd2e6-3bdc-4129-b560-ab0670c114de succeeded

    STEP: deleting the pod 01/30/23 23:52:45.674
    STEP: deleting the test headless service 01/30/23 23:52:45.738
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:52:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5277" for this suite. 01/30/23 23:52:45.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:52:45.829
Jan 30 23:52:45.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/30/23 23:52:45.832
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:45.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:45.946
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-ed70c02f-9f76-4101-95fb-2f789d33dc6b 01/30/23 23:52:45.957
STEP: Creating a pod to test consume secrets 01/30/23 23:52:46.014
Jan 30 23:52:46.044: INFO: Waiting up to 5m0s for pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503" in namespace "secrets-1705" to be "Succeeded or Failed"
Jan 30 23:52:46.066: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Pending", Reason="", readiness=false. Elapsed: 21.824721ms
Jan 30 23:52:48.081: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Running", Reason="", readiness=true. Elapsed: 2.036741445s
Jan 30 23:52:50.086: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Running", Reason="", readiness=false. Elapsed: 4.041891475s
Jan 30 23:52:52.082: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037671783s
STEP: Saw pod success 01/30/23 23:52:52.082
Jan 30 23:52:52.083: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503" satisfied condition "Succeeded or Failed"
Jan 30 23:52:52.098: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 container secret-volume-test: <nil>
STEP: delete the pod 01/30/23 23:52:52.198
Jan 30 23:52:52.247: INFO: Waiting for pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 to disappear
Jan 30 23:52:52.261: INFO: Pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 30 23:52:52.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1705" for this suite. 01/30/23 23:52:52.281
------------------------------
• [SLOW TEST] [6.479 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:52:45.829
    Jan 30 23:52:45.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/30/23 23:52:45.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:45.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:45.946
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-ed70c02f-9f76-4101-95fb-2f789d33dc6b 01/30/23 23:52:45.957
    STEP: Creating a pod to test consume secrets 01/30/23 23:52:46.014
    Jan 30 23:52:46.044: INFO: Waiting up to 5m0s for pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503" in namespace "secrets-1705" to be "Succeeded or Failed"
    Jan 30 23:52:46.066: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Pending", Reason="", readiness=false. Elapsed: 21.824721ms
    Jan 30 23:52:48.081: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Running", Reason="", readiness=true. Elapsed: 2.036741445s
    Jan 30 23:52:50.086: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Running", Reason="", readiness=false. Elapsed: 4.041891475s
    Jan 30 23:52:52.082: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037671783s
    STEP: Saw pod success 01/30/23 23:52:52.082
    Jan 30 23:52:52.083: INFO: Pod "pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503" satisfied condition "Succeeded or Failed"
    Jan 30 23:52:52.098: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 container secret-volume-test: <nil>
    STEP: delete the pod 01/30/23 23:52:52.198
    Jan 30 23:52:52.247: INFO: Waiting for pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 to disappear
    Jan 30 23:52:52.261: INFO: Pod pod-secrets-c74e4324-08aa-420e-aeab-d3995453b503 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:52:52.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1705" for this suite. 01/30/23 23:52:52.281
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:52:52.313
Jan 30 23:52:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/30/23 23:52:52.315
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:52.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:52.41
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/30/23 23:52:52.428
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:52.515
STEP: Creating a service in the namespace 01/30/23 23:52:52.529
STEP: Deleting the namespace 01/30/23 23:52:52.632
STEP: Waiting for the namespace to be removed. 01/30/23 23:52:52.659
STEP: Recreating the namespace 01/30/23 23:52:59.674
STEP: Verifying there is no service in the namespace 01/30/23 23:52:59.732
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:52:59.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9092" for this suite. 01/30/23 23:52:59.775
STEP: Destroying namespace "nsdeletetest-6236" for this suite. 01/30/23 23:52:59.8
Jan 30 23:52:59.815: INFO: Namespace nsdeletetest-6236 was already deleted
STEP: Destroying namespace "nsdeletetest-7210" for this suite. 01/30/23 23:52:59.815
------------------------------
• [SLOW TEST] [7.536 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:52:52.313
    Jan 30 23:52:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/30/23 23:52:52.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:52.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:52.41
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/30/23 23:52:52.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:52.515
    STEP: Creating a service in the namespace 01/30/23 23:52:52.529
    STEP: Deleting the namespace 01/30/23 23:52:52.632
    STEP: Waiting for the namespace to be removed. 01/30/23 23:52:52.659
    STEP: Recreating the namespace 01/30/23 23:52:59.674
    STEP: Verifying there is no service in the namespace 01/30/23 23:52:59.732
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:52:59.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9092" for this suite. 01/30/23 23:52:59.775
    STEP: Destroying namespace "nsdeletetest-6236" for this suite. 01/30/23 23:52:59.8
    Jan 30 23:52:59.815: INFO: Namespace nsdeletetest-6236 was already deleted
    STEP: Destroying namespace "nsdeletetest-7210" for this suite. 01/30/23 23:52:59.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:52:59.862
Jan 30 23:52:59.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/30/23 23:52:59.863
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:59.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:59.979
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/30/23 23:53:00.073
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:53:00.103
Jan 30 23:53:00.133: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:53:00.134: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:53:01.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:53:01.202: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:53:02.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:53:02.194: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:53:03.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:53:03.180: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/30/23 23:53:03.193
Jan 30 23:53:03.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:53:03.305: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:53:04.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:53:04.370: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:53:05.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 30 23:53:05.342: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 30 23:53:06.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 30 23:53:06.342: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/30/23 23:53:06.342
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:53:06.372
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9147, will wait for the garbage collector to delete the pods 01/30/23 23:53:06.372
Jan 30 23:53:06.462: INFO: Deleting DaemonSet.extensions daemon-set took: 26.566197ms
Jan 30 23:53:06.563: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.404332ms
Jan 30 23:53:09.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 30 23:53:09.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 30 23:53:09.499: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43027"},"items":null}

Jan 30 23:53:09.513: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43027"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:53:09.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9147" for this suite. 01/30/23 23:53:09.602
------------------------------
• [SLOW TEST] [9.769 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:52:59.862
    Jan 30 23:52:59.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/30/23 23:52:59.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:52:59.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:52:59.979
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/30/23 23:53:00.073
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:53:00.103
    Jan 30 23:53:00.133: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:53:00.134: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:53:01.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:53:01.202: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:53:02.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:53:02.194: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:53:03.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:53:03.180: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/30/23 23:53:03.193
    Jan 30 23:53:03.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:53:03.305: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:53:04.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:53:04.370: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:53:05.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 30 23:53:05.342: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 30 23:53:06.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 30 23:53:06.342: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/30/23 23:53:06.342
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/30/23 23:53:06.372
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9147, will wait for the garbage collector to delete the pods 01/30/23 23:53:06.372
    Jan 30 23:53:06.462: INFO: Deleting DaemonSet.extensions daemon-set took: 26.566197ms
    Jan 30 23:53:06.563: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.404332ms
    Jan 30 23:53:09.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 30 23:53:09.482: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 30 23:53:09.499: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43027"},"items":null}

    Jan 30 23:53:09.513: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43027"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:53:09.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9147" for this suite. 01/30/23 23:53:09.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:53:09.636
Jan 30 23:53:09.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename podtemplate 01/30/23 23:53:09.638
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:09.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:09.709
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 30 23:53:09.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6455" for this suite. 01/30/23 23:53:09.879
------------------------------
• [0.268 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:53:09.636
    Jan 30 23:53:09.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename podtemplate 01/30/23 23:53:09.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:09.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:09.709
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:53:09.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6455" for this suite. 01/30/23 23:53:09.879
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:53:09.905
Jan 30 23:53:09.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-runtime 01/30/23 23:53:09.908
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:09.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:09.985
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/30/23 23:53:10.026
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/30/23 23:53:29.413
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/30/23 23:53:29.425
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/30/23 23:53:29.488
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/30/23 23:53:29.488
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/30/23 23:53:29.563
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/30/23 23:53:33.687
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/30/23 23:53:35.752
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/30/23 23:53:35.788
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/30/23 23:53:35.788
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/30/23 23:53:35.89
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/30/23 23:53:36.93
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/30/23 23:53:41.061
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/30/23 23:53:41.095
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/30/23 23:53:41.096
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 30 23:53:41.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1948" for this suite. 01/30/23 23:53:41.231
------------------------------
• [SLOW TEST] [31.353 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:53:09.905
    Jan 30 23:53:09.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-runtime 01/30/23 23:53:09.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:09.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:09.985
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/30/23 23:53:10.026
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/30/23 23:53:29.413
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/30/23 23:53:29.425
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/30/23 23:53:29.488
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/30/23 23:53:29.488
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/30/23 23:53:29.563
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/30/23 23:53:33.687
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/30/23 23:53:35.752
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/30/23 23:53:35.788
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/30/23 23:53:35.788
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/30/23 23:53:35.89
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/30/23 23:53:36.93
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/30/23 23:53:41.061
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/30/23 23:53:41.095
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/30/23 23:53:41.096
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:53:41.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1948" for this suite. 01/30/23 23:53:41.231
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:53:41.258
Jan 30 23:53:41.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/30/23 23:53:41.261
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:41.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:41.414
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/30/23 23:53:41.458
STEP: waiting for available Endpoint 01/30/23 23:53:41.476
STEP: listing all Endpoints 01/30/23 23:53:41.486
STEP: updating the Endpoint 01/30/23 23:53:41.515
STEP: fetching the Endpoint 01/30/23 23:53:41.539
STEP: patching the Endpoint 01/30/23 23:53:41.553
STEP: fetching the Endpoint 01/30/23 23:53:41.584
STEP: deleting the Endpoint by Collection 01/30/23 23:53:41.598
STEP: waiting for Endpoint deletion 01/30/23 23:53:41.629
STEP: fetching the Endpoint 01/30/23 23:53:41.636
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 30 23:53:41.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3944" for this suite. 01/30/23 23:53:41.677
------------------------------
• [0.454 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:53:41.258
    Jan 30 23:53:41.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/30/23 23:53:41.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:41.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:41.414
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/30/23 23:53:41.458
    STEP: waiting for available Endpoint 01/30/23 23:53:41.476
    STEP: listing all Endpoints 01/30/23 23:53:41.486
    STEP: updating the Endpoint 01/30/23 23:53:41.515
    STEP: fetching the Endpoint 01/30/23 23:53:41.539
    STEP: patching the Endpoint 01/30/23 23:53:41.553
    STEP: fetching the Endpoint 01/30/23 23:53:41.584
    STEP: deleting the Endpoint by Collection 01/30/23 23:53:41.598
    STEP: waiting for Endpoint deletion 01/30/23 23:53:41.629
    STEP: fetching the Endpoint 01/30/23 23:53:41.636
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:53:41.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3944" for this suite. 01/30/23 23:53:41.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:53:41.718
Jan 30 23:53:41.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename subpath 01/30/23 23:53:41.721
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:41.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:41.816
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/30/23 23:53:41.831
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-p79w 01/30/23 23:53:41.876
STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:53:41.876
Jan 30 23:53:41.902: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p79w" in namespace "subpath-6775" to be "Succeeded or Failed"
Jan 30 23:53:41.917: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Pending", Reason="", readiness=false. Elapsed: 14.130439ms
Jan 30 23:53:43.939: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03616479s
Jan 30 23:53:45.931: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 4.028137657s
Jan 30 23:53:47.937: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 6.034237914s
Jan 30 23:53:49.934: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 8.031669277s
Jan 30 23:53:51.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 10.029333399s
Jan 30 23:53:53.937: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 12.034121553s
Jan 30 23:53:55.934: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 14.031366936s
Jan 30 23:53:57.930: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 16.02761691s
Jan 30 23:53:59.936: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 18.033273061s
Jan 30 23:54:01.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 20.029550009s
Jan 30 23:54:03.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 22.029103287s
Jan 30 23:54:05.947: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=false. Elapsed: 24.04471159s
Jan 30 23:54:07.935: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.032036878s
STEP: Saw pod success 01/30/23 23:54:07.935
Jan 30 23:54:07.935: INFO: Pod "pod-subpath-test-projected-p79w" satisfied condition "Succeeded or Failed"
Jan 30 23:54:07.950: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-projected-p79w container test-container-subpath-projected-p79w: <nil>
STEP: delete the pod 01/30/23 23:54:07.984
Jan 30 23:54:08.030: INFO: Waiting for pod pod-subpath-test-projected-p79w to disappear
Jan 30 23:54:08.044: INFO: Pod pod-subpath-test-projected-p79w no longer exists
STEP: Deleting pod pod-subpath-test-projected-p79w 01/30/23 23:54:08.044
Jan 30 23:54:08.045: INFO: Deleting pod "pod-subpath-test-projected-p79w" in namespace "subpath-6775"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 30 23:54:08.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6775" for this suite. 01/30/23 23:54:08.082
------------------------------
• [SLOW TEST] [26.393 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:53:41.718
    Jan 30 23:53:41.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename subpath 01/30/23 23:53:41.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:53:41.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:53:41.816
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/30/23 23:53:41.831
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-p79w 01/30/23 23:53:41.876
    STEP: Creating a pod to test atomic-volume-subpath 01/30/23 23:53:41.876
    Jan 30 23:53:41.902: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p79w" in namespace "subpath-6775" to be "Succeeded or Failed"
    Jan 30 23:53:41.917: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Pending", Reason="", readiness=false. Elapsed: 14.130439ms
    Jan 30 23:53:43.939: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03616479s
    Jan 30 23:53:45.931: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 4.028137657s
    Jan 30 23:53:47.937: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 6.034237914s
    Jan 30 23:53:49.934: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 8.031669277s
    Jan 30 23:53:51.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 10.029333399s
    Jan 30 23:53:53.937: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 12.034121553s
    Jan 30 23:53:55.934: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 14.031366936s
    Jan 30 23:53:57.930: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 16.02761691s
    Jan 30 23:53:59.936: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 18.033273061s
    Jan 30 23:54:01.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 20.029550009s
    Jan 30 23:54:03.932: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=true. Elapsed: 22.029103287s
    Jan 30 23:54:05.947: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Running", Reason="", readiness=false. Elapsed: 24.04471159s
    Jan 30 23:54:07.935: INFO: Pod "pod-subpath-test-projected-p79w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.032036878s
    STEP: Saw pod success 01/30/23 23:54:07.935
    Jan 30 23:54:07.935: INFO: Pod "pod-subpath-test-projected-p79w" satisfied condition "Succeeded or Failed"
    Jan 30 23:54:07.950: INFO: Trying to get logs from node 10.15.28.227 pod pod-subpath-test-projected-p79w container test-container-subpath-projected-p79w: <nil>
    STEP: delete the pod 01/30/23 23:54:07.984
    Jan 30 23:54:08.030: INFO: Waiting for pod pod-subpath-test-projected-p79w to disappear
    Jan 30 23:54:08.044: INFO: Pod pod-subpath-test-projected-p79w no longer exists
    STEP: Deleting pod pod-subpath-test-projected-p79w 01/30/23 23:54:08.044
    Jan 30 23:54:08.045: INFO: Deleting pod "pod-subpath-test-projected-p79w" in namespace "subpath-6775"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:54:08.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6775" for this suite. 01/30/23 23:54:08.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:54:08.117
Jan 30 23:54:08.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/30/23 23:54:08.119
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:08.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:08.194
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/30/23 23:54:08.217
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_tcp@PTR;sleep 1; done
 01/30/23 23:54:08.291
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_tcp@PTR;sleep 1; done
 01/30/23 23:54:08.292
STEP: creating a pod to probe DNS 01/30/23 23:54:08.292
STEP: submitting the pod to kubernetes 01/30/23 23:54:08.292
Jan 30 23:54:08.343: INFO: Waiting up to 15m0s for pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d" in namespace "dns-4479" to be "running"
Jan 30 23:54:08.358: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.611588ms
Jan 30 23:54:10.380: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037764687s
Jan 30 23:54:12.373: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.030862049s
Jan 30 23:54:12.374: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d" satisfied condition "running"
STEP: retrieving the pod 01/30/23 23:54:12.374
STEP: looking for the results for each expected name from probers 01/30/23 23:54:12.386
Jan 30 23:54:12.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.496: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.517: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.537: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.665: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.718: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.742: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:12.841: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:17.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:17.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:17.914: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:17.944: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:18.076: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:18.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:18.120: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:18.143: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:18.333: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:22.871: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:22.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:22.926: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:22.955: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:23.102: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:23.131: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:23.159: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:23.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:23.327: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:27.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:27.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:27.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:27.960: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:28.138: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:28.168: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:28.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:28.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:28.431: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:32.874: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:32.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:32.934: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:32.970: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:33.127: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:33.157: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:33.188: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:33.219: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:33.358: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:37.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:37.903: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:37.933: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:37.962: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:38.149: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:38.179: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:38.208: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:38.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
Jan 30 23:54:38.371: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jan 30 23:54:43.360: INFO: DNS probes using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d succeeded

STEP: deleting the pod 01/30/23 23:54:43.36
STEP: deleting the test service 01/30/23 23:54:43.415
STEP: deleting the test headless service 01/30/23 23:54:43.537
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 30 23:54:43.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4479" for this suite. 01/30/23 23:54:43.609
------------------------------
• [SLOW TEST] [35.526 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:54:08.117
    Jan 30 23:54:08.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/30/23 23:54:08.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:08.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:08.194
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/30/23 23:54:08.217
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_tcp@PTR;sleep 1; done
     01/30/23 23:54:08.291
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.119.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.119.108_tcp@PTR;sleep 1; done
     01/30/23 23:54:08.292
    STEP: creating a pod to probe DNS 01/30/23 23:54:08.292
    STEP: submitting the pod to kubernetes 01/30/23 23:54:08.292
    Jan 30 23:54:08.343: INFO: Waiting up to 15m0s for pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d" in namespace "dns-4479" to be "running"
    Jan 30 23:54:08.358: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.611588ms
    Jan 30 23:54:10.380: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037764687s
    Jan 30 23:54:12.373: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d": Phase="Running", Reason="", readiness=true. Elapsed: 4.030862049s
    Jan 30 23:54:12.374: INFO: Pod "dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d" satisfied condition "running"
    STEP: retrieving the pod 01/30/23 23:54:12.374
    STEP: looking for the results for each expected name from probers 01/30/23 23:54:12.386
    Jan 30 23:54:12.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.496: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.517: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.537: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.665: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.718: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.742: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:12.841: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:17.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:17.889: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:17.914: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:17.944: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:18.076: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:18.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:18.120: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:18.143: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:18.333: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:22.871: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:22.899: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:22.926: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:22.955: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:23.102: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:23.131: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:23.159: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:23.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:23.327: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:27.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:27.901: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:27.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:27.960: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:28.138: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:28.168: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:28.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:28.279: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:28.431: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:32.874: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:32.904: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:32.934: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:32.970: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:33.127: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:33.157: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:33.188: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:33.219: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:33.358: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:37.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:37.903: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:37.933: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:37.962: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:38.149: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:38.179: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:38.208: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:38.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d: the server could not find the requested resource (get pods dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d)
    Jan 30 23:54:38.371: INFO: Lookups using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jan 30 23:54:43.360: INFO: DNS probes using dns-4479/dns-test-ab4ba984-2ac4-46ec-8281-146b83100f8d succeeded

    STEP: deleting the pod 01/30/23 23:54:43.36
    STEP: deleting the test service 01/30/23 23:54:43.415
    STEP: deleting the test headless service 01/30/23 23:54:43.537
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:54:43.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4479" for this suite. 01/30/23 23:54:43.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:54:43.649
Jan 30 23:54:43.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/30/23 23:54:43.651
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:43.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:43.759
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 23:54:43.78
Jan 30 23:54:43.819: INFO: Waiting up to 5m0s for pod "pod-d658b569-ca6b-487a-9101-fe4956cae576" in namespace "emptydir-8765" to be "Succeeded or Failed"
Jan 30 23:54:43.859: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Pending", Reason="", readiness=false. Elapsed: 40.088032ms
Jan 30 23:54:45.883: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Running", Reason="", readiness=true. Elapsed: 2.063792723s
Jan 30 23:54:47.884: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Running", Reason="", readiness=false. Elapsed: 4.064739948s
Jan 30 23:54:49.886: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067038888s
STEP: Saw pod success 01/30/23 23:54:49.886
Jan 30 23:54:49.887: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576" satisfied condition "Succeeded or Failed"
Jan 30 23:54:49.910: INFO: Trying to get logs from node 10.15.28.227 pod pod-d658b569-ca6b-487a-9101-fe4956cae576 container test-container: <nil>
STEP: delete the pod 01/30/23 23:54:50.042
Jan 30 23:54:50.114: INFO: Waiting for pod pod-d658b569-ca6b-487a-9101-fe4956cae576 to disappear
Jan 30 23:54:50.139: INFO: Pod pod-d658b569-ca6b-487a-9101-fe4956cae576 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 30 23:54:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8765" for this suite. 01/30/23 23:54:50.167
------------------------------
• [SLOW TEST] [6.553 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:54:43.649
    Jan 30 23:54:43.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/30/23 23:54:43.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:43.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:43.759
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/30/23 23:54:43.78
    Jan 30 23:54:43.819: INFO: Waiting up to 5m0s for pod "pod-d658b569-ca6b-487a-9101-fe4956cae576" in namespace "emptydir-8765" to be "Succeeded or Failed"
    Jan 30 23:54:43.859: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Pending", Reason="", readiness=false. Elapsed: 40.088032ms
    Jan 30 23:54:45.883: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Running", Reason="", readiness=true. Elapsed: 2.063792723s
    Jan 30 23:54:47.884: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Running", Reason="", readiness=false. Elapsed: 4.064739948s
    Jan 30 23:54:49.886: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067038888s
    STEP: Saw pod success 01/30/23 23:54:49.886
    Jan 30 23:54:49.887: INFO: Pod "pod-d658b569-ca6b-487a-9101-fe4956cae576" satisfied condition "Succeeded or Failed"
    Jan 30 23:54:49.910: INFO: Trying to get logs from node 10.15.28.227 pod pod-d658b569-ca6b-487a-9101-fe4956cae576 container test-container: <nil>
    STEP: delete the pod 01/30/23 23:54:50.042
    Jan 30 23:54:50.114: INFO: Waiting for pod pod-d658b569-ca6b-487a-9101-fe4956cae576 to disappear
    Jan 30 23:54:50.139: INFO: Pod pod-d658b569-ca6b-487a-9101-fe4956cae576 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:54:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8765" for this suite. 01/30/23 23:54:50.167
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:54:50.205
Jan 30 23:54:50.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:54:50.207
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:50.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:50.299
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 in namespace container-probe-7617 01/30/23 23:54:50.324
Jan 30 23:54:50.357: INFO: Waiting up to 5m0s for pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1" in namespace "container-probe-7617" to be "not pending"
Jan 30 23:54:50.378: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.500905ms
Jan 30 23:54:52.402: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044958144s
Jan 30 23:54:54.401: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.044700349s
Jan 30 23:54:54.401: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1" satisfied condition "not pending"
Jan 30 23:54:54.402: INFO: Started pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 in namespace container-probe-7617
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:54:54.402
Jan 30 23:54:54.421: INFO: Initial restart count of pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 is 0
Jan 30 23:55:12.639: INFO: Restart count of pod container-probe-7617/liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 is now 1 (18.217291332s elapsed)
STEP: deleting the pod 01/30/23 23:55:12.639
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 23:55:12.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7617" for this suite. 01/30/23 23:55:12.722
------------------------------
• [SLOW TEST] [22.553 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:54:50.205
    Jan 30 23:54:50.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:54:50.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:54:50.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:54:50.299
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 in namespace container-probe-7617 01/30/23 23:54:50.324
    Jan 30 23:54:50.357: INFO: Waiting up to 5m0s for pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1" in namespace "container-probe-7617" to be "not pending"
    Jan 30 23:54:50.378: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.500905ms
    Jan 30 23:54:52.402: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044958144s
    Jan 30 23:54:54.401: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.044700349s
    Jan 30 23:54:54.401: INFO: Pod "liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1" satisfied condition "not pending"
    Jan 30 23:54:54.402: INFO: Started pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 in namespace container-probe-7617
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:54:54.402
    Jan 30 23:54:54.421: INFO: Initial restart count of pod liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 is 0
    Jan 30 23:55:12.639: INFO: Restart count of pod container-probe-7617/liveness-94bbff4e-fbba-4f1f-a257-2d0c5a8b56b1 is now 1 (18.217291332s elapsed)
    STEP: deleting the pod 01/30/23 23:55:12.639
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:55:12.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7617" for this suite. 01/30/23 23:55:12.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:55:12.777
Jan 30 23:55:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename proxy 01/30/23 23:55:12.779
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:12.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:12.882
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/30/23 23:55:12.963
STEP: creating replication controller proxy-service-rd94v in namespace proxy-1746 01/30/23 23:55:12.964
I0130 23:55:12.988202      23 runners.go:193] Created replication controller with name: proxy-service-rd94v, namespace: proxy-1746, replica count: 1
I0130 23:55:14.039149      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 23:55:15.040229      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0130 23:55:16.041106      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 30 23:55:16.056: INFO: setup took 3.150756539s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/30/23 23:55:16.056
Jan 30 23:55:16.143: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 85.706466ms)
Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 104.219926ms)
Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 104.679954ms)
Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 104.317471ms)
Jan 30 23:55:16.178: INFO: (0) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 121.162117ms)
Jan 30 23:55:16.178: INFO: (0) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 121.275218ms)
Jan 30 23:55:16.181: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 124.238834ms)
Jan 30 23:55:16.182: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 125.067768ms)
Jan 30 23:55:16.182: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 125.260352ms)
Jan 30 23:55:16.193: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 136.194091ms)
Jan 30 23:55:16.193: INFO: (0) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 136.629848ms)
Jan 30 23:55:16.195: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 138.408701ms)
Jan 30 23:55:16.198: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 140.580384ms)
Jan 30 23:55:16.212: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 155.291668ms)
Jan 30 23:55:16.215: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 158.516626ms)
Jan 30 23:55:16.217: INFO: (0) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 160.028078ms)
Jan 30 23:55:16.245: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 27.078219ms)
Jan 30 23:55:16.258: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 39.723856ms)
Jan 30 23:55:16.258: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 40.377947ms)
Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.647713ms)
Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 42.097036ms)
Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 42.142125ms)
Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.245334ms)
Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.897201ms)
Jan 30 23:55:16.263: INFO: (1) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 44.390378ms)
Jan 30 23:55:16.264: INFO: (1) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 46.985954ms)
Jan 30 23:55:16.264: INFO: (1) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 46.179032ms)
Jan 30 23:55:16.273: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 55.157426ms)
Jan 30 23:55:16.273: INFO: (1) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 55.639198ms)
Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 55.441044ms)
Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 55.461398ms)
Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 55.539249ms)
Jan 30 23:55:16.312: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 37.677591ms)
Jan 30 23:55:16.319: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 43.460803ms)
Jan 30 23:55:16.321: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 44.618152ms)
Jan 30 23:55:16.321: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.743312ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 46.163991ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 45.277077ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 45.584132ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 46.252763ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.679238ms)
Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 46.493316ms)
Jan 30 23:55:16.327: INFO: (2) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 51.37745ms)
Jan 30 23:55:16.330: INFO: (2) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 53.887882ms)
Jan 30 23:55:16.331: INFO: (2) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 54.813791ms)
Jan 30 23:55:16.331: INFO: (2) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 55.012894ms)
Jan 30 23:55:16.333: INFO: (2) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 57.400561ms)
Jan 30 23:55:16.333: INFO: (2) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 56.982205ms)
Jan 30 23:55:16.364: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 30.060801ms)
Jan 30 23:55:16.376: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 42.339736ms)
Jan 30 23:55:16.376: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 42.136163ms)
Jan 30 23:55:16.377: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 41.994125ms)
Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 43.117534ms)
Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 44.284109ms)
Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 44.243645ms)
Jan 30 23:55:16.379: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 45.417174ms)
Jan 30 23:55:16.379: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 45.167348ms)
Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.819954ms)
Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.838439ms)
Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 46.357691ms)
Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.938155ms)
Jan 30 23:55:16.384: INFO: (3) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.811805ms)
Jan 30 23:55:16.392: INFO: (3) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 57.962658ms)
Jan 30 23:55:16.393: INFO: (3) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 58.030169ms)
Jan 30 23:55:16.427: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 33.438569ms)
Jan 30 23:55:16.437: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 43.567857ms)
Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 45.121018ms)
Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 44.152559ms)
Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 44.562352ms)
Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 45.396305ms)
Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 44.406567ms)
Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.022401ms)
Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 46.525495ms)
Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 51.761353ms)
Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 52.662927ms)
Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 51.51998ms)
Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 62.673759ms)
Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.56435ms)
Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 63.106834ms)
Jan 30 23:55:16.458: INFO: (4) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 64.133566ms)
Jan 30 23:55:16.488: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 29.211171ms)
Jan 30 23:55:16.497: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.868257ms)
Jan 30 23:55:16.497: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 38.020055ms)
Jan 30 23:55:16.498: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 38.848269ms)
Jan 30 23:55:16.500: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 40.347084ms)
Jan 30 23:55:16.500: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 40.738294ms)
Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 41.633404ms)
Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 41.707759ms)
Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.3976ms)
Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 42.020439ms)
Jan 30 23:55:16.504: INFO: (5) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 44.746215ms)
Jan 30 23:55:16.504: INFO: (5) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 44.753914ms)
Jan 30 23:55:16.505: INFO: (5) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 46.289448ms)
Jan 30 23:55:16.508: INFO: (5) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.720916ms)
Jan 30 23:55:16.516: INFO: (5) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 57.87554ms)
Jan 30 23:55:16.522: INFO: (5) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.355524ms)
Jan 30 23:55:16.569: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 46.138391ms)
Jan 30 23:55:16.570: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 47.218123ms)
Jan 30 23:55:16.571: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 48.569143ms)
Jan 30 23:55:16.574: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 51.055561ms)
Jan 30 23:55:16.575: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 51.905703ms)
Jan 30 23:55:16.576: INFO: (6) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 53.291988ms)
Jan 30 23:55:16.577: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 54.300786ms)
Jan 30 23:55:16.577: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 53.656839ms)
Jan 30 23:55:16.576: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 53.005933ms)
Jan 30 23:55:16.609: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 85.980183ms)
Jan 30 23:55:16.610: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 86.201235ms)
Jan 30 23:55:16.614: INFO: (6) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 91.066267ms)
Jan 30 23:55:16.614: INFO: (6) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 90.896492ms)
Jan 30 23:55:16.616: INFO: (6) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 93.392646ms)
Jan 30 23:55:16.616: INFO: (6) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 93.742208ms)
Jan 30 23:55:16.624: INFO: (6) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 100.545933ms)
Jan 30 23:55:16.661: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.145399ms)
Jan 30 23:55:16.662: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.364374ms)
Jan 30 23:55:16.663: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 38.347002ms)
Jan 30 23:55:16.664: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 39.934154ms)
Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 41.475822ms)
Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.254373ms)
Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.298791ms)
Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 42.104454ms)
Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 41.32925ms)
Jan 30 23:55:16.668: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 43.172675ms)
Jan 30 23:55:16.669: INFO: (7) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 44.218296ms)
Jan 30 23:55:16.675: INFO: (7) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 50.791481ms)
Jan 30 23:55:16.676: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 51.366847ms)
Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 51.964703ms)
Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 51.984251ms)
Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 52.251901ms)
Jan 30 23:55:16.706: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 28.570152ms)
Jan 30 23:55:16.720: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.805238ms)
Jan 30 23:55:16.722: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 43.034199ms)
Jan 30 23:55:16.722: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 43.417591ms)
Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 45.117857ms)
Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.861614ms)
Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 44.734251ms)
Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 46.639904ms)
Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 47.159914ms)
Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 46.521512ms)
Jan 30 23:55:16.725: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.675035ms)
Jan 30 23:55:16.726: INFO: (8) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 48.862609ms)
Jan 30 23:55:16.729: INFO: (8) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 50.811494ms)
Jan 30 23:55:16.731: INFO: (8) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 53.519921ms)
Jan 30 23:55:16.738: INFO: (8) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 58.698466ms)
Jan 30 23:55:16.738: INFO: (8) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 61.197907ms)
Jan 30 23:55:16.767: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 27.659084ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.519116ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 56.709852ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 56.743606ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 56.293015ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 56.525902ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 57.313507ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.879981ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 57.195817ms)
Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 56.928773ms)
Jan 30 23:55:16.807: INFO: (9) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 68.290626ms)
Jan 30 23:55:16.808: INFO: (9) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 68.585308ms)
Jan 30 23:55:16.809: INFO: (9) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 70.274268ms)
Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 72.702426ms)
Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 72.360925ms)
Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 71.731837ms)
Jan 30 23:55:16.843: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.80683ms)
Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 42.957746ms)
Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.755587ms)
Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 42.885851ms)
Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 43.57823ms)
Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 42.686664ms)
Jan 30 23:55:16.856: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 42.642454ms)
Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 43.2958ms)
Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 44.78697ms)
Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 43.686781ms)
Jan 30 23:55:16.858: INFO: (10) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.527018ms)
Jan 30 23:55:16.858: INFO: (10) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.550417ms)
Jan 30 23:55:16.864: INFO: (10) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 51.77348ms)
Jan 30 23:55:16.864: INFO: (10) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 50.576045ms)
Jan 30 23:55:16.866: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 52.382416ms)
Jan 30 23:55:16.866: INFO: (10) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 54.317419ms)
Jan 30 23:55:16.895: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 28.342818ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 40.497534ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 40.429274ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 40.376513ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.780844ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 40.243502ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 40.602052ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 40.830689ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 41.650689ms)
Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 40.520124ms)
Jan 30 23:55:16.909: INFO: (11) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 42.250553ms)
Jan 30 23:55:16.913: INFO: (11) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.452054ms)
Jan 30 23:55:16.916: INFO: (11) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.184116ms)
Jan 30 23:55:16.917: INFO: (11) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 48.653ms)
Jan 30 23:55:16.917: INFO: (11) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 49.5708ms)
Jan 30 23:55:16.919: INFO: (11) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 51.172281ms)
Jan 30 23:55:16.947: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 28.398637ms)
Jan 30 23:55:16.955: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 35.098545ms)
Jan 30 23:55:16.957: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.14851ms)
Jan 30 23:55:16.957: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 36.292516ms)
Jan 30 23:55:16.958: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 38.577144ms)
Jan 30 23:55:16.959: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 40.104433ms)
Jan 30 23:55:16.960: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 39.164071ms)
Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.002407ms)
Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.33654ms)
Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 40.16829ms)
Jan 30 23:55:16.963: INFO: (12) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 42.167637ms)
Jan 30 23:55:16.963: INFO: (12) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 42.1932ms)
Jan 30 23:55:16.964: INFO: (12) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 42.946278ms)
Jan 30 23:55:16.965: INFO: (12) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 46.355434ms)
Jan 30 23:55:16.970: INFO: (12) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 50.11698ms)
Jan 30 23:55:16.970: INFO: (12) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 50.217852ms)
Jan 30 23:55:17.015: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 43.667491ms)
Jan 30 23:55:17.026: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 54.516677ms)
Jan 30 23:55:17.026: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 54.918603ms)
Jan 30 23:55:17.027: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 55.303341ms)
Jan 30 23:55:17.028: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.552198ms)
Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 57.848184ms)
Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 58.672881ms)
Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 57.795359ms)
Jan 30 23:55:17.030: INFO: (13) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 59.44016ms)
Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 59.320063ms)
Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 59.592312ms)
Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 59.944469ms)
Jan 30 23:55:17.034: INFO: (13) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.213514ms)
Jan 30 23:55:17.036: INFO: (13) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 65.307107ms)
Jan 30 23:55:17.039: INFO: (13) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 67.922884ms)
Jan 30 23:55:17.040: INFO: (13) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 68.543715ms)
Jan 30 23:55:17.070: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.933594ms)
Jan 30 23:55:17.076: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 34.92999ms)
Jan 30 23:55:17.077: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 36.286304ms)
Jan 30 23:55:17.077: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.984012ms)
Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.966655ms)
Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.445057ms)
Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 37.158786ms)
Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.83321ms)
Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 38.051379ms)
Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.616158ms)
Jan 30 23:55:17.088: INFO: (14) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 47.929581ms)
Jan 30 23:55:17.089: INFO: (14) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 48.88223ms)
Jan 30 23:55:17.090: INFO: (14) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 48.784651ms)
Jan 30 23:55:17.091: INFO: (14) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 49.718748ms)
Jan 30 23:55:17.091: INFO: (14) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 49.418205ms)
Jan 30 23:55:17.092: INFO: (14) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 50.452992ms)
Jan 30 23:55:17.256: INFO: (15) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 162.184759ms)
Jan 30 23:55:17.256: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 157.029271ms)
Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 169.971623ms)
Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 165.428345ms)
Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 170.309242ms)
Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 161.99144ms)
Jan 30 23:55:17.263: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 167.213746ms)
Jan 30 23:55:17.263: INFO: (15) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 164.065021ms)
Jan 30 23:55:17.264: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 171.397302ms)
Jan 30 23:55:17.272: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 172.17513ms)
Jan 30 23:55:17.276: INFO: (15) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 176.959051ms)
Jan 30 23:55:17.280: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 186.996895ms)
Jan 30 23:55:17.282: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 189.007555ms)
Jan 30 23:55:17.283: INFO: (15) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 187.437938ms)
Jan 30 23:55:17.285: INFO: (15) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 188.936003ms)
Jan 30 23:55:17.285: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 189.658379ms)
Jan 30 23:55:17.310: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 23.505499ms)
Jan 30 23:55:17.320: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.79021ms)
Jan 30 23:55:17.321: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 34.095251ms)
Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 34.888183ms)
Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.428716ms)
Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 35.313085ms)
Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 36.625434ms)
Jan 30 23:55:17.323: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 36.244919ms)
Jan 30 23:55:17.323: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.456201ms)
Jan 30 23:55:17.324: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.18675ms)
Jan 30 23:55:17.329: INFO: (16) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 42.25808ms)
Jan 30 23:55:17.330: INFO: (16) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 44.440298ms)
Jan 30 23:55:17.330: INFO: (16) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 43.909017ms)
Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.050726ms)
Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 45.249185ms)
Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 44.161708ms)
Jan 30 23:55:17.354: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 22.665211ms)
Jan 30 23:55:17.361: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 28.599639ms)
Jan 30 23:55:17.362: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.712267ms)
Jan 30 23:55:17.362: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 30.283973ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 35.292575ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 35.888916ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 35.726447ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 36.34325ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.217495ms)
Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.804054ms)
Jan 30 23:55:17.370: INFO: (17) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 38.055953ms)
Jan 30 23:55:17.370: INFO: (17) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 38.179456ms)
Jan 30 23:55:17.371: INFO: (17) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 38.724311ms)
Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 45.273833ms)
Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.200923ms)
Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 44.963003ms)
Jan 30 23:55:17.401: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 22.125608ms)
Jan 30 23:55:17.410: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 31.583286ms)
Jan 30 23:55:17.412: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.284146ms)
Jan 30 23:55:17.413: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 33.74285ms)
Jan 30 23:55:17.414: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 34.826191ms)
Jan 30 23:55:17.417: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.594291ms)
Jan 30 23:55:17.417: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 38.057642ms)
Jan 30 23:55:17.419: INFO: (18) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 40.148054ms)
Jan 30 23:55:17.420: INFO: (18) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 41.170851ms)
Jan 30 23:55:17.433: INFO: (18) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 54.222116ms)
Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 54.814259ms)
Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 54.968979ms)
Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 55.341351ms)
Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 55.956186ms)
Jan 30 23:55:17.435: INFO: (18) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 55.693322ms)
Jan 30 23:55:17.439: INFO: (18) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 59.674695ms)
Jan 30 23:55:17.462: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 22.744782ms)
Jan 30 23:55:17.471: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 32.207525ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 31.330679ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 31.886424ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 31.929211ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 31.721618ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 31.523039ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.550515ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 31.759792ms)
Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 31.782324ms)
Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 37.74308ms)
Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 38.974997ms)
Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 38.591618ms)
Jan 30 23:55:17.479: INFO: (19) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 39.33641ms)
Jan 30 23:55:17.484: INFO: (19) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 44.073402ms)
Jan 30 23:55:17.485: INFO: (19) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 44.849589ms)
STEP: deleting ReplicationController proxy-service-rd94v in namespace proxy-1746, will wait for the garbage collector to delete the pods 01/30/23 23:55:17.485
Jan 30 23:55:17.603: INFO: Deleting ReplicationController proxy-service-rd94v took: 51.896362ms
Jan 30 23:55:17.704: INFO: Terminating ReplicationController proxy-service-rd94v pods took: 101.054866ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 30 23:55:20.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1746" for this suite. 01/30/23 23:55:20.349
------------------------------
• [SLOW TEST] [7.604 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:55:12.777
    Jan 30 23:55:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename proxy 01/30/23 23:55:12.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:12.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:12.882
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/30/23 23:55:12.963
    STEP: creating replication controller proxy-service-rd94v in namespace proxy-1746 01/30/23 23:55:12.964
    I0130 23:55:12.988202      23 runners.go:193] Created replication controller with name: proxy-service-rd94v, namespace: proxy-1746, replica count: 1
    I0130 23:55:14.039149      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 23:55:15.040229      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0130 23:55:16.041106      23 runners.go:193] proxy-service-rd94v Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 30 23:55:16.056: INFO: setup took 3.150756539s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/30/23 23:55:16.056
    Jan 30 23:55:16.143: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 85.706466ms)
    Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 104.219926ms)
    Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 104.679954ms)
    Jan 30 23:55:16.161: INFO: (0) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 104.317471ms)
    Jan 30 23:55:16.178: INFO: (0) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 121.162117ms)
    Jan 30 23:55:16.178: INFO: (0) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 121.275218ms)
    Jan 30 23:55:16.181: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 124.238834ms)
    Jan 30 23:55:16.182: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 125.067768ms)
    Jan 30 23:55:16.182: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 125.260352ms)
    Jan 30 23:55:16.193: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 136.194091ms)
    Jan 30 23:55:16.193: INFO: (0) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 136.629848ms)
    Jan 30 23:55:16.195: INFO: (0) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 138.408701ms)
    Jan 30 23:55:16.198: INFO: (0) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 140.580384ms)
    Jan 30 23:55:16.212: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 155.291668ms)
    Jan 30 23:55:16.215: INFO: (0) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 158.516626ms)
    Jan 30 23:55:16.217: INFO: (0) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 160.028078ms)
    Jan 30 23:55:16.245: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 27.078219ms)
    Jan 30 23:55:16.258: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 39.723856ms)
    Jan 30 23:55:16.258: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 40.377947ms)
    Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.647713ms)
    Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 42.097036ms)
    Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 42.142125ms)
    Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.245334ms)
    Jan 30 23:55:16.260: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.897201ms)
    Jan 30 23:55:16.263: INFO: (1) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 44.390378ms)
    Jan 30 23:55:16.264: INFO: (1) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 46.985954ms)
    Jan 30 23:55:16.264: INFO: (1) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 46.179032ms)
    Jan 30 23:55:16.273: INFO: (1) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 55.157426ms)
    Jan 30 23:55:16.273: INFO: (1) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 55.639198ms)
    Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 55.441044ms)
    Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 55.461398ms)
    Jan 30 23:55:16.274: INFO: (1) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 55.539249ms)
    Jan 30 23:55:16.312: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 37.677591ms)
    Jan 30 23:55:16.319: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 43.460803ms)
    Jan 30 23:55:16.321: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 44.618152ms)
    Jan 30 23:55:16.321: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.743312ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 46.163991ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 45.277077ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 45.584132ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 46.252763ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.679238ms)
    Jan 30 23:55:16.322: INFO: (2) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 46.493316ms)
    Jan 30 23:55:16.327: INFO: (2) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 51.37745ms)
    Jan 30 23:55:16.330: INFO: (2) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 53.887882ms)
    Jan 30 23:55:16.331: INFO: (2) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 54.813791ms)
    Jan 30 23:55:16.331: INFO: (2) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 55.012894ms)
    Jan 30 23:55:16.333: INFO: (2) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 57.400561ms)
    Jan 30 23:55:16.333: INFO: (2) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 56.982205ms)
    Jan 30 23:55:16.364: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 30.060801ms)
    Jan 30 23:55:16.376: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 42.339736ms)
    Jan 30 23:55:16.376: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 42.136163ms)
    Jan 30 23:55:16.377: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 41.994125ms)
    Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 43.117534ms)
    Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 44.284109ms)
    Jan 30 23:55:16.378: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 44.243645ms)
    Jan 30 23:55:16.379: INFO: (3) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 45.417174ms)
    Jan 30 23:55:16.379: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 45.167348ms)
    Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.819954ms)
    Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.838439ms)
    Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 46.357691ms)
    Jan 30 23:55:16.380: INFO: (3) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.938155ms)
    Jan 30 23:55:16.384: INFO: (3) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.811805ms)
    Jan 30 23:55:16.392: INFO: (3) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 57.962658ms)
    Jan 30 23:55:16.393: INFO: (3) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 58.030169ms)
    Jan 30 23:55:16.427: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 33.438569ms)
    Jan 30 23:55:16.437: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 43.567857ms)
    Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 45.121018ms)
    Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 44.152559ms)
    Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 44.562352ms)
    Jan 30 23:55:16.438: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 45.396305ms)
    Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 44.406567ms)
    Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.022401ms)
    Jan 30 23:55:16.439: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 46.525495ms)
    Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 51.761353ms)
    Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 52.662927ms)
    Jan 30 23:55:16.446: INFO: (4) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 51.51998ms)
    Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 62.673759ms)
    Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.56435ms)
    Jan 30 23:55:16.457: INFO: (4) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 63.106834ms)
    Jan 30 23:55:16.458: INFO: (4) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 64.133566ms)
    Jan 30 23:55:16.488: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 29.211171ms)
    Jan 30 23:55:16.497: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.868257ms)
    Jan 30 23:55:16.497: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 38.020055ms)
    Jan 30 23:55:16.498: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 38.848269ms)
    Jan 30 23:55:16.500: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 40.347084ms)
    Jan 30 23:55:16.500: INFO: (5) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 40.738294ms)
    Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 41.633404ms)
    Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 41.707759ms)
    Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.3976ms)
    Jan 30 23:55:16.501: INFO: (5) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 42.020439ms)
    Jan 30 23:55:16.504: INFO: (5) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 44.746215ms)
    Jan 30 23:55:16.504: INFO: (5) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 44.753914ms)
    Jan 30 23:55:16.505: INFO: (5) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 46.289448ms)
    Jan 30 23:55:16.508: INFO: (5) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.720916ms)
    Jan 30 23:55:16.516: INFO: (5) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 57.87554ms)
    Jan 30 23:55:16.522: INFO: (5) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.355524ms)
    Jan 30 23:55:16.569: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 46.138391ms)
    Jan 30 23:55:16.570: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 47.218123ms)
    Jan 30 23:55:16.571: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 48.569143ms)
    Jan 30 23:55:16.574: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 51.055561ms)
    Jan 30 23:55:16.575: INFO: (6) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 51.905703ms)
    Jan 30 23:55:16.576: INFO: (6) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 53.291988ms)
    Jan 30 23:55:16.577: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 54.300786ms)
    Jan 30 23:55:16.577: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 53.656839ms)
    Jan 30 23:55:16.576: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 53.005933ms)
    Jan 30 23:55:16.609: INFO: (6) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 85.980183ms)
    Jan 30 23:55:16.610: INFO: (6) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 86.201235ms)
    Jan 30 23:55:16.614: INFO: (6) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 91.066267ms)
    Jan 30 23:55:16.614: INFO: (6) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 90.896492ms)
    Jan 30 23:55:16.616: INFO: (6) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 93.392646ms)
    Jan 30 23:55:16.616: INFO: (6) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 93.742208ms)
    Jan 30 23:55:16.624: INFO: (6) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 100.545933ms)
    Jan 30 23:55:16.661: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.145399ms)
    Jan 30 23:55:16.662: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.364374ms)
    Jan 30 23:55:16.663: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 38.347002ms)
    Jan 30 23:55:16.664: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 39.934154ms)
    Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 41.475822ms)
    Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.254373ms)
    Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.298791ms)
    Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 42.104454ms)
    Jan 30 23:55:16.666: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 41.32925ms)
    Jan 30 23:55:16.668: INFO: (7) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 43.172675ms)
    Jan 30 23:55:16.669: INFO: (7) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 44.218296ms)
    Jan 30 23:55:16.675: INFO: (7) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 50.791481ms)
    Jan 30 23:55:16.676: INFO: (7) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 51.366847ms)
    Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 51.964703ms)
    Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 51.984251ms)
    Jan 30 23:55:16.677: INFO: (7) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 52.251901ms)
    Jan 30 23:55:16.706: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 28.570152ms)
    Jan 30 23:55:16.720: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.805238ms)
    Jan 30 23:55:16.722: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 43.034199ms)
    Jan 30 23:55:16.722: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 43.417591ms)
    Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 45.117857ms)
    Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.861614ms)
    Jan 30 23:55:16.723: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 44.734251ms)
    Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 46.639904ms)
    Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 47.159914ms)
    Jan 30 23:55:16.724: INFO: (8) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 46.521512ms)
    Jan 30 23:55:16.725: INFO: (8) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 45.675035ms)
    Jan 30 23:55:16.726: INFO: (8) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 48.862609ms)
    Jan 30 23:55:16.729: INFO: (8) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 50.811494ms)
    Jan 30 23:55:16.731: INFO: (8) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 53.519921ms)
    Jan 30 23:55:16.738: INFO: (8) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 58.698466ms)
    Jan 30 23:55:16.738: INFO: (8) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 61.197907ms)
    Jan 30 23:55:16.767: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 27.659084ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.519116ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 56.709852ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 56.743606ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 56.293015ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 56.525902ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 57.313507ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.879981ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 57.195817ms)
    Jan 30 23:55:16.796: INFO: (9) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 56.928773ms)
    Jan 30 23:55:16.807: INFO: (9) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 68.290626ms)
    Jan 30 23:55:16.808: INFO: (9) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 68.585308ms)
    Jan 30 23:55:16.809: INFO: (9) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 70.274268ms)
    Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 72.702426ms)
    Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 72.360925ms)
    Jan 30 23:55:16.811: INFO: (9) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 71.731837ms)
    Jan 30 23:55:16.843: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.80683ms)
    Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 42.957746ms)
    Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.755587ms)
    Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 42.885851ms)
    Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 43.57823ms)
    Jan 30 23:55:16.855: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 42.686664ms)
    Jan 30 23:55:16.856: INFO: (10) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 42.642454ms)
    Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 43.2958ms)
    Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 44.78697ms)
    Jan 30 23:55:16.857: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 43.686781ms)
    Jan 30 23:55:16.858: INFO: (10) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.527018ms)
    Jan 30 23:55:16.858: INFO: (10) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.550417ms)
    Jan 30 23:55:16.864: INFO: (10) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 51.77348ms)
    Jan 30 23:55:16.864: INFO: (10) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 50.576045ms)
    Jan 30 23:55:16.866: INFO: (10) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 52.382416ms)
    Jan 30 23:55:16.866: INFO: (10) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 54.317419ms)
    Jan 30 23:55:16.895: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 28.342818ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 40.497534ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 40.429274ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 40.376513ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.780844ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 40.243502ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 40.602052ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 40.830689ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 41.650689ms)
    Jan 30 23:55:16.908: INFO: (11) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 40.520124ms)
    Jan 30 23:55:16.909: INFO: (11) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 42.250553ms)
    Jan 30 23:55:16.913: INFO: (11) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 45.452054ms)
    Jan 30 23:55:16.916: INFO: (11) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 49.184116ms)
    Jan 30 23:55:16.917: INFO: (11) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 48.653ms)
    Jan 30 23:55:16.917: INFO: (11) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 49.5708ms)
    Jan 30 23:55:16.919: INFO: (11) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 51.172281ms)
    Jan 30 23:55:16.947: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 28.398637ms)
    Jan 30 23:55:16.955: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 35.098545ms)
    Jan 30 23:55:16.957: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.14851ms)
    Jan 30 23:55:16.957: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 36.292516ms)
    Jan 30 23:55:16.958: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 38.577144ms)
    Jan 30 23:55:16.959: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 40.104433ms)
    Jan 30 23:55:16.960: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 39.164071ms)
    Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 41.002407ms)
    Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 41.33654ms)
    Jan 30 23:55:16.961: INFO: (12) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 40.16829ms)
    Jan 30 23:55:16.963: INFO: (12) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 42.167637ms)
    Jan 30 23:55:16.963: INFO: (12) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 42.1932ms)
    Jan 30 23:55:16.964: INFO: (12) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 42.946278ms)
    Jan 30 23:55:16.965: INFO: (12) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 46.355434ms)
    Jan 30 23:55:16.970: INFO: (12) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 50.11698ms)
    Jan 30 23:55:16.970: INFO: (12) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 50.217852ms)
    Jan 30 23:55:17.015: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 43.667491ms)
    Jan 30 23:55:17.026: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 54.516677ms)
    Jan 30 23:55:17.026: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 54.918603ms)
    Jan 30 23:55:17.027: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 55.303341ms)
    Jan 30 23:55:17.028: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 56.552198ms)
    Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 57.848184ms)
    Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 58.672881ms)
    Jan 30 23:55:17.029: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 57.795359ms)
    Jan 30 23:55:17.030: INFO: (13) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 59.44016ms)
    Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 59.320063ms)
    Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 59.592312ms)
    Jan 30 23:55:17.031: INFO: (13) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 59.944469ms)
    Jan 30 23:55:17.034: INFO: (13) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 63.213514ms)
    Jan 30 23:55:17.036: INFO: (13) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 65.307107ms)
    Jan 30 23:55:17.039: INFO: (13) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 67.922884ms)
    Jan 30 23:55:17.040: INFO: (13) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 68.543715ms)
    Jan 30 23:55:17.070: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.933594ms)
    Jan 30 23:55:17.076: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 34.92999ms)
    Jan 30 23:55:17.077: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 36.286304ms)
    Jan 30 23:55:17.077: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.984012ms)
    Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.966655ms)
    Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.445057ms)
    Jan 30 23:55:17.078: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 37.158786ms)
    Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 37.83321ms)
    Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 38.051379ms)
    Jan 30 23:55:17.079: INFO: (14) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.616158ms)
    Jan 30 23:55:17.088: INFO: (14) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 47.929581ms)
    Jan 30 23:55:17.089: INFO: (14) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 48.88223ms)
    Jan 30 23:55:17.090: INFO: (14) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 48.784651ms)
    Jan 30 23:55:17.091: INFO: (14) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 49.718748ms)
    Jan 30 23:55:17.091: INFO: (14) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 49.418205ms)
    Jan 30 23:55:17.092: INFO: (14) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 50.452992ms)
    Jan 30 23:55:17.256: INFO: (15) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 162.184759ms)
    Jan 30 23:55:17.256: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 157.029271ms)
    Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 169.971623ms)
    Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 165.428345ms)
    Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 170.309242ms)
    Jan 30 23:55:17.262: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 161.99144ms)
    Jan 30 23:55:17.263: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 167.213746ms)
    Jan 30 23:55:17.263: INFO: (15) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 164.065021ms)
    Jan 30 23:55:17.264: INFO: (15) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 171.397302ms)
    Jan 30 23:55:17.272: INFO: (15) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 172.17513ms)
    Jan 30 23:55:17.276: INFO: (15) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 176.959051ms)
    Jan 30 23:55:17.280: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 186.996895ms)
    Jan 30 23:55:17.282: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 189.007555ms)
    Jan 30 23:55:17.283: INFO: (15) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 187.437938ms)
    Jan 30 23:55:17.285: INFO: (15) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 188.936003ms)
    Jan 30 23:55:17.285: INFO: (15) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 189.658379ms)
    Jan 30 23:55:17.310: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 23.505499ms)
    Jan 30 23:55:17.320: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.79021ms)
    Jan 30 23:55:17.321: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 34.095251ms)
    Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 34.888183ms)
    Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.428716ms)
    Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 35.313085ms)
    Jan 30 23:55:17.322: INFO: (16) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 36.625434ms)
    Jan 30 23:55:17.323: INFO: (16) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 36.244919ms)
    Jan 30 23:55:17.323: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.456201ms)
    Jan 30 23:55:17.324: INFO: (16) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 37.18675ms)
    Jan 30 23:55:17.329: INFO: (16) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 42.25808ms)
    Jan 30 23:55:17.330: INFO: (16) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 44.440298ms)
    Jan 30 23:55:17.330: INFO: (16) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 43.909017ms)
    Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.050726ms)
    Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 45.249185ms)
    Jan 30 23:55:17.331: INFO: (16) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 44.161708ms)
    Jan 30 23:55:17.354: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 22.665211ms)
    Jan 30 23:55:17.361: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 28.599639ms)
    Jan 30 23:55:17.362: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 29.712267ms)
    Jan 30 23:55:17.362: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 30.283973ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 35.292575ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 35.888916ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 35.726447ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 36.34325ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 36.217495ms)
    Jan 30 23:55:17.368: INFO: (17) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 35.804054ms)
    Jan 30 23:55:17.370: INFO: (17) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 38.055953ms)
    Jan 30 23:55:17.370: INFO: (17) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 38.179456ms)
    Jan 30 23:55:17.371: INFO: (17) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 38.724311ms)
    Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 45.273833ms)
    Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 45.200923ms)
    Jan 30 23:55:17.378: INFO: (17) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 44.963003ms)
    Jan 30 23:55:17.401: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 22.125608ms)
    Jan 30 23:55:17.410: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 31.583286ms)
    Jan 30 23:55:17.412: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.284146ms)
    Jan 30 23:55:17.413: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 33.74285ms)
    Jan 30 23:55:17.414: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 34.826191ms)
    Jan 30 23:55:17.417: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 37.594291ms)
    Jan 30 23:55:17.417: INFO: (18) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 38.057642ms)
    Jan 30 23:55:17.419: INFO: (18) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 40.148054ms)
    Jan 30 23:55:17.420: INFO: (18) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 41.170851ms)
    Jan 30 23:55:17.433: INFO: (18) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 54.222116ms)
    Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 54.814259ms)
    Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 54.968979ms)
    Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 55.341351ms)
    Jan 30 23:55:17.434: INFO: (18) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 55.956186ms)
    Jan 30 23:55:17.435: INFO: (18) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 55.693322ms)
    Jan 30 23:55:17.439: INFO: (18) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 59.674695ms)
    Jan 30 23:55:17.462: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:462/proxy/: tls qux (200; 22.744782ms)
    Jan 30 23:55:17.471: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 32.207525ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:162/proxy/: bar (200; 31.330679ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh/proxy/rewriteme">test</a> (200; 31.886424ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 31.929211ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">test<... (200; 31.721618ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:443/proxy/tlsrewritem... (200; 31.523039ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:160/proxy/: foo (200; 32.550515ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/https:proxy-service-rd94v-9t6wh:460/proxy/: tls baz (200; 31.759792ms)
    Jan 30 23:55:17.472: INFO: (19) /api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1746/pods/http:proxy-service-rd94v-9t6wh:1080/proxy/rewriteme">... (200; 31.782324ms)
    Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname2/proxy/: bar (200; 37.74308ms)
    Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname1/proxy/: tls baz (200; 38.974997ms)
    Jan 30 23:55:17.478: INFO: (19) /api/v1/namespaces/proxy-1746/services/https:proxy-service-rd94v:tlsportname2/proxy/: tls qux (200; 38.591618ms)
    Jan 30 23:55:17.479: INFO: (19) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname1/proxy/: foo (200; 39.33641ms)
    Jan 30 23:55:17.484: INFO: (19) /api/v1/namespaces/proxy-1746/services/proxy-service-rd94v:portname2/proxy/: bar (200; 44.073402ms)
    Jan 30 23:55:17.485: INFO: (19) /api/v1/namespaces/proxy-1746/services/http:proxy-service-rd94v:portname1/proxy/: foo (200; 44.849589ms)
    STEP: deleting ReplicationController proxy-service-rd94v in namespace proxy-1746, will wait for the garbage collector to delete the pods 01/30/23 23:55:17.485
    Jan 30 23:55:17.603: INFO: Deleting ReplicationController proxy-service-rd94v took: 51.896362ms
    Jan 30 23:55:17.704: INFO: Terminating ReplicationController proxy-service-rd94v pods took: 101.054866ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:55:20.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1746" for this suite. 01/30/23 23:55:20.349
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:55:20.385
Jan 30 23:55:20.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/30/23 23:55:20.39
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:20.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:20.459
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 30 23:55:20.566: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 30 23:55:25.588: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/30/23 23:55:25.588
Jan 30 23:55:25.588: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 30 23:55:27.608: INFO: Creating deployment "test-rollover-deployment"
Jan 30 23:55:27.655: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 30 23:55:29.693: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 30 23:55:29.728: INFO: Ensure that both replica sets have 1 created replica
Jan 30 23:55:29.761: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 30 23:55:29.805: INFO: Updating deployment test-rollover-deployment
Jan 30 23:55:29.805: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 30 23:55:31.843: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 30 23:55:31.879: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 30 23:55:31.917: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:31.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:34.012: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:34.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:35.967: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:35.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:37.957: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:37.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:39.956: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:39.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:41.956: INFO: all replica sets need to contain the pod-template-hash label
Jan 30 23:55:41.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 30 23:55:43.954: INFO: 
Jan 30 23:55:43.954: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 30 23:55:44.048: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4759  0b49e2b4-9e51-4b1b-beca-e351e4a12f31 43679 2 2023-01-30 23:55:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fa9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 23:55:27 +0000 UTC,LastTransitionTime:2023-01-30 23:55:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-30 23:55:42 +0000 UTC,LastTransitionTime:2023-01-30 23:55:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 30 23:55:44.071: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4759  2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a 43669 2 2023-01-30 23:55:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045faeb7 0xc0045faeb8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045faf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:55:44.071: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 30 23:55:44.071: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4759  e0fdeded-5a6d-478a-8c82-04b6561a4521 43678 2 2023-01-30 23:55:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045fad77 0xc0045fad78}] [] [{e2e.test Update apps/v1 2023-01-30 23:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045fae38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:55:44.072: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4759  9a3bd219-1c1f-49f6-a35b-d42e6a31e6f3 43636 2 2023-01-30 23:55:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045fafd7 0xc0045fafd8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fb088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 30 23:55:44.089: INFO: Pod "test-rollover-deployment-6c6df9974f-jw7bj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-jw7bj test-rollover-deployment-6c6df9974f- deployment-4759  107207cf-a98c-4702-8bfc-b996fa449df2 43650 0 2023-01-30 23:55:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:ff92dc1b767f539b225b57d3e029d6f847535d79cf58f81de6a092a68ccf61fd cni.projectcalico.org/podIP:172.30.199.51/32 cni.projectcalico.org/podIPs:172.30.199.51/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a 0xc0045fb607 0xc0045fb608}] [] [{kube-controller-manager Update v1 2023-01-30 23:55:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:55:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:55:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhdxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhdxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.51,StartTime:2023-01-30 23:55:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:55:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f426930171ca179bbb859d0c25a384ae7d827b07a39c1acdda5681de16398818,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 30 23:55:44.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4759" for this suite. 01/30/23 23:55:44.156
------------------------------
• [SLOW TEST] [23.799 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:55:20.385
    Jan 30 23:55:20.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/30/23 23:55:20.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:20.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:20.459
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 30 23:55:20.566: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 30 23:55:25.588: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/30/23 23:55:25.588
    Jan 30 23:55:25.588: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 30 23:55:27.608: INFO: Creating deployment "test-rollover-deployment"
    Jan 30 23:55:27.655: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 30 23:55:29.693: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 30 23:55:29.728: INFO: Ensure that both replica sets have 1 created replica
    Jan 30 23:55:29.761: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 30 23:55:29.805: INFO: Updating deployment test-rollover-deployment
    Jan 30 23:55:29.805: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 30 23:55:31.843: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 30 23:55:31.879: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 30 23:55:31.917: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:31.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:34.012: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:34.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:35.967: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:35.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:37.957: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:37.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:39.956: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:39.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:41.956: INFO: all replica sets need to contain the pod-template-hash label
    Jan 30 23:55:41.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 55, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 55, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 30 23:55:43.954: INFO: 
    Jan 30 23:55:43.954: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 30 23:55:44.048: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4759  0b49e2b4-9e51-4b1b-beca-e351e4a12f31 43679 2 2023-01-30 23:55:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fa9e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-30 23:55:27 +0000 UTC,LastTransitionTime:2023-01-30 23:55:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-30 23:55:42 +0000 UTC,LastTransitionTime:2023-01-30 23:55:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 30 23:55:44.071: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-4759  2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a 43669 2 2023-01-30 23:55:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045faeb7 0xc0045faeb8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045faf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:55:44.071: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 30 23:55:44.071: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4759  e0fdeded-5a6d-478a-8c82-04b6561a4521 43678 2 2023-01-30 23:55:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045fad77 0xc0045fad78}] [] [{e2e.test Update apps/v1 2023-01-30 23:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0045fae38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:55:44.072: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-4759  9a3bd219-1c1f-49f6-a35b-d42e6a31e6f3 43636 2 2023-01-30 23:55:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0b49e2b4-9e51-4b1b-beca-e351e4a12f31 0xc0045fafd7 0xc0045fafd8}] [] [{kube-controller-manager Update apps/v1 2023-01-30 23:55:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b49e2b4-9e51-4b1b-beca-e351e4a12f31\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-30 23:55:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045fb088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 30 23:55:44.089: INFO: Pod "test-rollover-deployment-6c6df9974f-jw7bj" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-jw7bj test-rollover-deployment-6c6df9974f- deployment-4759  107207cf-a98c-4702-8bfc-b996fa449df2 43650 0 2023-01-30 23:55:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:ff92dc1b767f539b225b57d3e029d6f847535d79cf58f81de6a092a68ccf61fd cni.projectcalico.org/podIP:172.30.199.51/32 cni.projectcalico.org/podIPs:172.30.199.51/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a 0xc0045fb607 0xc0045fb608}] [] [{kube-controller-manager Update v1 2023-01-30 23:55:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b2a5a5c-aaa2-49ca-81d6-fefe94d11d7a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-30 23:55:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-30 23:55:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhdxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhdxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-30 23:55:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.51,StartTime:2023-01-30 23:55:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-30 23:55:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f426930171ca179bbb859d0c25a384ae7d827b07a39c1acdda5681de16398818,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:55:44.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4759" for this suite. 01/30/23 23:55:44.156
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:55:44.184
Jan 30 23:55:44.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:55:44.187
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:44.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:44.253
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/30/23 23:55:44.269
Jan 30 23:55:44.307: INFO: Waiting up to 5m0s for pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134" in namespace "svcaccounts-495" to be "Succeeded or Failed"
Jan 30 23:55:44.326: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 18.913274ms
Jan 30 23:55:46.345: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038169426s
Jan 30 23:55:48.365: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058397477s
Jan 30 23:55:50.346: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038763587s
STEP: Saw pod success 01/30/23 23:55:50.346
Jan 30 23:55:50.346: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134" satisfied condition "Succeeded or Failed"
Jan 30 23:55:50.392: INFO: Trying to get logs from node 10.15.28.227 pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:55:50.523
Jan 30 23:55:50.631: INFO: Waiting for pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 to disappear
Jan 30 23:55:50.652: INFO: Pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 30 23:55:50.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-495" for this suite. 01/30/23 23:55:50.691
------------------------------
• [SLOW TEST] [6.580 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:55:44.184
    Jan 30 23:55:44.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/30/23 23:55:44.187
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:44.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:44.253
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/30/23 23:55:44.269
    Jan 30 23:55:44.307: INFO: Waiting up to 5m0s for pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134" in namespace "svcaccounts-495" to be "Succeeded or Failed"
    Jan 30 23:55:44.326: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 18.913274ms
    Jan 30 23:55:46.345: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038169426s
    Jan 30 23:55:48.365: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058397477s
    Jan 30 23:55:50.346: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038763587s
    STEP: Saw pod success 01/30/23 23:55:50.346
    Jan 30 23:55:50.346: INFO: Pod "test-pod-33df6077-932a-46de-a8d5-1ea7b048a134" satisfied condition "Succeeded or Failed"
    Jan 30 23:55:50.392: INFO: Trying to get logs from node 10.15.28.227 pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:55:50.523
    Jan 30 23:55:50.631: INFO: Waiting for pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 to disappear
    Jan 30 23:55:50.652: INFO: Pod test-pod-33df6077-932a-46de-a8d5-1ea7b048a134 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:55:50.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-495" for this suite. 01/30/23 23:55:50.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:55:50.774
Jan 30 23:55:50.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:55:50.776
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:50.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:50.848
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:55:50.867
Jan 30 23:55:50.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187" in namespace "downward-api-2737" to be "Succeeded or Failed"
Jan 30 23:55:50.961: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 22.378891ms
Jan 30 23:55:52.981: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042726556s
Jan 30 23:55:54.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041530399s
Jan 30 23:55:56.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041856208s
STEP: Saw pod success 01/30/23 23:55:56.98
Jan 30 23:55:56.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187" satisfied condition "Succeeded or Failed"
Jan 30 23:55:57.000: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 container client-container: <nil>
STEP: delete the pod 01/30/23 23:55:57.044
Jan 30 23:55:57.089: INFO: Waiting for pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 to disappear
Jan 30 23:55:57.107: INFO: Pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:55:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2737" for this suite. 01/30/23 23:55:57.146
------------------------------
• [SLOW TEST] [6.400 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:55:50.774
    Jan 30 23:55:50.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:55:50.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:50.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:50.848
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:55:50.867
    Jan 30 23:55:50.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187" in namespace "downward-api-2737" to be "Succeeded or Failed"
    Jan 30 23:55:50.961: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 22.378891ms
    Jan 30 23:55:52.981: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042726556s
    Jan 30 23:55:54.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041530399s
    Jan 30 23:55:56.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041856208s
    STEP: Saw pod success 01/30/23 23:55:56.98
    Jan 30 23:55:56.980: INFO: Pod "downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187" satisfied condition "Succeeded or Failed"
    Jan 30 23:55:57.000: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:55:57.044
    Jan 30 23:55:57.089: INFO: Waiting for pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 to disappear
    Jan 30 23:55:57.107: INFO: Pod downwardapi-volume-d578d9ec-bf98-48d3-b3c8-cabb4597b187 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:55:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2737" for this suite. 01/30/23 23:55:57.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:55:57.178
Jan 30 23:55:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename job 01/30/23 23:55:57.182
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:57.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:57.265
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/30/23 23:55:57.283
STEP: Ensuring job reaches completions 01/30/23 23:55:57.305
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 30 23:56:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2229" for this suite. 01/30/23 23:56:13.368
------------------------------
• [SLOW TEST] [16.218 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:55:57.178
    Jan 30 23:55:57.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename job 01/30/23 23:55:57.182
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:55:57.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:55:57.265
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/30/23 23:55:57.283
    STEP: Ensuring job reaches completions 01/30/23 23:55:57.305
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:56:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2229" for this suite. 01/30/23 23:56:13.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:56:13.402
Jan 30 23:56:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/30/23 23:56:13.404
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:13.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:13.491
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/30/23 23:56:13.532
Jan 30 23:56:13.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-290 create -f -'
Jan 30 23:56:13.901: INFO: stderr: ""
Jan 30 23:56:13.901: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/30/23 23:56:13.901
Jan 30 23:56:14.920: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:56:14.920: INFO: Found 0 / 1
Jan 30 23:56:15.920: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:56:15.920: INFO: Found 0 / 1
Jan 30 23:56:16.921: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:56:16.921: INFO: Found 1 / 1
Jan 30 23:56:16.921: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/30/23 23:56:16.921
Jan 30 23:56:16.938: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:56:16.938: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 30 23:56:16.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-290 patch pod agnhost-primary-27nc2 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 30 23:56:17.107: INFO: stderr: ""
Jan 30 23:56:17.107: INFO: stdout: "pod/agnhost-primary-27nc2 patched\n"
STEP: checking annotations 01/30/23 23:56:17.107
Jan 30 23:56:17.126: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 30 23:56:17.126: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 30 23:56:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-290" for this suite. 01/30/23 23:56:17.161
------------------------------
• [3.794 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:56:13.402
    Jan 30 23:56:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/30/23 23:56:13.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:13.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:13.491
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/30/23 23:56:13.532
    Jan 30 23:56:13.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-290 create -f -'
    Jan 30 23:56:13.901: INFO: stderr: ""
    Jan 30 23:56:13.901: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/30/23 23:56:13.901
    Jan 30 23:56:14.920: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:56:14.920: INFO: Found 0 / 1
    Jan 30 23:56:15.920: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:56:15.920: INFO: Found 0 / 1
    Jan 30 23:56:16.921: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:56:16.921: INFO: Found 1 / 1
    Jan 30 23:56:16.921: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/30/23 23:56:16.921
    Jan 30 23:56:16.938: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:56:16.938: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 30 23:56:16.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-290 patch pod agnhost-primary-27nc2 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 30 23:56:17.107: INFO: stderr: ""
    Jan 30 23:56:17.107: INFO: stdout: "pod/agnhost-primary-27nc2 patched\n"
    STEP: checking annotations 01/30/23 23:56:17.107
    Jan 30 23:56:17.126: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 30 23:56:17.126: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:56:17.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-290" for this suite. 01/30/23 23:56:17.161
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:56:17.196
Jan 30 23:56:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/30/23 23:56:17.199
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:17.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:17.271
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/30/23 23:56:17.288
Jan 30 23:56:17.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1" in namespace "downward-api-139" to be "Succeeded or Failed"
Jan 30 23:56:17.361: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.449241ms
Jan 30 23:56:19.377: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035893116s
Jan 30 23:56:21.379: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037240107s
STEP: Saw pod success 01/30/23 23:56:21.379
Jan 30 23:56:21.379: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1" satisfied condition "Succeeded or Failed"
Jan 30 23:56:21.395: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 container client-container: <nil>
STEP: delete the pod 01/30/23 23:56:21.438
Jan 30 23:56:21.524: INFO: Waiting for pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 to disappear
Jan 30 23:56:21.543: INFO: Pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 30 23:56:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-139" for this suite. 01/30/23 23:56:21.571
------------------------------
• [4.407 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:56:17.196
    Jan 30 23:56:17.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/30/23 23:56:17.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:17.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:17.271
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/30/23 23:56:17.288
    Jan 30 23:56:17.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1" in namespace "downward-api-139" to be "Succeeded or Failed"
    Jan 30 23:56:17.361: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.449241ms
    Jan 30 23:56:19.377: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035893116s
    Jan 30 23:56:21.379: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037240107s
    STEP: Saw pod success 01/30/23 23:56:21.379
    Jan 30 23:56:21.379: INFO: Pod "downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1" satisfied condition "Succeeded or Failed"
    Jan 30 23:56:21.395: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 container client-container: <nil>
    STEP: delete the pod 01/30/23 23:56:21.438
    Jan 30 23:56:21.524: INFO: Waiting for pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 to disappear
    Jan 30 23:56:21.543: INFO: Pod downwardapi-volume-edfb1c78-da00-45c9-a29c-b5d186c3d6b1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:56:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-139" for this suite. 01/30/23 23:56:21.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:56:21.604
Jan 30 23:56:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 23:56:21.609
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:21.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:21.691
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/30/23 23:56:21.733
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/30/23 23:56:21.752
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 23:56:21.752
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/30/23 23:56:21.752
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/30/23 23:56:21.773
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 23:56:21.773
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 23:56:21.792
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:56:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8171" for this suite. 01/30/23 23:56:21.828
------------------------------
• [0.254 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:56:21.604
    Jan 30 23:56:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename custom-resource-definition 01/30/23 23:56:21.609
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:21.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:21.691
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/30/23 23:56:21.733
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/30/23 23:56:21.752
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/30/23 23:56:21.752
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/30/23 23:56:21.752
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/30/23 23:56:21.773
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 23:56:21.773
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/30/23 23:56:21.792
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:56:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8171" for this suite. 01/30/23 23:56:21.828
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:56:21.861
Jan 30 23:56:21.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename cronjob 01/30/23 23:56:21.863
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:21.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:21.942
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/30/23 23:56:21.957
STEP: Ensuring a job is scheduled 01/30/23 23:56:21.981
STEP: Ensuring exactly one is scheduled 01/30/23 23:57:02.001
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 23:57:02.019
STEP: Ensuring the job is replaced with a new one 01/30/23 23:57:02.036
STEP: Removing cronjob 01/30/23 23:58:02.061
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 30 23:58:02.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7103" for this suite. 01/30/23 23:58:02.12
------------------------------
• [SLOW TEST] [100.284 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:56:21.861
    Jan 30 23:56:21.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename cronjob 01/30/23 23:56:21.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:56:21.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:56:21.942
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/30/23 23:56:21.957
    STEP: Ensuring a job is scheduled 01/30/23 23:56:21.981
    STEP: Ensuring exactly one is scheduled 01/30/23 23:57:02.001
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/30/23 23:57:02.019
    STEP: Ensuring the job is replaced with a new one 01/30/23 23:57:02.036
    STEP: Removing cronjob 01/30/23 23:58:02.061
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:58:02.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7103" for this suite. 01/30/23 23:58:02.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:58:02.155
Jan 30 23:58:02.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/30/23 23:58:02.158
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:02.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:02.263
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-f81985d6-1dd2-4b2f-aa4f-156903881a9e 01/30/23 23:58:02.281
STEP: Creating a pod to test consume configMaps 01/30/23 23:58:02.303
Jan 30 23:58:02.354: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9" in namespace "projected-5856" to be "Succeeded or Failed"
Jan 30 23:58:02.376: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.985369ms
Jan 30 23:58:04.413: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059393218s
Jan 30 23:58:06.405: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051445713s
Jan 30 23:58:08.404: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049590114s
STEP: Saw pod success 01/30/23 23:58:08.404
Jan 30 23:58:08.404: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9" satisfied condition "Succeeded or Failed"
Jan 30 23:58:08.424: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 container agnhost-container: <nil>
STEP: delete the pod 01/30/23 23:58:08.546
Jan 30 23:58:08.643: INFO: Waiting for pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 to disappear
Jan 30 23:58:08.663: INFO: Pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 30 23:58:08.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5856" for this suite. 01/30/23 23:58:08.694
------------------------------
• [SLOW TEST] [6.597 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:58:02.155
    Jan 30 23:58:02.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/30/23 23:58:02.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:02.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:02.263
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-f81985d6-1dd2-4b2f-aa4f-156903881a9e 01/30/23 23:58:02.281
    STEP: Creating a pod to test consume configMaps 01/30/23 23:58:02.303
    Jan 30 23:58:02.354: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9" in namespace "projected-5856" to be "Succeeded or Failed"
    Jan 30 23:58:02.376: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.985369ms
    Jan 30 23:58:04.413: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059393218s
    Jan 30 23:58:06.405: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051445713s
    Jan 30 23:58:08.404: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049590114s
    STEP: Saw pod success 01/30/23 23:58:08.404
    Jan 30 23:58:08.404: INFO: Pod "pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9" satisfied condition "Succeeded or Failed"
    Jan 30 23:58:08.424: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 container agnhost-container: <nil>
    STEP: delete the pod 01/30/23 23:58:08.546
    Jan 30 23:58:08.643: INFO: Waiting for pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 to disappear
    Jan 30 23:58:08.663: INFO: Pod pod-projected-configmaps-58eb3479-d319-4bcc-b03f-20c5b7af6ba9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:58:08.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5856" for this suite. 01/30/23 23:58:08.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:58:08.765
Jan 30 23:58:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename controllerrevisions 01/30/23 23:58:08.767
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:08.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:08.894
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-l8mcz-daemon-set" 01/30/23 23:58:09.07
STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:58:09.093
Jan 30 23:58:09.163: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
Jan 30 23:58:09.163: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:58:10.243: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
Jan 30 23:58:10.243: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:58:11.261: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
Jan 30 23:58:11.261: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
Jan 30 23:58:12.239: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 3
Jan 30 23:58:12.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-l8mcz-daemon-set
STEP: Confirm DaemonSet "e2e-l8mcz-daemon-set" successfully created with "daemonset-name=e2e-l8mcz-daemon-set" label 01/30/23 23:58:12.261
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-l8mcz-daemon-set" 01/30/23 23:58:12.303
Jan 30 23:58:12.328: INFO: Located ControllerRevision: "e2e-l8mcz-daemon-set-6d898779b7"
STEP: Patching ControllerRevision "e2e-l8mcz-daemon-set-6d898779b7" 01/30/23 23:58:12.343
Jan 30 23:58:12.364: INFO: e2e-l8mcz-daemon-set-6d898779b7 has been patched
STEP: Create a new ControllerRevision 01/30/23 23:58:12.364
Jan 30 23:58:12.391: INFO: Created ControllerRevision: e2e-l8mcz-daemon-set-7765964cb7
STEP: Confirm that there are two ControllerRevisions 01/30/23 23:58:12.391
Jan 30 23:58:12.392: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 23:58:12.420: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-l8mcz-daemon-set-6d898779b7" 01/30/23 23:58:12.42
STEP: Confirm that there is only one ControllerRevision 01/30/23 23:58:12.449
Jan 30 23:58:12.449: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 23:58:12.465: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-l8mcz-daemon-set-7765964cb7" 01/30/23 23:58:12.481
Jan 30 23:58:12.520: INFO: e2e-l8mcz-daemon-set-7765964cb7 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/30/23 23:58:12.52
W0130 23:58:12.558326      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/30/23 23:58:12.559
Jan 30 23:58:12.559: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 23:58:13.609: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 23:58:13.626: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-l8mcz-daemon-set-7765964cb7=updated" 01/30/23 23:58:13.626
STEP: Confirm that there is only one ControllerRevision 01/30/23 23:58:13.663
Jan 30 23:58:13.664: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 30 23:58:13.681: INFO: Found 1 ControllerRevisions
Jan 30 23:58:13.700: INFO: ControllerRevision "e2e-l8mcz-daemon-set-746cc8646" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-l8mcz-daemon-set" 01/30/23 23:58:13.746
STEP: deleting DaemonSet.extensions e2e-l8mcz-daemon-set in namespace controllerrevisions-3952, will wait for the garbage collector to delete the pods 01/30/23 23:58:13.746
Jan 30 23:58:13.838: INFO: Deleting DaemonSet.extensions e2e-l8mcz-daemon-set took: 25.073819ms
Jan 30 23:58:14.039: INFO: Terminating DaemonSet.extensions e2e-l8mcz-daemon-set pods took: 201.084841ms
Jan 30 23:58:16.058: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
Jan 30 23:58:16.059: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-l8mcz-daemon-set
Jan 30 23:58:16.073: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44371"},"items":null}

Jan 30 23:58:16.089: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44371"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:58:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3952" for this suite. 01/30/23 23:58:16.308
------------------------------
• [SLOW TEST] [7.579 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:58:08.765
    Jan 30 23:58:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename controllerrevisions 01/30/23 23:58:08.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:08.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:08.894
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-l8mcz-daemon-set" 01/30/23 23:58:09.07
    STEP: Check that daemon pods launch on every node of the cluster. 01/30/23 23:58:09.093
    Jan 30 23:58:09.163: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
    Jan 30 23:58:09.163: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:58:10.243: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
    Jan 30 23:58:10.243: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:58:11.261: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
    Jan 30 23:58:11.261: INFO: Node 10.15.28.225 is running 0 daemon pod, expected 1
    Jan 30 23:58:12.239: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 3
    Jan 30 23:58:12.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-l8mcz-daemon-set
    STEP: Confirm DaemonSet "e2e-l8mcz-daemon-set" successfully created with "daemonset-name=e2e-l8mcz-daemon-set" label 01/30/23 23:58:12.261
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-l8mcz-daemon-set" 01/30/23 23:58:12.303
    Jan 30 23:58:12.328: INFO: Located ControllerRevision: "e2e-l8mcz-daemon-set-6d898779b7"
    STEP: Patching ControllerRevision "e2e-l8mcz-daemon-set-6d898779b7" 01/30/23 23:58:12.343
    Jan 30 23:58:12.364: INFO: e2e-l8mcz-daemon-set-6d898779b7 has been patched
    STEP: Create a new ControllerRevision 01/30/23 23:58:12.364
    Jan 30 23:58:12.391: INFO: Created ControllerRevision: e2e-l8mcz-daemon-set-7765964cb7
    STEP: Confirm that there are two ControllerRevisions 01/30/23 23:58:12.391
    Jan 30 23:58:12.392: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 23:58:12.420: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-l8mcz-daemon-set-6d898779b7" 01/30/23 23:58:12.42
    STEP: Confirm that there is only one ControllerRevision 01/30/23 23:58:12.449
    Jan 30 23:58:12.449: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 23:58:12.465: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-l8mcz-daemon-set-7765964cb7" 01/30/23 23:58:12.481
    Jan 30 23:58:12.520: INFO: e2e-l8mcz-daemon-set-7765964cb7 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/30/23 23:58:12.52
    W0130 23:58:12.558326      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/30/23 23:58:12.559
    Jan 30 23:58:12.559: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 23:58:13.609: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 23:58:13.626: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-l8mcz-daemon-set-7765964cb7=updated" 01/30/23 23:58:13.626
    STEP: Confirm that there is only one ControllerRevision 01/30/23 23:58:13.663
    Jan 30 23:58:13.664: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 30 23:58:13.681: INFO: Found 1 ControllerRevisions
    Jan 30 23:58:13.700: INFO: ControllerRevision "e2e-l8mcz-daemon-set-746cc8646" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-l8mcz-daemon-set" 01/30/23 23:58:13.746
    STEP: deleting DaemonSet.extensions e2e-l8mcz-daemon-set in namespace controllerrevisions-3952, will wait for the garbage collector to delete the pods 01/30/23 23:58:13.746
    Jan 30 23:58:13.838: INFO: Deleting DaemonSet.extensions e2e-l8mcz-daemon-set took: 25.073819ms
    Jan 30 23:58:14.039: INFO: Terminating DaemonSet.extensions e2e-l8mcz-daemon-set pods took: 201.084841ms
    Jan 30 23:58:16.058: INFO: Number of nodes with available pods controlled by daemonset e2e-l8mcz-daemon-set: 0
    Jan 30 23:58:16.059: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-l8mcz-daemon-set
    Jan 30 23:58:16.073: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44371"},"items":null}

    Jan 30 23:58:16.089: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44371"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:58:16.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3952" for this suite. 01/30/23 23:58:16.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:58:16.347
Jan 30 23:58:16.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/30/23 23:58:16.354
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:16.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:16.42
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/30/23 23:58:16.545
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:58:16.789
STEP: Deploying the webhook pod 01/30/23 23:58:16.829
STEP: Wait for the deployment to be ready 01/30/23 23:58:16.875
Jan 30 23:58:16.943: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 30 23:58:19.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 58, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/30/23 23:58:21.034
STEP: Verifying the service has paired with the endpoint 01/30/23 23:58:21.093
Jan 30 23:58:22.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 23:58:22.111
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 23:58:22.274
STEP: Creating a dummy validating-webhook-configuration object 01/30/23 23:58:22.36
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/30/23 23:58:22.422
STEP: Creating a dummy mutating-webhook-configuration object 01/30/23 23:58:22.45
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/30/23 23:58:22.506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 30 23:58:22.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7146" for this suite. 01/30/23 23:58:22.965
STEP: Destroying namespace "webhook-7146-markers" for this suite. 01/30/23 23:58:23.035
------------------------------
• [SLOW TEST] [6.718 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:58:16.347
    Jan 30 23:58:16.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/30/23 23:58:16.354
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:16.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:16.42
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/30/23 23:58:16.545
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/30/23 23:58:16.789
    STEP: Deploying the webhook pod 01/30/23 23:58:16.829
    STEP: Wait for the deployment to be ready 01/30/23 23:58:16.875
    Jan 30 23:58:16.943: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 30 23:58:19.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 30, 23, 58, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 30, 23, 58, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/30/23 23:58:21.034
    STEP: Verifying the service has paired with the endpoint 01/30/23 23:58:21.093
    Jan 30 23:58:22.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 23:58:22.111
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/30/23 23:58:22.274
    STEP: Creating a dummy validating-webhook-configuration object 01/30/23 23:58:22.36
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/30/23 23:58:22.422
    STEP: Creating a dummy mutating-webhook-configuration object 01/30/23 23:58:22.45
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/30/23 23:58:22.506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:58:22.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7146" for this suite. 01/30/23 23:58:22.965
    STEP: Destroying namespace "webhook-7146-markers" for this suite. 01/30/23 23:58:23.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:58:23.077
Jan 30 23:58:23.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:58:23.078
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:23.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:23.196
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 30 23:59:23.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6951" for this suite. 01/30/23 23:59:23.308
------------------------------
• [SLOW TEST] [60.262 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:58:23.077
    Jan 30 23:58:23.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:58:23.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:58:23.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:58:23.196
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:59:23.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6951" for this suite. 01/30/23 23:59:23.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:59:23.341
Jan 30 23:59:23.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:59:23.344
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:59:23.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:59:23.441
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 30 23:59:23.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6745" for this suite. 01/30/23 23:59:23.522
------------------------------
• [0.242 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:59:23.341
    Jan 30 23:59:23.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename runtimeclass 01/30/23 23:59:23.344
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:59:23.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:59:23.441
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 30 23:59:23.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6745" for this suite. 01/30/23 23:59:23.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/30/23 23:59:23.596
Jan 30 23:59:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/30/23 23:59:23.598
STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:59:23.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:59:23.683
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a in namespace container-probe-1977 01/30/23 23:59:23.697
Jan 30 23:59:23.782: INFO: Waiting up to 5m0s for pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a" in namespace "container-probe-1977" to be "not pending"
Jan 30 23:59:23.806: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.366244ms
Jan 30 23:59:25.827: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043766923s
Jan 30 23:59:27.825: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.042436697s
Jan 30 23:59:27.825: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a" satisfied condition "not pending"
Jan 30 23:59:27.825: INFO: Started pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a in namespace container-probe-1977
STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:59:27.825
Jan 30 23:59:27.844: INFO: Initial restart count of pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a is 0
STEP: deleting the pod 01/31/23 00:03:28.352
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 31 00:03:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1977" for this suite. 01/31/23 00:03:28.443
------------------------------
• [SLOW TEST] [244.877 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/30/23 23:59:23.596
    Jan 30 23:59:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/30/23 23:59:23.598
    STEP: Waiting for a default service account to be provisioned in namespace 01/30/23 23:59:23.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/30/23 23:59:23.683
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a in namespace container-probe-1977 01/30/23 23:59:23.697
    Jan 30 23:59:23.782: INFO: Waiting up to 5m0s for pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a" in namespace "container-probe-1977" to be "not pending"
    Jan 30 23:59:23.806: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.366244ms
    Jan 30 23:59:25.827: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043766923s
    Jan 30 23:59:27.825: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.042436697s
    Jan 30 23:59:27.825: INFO: Pod "test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a" satisfied condition "not pending"
    Jan 30 23:59:27.825: INFO: Started pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a in namespace container-probe-1977
    STEP: checking the pod's current state and verifying that restartCount is present 01/30/23 23:59:27.825
    Jan 30 23:59:27.844: INFO: Initial restart count of pod test-webserver-15f9a89e-a037-4fc0-89f1-e20e0d5f1f3a is 0
    STEP: deleting the pod 01/31/23 00:03:28.352
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:03:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1977" for this suite. 01/31/23 00:03:28.443
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:03:28.474
Jan 31 00:03:28.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/31/23 00:03:28.477
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:03:28.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:03:28.592
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 31 00:03:28.711: INFO: created pod
Jan 31 00:03:28.711: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7473" to be "Succeeded or Failed"
Jan 31 00:03:28.732: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 21.129479ms
Jan 31 00:03:30.753: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041640097s
Jan 31 00:03:32.749: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037607712s
Jan 31 00:03:34.755: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043241015s
STEP: Saw pod success 01/31/23 00:03:34.755
Jan 31 00:03:34.756: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 31 00:04:04.759: INFO: polling logs
Jan 31 00:04:04.894: INFO: Pod logs: 
I0131 00:03:30.589254       1 log.go:198] OK: Got token
I0131 00:03:30.589308       1 log.go:198] validating with in-cluster discovery
I0131 00:03:30.589858       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0131 00:03:30.589899       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7473:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675124008, NotBefore:1675123408, IssuedAt:1675123408, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7473", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"809309b0-97a4-46ec-bcac-e2717168ab0b"}}}
I0131 00:03:30.617360       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0131 00:03:30.647827       1 log.go:198] OK: Validated signature on JWT
I0131 00:03:30.648275       1 log.go:198] OK: Got valid claims from token!
I0131 00:03:30.648459       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7473:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675124008, NotBefore:1675123408, IssuedAt:1675123408, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7473", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"809309b0-97a4-46ec-bcac-e2717168ab0b"}}}

Jan 31 00:04:04.894: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 31 00:04:04.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7473" for this suite. 01/31/23 00:04:04.981
------------------------------
• [SLOW TEST] [36.537 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:03:28.474
    Jan 31 00:03:28.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/31/23 00:03:28.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:03:28.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:03:28.592
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 31 00:03:28.711: INFO: created pod
    Jan 31 00:03:28.711: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7473" to be "Succeeded or Failed"
    Jan 31 00:03:28.732: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 21.129479ms
    Jan 31 00:03:30.753: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041640097s
    Jan 31 00:03:32.749: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037607712s
    Jan 31 00:03:34.755: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043241015s
    STEP: Saw pod success 01/31/23 00:03:34.755
    Jan 31 00:03:34.756: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 31 00:04:04.759: INFO: polling logs
    Jan 31 00:04:04.894: INFO: Pod logs: 
    I0131 00:03:30.589254       1 log.go:198] OK: Got token
    I0131 00:03:30.589308       1 log.go:198] validating with in-cluster discovery
    I0131 00:03:30.589858       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0131 00:03:30.589899       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7473:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675124008, NotBefore:1675123408, IssuedAt:1675123408, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7473", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"809309b0-97a4-46ec-bcac-e2717168ab0b"}}}
    I0131 00:03:30.617360       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0131 00:03:30.647827       1 log.go:198] OK: Validated signature on JWT
    I0131 00:03:30.648275       1 log.go:198] OK: Got valid claims from token!
    I0131 00:03:30.648459       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7473:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1675124008, NotBefore:1675123408, IssuedAt:1675123408, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7473", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"809309b0-97a4-46ec-bcac-e2717168ab0b"}}}

    Jan 31 00:04:04.894: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:04:04.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7473" for this suite. 01/31/23 00:04:04.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:04:05.012
Jan 31 00:04:05.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:04:05.015
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:05.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:05.099
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 31 00:04:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/31/23 00:04:07.698
Jan 31 00:04:07.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
Jan 31 00:04:08.633: INFO: stderr: ""
Jan 31 00:04:08.633: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 31 00:04:08.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 delete e2e-test-crd-publish-openapi-1809-crds test-foo'
Jan 31 00:04:08.872: INFO: stderr: ""
Jan 31 00:04:08.872: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 31 00:04:08.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
Jan 31 00:04:09.237: INFO: stderr: ""
Jan 31 00:04:09.237: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 31 00:04:09.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 delete e2e-test-crd-publish-openapi-1809-crds test-foo'
Jan 31 00:04:09.448: INFO: stderr: ""
Jan 31 00:04:09.448: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/31/23 00:04:09.448
Jan 31 00:04:09.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
Jan 31 00:04:10.225: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/31/23 00:04:10.225
Jan 31 00:04:10.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
Jan 31 00:04:11.001: INFO: rc: 1
Jan 31 00:04:11.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
Jan 31 00:04:11.391: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/31/23 00:04:11.391
Jan 31 00:04:11.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
Jan 31 00:04:11.775: INFO: rc: 1
Jan 31 00:04:11.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
Jan 31 00:04:12.131: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/31/23 00:04:12.131
Jan 31 00:04:12.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds'
Jan 31 00:04:12.460: INFO: stderr: ""
Jan 31 00:04:12.460: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/31/23 00:04:12.46
Jan 31 00:04:12.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.metadata'
Jan 31 00:04:12.775: INFO: stderr: ""
Jan 31 00:04:12.775: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 31 00:04:12.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec'
Jan 31 00:04:13.108: INFO: stderr: ""
Jan 31 00:04:13.108: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 31 00:04:13.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec.bars'
Jan 31 00:04:13.392: INFO: stderr: ""
Jan 31 00:04:13.392: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/31/23 00:04:13.392
Jan 31 00:04:13.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec.bars2'
Jan 31 00:04:13.701: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:04:16.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1252" for this suite. 01/31/23 00:04:16.211
------------------------------
• [SLOW TEST] [11.223 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:04:05.012
    Jan 31 00:04:05.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:04:05.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:05.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:05.099
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 31 00:04:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/31/23 00:04:07.698
    Jan 31 00:04:07.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
    Jan 31 00:04:08.633: INFO: stderr: ""
    Jan 31 00:04:08.633: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 31 00:04:08.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 delete e2e-test-crd-publish-openapi-1809-crds test-foo'
    Jan 31 00:04:08.872: INFO: stderr: ""
    Jan 31 00:04:08.872: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 31 00:04:08.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
    Jan 31 00:04:09.237: INFO: stderr: ""
    Jan 31 00:04:09.237: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 31 00:04:09.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 delete e2e-test-crd-publish-openapi-1809-crds test-foo'
    Jan 31 00:04:09.448: INFO: stderr: ""
    Jan 31 00:04:09.448: INFO: stdout: "e2e-test-crd-publish-openapi-1809-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/31/23 00:04:09.448
    Jan 31 00:04:09.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
    Jan 31 00:04:10.225: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/31/23 00:04:10.225
    Jan 31 00:04:10.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
    Jan 31 00:04:11.001: INFO: rc: 1
    Jan 31 00:04:11.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
    Jan 31 00:04:11.391: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/31/23 00:04:11.391
    Jan 31 00:04:11.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 create -f -'
    Jan 31 00:04:11.775: INFO: rc: 1
    Jan 31 00:04:11.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 --namespace=crd-publish-openapi-1252 apply -f -'
    Jan 31 00:04:12.131: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/31/23 00:04:12.131
    Jan 31 00:04:12.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds'
    Jan 31 00:04:12.460: INFO: stderr: ""
    Jan 31 00:04:12.460: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/31/23 00:04:12.46
    Jan 31 00:04:12.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.metadata'
    Jan 31 00:04:12.775: INFO: stderr: ""
    Jan 31 00:04:12.775: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 31 00:04:12.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec'
    Jan 31 00:04:13.108: INFO: stderr: ""
    Jan 31 00:04:13.108: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 31 00:04:13.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec.bars'
    Jan 31 00:04:13.392: INFO: stderr: ""
    Jan 31 00:04:13.392: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1809-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/31/23 00:04:13.392
    Jan 31 00:04:13.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-1252 explain e2e-test-crd-publish-openapi-1809-crds.spec.bars2'
    Jan 31 00:04:13.701: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:04:16.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1252" for this suite. 01/31/23 00:04:16.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:04:16.239
Jan 31 00:04:16.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/31/23 00:04:16.242
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:16.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:16.346
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/31/23 00:04:16.365
Jan 31 00:04:16.413: INFO: Waiting up to 5m0s for pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0" in namespace "downward-api-483" to be "running and ready"
Jan 31 00:04:16.439: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.259007ms
Jan 31 00:04:16.439: INFO: The phase of Pod labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:04:18.459: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0": Phase="Running", Reason="", readiness=true. Elapsed: 2.046299783s
Jan 31 00:04:18.459: INFO: The phase of Pod labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0 is Running (Ready = true)
Jan 31 00:04:18.459: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0" satisfied condition "running and ready"
Jan 31 00:04:19.103: INFO: Successfully updated pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 31 00:04:21.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-483" for this suite. 01/31/23 00:04:21.228
------------------------------
• [SLOW TEST] [5.016 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:04:16.239
    Jan 31 00:04:16.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/31/23 00:04:16.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:16.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:16.346
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/31/23 00:04:16.365
    Jan 31 00:04:16.413: INFO: Waiting up to 5m0s for pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0" in namespace "downward-api-483" to be "running and ready"
    Jan 31 00:04:16.439: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.259007ms
    Jan 31 00:04:16.439: INFO: The phase of Pod labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:04:18.459: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0": Phase="Running", Reason="", readiness=true. Elapsed: 2.046299783s
    Jan 31 00:04:18.459: INFO: The phase of Pod labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0 is Running (Ready = true)
    Jan 31 00:04:18.459: INFO: Pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0" satisfied condition "running and ready"
    Jan 31 00:04:19.103: INFO: Successfully updated pod "labelsupdate63675f4b-cf6e-43ef-b36a-92eed654dce0"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:04:21.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-483" for this suite. 01/31/23 00:04:21.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:04:21.262
Jan 31 00:04:21.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/31/23 00:04:21.266
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:21.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:21.332
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/31/23 00:04:21.344
Jan 31 00:04:21.384: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58" in namespace "downward-api-4293" to be "Succeeded or Failed"
Jan 31 00:04:21.401: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 16.998172ms
Jan 31 00:04:23.423: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039055408s
Jan 31 00:04:25.417: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032839146s
Jan 31 00:04:27.419: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035351404s
STEP: Saw pod success 01/31/23 00:04:27.419
Jan 31 00:04:27.420: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58" satisfied condition "Succeeded or Failed"
Jan 31 00:04:27.437: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 container client-container: <nil>
STEP: delete the pod 01/31/23 00:04:27.499
Jan 31 00:04:27.548: INFO: Waiting for pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 to disappear
Jan 31 00:04:27.563: INFO: Pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 31 00:04:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4293" for this suite. 01/31/23 00:04:27.637
------------------------------
• [SLOW TEST] [6.428 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:04:21.262
    Jan 31 00:04:21.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/31/23 00:04:21.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:21.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:21.332
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/31/23 00:04:21.344
    Jan 31 00:04:21.384: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58" in namespace "downward-api-4293" to be "Succeeded or Failed"
    Jan 31 00:04:21.401: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 16.998172ms
    Jan 31 00:04:23.423: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039055408s
    Jan 31 00:04:25.417: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032839146s
    Jan 31 00:04:27.419: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035351404s
    STEP: Saw pod success 01/31/23 00:04:27.419
    Jan 31 00:04:27.420: INFO: Pod "downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58" satisfied condition "Succeeded or Failed"
    Jan 31 00:04:27.437: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 container client-container: <nil>
    STEP: delete the pod 01/31/23 00:04:27.499
    Jan 31 00:04:27.548: INFO: Waiting for pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 to disappear
    Jan 31 00:04:27.563: INFO: Pod downwardapi-volume-0700e71a-38f7-4927-822f-48a2c762df58 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:04:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4293" for this suite. 01/31/23 00:04:27.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:04:27.696
Jan 31 00:04:27.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:04:27.699
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:27.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:27.814
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:04:27.907
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:04:28.269
STEP: Deploying the webhook pod 01/31/23 00:04:28.327
STEP: Wait for the deployment to be ready 01/31/23 00:04:28.394
Jan 31 00:04:28.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:04:30.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:04:32.581
STEP: Verifying the service has paired with the endpoint 01/31/23 00:04:32.644
Jan 31 00:04:33.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/31/23 00:04:33.662
STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:33.662
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/31/23 00:04:33.791
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/31/23 00:04:34.828
STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:34.828
STEP: Having no error when timeout is longer than webhook latency 01/31/23 00:04:35.981
STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:35.982
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/31/23 00:04:41.169
STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:41.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:04:46.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1145" for this suite. 01/31/23 00:04:46.56
STEP: Destroying namespace "webhook-1145-markers" for this suite. 01/31/23 00:04:46.589
------------------------------
• [SLOW TEST] [18.923 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:04:27.696
    Jan 31 00:04:27.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:04:27.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:27.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:27.814
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:04:27.907
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:04:28.269
    STEP: Deploying the webhook pod 01/31/23 00:04:28.327
    STEP: Wait for the deployment to be ready 01/31/23 00:04:28.394
    Jan 31 00:04:28.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:04:30.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:04:32.581
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:04:32.644
    Jan 31 00:04:33.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/31/23 00:04:33.662
    STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:33.662
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/31/23 00:04:33.791
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/31/23 00:04:34.828
    STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:34.828
    STEP: Having no error when timeout is longer than webhook latency 01/31/23 00:04:35.981
    STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:35.982
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/31/23 00:04:41.169
    STEP: Registering slow webhook via the AdmissionRegistration API 01/31/23 00:04:41.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:04:46.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1145" for this suite. 01/31/23 00:04:46.56
    STEP: Destroying namespace "webhook-1145-markers" for this suite. 01/31/23 00:04:46.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:04:46.637
Jan 31 00:04:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/31/23 00:04:46.638
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:46.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:46.701
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-m44w7" 01/31/23 00:04:46.763
Jan 31 00:04:46.802: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard cpu limit of 500m
Jan 31 00:04:46.802: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.802
STEP: Confirm /status for "e2e-rq-status-m44w7" resourceQuota via watch 01/31/23 00:04:46.843
Jan 31 00:04:46.853: INFO: observed resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList(nil)
Jan 31 00:04:46.853: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 31 00:04:46.853: INFO: ResourceQuota "e2e-rq-status-m44w7" /status was updated
STEP: Patching hard spec values for cpu & memory 01/31/23 00:04:46.876
Jan 31 00:04:46.904: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard cpu limit of 1
Jan 31 00:04:46.904: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.904
STEP: Confirm /status for "e2e-rq-status-m44w7" resourceQuota via watch 01/31/23 00:04:46.932
Jan 31 00:04:46.944: INFO: observed resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 31 00:04:46.944: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 31 00:04:46.944: INFO: ResourceQuota "e2e-rq-status-m44w7" /status was patched
STEP: Get "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.944
Jan 31 00:04:46.964: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard cpu of 1
Jan 31 00:04:46.964: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-m44w7" /status before checking Spec is unchanged 01/31/23 00:04:46.981
Jan 31 00:04:47.004: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard cpu of 2
Jan 31 00:04:47.004: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard memory of 2Gi
Jan 31 00:04:47.010: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 31 00:07:32.069: INFO: ResourceQuota "e2e-rq-status-m44w7" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 31 00:07:32.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6370" for this suite. 01/31/23 00:07:32.092
------------------------------
• [SLOW TEST] [165.483 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:04:46.637
    Jan 31 00:04:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/31/23 00:04:46.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:04:46.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:04:46.701
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-m44w7" 01/31/23 00:04:46.763
    Jan 31 00:04:46.802: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard cpu limit of 500m
    Jan 31 00:04:46.802: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.802
    STEP: Confirm /status for "e2e-rq-status-m44w7" resourceQuota via watch 01/31/23 00:04:46.843
    Jan 31 00:04:46.853: INFO: observed resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList(nil)
    Jan 31 00:04:46.853: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 31 00:04:46.853: INFO: ResourceQuota "e2e-rq-status-m44w7" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/31/23 00:04:46.876
    Jan 31 00:04:46.904: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard cpu limit of 1
    Jan 31 00:04:46.904: INFO: Resource quota "e2e-rq-status-m44w7" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.904
    STEP: Confirm /status for "e2e-rq-status-m44w7" resourceQuota via watch 01/31/23 00:04:46.932
    Jan 31 00:04:46.944: INFO: observed resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 31 00:04:46.944: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 31 00:04:46.944: INFO: ResourceQuota "e2e-rq-status-m44w7" /status was patched
    STEP: Get "e2e-rq-status-m44w7" /status 01/31/23 00:04:46.944
    Jan 31 00:04:46.964: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard cpu of 1
    Jan 31 00:04:46.964: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-m44w7" /status before checking Spec is unchanged 01/31/23 00:04:46.981
    Jan 31 00:04:47.004: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard cpu of 2
    Jan 31 00:04:47.004: INFO: Resourcequota "e2e-rq-status-m44w7" reports status: hard memory of 2Gi
    Jan 31 00:04:47.010: INFO: Found resourceQuota "e2e-rq-status-m44w7" in namespace "resourcequota-6370" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 31 00:07:32.069: INFO: ResourceQuota "e2e-rq-status-m44w7" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:07:32.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6370" for this suite. 01/31/23 00:07:32.092
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:07:32.125
Jan 31 00:07:32.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/31/23 00:07:32.128
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:32.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:32.204
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/31/23 00:07:32.218
Jan 31 00:07:32.219: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-563 proxy --unix-socket=/tmp/kubectl-proxy-unix3532828529/test'
STEP: retrieving proxy /api/ output 01/31/23 00:07:32.297
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 31 00:07:32.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-563" for this suite. 01/31/23 00:07:32.32
------------------------------
• [0.222 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:07:32.125
    Jan 31 00:07:32.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/31/23 00:07:32.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:32.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:32.204
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/31/23 00:07:32.218
    Jan 31 00:07:32.219: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-563 proxy --unix-socket=/tmp/kubectl-proxy-unix3532828529/test'
    STEP: retrieving proxy /api/ output 01/31/23 00:07:32.297
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:07:32.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-563" for this suite. 01/31/23 00:07:32.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:07:32.35
Jan 31 00:07:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:07:32.352
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:32.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:32.446
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:07:32.513
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:07:33.12
STEP: Deploying the webhook pod 01/31/23 00:07:33.157
STEP: Wait for the deployment to be ready 01/31/23 00:07:33.224
Jan 31 00:07:33.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:07:35.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:07:37.333
STEP: Verifying the service has paired with the endpoint 01/31/23 00:07:37.381
Jan 31 00:07:38.382: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 31 00:07:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-993-crds.webhook.example.com via the AdmissionRegistration API 01/31/23 00:07:38.962
STEP: Creating a custom resource while v1 is storage version 01/31/23 00:07:39.053
STEP: Patching Custom Resource Definition to set v2 as storage 01/31/23 00:07:41.283
STEP: Patching the custom resource while v2 is storage version 01/31/23 00:07:41.337
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:07:42.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9619" for this suite. 01/31/23 00:07:42.276
STEP: Destroying namespace "webhook-9619-markers" for this suite. 01/31/23 00:07:42.321
------------------------------
• [SLOW TEST] [9.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:07:32.35
    Jan 31 00:07:32.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:07:32.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:32.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:32.446
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:07:32.513
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:07:33.12
    STEP: Deploying the webhook pod 01/31/23 00:07:33.157
    STEP: Wait for the deployment to be ready 01/31/23 00:07:33.224
    Jan 31 00:07:33.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:07:35.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 7, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:07:37.333
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:07:37.381
    Jan 31 00:07:38.382: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 31 00:07:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-993-crds.webhook.example.com via the AdmissionRegistration API 01/31/23 00:07:38.962
    STEP: Creating a custom resource while v1 is storage version 01/31/23 00:07:39.053
    STEP: Patching Custom Resource Definition to set v2 as storage 01/31/23 00:07:41.283
    STEP: Patching the custom resource while v2 is storage version 01/31/23 00:07:41.337
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:07:42.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9619" for this suite. 01/31/23 00:07:42.276
    STEP: Destroying namespace "webhook-9619-markers" for this suite. 01/31/23 00:07:42.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:07:42.356
Jan 31 00:07:42.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename namespaces 01/31/23 00:07:42.358
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:42.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:42.466
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/31/23 00:07:42.477
STEP: patching the Namespace 01/31/23 00:07:42.593
STEP: get the Namespace and ensuring it has the label 01/31/23 00:07:42.62
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:07:42.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8216" for this suite. 01/31/23 00:07:42.667
STEP: Destroying namespace "nspatchtest-b969583d-b25e-40e0-841b-d9cdbc3edb2c-7499" for this suite. 01/31/23 00:07:42.693
------------------------------
• [0.370 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:07:42.356
    Jan 31 00:07:42.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename namespaces 01/31/23 00:07:42.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:42.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:42.466
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/31/23 00:07:42.477
    STEP: patching the Namespace 01/31/23 00:07:42.593
    STEP: get the Namespace and ensuring it has the label 01/31/23 00:07:42.62
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:07:42.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8216" for this suite. 01/31/23 00:07:42.667
    STEP: Destroying namespace "nspatchtest-b969583d-b25e-40e0-841b-d9cdbc3edb2c-7499" for this suite. 01/31/23 00:07:42.693
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:07:42.732
Jan 31 00:07:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context 01/31/23 00:07:42.735
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:42.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:42.814
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/31/23 00:07:42.831
Jan 31 00:07:42.868: INFO: Waiting up to 5m0s for pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b" in namespace "security-context-5528" to be "Succeeded or Failed"
Jan 31 00:07:42.908: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 39.750781ms
Jan 31 00:07:44.930: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062195337s
Jan 31 00:07:46.926: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057998724s
Jan 31 00:07:48.928: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060369973s
STEP: Saw pod success 01/31/23 00:07:48.928
Jan 31 00:07:48.929: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b" satisfied condition "Succeeded or Failed"
Jan 31 00:07:48.947: INFO: Trying to get logs from node 10.15.28.227 pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b container test-container: <nil>
STEP: delete the pod 01/31/23 00:07:49.029
Jan 31 00:07:49.069: INFO: Waiting for pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b to disappear
Jan 31 00:07:49.088: INFO: Pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 31 00:07:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5528" for this suite. 01/31/23 00:07:49.11
------------------------------
• [SLOW TEST] [6.410 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:07:42.732
    Jan 31 00:07:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context 01/31/23 00:07:42.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:42.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:42.814
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/31/23 00:07:42.831
    Jan 31 00:07:42.868: INFO: Waiting up to 5m0s for pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b" in namespace "security-context-5528" to be "Succeeded or Failed"
    Jan 31 00:07:42.908: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 39.750781ms
    Jan 31 00:07:44.930: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062195337s
    Jan 31 00:07:46.926: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057998724s
    Jan 31 00:07:48.928: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060369973s
    STEP: Saw pod success 01/31/23 00:07:48.928
    Jan 31 00:07:48.929: INFO: Pod "security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b" satisfied condition "Succeeded or Failed"
    Jan 31 00:07:48.947: INFO: Trying to get logs from node 10.15.28.227 pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b container test-container: <nil>
    STEP: delete the pod 01/31/23 00:07:49.029
    Jan 31 00:07:49.069: INFO: Waiting for pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b to disappear
    Jan 31 00:07:49.088: INFO: Pod security-context-e1ad4c08-3c70-4b46-8b73-d4979aa1a56b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:07:49.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5528" for this suite. 01/31/23 00:07:49.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:07:49.15
Jan 31 00:07:49.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pod-network-test 01/31/23 00:07:49.152
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:49.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:49.241
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8200 01/31/23 00:07:49.256
STEP: creating a selector 01/31/23 00:07:49.257
STEP: Creating the service pods in kubernetes 01/31/23 00:07:49.257
Jan 31 00:07:49.258: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 00:07:49.386: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8200" to be "running and ready"
Jan 31 00:07:49.426: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.002556ms
Jan 31 00:07:49.426: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:07:51.446: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060066968s
Jan 31 00:07:51.447: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:07:53.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.061386418s
Jan 31 00:07:53.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:07:55.445: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.058242186s
Jan 31 00:07:55.445: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:07:57.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.057307196s
Jan 31 00:07:57.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:07:59.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.059877645s
Jan 31 00:07:59.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:01.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.05801676s
Jan 31 00:08:01.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:03.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.070632515s
Jan 31 00:08:03.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:05.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.057656302s
Jan 31 00:08:05.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:07.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.057909466s
Jan 31 00:08:07.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:09.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.057386645s
Jan 31 00:08:09.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:08:11.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.060865101s
Jan 31 00:08:11.447: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 31 00:08:11.447: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 31 00:08:11.465: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8200" to be "running and ready"
Jan 31 00:08:11.483: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.231706ms
Jan 31 00:08:11.483: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 31 00:08:11.483: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 31 00:08:11.498: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8200" to be "running and ready"
Jan 31 00:08:11.515: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 16.836185ms
Jan 31 00:08:11.515: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 31 00:08:11.515: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/31/23 00:08:11.531
Jan 31 00:08:11.553: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8200" to be "running"
Jan 31 00:08:11.575: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.947054ms
Jan 31 00:08:13.627: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.074489903s
Jan 31 00:08:13.627: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 31 00:08:13.644: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 31 00:08:13.644: INFO: Breadth first check of 172.30.237.144 on host 10.15.28.225...
Jan 31 00:08:13.661: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.237.144&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:08:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:08:13.663: INFO: ExecWithOptions: Clientset creation
Jan 31 00:08:13.663: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.237.144%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 00:08:13.966: INFO: Waiting for responses: map[]
Jan 31 00:08:13.967: INFO: reached 172.30.237.144 after 0/1 tries
Jan 31 00:08:13.967: INFO: Breadth first check of 172.30.199.26 on host 10.15.28.227...
Jan 31 00:08:13.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.199.26&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:08:13.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:08:13.991: INFO: ExecWithOptions: Clientset creation
Jan 31 00:08:13.991: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.199.26%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 00:08:14.239: INFO: Waiting for responses: map[]
Jan 31 00:08:14.239: INFO: reached 172.30.199.26 after 0/1 tries
Jan 31 00:08:14.240: INFO: Breadth first check of 172.30.248.20 on host 10.15.28.237...
Jan 31 00:08:14.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.248.20&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:08:14.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:08:14.261: INFO: ExecWithOptions: Clientset creation
Jan 31 00:08:14.261: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.248.20%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 31 00:08:14.536: INFO: Waiting for responses: map[]
Jan 31 00:08:14.536: INFO: reached 172.30.248.20 after 0/1 tries
Jan 31 00:08:14.536: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 31 00:08:14.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8200" for this suite. 01/31/23 00:08:14.569
------------------------------
• [SLOW TEST] [25.446 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:07:49.15
    Jan 31 00:07:49.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pod-network-test 01/31/23 00:07:49.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:07:49.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:07:49.241
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8200 01/31/23 00:07:49.256
    STEP: creating a selector 01/31/23 00:07:49.257
    STEP: Creating the service pods in kubernetes 01/31/23 00:07:49.257
    Jan 31 00:07:49.258: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 31 00:07:49.386: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8200" to be "running and ready"
    Jan 31 00:07:49.426: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.002556ms
    Jan 31 00:07:49.426: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:07:51.446: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060066968s
    Jan 31 00:07:51.447: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:07:53.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.061386418s
    Jan 31 00:07:53.448: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:07:55.445: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.058242186s
    Jan 31 00:07:55.445: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:07:57.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.057307196s
    Jan 31 00:07:57.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:07:59.446: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.059877645s
    Jan 31 00:07:59.446: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:01.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.05801676s
    Jan 31 00:08:01.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:03.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.070632515s
    Jan 31 00:08:03.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:05.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.057656302s
    Jan 31 00:08:05.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:07.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.057909466s
    Jan 31 00:08:07.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:09.444: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.057386645s
    Jan 31 00:08:09.444: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:08:11.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.060865101s
    Jan 31 00:08:11.447: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 31 00:08:11.447: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 31 00:08:11.465: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8200" to be "running and ready"
    Jan 31 00:08:11.483: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.231706ms
    Jan 31 00:08:11.483: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 31 00:08:11.483: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 31 00:08:11.498: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8200" to be "running and ready"
    Jan 31 00:08:11.515: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 16.836185ms
    Jan 31 00:08:11.515: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 31 00:08:11.515: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/31/23 00:08:11.531
    Jan 31 00:08:11.553: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8200" to be "running"
    Jan 31 00:08:11.575: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.947054ms
    Jan 31 00:08:13.627: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.074489903s
    Jan 31 00:08:13.627: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 31 00:08:13.644: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 31 00:08:13.644: INFO: Breadth first check of 172.30.237.144 on host 10.15.28.225...
    Jan 31 00:08:13.661: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.237.144&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:08:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:08:13.663: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:08:13.663: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.237.144%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 31 00:08:13.966: INFO: Waiting for responses: map[]
    Jan 31 00:08:13.967: INFO: reached 172.30.237.144 after 0/1 tries
    Jan 31 00:08:13.967: INFO: Breadth first check of 172.30.199.26 on host 10.15.28.227...
    Jan 31 00:08:13.990: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.199.26&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:08:13.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:08:13.991: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:08:13.991: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.199.26%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 31 00:08:14.239: INFO: Waiting for responses: map[]
    Jan 31 00:08:14.239: INFO: reached 172.30.199.26 after 0/1 tries
    Jan 31 00:08:14.240: INFO: Breadth first check of 172.30.248.20 on host 10.15.28.237...
    Jan 31 00:08:14.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.199.48:9080/dial?request=hostname&protocol=udp&host=172.30.248.20&port=8081&tries=1'] Namespace:pod-network-test-8200 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:08:14.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:08:14.261: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:08:14.261: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8200/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.199.48%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.248.20%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 31 00:08:14.536: INFO: Waiting for responses: map[]
    Jan 31 00:08:14.536: INFO: reached 172.30.248.20 after 0/1 tries
    Jan 31 00:08:14.536: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:08:14.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8200" for this suite. 01/31/23 00:08:14.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:08:14.601
Jan 31 00:08:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/31/23 00:08:14.603
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:14.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:14.701
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2703 01/31/23 00:08:14.718
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-2703 01/31/23 00:08:14.758
Jan 31 00:08:14.795: INFO: Found 0 stateful pods, waiting for 1
Jan 31 00:08:24.819: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/31/23 00:08:24.857
STEP: Getting /status 01/31/23 00:08:24.883
Jan 31 00:08:24.903: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/31/23 00:08:24.904
Jan 31 00:08:24.952: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/31/23 00:08:24.952
Jan 31 00:08:24.961: INFO: Observed &StatefulSet event: ADDED
Jan 31 00:08:24.961: INFO: Found Statefulset ss in namespace statefulset-2703 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 00:08:24.961: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/31/23 00:08:24.961
Jan 31 00:08:24.961: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 31 00:08:25.012: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/31/23 00:08:25.012
Jan 31 00:08:25.023: INFO: Observed &StatefulSet event: ADDED
Jan 31 00:08:25.023: INFO: Observed Statefulset ss in namespace statefulset-2703 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 31 00:08:25.024: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 31 00:08:25.025: INFO: Deleting all statefulset in ns statefulset-2703
Jan 31 00:08:25.044: INFO: Scaling statefulset ss to 0
Jan 31 00:08:35.131: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 00:08:35.148: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:08:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2703" for this suite. 01/31/23 00:08:35.237
------------------------------
• [SLOW TEST] [20.664 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:08:14.601
    Jan 31 00:08:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/31/23 00:08:14.603
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:14.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:14.701
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2703 01/31/23 00:08:14.718
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-2703 01/31/23 00:08:14.758
    Jan 31 00:08:14.795: INFO: Found 0 stateful pods, waiting for 1
    Jan 31 00:08:24.819: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/31/23 00:08:24.857
    STEP: Getting /status 01/31/23 00:08:24.883
    Jan 31 00:08:24.903: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/31/23 00:08:24.904
    Jan 31 00:08:24.952: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/31/23 00:08:24.952
    Jan 31 00:08:24.961: INFO: Observed &StatefulSet event: ADDED
    Jan 31 00:08:24.961: INFO: Found Statefulset ss in namespace statefulset-2703 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 31 00:08:24.961: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/31/23 00:08:24.961
    Jan 31 00:08:24.961: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 31 00:08:25.012: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/31/23 00:08:25.012
    Jan 31 00:08:25.023: INFO: Observed &StatefulSet event: ADDED
    Jan 31 00:08:25.023: INFO: Observed Statefulset ss in namespace statefulset-2703 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 31 00:08:25.024: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 31 00:08:25.025: INFO: Deleting all statefulset in ns statefulset-2703
    Jan 31 00:08:25.044: INFO: Scaling statefulset ss to 0
    Jan 31 00:08:35.131: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 31 00:08:35.148: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:08:35.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2703" for this suite. 01/31/23 00:08:35.237
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:08:35.267
Jan 31 00:08:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/31/23 00:08:35.27
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:35.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:35.366
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 31 00:08:35.425: INFO: Waiting up to 5m0s for pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012" in namespace "pods-5784" to be "running and ready"
Jan 31 00:08:35.448: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Pending", Reason="", readiness=false. Elapsed: 22.094669ms
Jan 31 00:08:35.448: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:08:37.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042016622s
Jan 31 00:08:37.467: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:08:39.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Running", Reason="", readiness=true. Elapsed: 4.041282662s
Jan 31 00:08:39.467: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Running (Ready = true)
Jan 31 00:08:39.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012" satisfied condition "running and ready"
Jan 31 00:08:39.605: INFO: Waiting up to 5m0s for pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e" in namespace "pods-5784" to be "Succeeded or Failed"
Jan 31 00:08:39.622: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.337871ms
Jan 31 00:08:41.641: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036434881s
Jan 31 00:08:43.643: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038484935s
Jan 31 00:08:45.644: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039155352s
STEP: Saw pod success 01/31/23 00:08:45.644
Jan 31 00:08:45.645: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e" satisfied condition "Succeeded or Failed"
Jan 31 00:08:45.668: INFO: Trying to get logs from node 10.15.28.227 pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e container env3cont: <nil>
STEP: delete the pod 01/31/23 00:08:45.71
Jan 31 00:08:45.783: INFO: Waiting for pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e to disappear
Jan 31 00:08:45.801: INFO: Pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 31 00:08:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5784" for this suite. 01/31/23 00:08:45.824
------------------------------
• [SLOW TEST] [10.590 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:08:35.267
    Jan 31 00:08:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/31/23 00:08:35.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:35.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:35.366
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 31 00:08:35.425: INFO: Waiting up to 5m0s for pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012" in namespace "pods-5784" to be "running and ready"
    Jan 31 00:08:35.448: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Pending", Reason="", readiness=false. Elapsed: 22.094669ms
    Jan 31 00:08:35.448: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:08:37.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042016622s
    Jan 31 00:08:37.467: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:08:39.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012": Phase="Running", Reason="", readiness=true. Elapsed: 4.041282662s
    Jan 31 00:08:39.467: INFO: The phase of Pod server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012 is Running (Ready = true)
    Jan 31 00:08:39.467: INFO: Pod "server-envvars-ec762975-b64d-4b33-9f0a-4b9576d50012" satisfied condition "running and ready"
    Jan 31 00:08:39.605: INFO: Waiting up to 5m0s for pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e" in namespace "pods-5784" to be "Succeeded or Failed"
    Jan 31 00:08:39.622: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.337871ms
    Jan 31 00:08:41.641: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036434881s
    Jan 31 00:08:43.643: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038484935s
    Jan 31 00:08:45.644: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039155352s
    STEP: Saw pod success 01/31/23 00:08:45.644
    Jan 31 00:08:45.645: INFO: Pod "client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e" satisfied condition "Succeeded or Failed"
    Jan 31 00:08:45.668: INFO: Trying to get logs from node 10.15.28.227 pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e container env3cont: <nil>
    STEP: delete the pod 01/31/23 00:08:45.71
    Jan 31 00:08:45.783: INFO: Waiting for pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e to disappear
    Jan 31 00:08:45.801: INFO: Pod client-envvars-efe2b32a-27c5-447b-b1bf-71d7f395c23e no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:08:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5784" for this suite. 01/31/23 00:08:45.824
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:08:45.86
Jan 31 00:08:45.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/31/23 00:08:45.862
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:45.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:45.931
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5340 01/31/23 00:08:45.948
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/31/23 00:08:46.016
STEP: creating service externalsvc in namespace services-5340 01/31/23 00:08:46.017
STEP: creating replication controller externalsvc in namespace services-5340 01/31/23 00:08:46.066
I0131 00:08:46.089229      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5340, replica count: 2
I0131 00:08:49.141428      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/31/23 00:08:49.159
Jan 31 00:08:49.244: INFO: Creating new exec pod
Jan 31 00:08:49.267: INFO: Waiting up to 5m0s for pod "execpod2q9hj" in namespace "services-5340" to be "running"
Jan 31 00:08:49.286: INFO: Pod "execpod2q9hj": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800069ms
Jan 31 00:08:51.320: INFO: Pod "execpod2q9hj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052787058s
Jan 31 00:08:53.306: INFO: Pod "execpod2q9hj": Phase="Running", Reason="", readiness=true. Elapsed: 4.038727904s
Jan 31 00:08:53.306: INFO: Pod "execpod2q9hj" satisfied condition "running"
Jan 31 00:08:53.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5340 exec execpod2q9hj -- /bin/sh -x -c nslookup nodeport-service.services-5340.svc.cluster.local'
Jan 31 00:08:53.740: INFO: stderr: "+ nslookup nodeport-service.services-5340.svc.cluster.local\n"
Jan 31 00:08:53.740: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5340.svc.cluster.local\tcanonical name = externalsvc.services-5340.svc.cluster.local.\nName:\texternalsvc.services-5340.svc.cluster.local\nAddress: 172.21.221.115\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5340, will wait for the garbage collector to delete the pods 01/31/23 00:08:53.74
Jan 31 00:08:53.847: INFO: Deleting ReplicationController externalsvc took: 35.317358ms
Jan 31 00:08:53.947: INFO: Terminating ReplicationController externalsvc pods took: 100.540463ms
Jan 31 00:08:57.024: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 31 00:08:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5340" for this suite. 01/31/23 00:08:57.097
------------------------------
• [SLOW TEST] [11.264 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:08:45.86
    Jan 31 00:08:45.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/31/23 00:08:45.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:45.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:45.931
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5340 01/31/23 00:08:45.948
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/31/23 00:08:46.016
    STEP: creating service externalsvc in namespace services-5340 01/31/23 00:08:46.017
    STEP: creating replication controller externalsvc in namespace services-5340 01/31/23 00:08:46.066
    I0131 00:08:46.089229      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5340, replica count: 2
    I0131 00:08:49.141428      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/31/23 00:08:49.159
    Jan 31 00:08:49.244: INFO: Creating new exec pod
    Jan 31 00:08:49.267: INFO: Waiting up to 5m0s for pod "execpod2q9hj" in namespace "services-5340" to be "running"
    Jan 31 00:08:49.286: INFO: Pod "execpod2q9hj": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800069ms
    Jan 31 00:08:51.320: INFO: Pod "execpod2q9hj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052787058s
    Jan 31 00:08:53.306: INFO: Pod "execpod2q9hj": Phase="Running", Reason="", readiness=true. Elapsed: 4.038727904s
    Jan 31 00:08:53.306: INFO: Pod "execpod2q9hj" satisfied condition "running"
    Jan 31 00:08:53.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5340 exec execpod2q9hj -- /bin/sh -x -c nslookup nodeport-service.services-5340.svc.cluster.local'
    Jan 31 00:08:53.740: INFO: stderr: "+ nslookup nodeport-service.services-5340.svc.cluster.local\n"
    Jan 31 00:08:53.740: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5340.svc.cluster.local\tcanonical name = externalsvc.services-5340.svc.cluster.local.\nName:\texternalsvc.services-5340.svc.cluster.local\nAddress: 172.21.221.115\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5340, will wait for the garbage collector to delete the pods 01/31/23 00:08:53.74
    Jan 31 00:08:53.847: INFO: Deleting ReplicationController externalsvc took: 35.317358ms
    Jan 31 00:08:53.947: INFO: Terminating ReplicationController externalsvc pods took: 100.540463ms
    Jan 31 00:08:57.024: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:08:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5340" for this suite. 01/31/23 00:08:57.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:08:57.128
Jan 31 00:08:57.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/31/23 00:08:57.129
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:57.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:57.217
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 31 00:08:57.273: INFO: Waiting up to 2m0s for pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" in namespace "var-expansion-767" to be "container 0 failed with reason CreateContainerConfigError"
Jan 31 00:08:57.292: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.639936ms
Jan 31 00:08:59.313: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039853493s
Jan 31 00:08:59.313: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 31 00:08:59.314: INFO: Deleting pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" in namespace "var-expansion-767"
Jan 31 00:08:59.343: INFO: Wait up to 5m0s for pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:03.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-767" for this suite. 01/31/23 00:09:03.408
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:08:57.128
    Jan 31 00:08:57.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/31/23 00:08:57.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:08:57.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:08:57.217
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 31 00:08:57.273: INFO: Waiting up to 2m0s for pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" in namespace "var-expansion-767" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 31 00:08:57.292: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.639936ms
    Jan 31 00:08:59.313: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039853493s
    Jan 31 00:08:59.313: INFO: Pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 31 00:08:59.314: INFO: Deleting pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" in namespace "var-expansion-767"
    Jan 31 00:08:59.343: INFO: Wait up to 5m0s for pod "var-expansion-749394e7-69d7-45b7-a801-d8a9e402e8e3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:03.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-767" for this suite. 01/31/23 00:09:03.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:03.446
Jan 31 00:09:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename proxy 01/31/23 00:09:03.449
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:03.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:03.516
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 31 00:09:03.532: INFO: Creating pod...
Jan 31 00:09:03.573: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3724" to be "running"
Jan 31 00:09:03.591: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.535409ms
Jan 31 00:09:05.608: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035099426s
Jan 31 00:09:07.612: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.039618031s
Jan 31 00:09:07.612: INFO: Pod "agnhost" satisfied condition "running"
Jan 31 00:09:07.613: INFO: Creating service...
Jan 31 00:09:07.665: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/DELETE
Jan 31 00:09:07.774: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 00:09:07.775: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/GET
Jan 31 00:09:07.810: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 31 00:09:07.811: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/HEAD
Jan 31 00:09:07.831: INFO: http.Client request:HEAD | StatusCode:200
Jan 31 00:09:07.831: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 31 00:09:07.852: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 00:09:07.853: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/PATCH
Jan 31 00:09:07.875: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 00:09:07.875: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/POST
Jan 31 00:09:07.915: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 00:09:07.915: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/PUT
Jan 31 00:09:07.938: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 31 00:09:07.939: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/DELETE
Jan 31 00:09:07.997: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 31 00:09:07.997: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/GET
Jan 31 00:09:08.029: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 31 00:09:08.029: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/HEAD
Jan 31 00:09:08.059: INFO: http.Client request:HEAD | StatusCode:200
Jan 31 00:09:08.059: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/OPTIONS
Jan 31 00:09:08.092: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 31 00:09:08.092: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/PATCH
Jan 31 00:09:08.152: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 31 00:09:08.152: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/POST
Jan 31 00:09:08.211: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 31 00:09:08.211: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/PUT
Jan 31 00:09:08.251: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3724" for this suite. 01/31/23 00:09:08.301
------------------------------
• [4.882 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:03.446
    Jan 31 00:09:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename proxy 01/31/23 00:09:03.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:03.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:03.516
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 31 00:09:03.532: INFO: Creating pod...
    Jan 31 00:09:03.573: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3724" to be "running"
    Jan 31 00:09:03.591: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.535409ms
    Jan 31 00:09:05.608: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035099426s
    Jan 31 00:09:07.612: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.039618031s
    Jan 31 00:09:07.612: INFO: Pod "agnhost" satisfied condition "running"
    Jan 31 00:09:07.613: INFO: Creating service...
    Jan 31 00:09:07.665: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/DELETE
    Jan 31 00:09:07.774: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 31 00:09:07.775: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/GET
    Jan 31 00:09:07.810: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 31 00:09:07.811: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/HEAD
    Jan 31 00:09:07.831: INFO: http.Client request:HEAD | StatusCode:200
    Jan 31 00:09:07.831: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 31 00:09:07.852: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 31 00:09:07.853: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/PATCH
    Jan 31 00:09:07.875: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 31 00:09:07.875: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/POST
    Jan 31 00:09:07.915: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 31 00:09:07.915: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/pods/agnhost/proxy/some/path/with/PUT
    Jan 31 00:09:07.938: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 31 00:09:07.939: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/DELETE
    Jan 31 00:09:07.997: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 31 00:09:07.997: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/GET
    Jan 31 00:09:08.029: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 31 00:09:08.029: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/HEAD
    Jan 31 00:09:08.059: INFO: http.Client request:HEAD | StatusCode:200
    Jan 31 00:09:08.059: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/OPTIONS
    Jan 31 00:09:08.092: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 31 00:09:08.092: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/PATCH
    Jan 31 00:09:08.152: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 31 00:09:08.152: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/POST
    Jan 31 00:09:08.211: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 31 00:09:08.211: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-3724/services/test-service/proxy/some/path/with/PUT
    Jan 31 00:09:08.251: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:08.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3724" for this suite. 01/31/23 00:09:08.301
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:08.331
Jan 31 00:09:08.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:09:08.333
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:08.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:08.419
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-e70ae24f-2889-4910-8106-9cae9d004873 01/31/23 00:09:08.433
STEP: Creating a pod to test consume configMaps 01/31/23 00:09:08.457
Jan 31 00:09:08.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a" in namespace "projected-174" to be "Succeeded or Failed"
Jan 31 00:09:08.514: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.101485ms
Jan 31 00:09:10.533: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038202246s
Jan 31 00:09:12.532: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036465666s
Jan 31 00:09:14.533: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037970167s
STEP: Saw pod success 01/31/23 00:09:14.533
Jan 31 00:09:14.534: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a" satisfied condition "Succeeded or Failed"
Jan 31 00:09:14.553: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a container agnhost-container: <nil>
STEP: delete the pod 01/31/23 00:09:14.606
Jan 31 00:09:14.652: INFO: Waiting for pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a to disappear
Jan 31 00:09:14.674: INFO: Pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-174" for this suite. 01/31/23 00:09:14.702
------------------------------
• [SLOW TEST] [6.403 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:08.331
    Jan 31 00:09:08.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:09:08.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:08.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:08.419
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-e70ae24f-2889-4910-8106-9cae9d004873 01/31/23 00:09:08.433
    STEP: Creating a pod to test consume configMaps 01/31/23 00:09:08.457
    Jan 31 00:09:08.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a" in namespace "projected-174" to be "Succeeded or Failed"
    Jan 31 00:09:08.514: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.101485ms
    Jan 31 00:09:10.533: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038202246s
    Jan 31 00:09:12.532: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036465666s
    Jan 31 00:09:14.533: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037970167s
    STEP: Saw pod success 01/31/23 00:09:14.533
    Jan 31 00:09:14.534: INFO: Pod "pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a" satisfied condition "Succeeded or Failed"
    Jan 31 00:09:14.553: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a container agnhost-container: <nil>
    STEP: delete the pod 01/31/23 00:09:14.606
    Jan 31 00:09:14.652: INFO: Waiting for pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a to disappear
    Jan 31 00:09:14.674: INFO: Pod pod-projected-configmaps-84445daf-ba6e-44c8-a63e-cb48dc30ad7a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:14.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-174" for this suite. 01/31/23 00:09:14.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:14.738
Jan 31 00:09:14.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/31/23 00:09:14.74
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:14.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:14.808
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/31/23 00:09:14.827
STEP: delete the rc 01/31/23 00:09:19.876
STEP: wait for all pods to be garbage collected 01/31/23 00:09:19.909
STEP: Gathering metrics 01/31/23 00:09:24.942
W0131 00:09:24.986070      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 31 00:09:24.986: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:24.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2814" for this suite. 01/31/23 00:09:25.013
------------------------------
• [SLOW TEST] [10.303 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:14.738
    Jan 31 00:09:14.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/31/23 00:09:14.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:14.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:14.808
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/31/23 00:09:14.827
    STEP: delete the rc 01/31/23 00:09:19.876
    STEP: wait for all pods to be garbage collected 01/31/23 00:09:19.909
    STEP: Gathering metrics 01/31/23 00:09:24.942
    W0131 00:09:24.986070      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 31 00:09:24.986: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:24.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2814" for this suite. 01/31/23 00:09:25.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:25.049
Jan 31 00:09:25.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:09:25.051
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:25.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:25.127
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/31/23 00:09:25.143
Jan 31 00:09:25.180: INFO: Waiting up to 5m0s for pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928" in namespace "emptydir-9685" to be "Succeeded or Failed"
Jan 31 00:09:25.205: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 24.927836ms
Jan 31 00:09:27.225: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044451325s
Jan 31 00:09:29.224: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044036941s
Jan 31 00:09:31.224: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04437453s
STEP: Saw pod success 01/31/23 00:09:31.225
Jan 31 00:09:31.225: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928" satisfied condition "Succeeded or Failed"
Jan 31 00:09:31.244: INFO: Trying to get logs from node 10.15.28.227 pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 container test-container: <nil>
STEP: delete the pod 01/31/23 00:09:31.315
Jan 31 00:09:31.384: INFO: Waiting for pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 to disappear
Jan 31 00:09:31.402: INFO: Pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:31.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9685" for this suite. 01/31/23 00:09:31.429
------------------------------
• [SLOW TEST] [6.407 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:25.049
    Jan 31 00:09:25.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:09:25.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:25.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:25.127
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/31/23 00:09:25.143
    Jan 31 00:09:25.180: INFO: Waiting up to 5m0s for pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928" in namespace "emptydir-9685" to be "Succeeded or Failed"
    Jan 31 00:09:25.205: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 24.927836ms
    Jan 31 00:09:27.225: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044451325s
    Jan 31 00:09:29.224: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044036941s
    Jan 31 00:09:31.224: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04437453s
    STEP: Saw pod success 01/31/23 00:09:31.225
    Jan 31 00:09:31.225: INFO: Pod "pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928" satisfied condition "Succeeded or Failed"
    Jan 31 00:09:31.244: INFO: Trying to get logs from node 10.15.28.227 pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 container test-container: <nil>
    STEP: delete the pod 01/31/23 00:09:31.315
    Jan 31 00:09:31.384: INFO: Waiting for pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 to disappear
    Jan 31 00:09:31.402: INFO: Pod pod-d97bf26a-5b0b-4af5-915c-ee1c2ac72928 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:31.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9685" for this suite. 01/31/23 00:09:31.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:31.461
Jan 31 00:09:31.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-lifecycle-hook 01/31/23 00:09:31.463
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:31.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:31.531
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/31/23 00:09:31.572
Jan 31 00:09:31.607: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1486" to be "running and ready"
Jan 31 00:09:31.626: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.461733ms
Jan 31 00:09:31.626: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:09:33.641: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034129004s
Jan 31 00:09:33.641: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:09:35.644: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.036488054s
Jan 31 00:09:35.644: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 31 00:09:35.644: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/31/23 00:09:35.662
Jan 31 00:09:35.688: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1486" to be "running and ready"
Jan 31 00:09:35.708: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.117997ms
Jan 31 00:09:35.708: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:09:37.725: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036418489s
Jan 31 00:09:37.726: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:09:39.730: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041024366s
Jan 31 00:09:39.730: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 31 00:09:39.730: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/31/23 00:09:39.749
Jan 31 00:09:39.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 31 00:09:39.830: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 31 00:09:41.830: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 31 00:09:41.854: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/31/23 00:09:41.854
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1486" for this suite. 01/31/23 00:09:41.976
------------------------------
• [SLOW TEST] [10.545 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:31.461
    Jan 31 00:09:31.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/31/23 00:09:31.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:31.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:31.531
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/31/23 00:09:31.572
    Jan 31 00:09:31.607: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1486" to be "running and ready"
    Jan 31 00:09:31.626: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.461733ms
    Jan 31 00:09:31.626: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:09:33.641: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034129004s
    Jan 31 00:09:33.641: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:09:35.644: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.036488054s
    Jan 31 00:09:35.644: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 31 00:09:35.644: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/31/23 00:09:35.662
    Jan 31 00:09:35.688: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1486" to be "running and ready"
    Jan 31 00:09:35.708: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.117997ms
    Jan 31 00:09:35.708: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:09:37.725: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036418489s
    Jan 31 00:09:37.726: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:09:39.730: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041024366s
    Jan 31 00:09:39.730: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 31 00:09:39.730: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/31/23 00:09:39.749
    Jan 31 00:09:39.811: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 31 00:09:39.830: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 31 00:09:41.830: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 31 00:09:41.854: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/31/23 00:09:41.854
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1486" for this suite. 01/31/23 00:09:41.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:42.024
Jan 31 00:09:42.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/31/23 00:09:42.025
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:42.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:42.101
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/31/23 00:09:42.15
STEP: waiting for Deployment to be created 01/31/23 00:09:42.176
STEP: waiting for all Replicas to be Ready 01/31/23 00:09:42.183
Jan 31 00:09:42.191: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.191: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.216: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.257: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.257: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.361: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:42.362: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 31 00:09:44.149: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 31 00:09:44.149: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 31 00:09:45.156: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/31/23 00:09:45.156
W0131 00:09:45.191711      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 31 00:09:45.200: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/31/23 00:09:45.2
Jan 31 00:09:45.209: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.209: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.211: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.211: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.233: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.233: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.281: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.281: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:45.314: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:45.314: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:45.364: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:45.364: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:47.190: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:47.190: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:47.272: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
STEP: listing Deployments 01/31/23 00:09:47.272
Jan 31 00:09:47.303: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/31/23 00:09:47.303
Jan 31 00:09:47.348: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/31/23 00:09:47.348
Jan 31 00:09:47.385: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:47.385: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:47.494: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:47.543: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:50.163: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:50.204: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:50.309: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:50.329: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 31 00:09:52.201: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/31/23 00:09:52.274
STEP: fetching the DeploymentStatus 01/31/23 00:09:52.302
Jan 31 00:09:52.332: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3
Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:52.334: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
Jan 31 00:09:52.334: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3
STEP: deleting the Deployment 01/31/23 00:09:52.334
Jan 31 00:09:52.382: INFO: observed event type MODIFIED
Jan 31 00:09:52.383: INFO: observed event type MODIFIED
Jan 31 00:09:52.383: INFO: observed event type MODIFIED
Jan 31 00:09:52.384: INFO: observed event type MODIFIED
Jan 31 00:09:52.384: INFO: observed event type MODIFIED
Jan 31 00:09:52.384: INFO: observed event type MODIFIED
Jan 31 00:09:52.385: INFO: observed event type MODIFIED
Jan 31 00:09:52.385: INFO: observed event type MODIFIED
Jan 31 00:09:52.385: INFO: observed event type MODIFIED
Jan 31 00:09:52.386: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 00:09:52.414: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 31 00:09:52.434: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-2755  75236bd8-98b8-4202-987e-0dfa4c3b6477 46542 2 2023-01-31 00:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64ce7 0xc005a64ce8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64d70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 31 00:09:52.458: INFO: pod: "test-deployment-7b7876f9d6-4vqnc":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-4vqnc test-deployment-7b7876f9d6- deployment-2755  a819d574-1285-42c1-955d-e2d8c6c3d495 46541 0 2023-01-31 00:09:50 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:b44eafd1fdaf93e57be0d7a1795f24cd312df20921bf4d5cca343273be47db75 cni.projectcalico.org/podIP:172.30.237.150/32 cni.projectcalico.org/podIPs:172.30.237.150/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75236bd8-98b8-4202-987e-0dfa4c3b6477 0xc005a651f7 0xc005a651f8}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75236bd8-98b8-4202-987e-0dfa4c3b6477\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kr64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kr64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.150,StartTime:2023-01-31 00:09:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7d9d4a5186ef1a680ac99dd27f6acae593d62b228027ade4e8341ab6cb84160,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 00:09:52.458: INFO: pod: "test-deployment-7b7876f9d6-j7kbq":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-j7kbq test-deployment-7b7876f9d6- deployment-2755  497f7bfc-d059-4071-877c-3c43537bcee3 46504 0 2023-01-31 00:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:0bea20661f7a8869f8a3c3b100e623c7fa7a9a20dfec2dd8063716573d97cfdc cni.projectcalico.org/podIP:172.30.199.52/32 cni.projectcalico.org/podIPs:172.30.199.52/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75236bd8-98b8-4202-987e-0dfa4c3b6477 0xc005a65427 0xc005a65428}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75236bd8-98b8-4202-987e-0dfa4c3b6477\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5t8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5t8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.52,StartTime:2023-01-31 00:09:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1fef1012d3c7ca01e7f6a1d089518ab2732a9c11b3e73acabce0120c7045e8df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 00:09:52.459: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-2755  f420b873-b58d-4c03-9fe3-01da96c49215 46550 4 2023-01-31 00:09:45 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64dd7 0xc005a64dd8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64e70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 31 00:09:52.489: INFO: pod: "test-deployment-7df74c55ff-rt2jq":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-rt2jq test-deployment-7df74c55ff- deployment-2755  098bd6aa-929f-45c1-b79e-332292bb4375 46546 0 2023-01-31 00:09:47 +0000 UTC 2023-01-31 00:09:53 +0000 UTC 0xc005be65a8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7a6068bac510fbf3bd45e0c1bea5957e68d81b992c5d87f8898d5f4b5bd7257b cni.projectcalico.org/podIP:172.30.248.52/32 cni.projectcalico.org/podIPs:172.30.248.52/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff f420b873-b58d-4c03-9fe3-01da96c49215 0xc005be65f7 0xc005be65f8}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f420b873-b58d-4c03-9fe3-01da96c49215\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99rf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99rf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.52,StartTime:2023-01-31 00:09:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://86c0b2813c42b6edc73f771126bc7945a877b6693b3ecae5aaa404a518d2502d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 31 00:09:52.491: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-2755  a3ac56d4-da97-4557-9383-cf5569515b12 46430 3 2023-01-31 00:09:42 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64ed7 0xc005a64ed8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64f60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:52.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2755" for this suite. 01/31/23 00:09:52.532
------------------------------
• [SLOW TEST] [10.542 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:42.024
    Jan 31 00:09:42.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/31/23 00:09:42.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:42.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:42.101
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/31/23 00:09:42.15
    STEP: waiting for Deployment to be created 01/31/23 00:09:42.176
    STEP: waiting for all Replicas to be Ready 01/31/23 00:09:42.183
    Jan 31 00:09:42.191: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.191: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.216: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.257: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.257: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.361: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:42.362: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 31 00:09:44.149: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 31 00:09:44.149: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 31 00:09:45.156: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/31/23 00:09:45.156
    W0131 00:09:45.191711      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 31 00:09:45.200: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/31/23 00:09:45.2
    Jan 31 00:09:45.209: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.209: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.210: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.211: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.211: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 0
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.215: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.233: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.233: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.281: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.281: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:45.314: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:45.314: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:45.364: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:45.364: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:47.190: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:47.190: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:47.272: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    STEP: listing Deployments 01/31/23 00:09:47.272
    Jan 31 00:09:47.303: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/31/23 00:09:47.303
    Jan 31 00:09:47.348: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/31/23 00:09:47.348
    Jan 31 00:09:47.385: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:47.385: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:47.494: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:47.543: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:50.163: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:50.204: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:50.309: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:50.329: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 31 00:09:52.201: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/31/23 00:09:52.274
    STEP: fetching the DeploymentStatus 01/31/23 00:09:52.302
    Jan 31 00:09:52.332: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 1
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3
    Jan 31 00:09:52.333: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:52.334: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 2
    Jan 31 00:09:52.334: INFO: observed Deployment test-deployment in namespace deployment-2755 with ReadyReplicas 3
    STEP: deleting the Deployment 01/31/23 00:09:52.334
    Jan 31 00:09:52.382: INFO: observed event type MODIFIED
    Jan 31 00:09:52.383: INFO: observed event type MODIFIED
    Jan 31 00:09:52.383: INFO: observed event type MODIFIED
    Jan 31 00:09:52.384: INFO: observed event type MODIFIED
    Jan 31 00:09:52.384: INFO: observed event type MODIFIED
    Jan 31 00:09:52.384: INFO: observed event type MODIFIED
    Jan 31 00:09:52.385: INFO: observed event type MODIFIED
    Jan 31 00:09:52.385: INFO: observed event type MODIFIED
    Jan 31 00:09:52.385: INFO: observed event type MODIFIED
    Jan 31 00:09:52.386: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 31 00:09:52.414: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 31 00:09:52.434: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-2755  75236bd8-98b8-4202-987e-0dfa4c3b6477 46542 2 2023-01-31 00:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64ce7 0xc005a64ce8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64d70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 31 00:09:52.458: INFO: pod: "test-deployment-7b7876f9d6-4vqnc":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-4vqnc test-deployment-7b7876f9d6- deployment-2755  a819d574-1285-42c1-955d-e2d8c6c3d495 46541 0 2023-01-31 00:09:50 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:b44eafd1fdaf93e57be0d7a1795f24cd312df20921bf4d5cca343273be47db75 cni.projectcalico.org/podIP:172.30.237.150/32 cni.projectcalico.org/podIPs:172.30.237.150/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75236bd8-98b8-4202-987e-0dfa4c3b6477 0xc005a651f7 0xc005a651f8}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75236bd8-98b8-4202-987e-0dfa4c3b6477\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kr64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kr64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.150,StartTime:2023-01-31 00:09:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f7d9d4a5186ef1a680ac99dd27f6acae593d62b228027ade4e8341ab6cb84160,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 31 00:09:52.458: INFO: pod: "test-deployment-7b7876f9d6-j7kbq":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-j7kbq test-deployment-7b7876f9d6- deployment-2755  497f7bfc-d059-4071-877c-3c43537bcee3 46504 0 2023-01-31 00:09:47 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:0bea20661f7a8869f8a3c3b100e623c7fa7a9a20dfec2dd8063716573d97cfdc cni.projectcalico.org/podIP:172.30.199.52/32 cni.projectcalico.org/podIPs:172.30.199.52/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 75236bd8-98b8-4202-987e-0dfa4c3b6477 0xc005a65427 0xc005a65428}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75236bd8-98b8-4202-987e-0dfa4c3b6477\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5t8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5t8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.52,StartTime:2023-01-31 00:09:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1fef1012d3c7ca01e7f6a1d089518ab2732a9c11b3e73acabce0120c7045e8df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 31 00:09:52.459: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-2755  f420b873-b58d-4c03-9fe3-01da96c49215 46550 4 2023-01-31 00:09:45 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64dd7 0xc005a64dd8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64e70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 31 00:09:52.489: INFO: pod: "test-deployment-7df74c55ff-rt2jq":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-rt2jq test-deployment-7df74c55ff- deployment-2755  098bd6aa-929f-45c1-b79e-332292bb4375 46546 0 2023-01-31 00:09:47 +0000 UTC 2023-01-31 00:09:53 +0000 UTC 0xc005be65a8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7a6068bac510fbf3bd45e0c1bea5957e68d81b992c5d87f8898d5f4b5bd7257b cni.projectcalico.org/podIP:172.30.248.52/32 cni.projectcalico.org/podIPs:172.30.248.52/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff f420b873-b58d-4c03-9fe3-01da96c49215 0xc005be65f7 0xc005be65f8}] [] [{kube-controller-manager Update v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f420b873-b58d-4c03-9fe3-01da96c49215\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:09:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:09:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99rf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99rf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:09:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.52,StartTime:2023-01-31 00:09:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:09:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://86c0b2813c42b6edc73f771126bc7945a877b6693b3ecae5aaa404a518d2502d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 31 00:09:52.491: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-2755  a3ac56d4-da97-4557-9383-cf5569515b12 46430 3 2023-01-31 00:09:42 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6a46cbde-aa57-41fe-b1f3-39690081de49 0xc005a64ed7 0xc005a64ed8}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a46cbde-aa57-41fe-b1f3-39690081de49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:09:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a64f60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:52.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2755" for this suite. 01/31/23 00:09:52.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:52.567
Jan 31 00:09:52.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:09:52.572
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:52.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:52.657
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:09:52.734
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:09:53.295
STEP: Deploying the webhook pod 01/31/23 00:09:53.372
STEP: Wait for the deployment to be ready 01/31/23 00:09:53.415
Jan 31 00:09:53.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:09:55.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:09:57.549
STEP: Verifying the service has paired with the endpoint 01/31/23 00:09:57.617
Jan 31 00:09:58.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/31/23 00:09:58.634
STEP: create a namespace for the webhook 01/31/23 00:09:58.732
STEP: create a configmap should be unconditionally rejected by the webhook 01/31/23 00:09:58.766
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:58.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4308" for this suite. 01/31/23 00:09:59.173
STEP: Destroying namespace "webhook-4308-markers" for this suite. 01/31/23 00:09:59.201
------------------------------
• [SLOW TEST] [6.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:52.567
    Jan 31 00:09:52.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:09:52.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:52.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:52.657
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:09:52.734
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:09:53.295
    STEP: Deploying the webhook pod 01/31/23 00:09:53.372
    STEP: Wait for the deployment to be ready 01/31/23 00:09:53.415
    Jan 31 00:09:53.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:09:55.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:09:57.549
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:09:57.617
    Jan 31 00:09:58.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/31/23 00:09:58.634
    STEP: create a namespace for the webhook 01/31/23 00:09:58.732
    STEP: create a configmap should be unconditionally rejected by the webhook 01/31/23 00:09:58.766
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:58.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4308" for this suite. 01/31/23 00:09:59.173
    STEP: Destroying namespace "webhook-4308-markers" for this suite. 01/31/23 00:09:59.201
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:59.228
Jan 31 00:09:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/31/23 00:09:59.229
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:59.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:59.286
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/31/23 00:09:59.3
Jan 31 00:09:59.300: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-5537 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/31/23 00:09:59.376
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 31 00:09:59.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5537" for this suite. 01/31/23 00:09:59.444
------------------------------
• [0.242 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:59.228
    Jan 31 00:09:59.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/31/23 00:09:59.229
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:59.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:59.286
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/31/23 00:09:59.3
    Jan 31 00:09:59.300: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-5537 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/31/23 00:09:59.376
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:09:59.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5537" for this suite. 01/31/23 00:09:59.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:09:59.472
Jan 31 00:09:59.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/31/23 00:09:59.473
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:59.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:59.537
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5036 01/31/23 00:09:59.551
STEP: creating service affinity-clusterip-transition in namespace services-5036 01/31/23 00:09:59.552
STEP: creating replication controller affinity-clusterip-transition in namespace services-5036 01/31/23 00:09:59.603
I0131 00:09:59.626243      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5036, replica count: 3
I0131 00:10:02.678463      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 00:10:02.714: INFO: Creating new exec pod
Jan 31 00:10:02.743: INFO: Waiting up to 5m0s for pod "execpod-affinity8b8ss" in namespace "services-5036" to be "running"
Jan 31 00:10:02.761: INFO: Pod "execpod-affinity8b8ss": Phase="Pending", Reason="", readiness=false. Elapsed: 18.428635ms
Jan 31 00:10:04.779: INFO: Pod "execpod-affinity8b8ss": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036308916s
Jan 31 00:10:06.796: INFO: Pod "execpod-affinity8b8ss": Phase="Running", Reason="", readiness=true. Elapsed: 4.052726845s
Jan 31 00:10:06.796: INFO: Pod "execpod-affinity8b8ss" satisfied condition "running"
Jan 31 00:10:07.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 31 00:10:08.231: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 31 00:10:08.231: INFO: stdout: ""
Jan 31 00:10:08.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c nc -v -z -w 2 172.21.23.147 80'
Jan 31 00:10:08.633: INFO: stderr: "+ nc -v -z -w 2 172.21.23.147 80\nConnection to 172.21.23.147 80 port [tcp/http] succeeded!\n"
Jan 31 00:10:08.633: INFO: stdout: ""
Jan 31 00:10:08.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
Jan 31 00:10:09.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
Jan 31 00:10:09.250: INFO: stdout: "\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj"
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
Jan 31 00:10:09.912: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
Jan 31 00:10:09.912: INFO: stdout: "\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj"
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
Jan 31 00:10:39.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
Jan 31 00:10:40.482: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
Jan 31 00:10:40.482: INFO: stdout: "\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk"
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
Jan 31 00:10:40.483: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5036, will wait for the garbage collector to delete the pods 01/31/23 00:10:40.534
Jan 31 00:10:40.632: INFO: Deleting ReplicationController affinity-clusterip-transition took: 30.533825ms
Jan 31 00:10:40.735: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 103.330415ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 31 00:10:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5036" for this suite. 01/31/23 00:10:43.759
------------------------------
• [SLOW TEST] [44.316 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:09:59.472
    Jan 31 00:09:59.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/31/23 00:09:59.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:09:59.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:09:59.537
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5036 01/31/23 00:09:59.551
    STEP: creating service affinity-clusterip-transition in namespace services-5036 01/31/23 00:09:59.552
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5036 01/31/23 00:09:59.603
    I0131 00:09:59.626243      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5036, replica count: 3
    I0131 00:10:02.678463      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 31 00:10:02.714: INFO: Creating new exec pod
    Jan 31 00:10:02.743: INFO: Waiting up to 5m0s for pod "execpod-affinity8b8ss" in namespace "services-5036" to be "running"
    Jan 31 00:10:02.761: INFO: Pod "execpod-affinity8b8ss": Phase="Pending", Reason="", readiness=false. Elapsed: 18.428635ms
    Jan 31 00:10:04.779: INFO: Pod "execpod-affinity8b8ss": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036308916s
    Jan 31 00:10:06.796: INFO: Pod "execpod-affinity8b8ss": Phase="Running", Reason="", readiness=true. Elapsed: 4.052726845s
    Jan 31 00:10:06.796: INFO: Pod "execpod-affinity8b8ss" satisfied condition "running"
    Jan 31 00:10:07.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 31 00:10:08.231: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 31 00:10:08.231: INFO: stdout: ""
    Jan 31 00:10:08.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c nc -v -z -w 2 172.21.23.147 80'
    Jan 31 00:10:08.633: INFO: stderr: "+ nc -v -z -w 2 172.21.23.147 80\nConnection to 172.21.23.147 80 port [tcp/http] succeeded!\n"
    Jan 31 00:10:08.633: INFO: stdout: ""
    Jan 31 00:10:08.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
    Jan 31 00:10:09.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
    Jan 31 00:10:09.250: INFO: stdout: "\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj"
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.251: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.252: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
    Jan 31 00:10:09.912: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
    Jan 31 00:10:09.912: INFO: stdout: "\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-jzjt9\naffinity-clusterip-transition-x7trj\naffinity-clusterip-transition-x7trj"
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-jzjt9
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:09.912: INFO: Received response from host: affinity-clusterip-transition-x7trj
    Jan 31 00:10:39.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-5036 exec execpod-affinity8b8ss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.23.147:80/ ; done'
    Jan 31 00:10:40.482: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.23.147:80/\n"
    Jan 31 00:10:40.482: INFO: stdout: "\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk\naffinity-clusterip-transition-m4kwk"
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Received response from host: affinity-clusterip-transition-m4kwk
    Jan 31 00:10:40.483: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5036, will wait for the garbage collector to delete the pods 01/31/23 00:10:40.534
    Jan 31 00:10:40.632: INFO: Deleting ReplicationController affinity-clusterip-transition took: 30.533825ms
    Jan 31 00:10:40.735: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 103.330415ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:10:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5036" for this suite. 01/31/23 00:10:43.759
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:10:43.788
Jan 31 00:10:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/31/23 00:10:43.79
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:43.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:43.88
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/31/23 00:10:43.896
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/31/23 00:10:43.896
STEP: creating a pod to probe DNS 01/31/23 00:10:43.896
STEP: submitting the pod to kubernetes 01/31/23 00:10:43.896
Jan 31 00:10:43.946: INFO: Waiting up to 15m0s for pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552" in namespace "dns-9530" to be "running"
Jan 31 00:10:43.963: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Pending", Reason="", readiness=false. Elapsed: 17.150459ms
Jan 31 00:10:45.981: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034551502s
Jan 31 00:10:47.984: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Running", Reason="", readiness=true. Elapsed: 4.038197581s
Jan 31 00:10:47.985: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552" satisfied condition "running"
STEP: retrieving the pod 01/31/23 00:10:47.985
STEP: looking for the results for each expected name from probers 01/31/23 00:10:48.002
Jan 31 00:10:48.134: INFO: DNS probes using dns-9530/dns-test-030cf8eb-9abd-48f2-9842-81ce04299552 succeeded

STEP: deleting the pod 01/31/23 00:10:48.134
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 31 00:10:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9530" for this suite. 01/31/23 00:10:48.233
------------------------------
• [4.472 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:10:43.788
    Jan 31 00:10:43.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/31/23 00:10:43.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:43.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:43.88
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/31/23 00:10:43.896
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/31/23 00:10:43.896
    STEP: creating a pod to probe DNS 01/31/23 00:10:43.896
    STEP: submitting the pod to kubernetes 01/31/23 00:10:43.896
    Jan 31 00:10:43.946: INFO: Waiting up to 15m0s for pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552" in namespace "dns-9530" to be "running"
    Jan 31 00:10:43.963: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Pending", Reason="", readiness=false. Elapsed: 17.150459ms
    Jan 31 00:10:45.981: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034551502s
    Jan 31 00:10:47.984: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552": Phase="Running", Reason="", readiness=true. Elapsed: 4.038197581s
    Jan 31 00:10:47.985: INFO: Pod "dns-test-030cf8eb-9abd-48f2-9842-81ce04299552" satisfied condition "running"
    STEP: retrieving the pod 01/31/23 00:10:47.985
    STEP: looking for the results for each expected name from probers 01/31/23 00:10:48.002
    Jan 31 00:10:48.134: INFO: DNS probes using dns-9530/dns-test-030cf8eb-9abd-48f2-9842-81ce04299552 succeeded

    STEP: deleting the pod 01/31/23 00:10:48.134
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:10:48.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9530" for this suite. 01/31/23 00:10:48.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:10:48.263
Jan 31 00:10:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/31/23 00:10:48.266
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:48.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:48.335
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 31 00:10:48.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3186" for this suite. 01/31/23 00:10:48.408
------------------------------
• [0.174 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:10:48.263
    Jan 31 00:10:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/31/23 00:10:48.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:48.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:48.335
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:10:48.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3186" for this suite. 01/31/23 00:10:48.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:10:48.45
Jan 31 00:10:48.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/31/23 00:10:48.451
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:48.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:48.515
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-efa92170-f456-4eab-b35b-6abe298db501 01/31/23 00:10:48.529
STEP: Creating a pod to test consume configMaps 01/31/23 00:10:48.552
Jan 31 00:10:48.592: INFO: Waiting up to 5m0s for pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc" in namespace "configmap-7187" to be "Succeeded or Failed"
Jan 31 00:10:48.613: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.616029ms
Jan 31 00:10:50.634: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.041345951s
Jan 31 00:10:52.632: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Running", Reason="", readiness=false. Elapsed: 4.039402646s
Jan 31 00:10:54.634: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041794046s
STEP: Saw pod success 01/31/23 00:10:54.634
Jan 31 00:10:54.635: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc" satisfied condition "Succeeded or Failed"
Jan 31 00:10:54.654: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc container agnhost-container: <nil>
STEP: delete the pod 01/31/23 00:10:54.695
Jan 31 00:10:54.771: INFO: Waiting for pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc to disappear
Jan 31 00:10:54.788: INFO: Pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 31 00:10:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7187" for this suite. 01/31/23 00:10:54.817
------------------------------
• [SLOW TEST] [6.398 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:10:48.45
    Jan 31 00:10:48.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/31/23 00:10:48.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:48.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:48.515
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-efa92170-f456-4eab-b35b-6abe298db501 01/31/23 00:10:48.529
    STEP: Creating a pod to test consume configMaps 01/31/23 00:10:48.552
    Jan 31 00:10:48.592: INFO: Waiting up to 5m0s for pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc" in namespace "configmap-7187" to be "Succeeded or Failed"
    Jan 31 00:10:48.613: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.616029ms
    Jan 31 00:10:50.634: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.041345951s
    Jan 31 00:10:52.632: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Running", Reason="", readiness=false. Elapsed: 4.039402646s
    Jan 31 00:10:54.634: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041794046s
    STEP: Saw pod success 01/31/23 00:10:54.634
    Jan 31 00:10:54.635: INFO: Pod "pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc" satisfied condition "Succeeded or Failed"
    Jan 31 00:10:54.654: INFO: Trying to get logs from node 10.15.28.227 pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc container agnhost-container: <nil>
    STEP: delete the pod 01/31/23 00:10:54.695
    Jan 31 00:10:54.771: INFO: Waiting for pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc to disappear
    Jan 31 00:10:54.788: INFO: Pod pod-configmaps-203c63c6-a0f3-441a-ac5e-1a49baf0b1fc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:10:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7187" for this suite. 01/31/23 00:10:54.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:10:54.851
Jan 31 00:10:54.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:10:54.853
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:54.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:54.939
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c69157ee-9978-4ad5-ae47-2259bb194ad0 01/31/23 00:10:54.981
STEP: Creating the pod 01/31/23 00:10:55.005
Jan 31 00:10:55.047: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f" in namespace "projected-9828" to be "running and ready"
Jan 31 00:10:55.068: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.274813ms
Jan 31 00:10:55.068: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:10:57.089: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042079526s
Jan 31 00:10:57.090: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:10:59.088: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Running", Reason="", readiness=true. Elapsed: 4.040285103s
Jan 31 00:10:59.088: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Running (Ready = true)
Jan 31 00:10:59.088: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-c69157ee-9978-4ad5-ae47-2259bb194ad0 01/31/23 00:10:59.147
STEP: waiting to observe update in volume 01/31/23 00:10:59.169
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:15.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9828" for this suite. 01/31/23 00:12:15.051
------------------------------
• [SLOW TEST] [80.255 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:10:54.851
    Jan 31 00:10:54.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:10:54.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:10:54.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:10:54.939
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-c69157ee-9978-4ad5-ae47-2259bb194ad0 01/31/23 00:10:54.981
    STEP: Creating the pod 01/31/23 00:10:55.005
    Jan 31 00:10:55.047: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f" in namespace "projected-9828" to be "running and ready"
    Jan 31 00:10:55.068: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.274813ms
    Jan 31 00:10:55.068: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:10:57.089: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042079526s
    Jan 31 00:10:57.090: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:10:59.088: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f": Phase="Running", Reason="", readiness=true. Elapsed: 4.040285103s
    Jan 31 00:10:59.088: INFO: The phase of Pod pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f is Running (Ready = true)
    Jan 31 00:10:59.088: INFO: Pod "pod-projected-configmaps-669e447f-e9e1-492b-944c-0ad83b96905f" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-c69157ee-9978-4ad5-ae47-2259bb194ad0 01/31/23 00:10:59.147
    STEP: waiting to observe update in volume 01/31/23 00:10:59.169
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:15.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9828" for this suite. 01/31/23 00:12:15.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:15.107
Jan 31 00:12:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename configmap 01/31/23 00:12:15.109
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.194
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-71eb156f-025f-42b3-9bd8-1e5040bd6516 01/31/23 00:12:15.211
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:15.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-265" for this suite. 01/31/23 00:12:15.247
------------------------------
• [0.229 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:15.107
    Jan 31 00:12:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename configmap 01/31/23 00:12:15.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.194
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-71eb156f-025f-42b3-9bd8-1e5040bd6516 01/31/23 00:12:15.211
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:15.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-265" for this suite. 01/31/23 00:12:15.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:15.343
Jan 31 00:12:15.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/31/23 00:12:15.346
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.419
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/31/23 00:12:15.436
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/31/23 00:12:15.459
STEP: patching the secret 01/31/23 00:12:15.5
STEP: deleting the secret using a LabelSelector 01/31/23 00:12:15.535
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/31/23 00:12:15.575
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4904" for this suite. 01/31/23 00:12:15.626
------------------------------
• [0.311 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:15.343
    Jan 31 00:12:15.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/31/23 00:12:15.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.419
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/31/23 00:12:15.436
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/31/23 00:12:15.459
    STEP: patching the secret 01/31/23 00:12:15.5
    STEP: deleting the secret using a LabelSelector 01/31/23 00:12:15.535
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/31/23 00:12:15.575
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4904" for this suite. 01/31/23 00:12:15.626
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:15.664
Jan 31 00:12:15.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:12:15.665
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.736
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-aaacfc94-334e-488b-bb0c-a736714453ff 01/31/23 00:12:15.75
STEP: Creating a pod to test consume configMaps 01/31/23 00:12:15.802
Jan 31 00:12:15.842: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0" in namespace "projected-771" to be "Succeeded or Failed"
Jan 31 00:12:15.881: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.764676ms
Jan 31 00:12:17.921: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078848789s
Jan 31 00:12:19.899: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056531956s
STEP: Saw pod success 01/31/23 00:12:19.899
Jan 31 00:12:19.900: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0" satisfied condition "Succeeded or Failed"
Jan 31 00:12:19.917: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 container agnhost-container: <nil>
STEP: delete the pod 01/31/23 00:12:19.957
Jan 31 00:12:20.027: INFO: Waiting for pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 to disappear
Jan 31 00:12:20.044: INFO: Pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-771" for this suite. 01/31/23 00:12:20.07
------------------------------
• [4.433 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:15.664
    Jan 31 00:12:15.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:12:15.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:15.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:15.736
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-aaacfc94-334e-488b-bb0c-a736714453ff 01/31/23 00:12:15.75
    STEP: Creating a pod to test consume configMaps 01/31/23 00:12:15.802
    Jan 31 00:12:15.842: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0" in namespace "projected-771" to be "Succeeded or Failed"
    Jan 31 00:12:15.881: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.764676ms
    Jan 31 00:12:17.921: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078848789s
    Jan 31 00:12:19.899: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056531956s
    STEP: Saw pod success 01/31/23 00:12:19.899
    Jan 31 00:12:19.900: INFO: Pod "pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0" satisfied condition "Succeeded or Failed"
    Jan 31 00:12:19.917: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 container agnhost-container: <nil>
    STEP: delete the pod 01/31/23 00:12:19.957
    Jan 31 00:12:20.027: INFO: Waiting for pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 to disappear
    Jan 31 00:12:20.044: INFO: Pod pod-projected-configmaps-056fff40-6dc8-426c-aa4f-8023a8ccaad0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:20.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-771" for this suite. 01/31/23 00:12:20.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:20.097
Jan 31 00:12:20.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:12:20.099
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:20.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:20.19
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/31/23 00:12:20.205
Jan 31 00:12:20.238: INFO: Waiting up to 5m0s for pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58" in namespace "emptydir-7754" to be "Succeeded or Failed"
Jan 31 00:12:20.260: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 21.377573ms
Jan 31 00:12:22.302: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063139415s
Jan 31 00:12:24.279: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040439718s
Jan 31 00:12:26.277: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039129687s
STEP: Saw pod success 01/31/23 00:12:26.278
Jan 31 00:12:26.278: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58" satisfied condition "Succeeded or Failed"
Jan 31 00:12:26.295: INFO: Trying to get logs from node 10.15.28.227 pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 container test-container: <nil>
STEP: delete the pod 01/31/23 00:12:26.335
Jan 31 00:12:26.377: INFO: Waiting for pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 to disappear
Jan 31 00:12:26.396: INFO: Pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7754" for this suite. 01/31/23 00:12:26.421
------------------------------
• [SLOW TEST] [6.354 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:20.097
    Jan 31 00:12:20.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:12:20.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:20.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:20.19
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/31/23 00:12:20.205
    Jan 31 00:12:20.238: INFO: Waiting up to 5m0s for pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58" in namespace "emptydir-7754" to be "Succeeded or Failed"
    Jan 31 00:12:20.260: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 21.377573ms
    Jan 31 00:12:22.302: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063139415s
    Jan 31 00:12:24.279: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040439718s
    Jan 31 00:12:26.277: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039129687s
    STEP: Saw pod success 01/31/23 00:12:26.278
    Jan 31 00:12:26.278: INFO: Pod "pod-d5dc3810-ec32-411c-9cd9-7e760986ee58" satisfied condition "Succeeded or Failed"
    Jan 31 00:12:26.295: INFO: Trying to get logs from node 10.15.28.227 pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 container test-container: <nil>
    STEP: delete the pod 01/31/23 00:12:26.335
    Jan 31 00:12:26.377: INFO: Waiting for pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 to disappear
    Jan 31 00:12:26.396: INFO: Pod pod-d5dc3810-ec32-411c-9cd9-7e760986ee58 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7754" for this suite. 01/31/23 00:12:26.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:26.459
Jan 31 00:12:26.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:12:26.462
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:26.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:26.542
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:12:26.608
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:12:26.894
STEP: Deploying the webhook pod 01/31/23 00:12:26.933
STEP: Wait for the deployment to be ready 01/31/23 00:12:26.991
Jan 31 00:12:27.058: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/31/23 00:12:29.114
STEP: Verifying the service has paired with the endpoint 01/31/23 00:12:29.166
Jan 31 00:12:30.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/31/23 00:12:30.566
STEP: Creating a configMap that should be mutated 01/31/23 00:12:30.671
STEP: Deleting the collection of validation webhooks 01/31/23 00:12:30.955
STEP: Creating a configMap that should not be mutated 01/31/23 00:12:31.329
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:31.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5989" for this suite. 01/31/23 00:12:31.71
STEP: Destroying namespace "webhook-5989-markers" for this suite. 01/31/23 00:12:31.736
------------------------------
• [SLOW TEST] [5.329 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:26.459
    Jan 31 00:12:26.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:12:26.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:26.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:26.542
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:12:26.608
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:12:26.894
    STEP: Deploying the webhook pod 01/31/23 00:12:26.933
    STEP: Wait for the deployment to be ready 01/31/23 00:12:26.991
    Jan 31 00:12:27.058: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/31/23 00:12:29.114
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:12:29.166
    Jan 31 00:12:30.166: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/31/23 00:12:30.566
    STEP: Creating a configMap that should be mutated 01/31/23 00:12:30.671
    STEP: Deleting the collection of validation webhooks 01/31/23 00:12:30.955
    STEP: Creating a configMap that should not be mutated 01/31/23 00:12:31.329
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:31.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5989" for this suite. 01/31/23 00:12:31.71
    STEP: Destroying namespace "webhook-5989-markers" for this suite. 01/31/23 00:12:31.736
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:31.79
Jan 31 00:12:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename ingress 01/31/23 00:12:31.794
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:31.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:31.892
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/31/23 00:12:31.91
STEP: getting /apis/networking.k8s.io 01/31/23 00:12:31.925
STEP: getting /apis/networking.k8s.iov1 01/31/23 00:12:31.931
STEP: creating 01/31/23 00:12:31.939
STEP: getting 01/31/23 00:12:32.031
STEP: listing 01/31/23 00:12:32.049
STEP: watching 01/31/23 00:12:32.071
Jan 31 00:12:32.071: INFO: starting watch
STEP: cluster-wide listing 01/31/23 00:12:32.077
STEP: cluster-wide watching 01/31/23 00:12:32.103
Jan 31 00:12:32.103: INFO: starting watch
STEP: patching 01/31/23 00:12:32.11
STEP: updating 01/31/23 00:12:32.134
Jan 31 00:12:32.218: INFO: waiting for watch events with expected annotations
Jan 31 00:12:32.219: INFO: saw patched and updated annotations
STEP: patching /status 01/31/23 00:12:32.219
STEP: updating /status 01/31/23 00:12:32.243
STEP: get /status 01/31/23 00:12:32.285
STEP: deleting 01/31/23 00:12:32.306
STEP: deleting a collection 01/31/23 00:12:32.392
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5855" for this suite. 01/31/23 00:12:32.505
------------------------------
• [0.743 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:31.79
    Jan 31 00:12:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename ingress 01/31/23 00:12:31.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:31.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:31.892
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/31/23 00:12:31.91
    STEP: getting /apis/networking.k8s.io 01/31/23 00:12:31.925
    STEP: getting /apis/networking.k8s.iov1 01/31/23 00:12:31.931
    STEP: creating 01/31/23 00:12:31.939
    STEP: getting 01/31/23 00:12:32.031
    STEP: listing 01/31/23 00:12:32.049
    STEP: watching 01/31/23 00:12:32.071
    Jan 31 00:12:32.071: INFO: starting watch
    STEP: cluster-wide listing 01/31/23 00:12:32.077
    STEP: cluster-wide watching 01/31/23 00:12:32.103
    Jan 31 00:12:32.103: INFO: starting watch
    STEP: patching 01/31/23 00:12:32.11
    STEP: updating 01/31/23 00:12:32.134
    Jan 31 00:12:32.218: INFO: waiting for watch events with expected annotations
    Jan 31 00:12:32.219: INFO: saw patched and updated annotations
    STEP: patching /status 01/31/23 00:12:32.219
    STEP: updating /status 01/31/23 00:12:32.243
    STEP: get /status 01/31/23 00:12:32.285
    STEP: deleting 01/31/23 00:12:32.306
    STEP: deleting a collection 01/31/23 00:12:32.392
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5855" for this suite. 01/31/23 00:12:32.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:32.539
Jan 31 00:12:32.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:12:32.542
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:32.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:32.613
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:12:32.678
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:12:33.115
STEP: Deploying the webhook pod 01/31/23 00:12:33.136
STEP: Wait for the deployment to be ready 01/31/23 00:12:33.221
Jan 31 00:12:33.270: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:12:35.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:12:37.348
STEP: Verifying the service has paired with the endpoint 01/31/23 00:12:37.436
Jan 31 00:12:38.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/31/23 00:12:38.728
STEP: Creating a configMap that does not comply to the validation webhook rules 01/31/23 00:12:38.891
STEP: Deleting the collection of validation webhooks 01/31/23 00:12:39.014
STEP: Creating a configMap that does not comply to the validation webhook rules 01/31/23 00:12:39.323
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:39.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8246" for this suite. 01/31/23 00:12:39.632
STEP: Destroying namespace "webhook-8246-markers" for this suite. 01/31/23 00:12:39.66
------------------------------
• [SLOW TEST] [7.156 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:32.539
    Jan 31 00:12:32.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:12:32.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:32.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:32.613
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:12:32.678
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:12:33.115
    STEP: Deploying the webhook pod 01/31/23 00:12:33.136
    STEP: Wait for the deployment to be ready 01/31/23 00:12:33.221
    Jan 31 00:12:33.270: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:12:35.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 12, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:12:37.348
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:12:37.436
    Jan 31 00:12:38.437: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/31/23 00:12:38.728
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/31/23 00:12:38.891
    STEP: Deleting the collection of validation webhooks 01/31/23 00:12:39.014
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/31/23 00:12:39.323
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:39.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8246" for this suite. 01/31/23 00:12:39.632
    STEP: Destroying namespace "webhook-8246-markers" for this suite. 01/31/23 00:12:39.66
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:39.696
Jan 31 00:12:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/31/23 00:12:39.698
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:39.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:39.772
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/31/23 00:12:39.788
Jan 31 00:12:39.788: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 31 00:12:39.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:41.076: INFO: stderr: ""
Jan 31 00:12:41.076: INFO: stdout: "service/agnhost-replica created\n"
Jan 31 00:12:41.076: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 31 00:12:41.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:41.525: INFO: stderr: ""
Jan 31 00:12:41.525: INFO: stdout: "service/agnhost-primary created\n"
Jan 31 00:12:41.525: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 31 00:12:41.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:42.697: INFO: stderr: ""
Jan 31 00:12:42.697: INFO: stdout: "service/frontend created\n"
Jan 31 00:12:42.698: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 31 00:12:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:43.077: INFO: stderr: ""
Jan 31 00:12:43.077: INFO: stdout: "deployment.apps/frontend created\n"
Jan 31 00:12:43.077: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 31 00:12:43.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:43.434: INFO: stderr: ""
Jan 31 00:12:43.434: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 31 00:12:43.434: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 31 00:12:43.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
Jan 31 00:12:43.792: INFO: stderr: ""
Jan 31 00:12:43.792: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/31/23 00:12:43.792
Jan 31 00:12:43.793: INFO: Waiting for all frontend pods to be Running.
Jan 31 00:12:48.845: INFO: Waiting for frontend to serve content.
Jan 31 00:12:49.975: INFO: Trying to add a new entry to the guestbook.
Jan 31 00:12:50.021: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/31/23 00:12:50.091
Jan 31 00:12:50.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:50.316: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:50.316: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/31/23 00:12:50.317
Jan 31 00:12:50.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:50.590: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:50.590: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/31/23 00:12:50.59
Jan 31 00:12:50.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:50.810: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:50.810: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/31/23 00:12:50.811
Jan 31 00:12:50.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:50.974: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:50.974: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/31/23 00:12:50.975
Jan 31 00:12:50.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:51.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:51.185: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/31/23 00:12:51.185
Jan 31 00:12:51.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
Jan 31 00:12:51.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 31 00:12:51.408: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2172" for this suite. 01/31/23 00:12:51.444
------------------------------
• [SLOW TEST] [11.815 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:39.696
    Jan 31 00:12:39.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/31/23 00:12:39.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:39.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:39.772
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/31/23 00:12:39.788
    Jan 31 00:12:39.788: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 31 00:12:39.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:41.076: INFO: stderr: ""
    Jan 31 00:12:41.076: INFO: stdout: "service/agnhost-replica created\n"
    Jan 31 00:12:41.076: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 31 00:12:41.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:41.525: INFO: stderr: ""
    Jan 31 00:12:41.525: INFO: stdout: "service/agnhost-primary created\n"
    Jan 31 00:12:41.525: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 31 00:12:41.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:42.697: INFO: stderr: ""
    Jan 31 00:12:42.697: INFO: stdout: "service/frontend created\n"
    Jan 31 00:12:42.698: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 31 00:12:42.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:43.077: INFO: stderr: ""
    Jan 31 00:12:43.077: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 31 00:12:43.077: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 31 00:12:43.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:43.434: INFO: stderr: ""
    Jan 31 00:12:43.434: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 31 00:12:43.434: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 31 00:12:43.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 create -f -'
    Jan 31 00:12:43.792: INFO: stderr: ""
    Jan 31 00:12:43.792: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/31/23 00:12:43.792
    Jan 31 00:12:43.793: INFO: Waiting for all frontend pods to be Running.
    Jan 31 00:12:48.845: INFO: Waiting for frontend to serve content.
    Jan 31 00:12:49.975: INFO: Trying to add a new entry to the guestbook.
    Jan 31 00:12:50.021: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/31/23 00:12:50.091
    Jan 31 00:12:50.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:50.316: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:50.316: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/31/23 00:12:50.317
    Jan 31 00:12:50.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:50.590: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:50.590: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/31/23 00:12:50.59
    Jan 31 00:12:50.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:50.810: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:50.810: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/31/23 00:12:50.811
    Jan 31 00:12:50.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:50.974: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:50.974: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/31/23 00:12:50.975
    Jan 31 00:12:50.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:51.185: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:51.185: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/31/23 00:12:51.185
    Jan 31 00:12:51.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-2172 delete --grace-period=0 --force -f -'
    Jan 31 00:12:51.408: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 31 00:12:51.408: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2172" for this suite. 01/31/23 00:12:51.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:51.513
Jan 31 00:12:51.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/31/23 00:12:51.513
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:51.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:51.713
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/31/23 00:12:51.732
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:51.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4183" for this suite. 01/31/23 00:12:51.804
------------------------------
• [0.319 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:51.513
    Jan 31 00:12:51.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/31/23 00:12:51.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:51.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:51.713
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/31/23 00:12:51.732
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:51.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4183" for this suite. 01/31/23 00:12:51.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:51.835
Jan 31 00:12:51.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/31/23 00:12:51.836
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:51.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:51.979
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-0bae1c16-7fb2-48a8-b8d2-0a40a185b94b 01/31/23 00:12:52.005
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:52.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-573" for this suite. 01/31/23 00:12:52.057
------------------------------
• [0.266 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:51.835
    Jan 31 00:12:51.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/31/23 00:12:51.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:51.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:51.979
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-0bae1c16-7fb2-48a8-b8d2-0a40a185b94b 01/31/23 00:12:52.005
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:52.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-573" for this suite. 01/31/23 00:12:52.057
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:52.103
Jan 31 00:12:52.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:12:52.105
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:52.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:52.222
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/31/23 00:12:52.239
Jan 31 00:12:52.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce" in namespace "projected-9430" to be "Succeeded or Failed"
Jan 31 00:12:52.353: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Pending", Reason="", readiness=false. Elapsed: 34.043323ms
Jan 31 00:12:54.371: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Running", Reason="", readiness=true. Elapsed: 2.051989761s
Jan 31 00:12:56.380: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Running", Reason="", readiness=false. Elapsed: 4.061882959s
Jan 31 00:12:58.369: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050620656s
STEP: Saw pod success 01/31/23 00:12:58.369
Jan 31 00:12:58.370: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce" satisfied condition "Succeeded or Failed"
Jan 31 00:12:58.385: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce container client-container: <nil>
STEP: delete the pod 01/31/23 00:12:58.44
Jan 31 00:12:58.495: INFO: Waiting for pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce to disappear
Jan 31 00:12:58.512: INFO: Pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:58.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9430" for this suite. 01/31/23 00:12:58.543
------------------------------
• [SLOW TEST] [6.473 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:52.103
    Jan 31 00:12:52.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:12:52.105
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:52.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:52.222
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/31/23 00:12:52.239
    Jan 31 00:12:52.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce" in namespace "projected-9430" to be "Succeeded or Failed"
    Jan 31 00:12:52.353: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Pending", Reason="", readiness=false. Elapsed: 34.043323ms
    Jan 31 00:12:54.371: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Running", Reason="", readiness=true. Elapsed: 2.051989761s
    Jan 31 00:12:56.380: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Running", Reason="", readiness=false. Elapsed: 4.061882959s
    Jan 31 00:12:58.369: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050620656s
    STEP: Saw pod success 01/31/23 00:12:58.369
    Jan 31 00:12:58.370: INFO: Pod "downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce" satisfied condition "Succeeded or Failed"
    Jan 31 00:12:58.385: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce container client-container: <nil>
    STEP: delete the pod 01/31/23 00:12:58.44
    Jan 31 00:12:58.495: INFO: Waiting for pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce to disappear
    Jan 31 00:12:58.512: INFO: Pod downwardapi-volume-1118f922-588c-4e1e-85e4-bf20cb898bce no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:58.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9430" for this suite. 01/31/23 00:12:58.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:58.582
Jan 31 00:12:58.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/31/23 00:12:58.584
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:58.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:58.665
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/31/23 00:12:58.679
STEP: Wait for the Deployment to create new ReplicaSet 01/31/23 00:12:58.705
STEP: delete the deployment 01/31/23 00:12:58.723
STEP: wait for all rs to be garbage collected 01/31/23 00:12:58.782
STEP: expected 0 rs, got 1 rs 01/31/23 00:12:58.89
STEP: expected 0 pods, got 2 pods 01/31/23 00:12:58.921
STEP: Gathering metrics 01/31/23 00:12:59.477
W0131 00:12:59.509678      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 31 00:12:59.509: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5887" for this suite. 01/31/23 00:12:59.531
------------------------------
• [0.974 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:58.582
    Jan 31 00:12:58.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/31/23 00:12:58.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:58.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:58.665
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/31/23 00:12:58.679
    STEP: Wait for the Deployment to create new ReplicaSet 01/31/23 00:12:58.705
    STEP: delete the deployment 01/31/23 00:12:58.723
    STEP: wait for all rs to be garbage collected 01/31/23 00:12:58.782
    STEP: expected 0 rs, got 1 rs 01/31/23 00:12:58.89
    STEP: expected 0 pods, got 2 pods 01/31/23 00:12:58.921
    STEP: Gathering metrics 01/31/23 00:12:59.477
    W0131 00:12:59.509678      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 31 00:12:59.509: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5887" for this suite. 01/31/23 00:12:59.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:59.564
Jan 31 00:12:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename watch 01/31/23 00:12:59.567
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:59.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:59.63
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/31/23 00:12:59.644
STEP: modifying the configmap once 01/31/23 00:12:59.668
STEP: modifying the configmap a second time 01/31/23 00:12:59.713
STEP: deleting the configmap 01/31/23 00:12:59.752
STEP: creating a watch on configmaps from the resource version returned by the first update 01/31/23 00:12:59.783
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/31/23 00:12:59.791
Jan 31 00:12:59.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7011  1942a0de-1ecf-481d-87c6-85aad95d23fc 47815 0 2023-01-31 00:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-31 00:12:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 31 00:12:59.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7011  1942a0de-1ecf-481d-87c6-85aad95d23fc 47817 0 2023-01-31 00:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-31 00:12:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 31 00:12:59.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7011" for this suite. 01/31/23 00:12:59.818
------------------------------
• [0.284 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:59.564
    Jan 31 00:12:59.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename watch 01/31/23 00:12:59.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:59.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:59.63
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/31/23 00:12:59.644
    STEP: modifying the configmap once 01/31/23 00:12:59.668
    STEP: modifying the configmap a second time 01/31/23 00:12:59.713
    STEP: deleting the configmap 01/31/23 00:12:59.752
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/31/23 00:12:59.783
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/31/23 00:12:59.791
    Jan 31 00:12:59.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7011  1942a0de-1ecf-481d-87c6-85aad95d23fc 47815 0 2023-01-31 00:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-31 00:12:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 31 00:12:59.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7011  1942a0de-1ecf-481d-87c6-85aad95d23fc 47817 0 2023-01-31 00:12:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-31 00:12:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:12:59.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7011" for this suite. 01/31/23 00:12:59.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:12:59.856
Jan 31 00:12:59.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:12:59.858
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:59.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:59.93
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/31/23 00:12:59.944
Jan 31 00:12:59.974: INFO: Waiting up to 5m0s for pod "pod-2eb5be16-9958-4495-b396-99adeca13c47" in namespace "emptydir-5661" to be "Succeeded or Failed"
Jan 31 00:12:59.992: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 17.937985ms
Jan 31 00:13:02.018: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043857478s
Jan 31 00:13:04.012: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03786599s
Jan 31 00:13:06.010: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036226877s
STEP: Saw pod success 01/31/23 00:13:06.01
Jan 31 00:13:06.011: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47" satisfied condition "Succeeded or Failed"
Jan 31 00:13:06.029: INFO: Trying to get logs from node 10.15.28.227 pod pod-2eb5be16-9958-4495-b396-99adeca13c47 container test-container: <nil>
STEP: delete the pod 01/31/23 00:13:06.069
Jan 31 00:13:06.114: INFO: Waiting for pod pod-2eb5be16-9958-4495-b396-99adeca13c47 to disappear
Jan 31 00:13:06.131: INFO: Pod pod-2eb5be16-9958-4495-b396-99adeca13c47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:13:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5661" for this suite. 01/31/23 00:13:06.156
------------------------------
• [SLOW TEST] [6.326 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:12:59.856
    Jan 31 00:12:59.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:12:59.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:12:59.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:12:59.93
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/31/23 00:12:59.944
    Jan 31 00:12:59.974: INFO: Waiting up to 5m0s for pod "pod-2eb5be16-9958-4495-b396-99adeca13c47" in namespace "emptydir-5661" to be "Succeeded or Failed"
    Jan 31 00:12:59.992: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 17.937985ms
    Jan 31 00:13:02.018: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043857478s
    Jan 31 00:13:04.012: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03786599s
    Jan 31 00:13:06.010: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036226877s
    STEP: Saw pod success 01/31/23 00:13:06.01
    Jan 31 00:13:06.011: INFO: Pod "pod-2eb5be16-9958-4495-b396-99adeca13c47" satisfied condition "Succeeded or Failed"
    Jan 31 00:13:06.029: INFO: Trying to get logs from node 10.15.28.227 pod pod-2eb5be16-9958-4495-b396-99adeca13c47 container test-container: <nil>
    STEP: delete the pod 01/31/23 00:13:06.069
    Jan 31 00:13:06.114: INFO: Waiting for pod pod-2eb5be16-9958-4495-b396-99adeca13c47 to disappear
    Jan 31 00:13:06.131: INFO: Pod pod-2eb5be16-9958-4495-b396-99adeca13c47 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:13:06.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5661" for this suite. 01/31/23 00:13:06.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:13:06.192
Jan 31 00:13:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:13:06.194
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:06.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:06.276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:13:06.345
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:13:06.519
STEP: Deploying the webhook pod 01/31/23 00:13:06.553
STEP: Wait for the deployment to be ready 01/31/23 00:13:06.601
Jan 31 00:13:06.651: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:13:08.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:13:10.741
STEP: Verifying the service has paired with the endpoint 01/31/23 00:13:10.806
Jan 31 00:13:11.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/31/23 00:13:11.838
STEP: create a pod that should be updated by the webhook 01/31/23 00:13:11.958
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:13:12.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5775" for this suite. 01/31/23 00:13:12.309
STEP: Destroying namespace "webhook-5775-markers" for this suite. 01/31/23 00:13:12.337
------------------------------
• [SLOW TEST] [6.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:13:06.192
    Jan 31 00:13:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:13:06.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:06.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:06.276
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:13:06.345
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:13:06.519
    STEP: Deploying the webhook pod 01/31/23 00:13:06.553
    STEP: Wait for the deployment to be ready 01/31/23 00:13:06.601
    Jan 31 00:13:06.651: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:13:08.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 13, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:13:10.741
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:13:10.806
    Jan 31 00:13:11.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/31/23 00:13:11.838
    STEP: create a pod that should be updated by the webhook 01/31/23 00:13:11.958
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:13:12.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5775" for this suite. 01/31/23 00:13:12.309
    STEP: Destroying namespace "webhook-5775-markers" for this suite. 01/31/23 00:13:12.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:13:12.373
Jan 31 00:13:12.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/31/23 00:13:12.376
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:12.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:12.455
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/31/23 00:13:12.472
STEP: waiting for pod running 01/31/23 00:13:12.507
Jan 31 00:13:12.507: INFO: Waiting up to 2m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239" to be "running"
Jan 31 00:13:12.525: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Pending", Reason="", readiness=false. Elapsed: 17.47366ms
Jan 31 00:13:14.545: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037033711s
Jan 31 00:13:16.546: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Running", Reason="", readiness=true. Elapsed: 4.038327321s
Jan 31 00:13:16.546: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" satisfied condition "running"
STEP: creating a file in subpath 01/31/23 00:13:16.547
Jan 31 00:13:16.568: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7239 PodName:var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:13:16.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:13:16.569: INFO: ExecWithOptions: Clientset creation
Jan 31 00:13:16.569: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7239/pods/var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/31/23 00:13:16.804
Jan 31 00:13:16.824: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7239 PodName:var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:13:16.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:13:16.826: INFO: ExecWithOptions: Clientset creation
Jan 31 00:13:16.827: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7239/pods/var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/31/23 00:13:17.061
Jan 31 00:13:17.601: INFO: Successfully updated pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958"
STEP: waiting for annotated pod running 01/31/23 00:13:17.601
Jan 31 00:13:17.602: INFO: Waiting up to 2m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239" to be "running"
Jan 31 00:13:17.620: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Running", Reason="", readiness=true. Elapsed: 17.881827ms
Jan 31 00:13:17.620: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" satisfied condition "running"
STEP: deleting the pod gracefully 01/31/23 00:13:17.62
Jan 31 00:13:17.621: INFO: Deleting pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239"
Jan 31 00:13:17.682: INFO: Wait up to 5m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 31 00:13:49.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7239" for this suite. 01/31/23 00:13:49.744
------------------------------
• [SLOW TEST] [37.398 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:13:12.373
    Jan 31 00:13:12.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/31/23 00:13:12.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:12.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:12.455
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/31/23 00:13:12.472
    STEP: waiting for pod running 01/31/23 00:13:12.507
    Jan 31 00:13:12.507: INFO: Waiting up to 2m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239" to be "running"
    Jan 31 00:13:12.525: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Pending", Reason="", readiness=false. Elapsed: 17.47366ms
    Jan 31 00:13:14.545: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037033711s
    Jan 31 00:13:16.546: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Running", Reason="", readiness=true. Elapsed: 4.038327321s
    Jan 31 00:13:16.546: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" satisfied condition "running"
    STEP: creating a file in subpath 01/31/23 00:13:16.547
    Jan 31 00:13:16.568: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7239 PodName:var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:13:16.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:13:16.569: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:13:16.569: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7239/pods/var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/31/23 00:13:16.804
    Jan 31 00:13:16.824: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7239 PodName:var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:13:16.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:13:16.826: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:13:16.827: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-7239/pods/var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/31/23 00:13:17.061
    Jan 31 00:13:17.601: INFO: Successfully updated pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958"
    STEP: waiting for annotated pod running 01/31/23 00:13:17.601
    Jan 31 00:13:17.602: INFO: Waiting up to 2m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239" to be "running"
    Jan 31 00:13:17.620: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958": Phase="Running", Reason="", readiness=true. Elapsed: 17.881827ms
    Jan 31 00:13:17.620: INFO: Pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" satisfied condition "running"
    STEP: deleting the pod gracefully 01/31/23 00:13:17.62
    Jan 31 00:13:17.621: INFO: Deleting pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" in namespace "var-expansion-7239"
    Jan 31 00:13:17.682: INFO: Wait up to 5m0s for pod "var-expansion-0a36fb9c-a3e9-4afc-9842-42d7b3cdc958" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:13:49.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7239" for this suite. 01/31/23 00:13:49.744
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:13:49.774
Jan 31 00:13:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/31/23 00:13:49.778
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:49.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:49.847
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/31/23 00:13:49.884
STEP: Creating a ResourceQuota 01/31/23 00:13:54.904
STEP: Ensuring resource quota status is calculated 01/31/23 00:13:54.927
STEP: Creating a Pod that fits quota 01/31/23 00:13:56.949
STEP: Ensuring ResourceQuota status captures the pod usage 01/31/23 00:13:57.009
STEP: Not allowing a pod to be created that exceeds remaining quota 01/31/23 00:13:59.03
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/31/23 00:13:59.046
STEP: Ensuring a pod cannot update its resource requirements 01/31/23 00:13:59.056
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/31/23 00:13:59.088
STEP: Deleting the pod 01/31/23 00:14:01.12
STEP: Ensuring resource quota status released the pod usage 01/31/23 00:14:01.17
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 31 00:14:03.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4052" for this suite. 01/31/23 00:14:03.224
------------------------------
• [SLOW TEST] [13.478 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:13:49.774
    Jan 31 00:13:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/31/23 00:13:49.778
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:13:49.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:13:49.847
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/31/23 00:13:49.884
    STEP: Creating a ResourceQuota 01/31/23 00:13:54.904
    STEP: Ensuring resource quota status is calculated 01/31/23 00:13:54.927
    STEP: Creating a Pod that fits quota 01/31/23 00:13:56.949
    STEP: Ensuring ResourceQuota status captures the pod usage 01/31/23 00:13:57.009
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/31/23 00:13:59.03
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/31/23 00:13:59.046
    STEP: Ensuring a pod cannot update its resource requirements 01/31/23 00:13:59.056
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/31/23 00:13:59.088
    STEP: Deleting the pod 01/31/23 00:14:01.12
    STEP: Ensuring resource quota status released the pod usage 01/31/23 00:14:01.17
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:14:03.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4052" for this suite. 01/31/23 00:14:03.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:14:03.256
Jan 31 00:14:03.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/31/23 00:14:03.258
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:03.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:03.363
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/31/23 00:14:03.378
STEP: Creating a ResourceQuota 01/31/23 00:14:08.398
STEP: Ensuring resource quota status is calculated 01/31/23 00:14:08.421
STEP: Creating a Service 01/31/23 00:14:10.44
STEP: Creating a NodePort Service 01/31/23 00:14:10.525
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/31/23 00:14:10.624
STEP: Ensuring resource quota status captures service creation 01/31/23 00:14:10.752
STEP: Deleting Services 01/31/23 00:14:12.799
STEP: Ensuring resource quota status released usage 01/31/23 00:14:12.96
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 31 00:14:14.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6677" for this suite. 01/31/23 00:14:15.018
------------------------------
• [SLOW TEST] [11.792 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:14:03.256
    Jan 31 00:14:03.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/31/23 00:14:03.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:03.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:03.363
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/31/23 00:14:03.378
    STEP: Creating a ResourceQuota 01/31/23 00:14:08.398
    STEP: Ensuring resource quota status is calculated 01/31/23 00:14:08.421
    STEP: Creating a Service 01/31/23 00:14:10.44
    STEP: Creating a NodePort Service 01/31/23 00:14:10.525
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/31/23 00:14:10.624
    STEP: Ensuring resource quota status captures service creation 01/31/23 00:14:10.752
    STEP: Deleting Services 01/31/23 00:14:12.799
    STEP: Ensuring resource quota status released usage 01/31/23 00:14:12.96
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:14:14.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6677" for this suite. 01/31/23 00:14:15.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:14:15.05
Jan 31 00:14:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubelet-test 01/31/23 00:14:15.052
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:15.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:15.151
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 31 00:14:15.206: INFO: Waiting up to 5m0s for pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b" in namespace "kubelet-test-2183" to be "running and ready"
Jan 31 00:14:15.226: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.330659ms
Jan 31 00:14:15.226: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:14:17.245: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038844535s
Jan 31 00:14:17.245: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:14:19.249: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.042247075s
Jan 31 00:14:19.249: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Running (Ready = true)
Jan 31 00:14:19.249: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:14:19.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2183" for this suite. 01/31/23 00:14:19.341
------------------------------
• [4.318 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:14:15.05
    Jan 31 00:14:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubelet-test 01/31/23 00:14:15.052
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:15.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:15.151
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 31 00:14:15.206: INFO: Waiting up to 5m0s for pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b" in namespace "kubelet-test-2183" to be "running and ready"
    Jan 31 00:14:15.226: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.330659ms
    Jan 31 00:14:15.226: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:14:17.245: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038844535s
    Jan 31 00:14:17.245: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:14:19.249: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b": Phase="Running", Reason="", readiness=true. Elapsed: 4.042247075s
    Jan 31 00:14:19.249: INFO: The phase of Pod busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b is Running (Ready = true)
    Jan 31 00:14:19.249: INFO: Pod "busybox-scheduling-89eb6dcd-fb7a-44f0-89a2-a6f52c3ddd0b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:14:19.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2183" for this suite. 01/31/23 00:14:19.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:14:19.372
Jan 31 00:14:19.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/31/23 00:14:19.375
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:19.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:19.456
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3770 01/31/23 00:14:19.47
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/31/23 00:14:19.493
STEP: Creating pod with conflicting port in namespace statefulset-3770 01/31/23 00:14:19.545
STEP: Waiting until pod test-pod will start running in namespace statefulset-3770 01/31/23 00:14:19.581
Jan 31 00:14:19.581: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3770" to be "running"
Jan 31 00:14:19.601: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.509715ms
Jan 31 00:14:21.619: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037593377s
Jan 31 00:14:21.619: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3770 01/31/23 00:14:21.619
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3770 01/31/23 00:14:21.641
Jan 31 00:14:21.673: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Pending. Waiting for statefulset controller to delete.
Jan 31 00:14:21.719: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Failed. Waiting for statefulset controller to delete.
Jan 31 00:14:21.764: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Failed. Waiting for statefulset controller to delete.
Jan 31 00:14:21.777: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3770
STEP: Removing pod with conflicting port in namespace statefulset-3770 01/31/23 00:14:21.777
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3770 and will be in running state 01/31/23 00:14:21.825
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 31 00:14:25.886: INFO: Deleting all statefulset in ns statefulset-3770
Jan 31 00:14:25.902: INFO: Scaling statefulset ss to 0
Jan 31 00:14:35.990: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 00:14:36.007: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:14:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3770" for this suite. 01/31/23 00:14:36.134
------------------------------
• [SLOW TEST] [16.790 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:14:19.372
    Jan 31 00:14:19.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/31/23 00:14:19.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:19.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:19.456
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3770 01/31/23 00:14:19.47
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/31/23 00:14:19.493
    STEP: Creating pod with conflicting port in namespace statefulset-3770 01/31/23 00:14:19.545
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3770 01/31/23 00:14:19.581
    Jan 31 00:14:19.581: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3770" to be "running"
    Jan 31 00:14:19.601: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.509715ms
    Jan 31 00:14:21.619: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037593377s
    Jan 31 00:14:21.619: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3770 01/31/23 00:14:21.619
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3770 01/31/23 00:14:21.641
    Jan 31 00:14:21.673: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 31 00:14:21.719: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 31 00:14:21.764: INFO: Observed stateful pod in namespace: statefulset-3770, name: ss-0, uid: 48d2d3a0-b7e4-4f0c-b20c-d8f3252ead57, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 31 00:14:21.777: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3770
    STEP: Removing pod with conflicting port in namespace statefulset-3770 01/31/23 00:14:21.777
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3770 and will be in running state 01/31/23 00:14:21.825
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 31 00:14:25.886: INFO: Deleting all statefulset in ns statefulset-3770
    Jan 31 00:14:25.902: INFO: Scaling statefulset ss to 0
    Jan 31 00:14:35.990: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 31 00:14:36.007: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:14:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3770" for this suite. 01/31/23 00:14:36.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:14:36.183
Jan 31 00:14:36.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubectl 01/31/23 00:14:36.185
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:36.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:36.252
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/31/23 00:14:36.28
Jan 31 00:14:36.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8776 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 31 00:14:36.456: INFO: stderr: ""
Jan 31 00:14:36.456: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/31/23 00:14:36.456
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 31 00:14:36.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8776 delete pods e2e-test-httpd-pod'
Jan 31 00:14:39.694: INFO: stderr: ""
Jan 31 00:14:39.694: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 31 00:14:39.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8776" for this suite. 01/31/23 00:14:39.746
------------------------------
• [3.592 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:14:36.183
    Jan 31 00:14:36.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubectl 01/31/23 00:14:36.185
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:36.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:36.252
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/31/23 00:14:36.28
    Jan 31 00:14:36.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8776 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 31 00:14:36.456: INFO: stderr: ""
    Jan 31 00:14:36.456: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/31/23 00:14:36.456
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 31 00:14:36.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=kubectl-8776 delete pods e2e-test-httpd-pod'
    Jan 31 00:14:39.694: INFO: stderr: ""
    Jan 31 00:14:39.694: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:14:39.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8776" for this suite. 01/31/23 00:14:39.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:14:39.782
Jan 31 00:14:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pod-network-test 01/31/23 00:14:39.784
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:39.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:39.877
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6825 01/31/23 00:14:39.897
STEP: creating a selector 01/31/23 00:14:39.898
STEP: Creating the service pods in kubernetes 01/31/23 00:14:39.898
Jan 31 00:14:39.898: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 31 00:14:40.043: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6825" to be "running and ready"
Jan 31 00:14:40.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.382029ms
Jan 31 00:14:40.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:14:42.094: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050329777s
Jan 31 00:14:42.094: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:14:44.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.051762663s
Jan 31 00:14:44.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:46.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.051137111s
Jan 31 00:14:46.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:48.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.053202437s
Jan 31 00:14:48.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:50.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.057671249s
Jan 31 00:14:50.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:52.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.05131185s
Jan 31 00:14:52.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:54.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053064263s
Jan 31 00:14:54.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:56.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049630237s
Jan 31 00:14:56.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:14:58.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049909746s
Jan 31 00:14:58.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:15:00.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052187321s
Jan 31 00:15:00.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 31 00:15:02.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05045223s
Jan 31 00:15:02.094: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 31 00:15:02.095: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 31 00:15:02.117: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6825" to be "running and ready"
Jan 31 00:15:02.136: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.59719ms
Jan 31 00:15:02.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 31 00:15:02.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 31 00:15:02.155: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6825" to be "running and ready"
Jan 31 00:15:02.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 23.783046ms
Jan 31 00:15:02.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 31 00:15:02.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/31/23 00:15:02.206
Jan 31 00:15:02.266: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6825" to be "running"
Jan 31 00:15:02.296: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.810362ms
Jan 31 00:15:04.326: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05985797s
Jan 31 00:15:06.321: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.055549371s
Jan 31 00:15:06.321: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 31 00:15:06.340: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6825" to be "running"
Jan 31 00:15:06.358: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.428383ms
Jan 31 00:15:06.358: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 31 00:15:06.376: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 31 00:15:06.376: INFO: Going to poll 172.30.237.159 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 31 00:15:06.406: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.237.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:15:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:15:06.408: INFO: ExecWithOptions: Clientset creation
Jan 31 00:15:06.408: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.237.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 00:15:07.686: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 31 00:15:07.686: INFO: Going to poll 172.30.199.12 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 31 00:15:07.712: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.199.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:15:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:15:07.713: INFO: ExecWithOptions: Clientset creation
Jan 31 00:15:07.713: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.199.12+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 00:15:09.016: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 31 00:15:09.016: INFO: Going to poll 172.30.248.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 31 00:15:09.056: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.248.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:15:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:15:09.057: INFO: ExecWithOptions: Clientset creation
Jan 31 00:15:09.057: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.248.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 00:15:10.313: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 31 00:15:10.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6825" for this suite. 01/31/23 00:15:10.34
------------------------------
• [SLOW TEST] [30.590 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:14:39.782
    Jan 31 00:14:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pod-network-test 01/31/23 00:14:39.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:14:39.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:14:39.877
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6825 01/31/23 00:14:39.897
    STEP: creating a selector 01/31/23 00:14:39.898
    STEP: Creating the service pods in kubernetes 01/31/23 00:14:39.898
    Jan 31 00:14:39.898: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 31 00:14:40.043: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6825" to be "running and ready"
    Jan 31 00:14:40.076: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.382029ms
    Jan 31 00:14:40.076: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:14:42.094: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050329777s
    Jan 31 00:14:42.094: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:14:44.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.051762663s
    Jan 31 00:14:44.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:46.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.051137111s
    Jan 31 00:14:46.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:48.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.053202437s
    Jan 31 00:14:48.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:50.101: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.057671249s
    Jan 31 00:14:50.101: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:52.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.05131185s
    Jan 31 00:14:52.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:54.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053064263s
    Jan 31 00:14:54.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:56.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049630237s
    Jan 31 00:14:56.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:14:58.093: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049909746s
    Jan 31 00:14:58.093: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:15:00.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.052187321s
    Jan 31 00:15:00.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 31 00:15:02.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.05045223s
    Jan 31 00:15:02.094: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 31 00:15:02.095: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 31 00:15:02.117: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6825" to be "running and ready"
    Jan 31 00:15:02.136: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 18.59719ms
    Jan 31 00:15:02.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 31 00:15:02.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 31 00:15:02.155: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6825" to be "running and ready"
    Jan 31 00:15:02.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 23.783046ms
    Jan 31 00:15:02.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 31 00:15:02.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/31/23 00:15:02.206
    Jan 31 00:15:02.266: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6825" to be "running"
    Jan 31 00:15:02.296: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 29.810362ms
    Jan 31 00:15:04.326: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05985797s
    Jan 31 00:15:06.321: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.055549371s
    Jan 31 00:15:06.321: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 31 00:15:06.340: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6825" to be "running"
    Jan 31 00:15:06.358: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.428383ms
    Jan 31 00:15:06.358: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 31 00:15:06.376: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 31 00:15:06.376: INFO: Going to poll 172.30.237.159 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 31 00:15:06.406: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.237.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:15:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:15:06.408: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:15:06.408: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.237.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 31 00:15:07.686: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 31 00:15:07.686: INFO: Going to poll 172.30.199.12 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 31 00:15:07.712: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.199.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:15:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:15:07.713: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:15:07.713: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.199.12+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 31 00:15:09.016: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 31 00:15:09.016: INFO: Going to poll 172.30.248.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 31 00:15:09.056: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.248.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6825 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:15:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:15:09.057: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:15:09.057: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6825/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.248.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 31 00:15:10.313: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:15:10.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6825" for this suite. 01/31/23 00:15:10.34
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:15:10.379
Jan 31 00:15:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename services 01/31/23 00:15:10.381
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:15:10.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:15:10.452
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-3562 01/31/23 00:15:10.474
STEP: creating service affinity-clusterip in namespace services-3562 01/31/23 00:15:10.475
STEP: creating replication controller affinity-clusterip in namespace services-3562 01/31/23 00:15:10.53
I0131 00:15:10.560439      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3562, replica count: 3
I0131 00:15:13.611759      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 00:15:13.642: INFO: Creating new exec pod
Jan 31 00:15:13.666: INFO: Waiting up to 5m0s for pod "execpod-affinitydqhjv" in namespace "services-3562" to be "running"
Jan 31 00:15:13.685: INFO: Pod "execpod-affinitydqhjv": Phase="Pending", Reason="", readiness=false. Elapsed: 18.687378ms
Jan 31 00:15:15.723: INFO: Pod "execpod-affinitydqhjv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056047822s
Jan 31 00:15:17.707: INFO: Pod "execpod-affinitydqhjv": Phase="Running", Reason="", readiness=true. Elapsed: 4.040710301s
Jan 31 00:15:17.707: INFO: Pod "execpod-affinitydqhjv" satisfied condition "running"
Jan 31 00:15:18.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 31 00:15:19.171: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 31 00:15:19.171: INFO: stdout: ""
Jan 31 00:15:19.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c nc -v -z -w 2 172.21.241.181 80'
Jan 31 00:15:19.571: INFO: stderr: "+ nc -v -z -w 2 172.21.241.181 80\nConnection to 172.21.241.181 80 port [tcp/http] succeeded!\n"
Jan 31 00:15:19.571: INFO: stdout: ""
Jan 31 00:15:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.241.181:80/ ; done'
Jan 31 00:15:20.116: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n"
Jan 31 00:15:20.117: INFO: stdout: "\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p"
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
Jan 31 00:15:20.118: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3562, will wait for the garbage collector to delete the pods 01/31/23 00:15:20.215
Jan 31 00:15:20.340: INFO: Deleting ReplicationController affinity-clusterip took: 28.228262ms
Jan 31 00:15:20.641: INFO: Terminating ReplicationController affinity-clusterip pods took: 300.991795ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 31 00:15:23.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3562" for this suite. 01/31/23 00:15:23.259
------------------------------
• [SLOW TEST] [12.916 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:15:10.379
    Jan 31 00:15:10.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename services 01/31/23 00:15:10.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:15:10.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:15:10.452
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-3562 01/31/23 00:15:10.474
    STEP: creating service affinity-clusterip in namespace services-3562 01/31/23 00:15:10.475
    STEP: creating replication controller affinity-clusterip in namespace services-3562 01/31/23 00:15:10.53
    I0131 00:15:10.560439      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3562, replica count: 3
    I0131 00:15:13.611759      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 31 00:15:13.642: INFO: Creating new exec pod
    Jan 31 00:15:13.666: INFO: Waiting up to 5m0s for pod "execpod-affinitydqhjv" in namespace "services-3562" to be "running"
    Jan 31 00:15:13.685: INFO: Pod "execpod-affinitydqhjv": Phase="Pending", Reason="", readiness=false. Elapsed: 18.687378ms
    Jan 31 00:15:15.723: INFO: Pod "execpod-affinitydqhjv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056047822s
    Jan 31 00:15:17.707: INFO: Pod "execpod-affinitydqhjv": Phase="Running", Reason="", readiness=true. Elapsed: 4.040710301s
    Jan 31 00:15:17.707: INFO: Pod "execpod-affinitydqhjv" satisfied condition "running"
    Jan 31 00:15:18.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 31 00:15:19.171: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 31 00:15:19.171: INFO: stdout: ""
    Jan 31 00:15:19.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c nc -v -z -w 2 172.21.241.181 80'
    Jan 31 00:15:19.571: INFO: stderr: "+ nc -v -z -w 2 172.21.241.181 80\nConnection to 172.21.241.181 80 port [tcp/http] succeeded!\n"
    Jan 31 00:15:19.571: INFO: stdout: ""
    Jan 31 00:15:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=services-3562 exec execpod-affinitydqhjv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.241.181:80/ ; done'
    Jan 31 00:15:20.116: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.241.181:80/\n"
    Jan 31 00:15:20.117: INFO: stdout: "\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p\naffinity-clusterip-5sf8p"
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.117: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.118: INFO: Received response from host: affinity-clusterip-5sf8p
    Jan 31 00:15:20.118: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3562, will wait for the garbage collector to delete the pods 01/31/23 00:15:20.215
    Jan 31 00:15:20.340: INFO: Deleting ReplicationController affinity-clusterip took: 28.228262ms
    Jan 31 00:15:20.641: INFO: Terminating ReplicationController affinity-clusterip pods took: 300.991795ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:15:23.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3562" for this suite. 01/31/23 00:15:23.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:15:23.301
Jan 31 00:15:23.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename container-probe 01/31/23 00:15:23.303
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:15:23.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:15:23.37
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 in namespace container-probe-10 01/31/23 00:15:23.384
Jan 31 00:15:23.447: INFO: Waiting up to 5m0s for pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1" in namespace "container-probe-10" to be "not pending"
Jan 31 00:15:23.516: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Pending", Reason="", readiness=false. Elapsed: 69.056149ms
Jan 31 00:15:25.533: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085661627s
Jan 31 00:15:27.561: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Running", Reason="", readiness=true. Elapsed: 4.113914913s
Jan 31 00:15:27.562: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1" satisfied condition "not pending"
Jan 31 00:15:27.563: INFO: Started pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 in namespace container-probe-10
STEP: checking the pod's current state and verifying that restartCount is present 01/31/23 00:15:27.563
Jan 31 00:15:27.582: INFO: Initial restart count of pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 is 0
STEP: deleting the pod 01/31/23 00:19:28.184
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 31 00:19:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-10" for this suite. 01/31/23 00:19:28.274
------------------------------
• [SLOW TEST] [245.000 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:15:23.301
    Jan 31 00:15:23.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename container-probe 01/31/23 00:15:23.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:15:23.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:15:23.37
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 in namespace container-probe-10 01/31/23 00:15:23.384
    Jan 31 00:15:23.447: INFO: Waiting up to 5m0s for pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1" in namespace "container-probe-10" to be "not pending"
    Jan 31 00:15:23.516: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Pending", Reason="", readiness=false. Elapsed: 69.056149ms
    Jan 31 00:15:25.533: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085661627s
    Jan 31 00:15:27.561: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1": Phase="Running", Reason="", readiness=true. Elapsed: 4.113914913s
    Jan 31 00:15:27.562: INFO: Pod "liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1" satisfied condition "not pending"
    Jan 31 00:15:27.563: INFO: Started pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 in namespace container-probe-10
    STEP: checking the pod's current state and verifying that restartCount is present 01/31/23 00:15:27.563
    Jan 31 00:15:27.582: INFO: Initial restart count of pod liveness-b5b89b49-cdf8-4374-a058-9b11b35782d1 is 0
    STEP: deleting the pod 01/31/23 00:19:28.184
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:19:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-10" for this suite. 01/31/23 00:19:28.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:19:28.307
Jan 31 00:19:28.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/31/23 00:19:28.309
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:28.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:28.414
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/31/23 00:19:28.449
STEP: waiting for RC to be added 01/31/23 00:19:28.47
STEP: waiting for available Replicas 01/31/23 00:19:28.47
STEP: patching ReplicationController 01/31/23 00:19:30.984
STEP: waiting for RC to be modified 01/31/23 00:19:31.013
STEP: patching ReplicationController status 01/31/23 00:19:31.014
STEP: waiting for RC to be modified 01/31/23 00:19:31.039
STEP: waiting for available Replicas 01/31/23 00:19:31.039
STEP: fetching ReplicationController status 01/31/23 00:19:31.055
STEP: patching ReplicationController scale 01/31/23 00:19:31.073
STEP: waiting for RC to be modified 01/31/23 00:19:31.099
STEP: waiting for ReplicationController's scale to be the max amount 01/31/23 00:19:31.099
STEP: fetching ReplicationController; ensuring that it's patched 01/31/23 00:19:33.936
STEP: updating ReplicationController status 01/31/23 00:19:33.956
STEP: waiting for RC to be modified 01/31/23 00:19:33.977
STEP: listing all ReplicationControllers 01/31/23 00:19:33.978
STEP: checking that ReplicationController has expected values 01/31/23 00:19:33.999
STEP: deleting ReplicationControllers by collection 01/31/23 00:19:33.999
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/31/23 00:19:34.037
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 31 00:19:34.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5124" for this suite. 01/31/23 00:19:34.219
------------------------------
• [SLOW TEST] [5.941 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:19:28.307
    Jan 31 00:19:28.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/31/23 00:19:28.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:28.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:28.414
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/31/23 00:19:28.449
    STEP: waiting for RC to be added 01/31/23 00:19:28.47
    STEP: waiting for available Replicas 01/31/23 00:19:28.47
    STEP: patching ReplicationController 01/31/23 00:19:30.984
    STEP: waiting for RC to be modified 01/31/23 00:19:31.013
    STEP: patching ReplicationController status 01/31/23 00:19:31.014
    STEP: waiting for RC to be modified 01/31/23 00:19:31.039
    STEP: waiting for available Replicas 01/31/23 00:19:31.039
    STEP: fetching ReplicationController status 01/31/23 00:19:31.055
    STEP: patching ReplicationController scale 01/31/23 00:19:31.073
    STEP: waiting for RC to be modified 01/31/23 00:19:31.099
    STEP: waiting for ReplicationController's scale to be the max amount 01/31/23 00:19:31.099
    STEP: fetching ReplicationController; ensuring that it's patched 01/31/23 00:19:33.936
    STEP: updating ReplicationController status 01/31/23 00:19:33.956
    STEP: waiting for RC to be modified 01/31/23 00:19:33.977
    STEP: listing all ReplicationControllers 01/31/23 00:19:33.978
    STEP: checking that ReplicationController has expected values 01/31/23 00:19:33.999
    STEP: deleting ReplicationControllers by collection 01/31/23 00:19:33.999
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/31/23 00:19:34.037
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:19:34.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5124" for this suite. 01/31/23 00:19:34.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:19:34.258
Jan 31 00:19:34.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/31/23 00:19:34.261
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:34.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:34.328
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2867 01/31/23 00:19:34.346
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2867 01/31/23 00:19:34.367
Jan 31 00:19:34.415: INFO: Found 0 stateful pods, waiting for 1
Jan 31 00:19:44.437: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/31/23 00:19:44.472
STEP: updating a scale subresource 01/31/23 00:19:44.49
STEP: verifying the statefulset Spec.Replicas was modified 01/31/23 00:19:44.513
STEP: Patch a scale subresource 01/31/23 00:19:44.531
STEP: verifying the statefulset Spec.Replicas was modified 01/31/23 00:19:44.577
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 31 00:19:44.606: INFO: Deleting all statefulset in ns statefulset-2867
Jan 31 00:19:44.624: INFO: Scaling statefulset ss to 0
Jan 31 00:19:54.719: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 00:19:54.753: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:19:54.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2867" for this suite. 01/31/23 00:19:54.864
------------------------------
• [SLOW TEST] [20.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:19:34.258
    Jan 31 00:19:34.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/31/23 00:19:34.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:34.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:34.328
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2867 01/31/23 00:19:34.346
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2867 01/31/23 00:19:34.367
    Jan 31 00:19:34.415: INFO: Found 0 stateful pods, waiting for 1
    Jan 31 00:19:44.437: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/31/23 00:19:44.472
    STEP: updating a scale subresource 01/31/23 00:19:44.49
    STEP: verifying the statefulset Spec.Replicas was modified 01/31/23 00:19:44.513
    STEP: Patch a scale subresource 01/31/23 00:19:44.531
    STEP: verifying the statefulset Spec.Replicas was modified 01/31/23 00:19:44.577
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 31 00:19:44.606: INFO: Deleting all statefulset in ns statefulset-2867
    Jan 31 00:19:44.624: INFO: Scaling statefulset ss to 0
    Jan 31 00:19:54.719: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 31 00:19:54.753: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:19:54.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2867" for this suite. 01/31/23 00:19:54.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:19:54.892
Jan 31 00:19:54.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename gc 01/31/23 00:19:54.895
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:54.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:55.002
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/31/23 00:19:55.02
STEP: Wait for the Deployment to create new ReplicaSet 01/31/23 00:19:55.04
STEP: delete the deployment 01/31/23 00:19:55.589
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/31/23 00:19:55.615
STEP: Gathering metrics 01/31/23 00:19:56.232
W0131 00:19:56.304306      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 31 00:19:56.304: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 31 00:19:56.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6802" for this suite. 01/31/23 00:19:56.327
------------------------------
• [1.465 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:19:54.892
    Jan 31 00:19:54.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename gc 01/31/23 00:19:54.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:54.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:55.002
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/31/23 00:19:55.02
    STEP: Wait for the Deployment to create new ReplicaSet 01/31/23 00:19:55.04
    STEP: delete the deployment 01/31/23 00:19:55.589
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/31/23 00:19:55.615
    STEP: Gathering metrics 01/31/23 00:19:56.232
    W0131 00:19:56.304306      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 31 00:19:56.304: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:19:56.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6802" for this suite. 01/31/23 00:19:56.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:19:56.36
Jan 31 00:19:56.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/31/23 00:19:56.364
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:56.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:56.447
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-1edf70e3-26d3-42b4-bd9e-a6b17fe0eb11 01/31/23 00:19:56.459
STEP: Creating a pod to test consume secrets 01/31/23 00:19:56.476
Jan 31 00:19:56.510: INFO: Waiting up to 5m0s for pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b" in namespace "secrets-11" to be "Succeeded or Failed"
Jan 31 00:19:56.554: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 43.893806ms
Jan 31 00:19:58.572: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061529341s
Jan 31 00:20:00.571: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060600819s
Jan 31 00:20:02.580: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069929963s
STEP: Saw pod success 01/31/23 00:20:02.581
Jan 31 00:20:02.581: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b" satisfied condition "Succeeded or Failed"
Jan 31 00:20:02.618: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b container secret-volume-test: <nil>
STEP: delete the pod 01/31/23 00:20:02.762
Jan 31 00:20:02.872: INFO: Waiting for pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b to disappear
Jan 31 00:20:02.911: INFO: Pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 31 00:20:02.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-11" for this suite. 01/31/23 00:20:02.93
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:19:56.36
    Jan 31 00:19:56.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/31/23 00:19:56.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:19:56.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:19:56.447
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-1edf70e3-26d3-42b4-bd9e-a6b17fe0eb11 01/31/23 00:19:56.459
    STEP: Creating a pod to test consume secrets 01/31/23 00:19:56.476
    Jan 31 00:19:56.510: INFO: Waiting up to 5m0s for pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b" in namespace "secrets-11" to be "Succeeded or Failed"
    Jan 31 00:19:56.554: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 43.893806ms
    Jan 31 00:19:58.572: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061529341s
    Jan 31 00:20:00.571: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060600819s
    Jan 31 00:20:02.580: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069929963s
    STEP: Saw pod success 01/31/23 00:20:02.581
    Jan 31 00:20:02.581: INFO: Pod "pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b" satisfied condition "Succeeded or Failed"
    Jan 31 00:20:02.618: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b container secret-volume-test: <nil>
    STEP: delete the pod 01/31/23 00:20:02.762
    Jan 31 00:20:02.872: INFO: Waiting for pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b to disappear
    Jan 31 00:20:02.911: INFO: Pod pod-secrets-a95cb85c-18d4-4cf6-907d-51138517f14b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:20:02.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-11" for this suite. 01/31/23 00:20:02.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:20:02.959
Jan 31 00:20:02.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/31/23 00:20:02.962
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:03.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:03.051
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 31 00:20:03.063: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 31 00:20:03.100: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 31 00:20:08.118: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/31/23 00:20:08.118
Jan 31 00:20:08.119: INFO: Creating deployment "test-rolling-update-deployment"
Jan 31 00:20:08.173: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 31 00:20:08.202: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 31 00:20:10.245: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 31 00:20:10.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 31 00:20:12.274: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 00:20:12.322: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4679  70c1b67c-1678-4488-b772-71b07031493f 49349 1 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003e5e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-31 00:20:08 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-31 00:20:10 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 31 00:20:12.339: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4679  bae4e776-1011-4482-91fd-a1e372385f34 49339 1 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 70c1b67c-1678-4488-b772-71b07031493f 0xc0037f0507 0xc0037f0508}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70c1b67c-1678-4488-b772-71b07031493f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f05b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 31 00:20:12.339: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 31 00:20:12.339: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4679  76e1ac5c-2148-4e55-8fb3-aca088305e20 49348 2 2023-01-31 00:20:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 70c1b67c-1678-4488-b772-71b07031493f 0xc0037f03b7 0xc0037f03b8}] [] [{e2e.test Update apps/v1 2023-01-31 00:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70c1b67c-1678-4488-b772-71b07031493f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037f0498 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 00:20:12.353: INFO: Pod "test-rolling-update-deployment-7549d9f46d-95x9m" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-95x9m test-rolling-update-deployment-7549d9f46d- deployment-4679  a057a13d-c1b4-49ff-8f07-84518857515a 49338 0 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:a84d1269a69f6a75604690f227008b8c03ef6b0b781394dadb3284010f6366df cni.projectcalico.org/podIP:172.30.199.28/32 cni.projectcalico.org/podIPs:172.30.199.28/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d bae4e776-1011-4482-91fd-a1e372385f34 0xc0037f0a37 0xc0037f0a38}] [] [{kube-controller-manager Update v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bae4e776-1011-4482-91fd-a1e372385f34\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:20:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6phb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6phb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.28,StartTime:2023-01-31 00:20:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:20:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1917f31abdca06df159cf2093800d443b45882865b618fdebeb9e591f8c0005e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 31 00:20:12.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4679" for this suite. 01/31/23 00:20:12.372
------------------------------
• [SLOW TEST] [9.439 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:20:02.959
    Jan 31 00:20:02.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/31/23 00:20:02.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:03.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:03.051
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 31 00:20:03.063: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 31 00:20:03.100: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 31 00:20:08.118: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/31/23 00:20:08.118
    Jan 31 00:20:08.119: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 31 00:20:08.173: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 31 00:20:08.202: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 31 00:20:10.245: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 31 00:20:10.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 20, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 31 00:20:12.274: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 31 00:20:12.322: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4679  70c1b67c-1678-4488-b772-71b07031493f 49349 1 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003e5e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-31 00:20:08 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-31 00:20:10 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 31 00:20:12.339: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4679  bae4e776-1011-4482-91fd-a1e372385f34 49339 1 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 70c1b67c-1678-4488-b772-71b07031493f 0xc0037f0507 0xc0037f0508}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70c1b67c-1678-4488-b772-71b07031493f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f05b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 31 00:20:12.339: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 31 00:20:12.339: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4679  76e1ac5c-2148-4e55-8fb3-aca088305e20 49348 2 2023-01-31 00:20:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 70c1b67c-1678-4488-b772-71b07031493f 0xc0037f03b7 0xc0037f03b8}] [] [{e2e.test Update apps/v1 2023-01-31 00:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70c1b67c-1678-4488-b772-71b07031493f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037f0498 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 31 00:20:12.353: INFO: Pod "test-rolling-update-deployment-7549d9f46d-95x9m" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-95x9m test-rolling-update-deployment-7549d9f46d- deployment-4679  a057a13d-c1b4-49ff-8f07-84518857515a 49338 0 2023-01-31 00:20:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:a84d1269a69f6a75604690f227008b8c03ef6b0b781394dadb3284010f6366df cni.projectcalico.org/podIP:172.30.199.28/32 cni.projectcalico.org/podIPs:172.30.199.28/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d bae4e776-1011-4482-91fd-a1e372385f34 0xc0037f0a37 0xc0037f0a38}] [] [{kube-controller-manager Update v1 2023-01-31 00:20:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bae4e776-1011-4482-91fd-a1e372385f34\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:20:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:20:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6phb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6phb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:20:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.28,StartTime:2023-01-31 00:20:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:20:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1917f31abdca06df159cf2093800d443b45882865b618fdebeb9e591f8c0005e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:20:12.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4679" for this suite. 01/31/23 00:20:12.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:20:12.405
Jan 31 00:20:12.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svc-latency 01/31/23 00:20:12.406
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:12.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:12.477
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 31 00:20:12.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8946 01/31/23 00:20:12.492
I0131 00:20:12.520442      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8946, replica count: 1
I0131 00:20:13.572249      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0131 00:20:14.573161      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0131 00:20:15.573819      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 31 00:20:15.729: INFO: Created: latency-svc-vkwm6
Jan 31 00:20:15.739: INFO: Got endpoints: latency-svc-vkwm6 [63.6411ms]
Jan 31 00:20:15.794: INFO: Created: latency-svc-ttdv2
Jan 31 00:20:15.808: INFO: Got endpoints: latency-svc-ttdv2 [68.688361ms]
Jan 31 00:20:15.822: INFO: Created: latency-svc-wtlwd
Jan 31 00:20:15.832: INFO: Got endpoints: latency-svc-wtlwd [93.038302ms]
Jan 31 00:20:15.856: INFO: Created: latency-svc-dfh9m
Jan 31 00:20:15.873: INFO: Got endpoints: latency-svc-dfh9m [133.562881ms]
Jan 31 00:20:15.888: INFO: Created: latency-svc-w46t6
Jan 31 00:20:15.898: INFO: Got endpoints: latency-svc-w46t6 [157.947293ms]
Jan 31 00:20:15.920: INFO: Created: latency-svc-8p7px
Jan 31 00:20:15.932: INFO: Got endpoints: latency-svc-8p7px [192.432155ms]
Jan 31 00:20:15.952: INFO: Created: latency-svc-c7fxf
Jan 31 00:20:15.965: INFO: Got endpoints: latency-svc-c7fxf [224.629009ms]
Jan 31 00:20:16.030: INFO: Created: latency-svc-tlj5s
Jan 31 00:20:16.042: INFO: Got endpoints: latency-svc-tlj5s [301.43912ms]
Jan 31 00:20:16.085: INFO: Created: latency-svc-bc9ln
Jan 31 00:20:16.098: INFO: Got endpoints: latency-svc-bc9ln [358.04801ms]
Jan 31 00:20:16.101: INFO: Created: latency-svc-l472t
Jan 31 00:20:16.118: INFO: Got endpoints: latency-svc-l472t [378.216465ms]
Jan 31 00:20:16.128: INFO: Created: latency-svc-kwpxs
Jan 31 00:20:16.140: INFO: Got endpoints: latency-svc-kwpxs [399.303244ms]
Jan 31 00:20:16.156: INFO: Created: latency-svc-njfv2
Jan 31 00:20:16.195: INFO: Got endpoints: latency-svc-njfv2 [454.341579ms]
Jan 31 00:20:16.205: INFO: Created: latency-svc-4jv96
Jan 31 00:20:16.222: INFO: Got endpoints: latency-svc-4jv96 [482.337643ms]
Jan 31 00:20:16.264: INFO: Created: latency-svc-99kgg
Jan 31 00:20:16.288: INFO: Got endpoints: latency-svc-99kgg [547.896174ms]
Jan 31 00:20:16.304: INFO: Created: latency-svc-b8zfs
Jan 31 00:20:16.318: INFO: Got endpoints: latency-svc-b8zfs [578.19825ms]
Jan 31 00:20:16.356: INFO: Created: latency-svc-rml8d
Jan 31 00:20:16.370: INFO: Created: latency-svc-4mwlr
Jan 31 00:20:16.370: INFO: Got endpoints: latency-svc-rml8d [630.410755ms]
Jan 31 00:20:16.387: INFO: Got endpoints: latency-svc-4mwlr [579.110239ms]
Jan 31 00:20:16.419: INFO: Created: latency-svc-lzfhv
Jan 31 00:20:16.440: INFO: Got endpoints: latency-svc-lzfhv [607.969611ms]
Jan 31 00:20:16.449: INFO: Created: latency-svc-6w2rm
Jan 31 00:20:16.463: INFO: Got endpoints: latency-svc-6w2rm [590.016798ms]
Jan 31 00:20:16.485: INFO: Created: latency-svc-qcq5z
Jan 31 00:20:16.506: INFO: Got endpoints: latency-svc-qcq5z [607.902997ms]
Jan 31 00:20:16.518: INFO: Created: latency-svc-7fttf
Jan 31 00:20:16.560: INFO: Got endpoints: latency-svc-7fttf [626.954785ms]
Jan 31 00:20:16.575: INFO: Created: latency-svc-ljq26
Jan 31 00:20:16.578: INFO: Created: latency-svc-rwztx
Jan 31 00:20:16.591: INFO: Got endpoints: latency-svc-ljq26 [625.870519ms]
Jan 31 00:20:16.598: INFO: Got endpoints: latency-svc-rwztx [556.609024ms]
Jan 31 00:20:16.639: INFO: Created: latency-svc-vh7dp
Jan 31 00:20:16.651: INFO: Got endpoints: latency-svc-vh7dp [552.793196ms]
Jan 31 00:20:16.721: INFO: Created: latency-svc-bjlnq
Jan 31 00:20:16.746: INFO: Got endpoints: latency-svc-bjlnq [627.585761ms]
Jan 31 00:20:16.769: INFO: Created: latency-svc-6x7bn
Jan 31 00:20:16.781: INFO: Got endpoints: latency-svc-6x7bn [641.248886ms]
Jan 31 00:20:16.812: INFO: Created: latency-svc-7zmcn
Jan 31 00:20:16.824: INFO: Got endpoints: latency-svc-7zmcn [629.157423ms]
Jan 31 00:20:16.842: INFO: Created: latency-svc-fp464
Jan 31 00:20:16.856: INFO: Got endpoints: latency-svc-fp464 [634.088311ms]
Jan 31 00:20:16.877: INFO: Created: latency-svc-4nbvx
Jan 31 00:20:16.892: INFO: Got endpoints: latency-svc-4nbvx [604.465898ms]
Jan 31 00:20:17.401: INFO: Created: latency-svc-5k2gv
Jan 31 00:20:17.401: INFO: Created: latency-svc-2mkxs
Jan 31 00:20:17.405: INFO: Created: latency-svc-2kb7f
Jan 31 00:20:17.405: INFO: Created: latency-svc-4hknr
Jan 31 00:20:17.414: INFO: Created: latency-svc-kdrxc
Jan 31 00:20:17.415: INFO: Created: latency-svc-5n2r2
Jan 31 00:20:17.416: INFO: Created: latency-svc-v75l7
Jan 31 00:20:17.416: INFO: Created: latency-svc-4mvjl
Jan 31 00:20:17.416: INFO: Created: latency-svc-k862m
Jan 31 00:20:17.417: INFO: Created: latency-svc-vbffs
Jan 31 00:20:17.417: INFO: Created: latency-svc-fwqbp
Jan 31 00:20:17.417: INFO: Created: latency-svc-qgq8x
Jan 31 00:20:17.417: INFO: Created: latency-svc-hk27g
Jan 31 00:20:17.417: INFO: Created: latency-svc-sv4h5
Jan 31 00:20:17.417: INFO: Created: latency-svc-ccktc
Jan 31 00:20:17.451: INFO: Got endpoints: latency-svc-2kb7f [1.08125858s]
Jan 31 00:20:17.460: INFO: Got endpoints: latency-svc-fwqbp [996.40759ms]
Jan 31 00:20:17.460: INFO: Got endpoints: latency-svc-5k2gv [1.072868017s]
Jan 31 00:20:17.461: INFO: Got endpoints: latency-svc-4hknr [1.019998118s]
Jan 31 00:20:17.461: INFO: Got endpoints: latency-svc-2mkxs [1.142685905s]
Jan 31 00:20:17.473: INFO: Got endpoints: latency-svc-kdrxc [913.183478ms]
Jan 31 00:20:17.487: INFO: Got endpoints: latency-svc-ccktc [981.61499ms]
Jan 31 00:20:17.488: INFO: Got endpoints: latency-svc-4mvjl [889.506196ms]
Jan 31 00:20:17.498: INFO: Got endpoints: latency-svc-hk27g [907.456826ms]
Jan 31 00:20:17.498: INFO: Got endpoints: latency-svc-5n2r2 [641.941125ms]
Jan 31 00:20:17.502: INFO: Got endpoints: latency-svc-qgq8x [609.380758ms]
Jan 31 00:20:17.506: INFO: Got endpoints: latency-svc-sv4h5 [760.579027ms]
Jan 31 00:20:17.507: INFO: Got endpoints: latency-svc-v75l7 [682.858411ms]
Jan 31 00:20:17.514: INFO: Got endpoints: latency-svc-k862m [862.165056ms]
Jan 31 00:20:17.517: INFO: Got endpoints: latency-svc-vbffs [736.38073ms]
Jan 31 00:20:17.524: INFO: Created: latency-svc-44n8r
Jan 31 00:20:17.544: INFO: Got endpoints: latency-svc-44n8r [92.102491ms]
Jan 31 00:20:17.583: INFO: Created: latency-svc-2dz2j
Jan 31 00:20:17.597: INFO: Got endpoints: latency-svc-2dz2j [136.566938ms]
Jan 31 00:20:17.642: INFO: Created: latency-svc-jcctc
Jan 31 00:20:17.652: INFO: Got endpoints: latency-svc-jcctc [191.586215ms]
Jan 31 00:20:17.675: INFO: Created: latency-svc-tlvgk
Jan 31 00:20:17.708: INFO: Got endpoints: latency-svc-tlvgk [246.901438ms]
Jan 31 00:20:17.748: INFO: Created: latency-svc-fhpn6
Jan 31 00:20:17.748: INFO: Got endpoints: latency-svc-fhpn6 [286.761951ms]
Jan 31 00:20:17.792: INFO: Created: latency-svc-tr6kt
Jan 31 00:20:17.797: INFO: Got endpoints: latency-svc-tr6kt [323.678452ms]
Jan 31 00:20:17.869: INFO: Created: latency-svc-4s9q8
Jan 31 00:20:17.907: INFO: Got endpoints: latency-svc-4s9q8 [419.738957ms]
Jan 31 00:20:17.980: INFO: Created: latency-svc-vpfzf
Jan 31 00:20:17.980: INFO: Got endpoints: latency-svc-vpfzf [491.524762ms]
Jan 31 00:20:17.990: INFO: Created: latency-svc-clv8z
Jan 31 00:20:17.991: INFO: Created: latency-svc-fxfcf
Jan 31 00:20:18.004: INFO: Got endpoints: latency-svc-clv8z [504.938994ms]
Jan 31 00:20:18.008: INFO: Got endpoints: latency-svc-fxfcf [508.731038ms]
Jan 31 00:20:18.020: INFO: Created: latency-svc-65zqg
Jan 31 00:20:18.033: INFO: Got endpoints: latency-svc-65zqg [530.41422ms]
Jan 31 00:20:18.053: INFO: Created: latency-svc-ljnzx
Jan 31 00:20:18.068: INFO: Got endpoints: latency-svc-ljnzx [561.347681ms]
Jan 31 00:20:18.569: INFO: Created: latency-svc-2pmbw
Jan 31 00:20:18.570: INFO: Created: latency-svc-wbtrb
Jan 31 00:20:18.571: INFO: Created: latency-svc-kvml2
Jan 31 00:20:18.571: INFO: Created: latency-svc-fg5pm
Jan 31 00:20:18.572: INFO: Created: latency-svc-nlcrt
Jan 31 00:20:18.572: INFO: Created: latency-svc-lst86
Jan 31 00:20:18.572: INFO: Created: latency-svc-4t27g
Jan 31 00:20:18.572: INFO: Created: latency-svc-jb74t
Jan 31 00:20:18.572: INFO: Created: latency-svc-227zg
Jan 31 00:20:18.572: INFO: Created: latency-svc-qfcjm
Jan 31 00:20:18.573: INFO: Created: latency-svc-9fmx7
Jan 31 00:20:18.573: INFO: Created: latency-svc-fljs6
Jan 31 00:20:18.573: INFO: Created: latency-svc-44tt2
Jan 31 00:20:18.573: INFO: Created: latency-svc-6tb4t
Jan 31 00:20:18.573: INFO: Created: latency-svc-ql5gv
Jan 31 00:20:18.635: INFO: Got endpoints: latency-svc-2pmbw [926.898597ms]
Jan 31 00:20:18.638: INFO: Got endpoints: latency-svc-wbtrb [1.12349472s]
Jan 31 00:20:18.638: INFO: Got endpoints: latency-svc-lst86 [730.728743ms]
Jan 31 00:20:18.640: INFO: Got endpoints: latency-svc-ql5gv [571.976543ms]
Jan 31 00:20:18.642: INFO: Got endpoints: latency-svc-44tt2 [894.035765ms]
Jan 31 00:20:18.655: INFO: Got endpoints: latency-svc-fg5pm [1.147995859s]
Jan 31 00:20:18.661: INFO: Got endpoints: latency-svc-nlcrt [681.464083ms]
Jan 31 00:20:18.662: INFO: Got endpoints: latency-svc-9fmx7 [653.822158ms]
Jan 31 00:20:18.662: INFO: Got endpoints: latency-svc-4t27g [1.009837273s]
Jan 31 00:20:18.663: INFO: Got endpoints: latency-svc-227zg [1.118794506s]
Jan 31 00:20:18.669: INFO: Got endpoints: latency-svc-fljs6 [1.072693167s]
Jan 31 00:20:18.692: INFO: Got endpoints: latency-svc-6tb4t [687.433162ms]
Jan 31 00:20:18.693: INFO: Got endpoints: latency-svc-qfcjm [1.175196555s]
Jan 31 00:20:18.693: INFO: Got endpoints: latency-svc-jb74t [660.400739ms]
Jan 31 00:20:18.694: INFO: Got endpoints: latency-svc-kvml2 [896.961237ms]
Jan 31 00:20:18.723: INFO: Created: latency-svc-gskb5
Jan 31 00:20:18.742: INFO: Created: latency-svc-7bgnr
Jan 31 00:20:18.762: INFO: Got endpoints: latency-svc-gskb5 [127.012013ms]
Jan 31 00:20:18.776: INFO: Got endpoints: latency-svc-7bgnr [137.100169ms]
Jan 31 00:20:18.804: INFO: Created: latency-svc-m5hkf
Jan 31 00:20:18.818: INFO: Got endpoints: latency-svc-m5hkf [179.031062ms]
Jan 31 00:20:18.850: INFO: Created: latency-svc-ttp88
Jan 31 00:20:18.872: INFO: Got endpoints: latency-svc-ttp88 [231.720729ms]
Jan 31 00:20:18.905: INFO: Created: latency-svc-qzcht
Jan 31 00:20:18.920: INFO: Got endpoints: latency-svc-qzcht [277.727112ms]
Jan 31 00:20:18.938: INFO: Created: latency-svc-h677m
Jan 31 00:20:18.938: INFO: Got endpoints: latency-svc-h677m [282.840613ms]
Jan 31 00:20:18.970: INFO: Created: latency-svc-tlxv7
Jan 31 00:20:18.985: INFO: Got endpoints: latency-svc-tlxv7 [322.694731ms]
Jan 31 00:20:19.027: INFO: Created: latency-svc-47tjg
Jan 31 00:20:19.039: INFO: Got endpoints: latency-svc-47tjg [375.943697ms]
Jan 31 00:20:19.062: INFO: Created: latency-svc-fqlzp
Jan 31 00:20:19.100: INFO: Got endpoints: latency-svc-fqlzp [437.524501ms]
Jan 31 00:20:19.154: INFO: Created: latency-svc-lss2k
Jan 31 00:20:19.173: INFO: Got endpoints: latency-svc-lss2k [509.986293ms]
Jan 31 00:20:19.192: INFO: Created: latency-svc-gmrzr
Jan 31 00:20:19.204: INFO: Got endpoints: latency-svc-gmrzr [534.619904ms]
Jan 31 00:20:19.222: INFO: Created: latency-svc-t4nsc
Jan 31 00:20:19.235: INFO: Got endpoints: latency-svc-t4nsc [542.129697ms]
Jan 31 00:20:19.262: INFO: Created: latency-svc-gttgs
Jan 31 00:20:19.273: INFO: Got endpoints: latency-svc-gttgs [579.077627ms]
Jan 31 00:20:19.338: INFO: Created: latency-svc-cx449
Jan 31 00:20:19.350: INFO: Got endpoints: latency-svc-cx449 [657.834448ms]
Jan 31 00:20:19.366: INFO: Created: latency-svc-n7dvg
Jan 31 00:20:19.381: INFO: Got endpoints: latency-svc-n7dvg [687.111606ms]
Jan 31 00:20:19.397: INFO: Created: latency-svc-xcrtv
Jan 31 00:20:19.410: INFO: Got endpoints: latency-svc-xcrtv [648.437976ms]
Jan 31 00:20:19.425: INFO: Created: latency-svc-btzmj
Jan 31 00:20:19.436: INFO: Got endpoints: latency-svc-btzmj [659.656111ms]
Jan 31 00:20:19.500: INFO: Created: latency-svc-kg5df
Jan 31 00:20:19.510: INFO: Created: latency-svc-tjl78
Jan 31 00:20:19.513: INFO: Got endpoints: latency-svc-kg5df [695.242794ms]
Jan 31 00:20:19.527: INFO: Got endpoints: latency-svc-tjl78 [654.596719ms]
Jan 31 00:20:19.585: INFO: Created: latency-svc-2r2wr
Jan 31 00:20:19.593: INFO: Got endpoints: latency-svc-2r2wr [673.331313ms]
Jan 31 00:20:19.612: INFO: Created: latency-svc-9xw6f
Jan 31 00:20:19.628: INFO: Got endpoints: latency-svc-9xw6f [690.060085ms]
Jan 31 00:20:19.645: INFO: Created: latency-svc-8mhgr
Jan 31 00:20:19.654: INFO: Got endpoints: latency-svc-8mhgr [668.879128ms]
Jan 31 00:20:19.695: INFO: Created: latency-svc-r92dq
Jan 31 00:20:19.713: INFO: Got endpoints: latency-svc-r92dq [673.533931ms]
Jan 31 00:20:19.715: INFO: Created: latency-svc-g7r64
Jan 31 00:20:19.729: INFO: Got endpoints: latency-svc-g7r64 [629.126054ms]
Jan 31 00:20:19.737: INFO: Created: latency-svc-xnj6t
Jan 31 00:20:19.802: INFO: Got endpoints: latency-svc-xnj6t [628.862342ms]
Jan 31 00:20:19.856: INFO: Created: latency-svc-blmrh
Jan 31 00:20:19.865: INFO: Got endpoints: latency-svc-blmrh [660.932923ms]
Jan 31 00:20:19.924: INFO: Created: latency-svc-2d6v9
Jan 31 00:20:19.925: INFO: Created: latency-svc-28l27
Jan 31 00:20:19.957: INFO: Got endpoints: latency-svc-2d6v9 [719.627297ms]
Jan 31 00:20:19.959: INFO: Got endpoints: latency-svc-28l27 [685.311232ms]
Jan 31 00:20:20.014: INFO: Created: latency-svc-dc5mv
Jan 31 00:20:20.025: INFO: Got endpoints: latency-svc-dc5mv [674.63397ms]
Jan 31 00:20:20.091: INFO: Created: latency-svc-2nbs4
Jan 31 00:20:20.091: INFO: Got endpoints: latency-svc-2nbs4 [710.400821ms]
Jan 31 00:20:20.125: INFO: Created: latency-svc-rh425
Jan 31 00:20:20.136: INFO: Got endpoints: latency-svc-rh425 [725.98253ms]
Jan 31 00:20:20.165: INFO: Created: latency-svc-zd9kt
Jan 31 00:20:20.175: INFO: Got endpoints: latency-svc-zd9kt [739.125524ms]
Jan 31 00:20:20.192: INFO: Created: latency-svc-m76wm
Jan 31 00:20:20.220: INFO: Got endpoints: latency-svc-m76wm [706.565857ms]
Jan 31 00:20:20.226: INFO: Created: latency-svc-k4vrt
Jan 31 00:20:20.235: INFO: Got endpoints: latency-svc-k4vrt [707.818662ms]
Jan 31 00:20:20.253: INFO: Created: latency-svc-5zgth
Jan 31 00:20:20.262: INFO: Got endpoints: latency-svc-5zgth [668.124144ms]
Jan 31 00:20:20.292: INFO: Created: latency-svc-sbj5r
Jan 31 00:20:20.305: INFO: Got endpoints: latency-svc-sbj5r [676.903803ms]
Jan 31 00:20:20.352: INFO: Created: latency-svc-9pzr6
Jan 31 00:20:20.352: INFO: Got endpoints: latency-svc-9pzr6 [698.876424ms]
Jan 31 00:20:20.357: INFO: Created: latency-svc-4jstk
Jan 31 00:20:20.368: INFO: Got endpoints: latency-svc-4jstk [655.248743ms]
Jan 31 00:20:20.383: INFO: Created: latency-svc-jp572
Jan 31 00:20:20.394: INFO: Got endpoints: latency-svc-jp572 [664.431143ms]
Jan 31 00:20:20.411: INFO: Created: latency-svc-7njzg
Jan 31 00:20:20.444: INFO: Got endpoints: latency-svc-7njzg [642.053644ms]
Jan 31 00:20:20.455: INFO: Created: latency-svc-4f7kp
Jan 31 00:20:20.473: INFO: Got endpoints: latency-svc-4f7kp [607.216869ms]
Jan 31 00:20:20.499: INFO: Created: latency-svc-ws2d9
Jan 31 00:20:20.511: INFO: Got endpoints: latency-svc-ws2d9 [553.716503ms]
Jan 31 00:20:20.559: INFO: Created: latency-svc-sq474
Jan 31 00:20:20.572: INFO: Got endpoints: latency-svc-sq474 [613.091906ms]
Jan 31 00:20:20.601: INFO: Created: latency-svc-597xm
Jan 31 00:20:20.615: INFO: Got endpoints: latency-svc-597xm [590.46786ms]
Jan 31 00:20:20.658: INFO: Created: latency-svc-jh54z
Jan 31 00:20:20.664: INFO: Got endpoints: latency-svc-jh54z [572.395488ms]
Jan 31 00:20:20.696: INFO: Created: latency-svc-2m4c6
Jan 31 00:20:20.710: INFO: Got endpoints: latency-svc-2m4c6 [573.304778ms]
Jan 31 00:20:20.743: INFO: Created: latency-svc-4nq2z
Jan 31 00:20:20.761: INFO: Got endpoints: latency-svc-4nq2z [585.929419ms]
Jan 31 00:20:20.771: INFO: Created: latency-svc-tmwc5
Jan 31 00:20:20.785: INFO: Got endpoints: latency-svc-tmwc5 [565.014689ms]
Jan 31 00:20:20.843: INFO: Created: latency-svc-shql4
Jan 31 00:20:20.843: INFO: Got endpoints: latency-svc-shql4 [607.918566ms]
Jan 31 00:20:20.853: INFO: Created: latency-svc-vw5jz
Jan 31 00:20:20.870: INFO: Got endpoints: latency-svc-vw5jz [608.440581ms]
Jan 31 00:20:20.884: INFO: Created: latency-svc-hqttj
Jan 31 00:20:20.908: INFO: Got endpoints: latency-svc-hqttj [602.081446ms]
Jan 31 00:20:20.922: INFO: Created: latency-svc-2ms46
Jan 31 00:20:20.934: INFO: Got endpoints: latency-svc-2ms46 [581.021715ms]
Jan 31 00:20:20.954: INFO: Created: latency-svc-7pr86
Jan 31 00:20:20.968: INFO: Got endpoints: latency-svc-7pr86 [600.18401ms]
Jan 31 00:20:20.994: INFO: Created: latency-svc-4tght
Jan 31 00:20:21.005: INFO: Got endpoints: latency-svc-4tght [610.778822ms]
Jan 31 00:20:21.083: INFO: Created: latency-svc-ggh8v
Jan 31 00:20:21.104: INFO: Got endpoints: latency-svc-ggh8v [660.270917ms]
Jan 31 00:20:21.596: INFO: Created: latency-svc-cq8dk
Jan 31 00:20:21.597: INFO: Created: latency-svc-srtgs
Jan 31 00:20:21.608: INFO: Created: latency-svc-lldx6
Jan 31 00:20:21.610: INFO: Created: latency-svc-dt2hf
Jan 31 00:20:21.611: INFO: Created: latency-svc-6nrft
Jan 31 00:20:21.611: INFO: Created: latency-svc-8nd2v
Jan 31 00:20:21.611: INFO: Created: latency-svc-k6t7j
Jan 31 00:20:21.613: INFO: Created: latency-svc-79csf
Jan 31 00:20:21.614: INFO: Created: latency-svc-snp8k
Jan 31 00:20:21.615: INFO: Created: latency-svc-gxs45
Jan 31 00:20:21.615: INFO: Created: latency-svc-l69br
Jan 31 00:20:21.616: INFO: Created: latency-svc-rz57t
Jan 31 00:20:21.616: INFO: Created: latency-svc-tqmww
Jan 31 00:20:21.616: INFO: Created: latency-svc-vw2bb
Jan 31 00:20:21.617: INFO: Created: latency-svc-frjmz
Jan 31 00:20:21.617: INFO: Got endpoints: latency-svc-cq8dk [612.07637ms]
Jan 31 00:20:21.618: INFO: Got endpoints: latency-svc-lldx6 [513.755176ms]
Jan 31 00:20:21.618: INFO: Got endpoints: latency-svc-srtgs [1.145617053s]
Jan 31 00:20:21.624: INFO: Got endpoints: latency-svc-dt2hf [689.661978ms]
Jan 31 00:20:21.624: INFO: Got endpoints: latency-svc-6nrft [655.521122ms]
Jan 31 00:20:21.629: INFO: Got endpoints: latency-svc-tqmww [1.117507651s]
Jan 31 00:20:21.636: INFO: Got endpoints: latency-svc-snp8k [728.290232ms]
Jan 31 00:20:21.637: INFO: Got endpoints: latency-svc-8nd2v [793.91699ms]
Jan 31 00:20:21.643: INFO: Got endpoints: latency-svc-rz57t [772.655049ms]
Jan 31 00:20:21.650: INFO: Got endpoints: latency-svc-l69br [1.077331804s]
Jan 31 00:20:21.650: INFO: Got endpoints: latency-svc-79csf [1.034738108s]
Jan 31 00:20:21.652: INFO: Got endpoints: latency-svc-k6t7j [942.097597ms]
Jan 31 00:20:21.656: INFO: Got endpoints: latency-svc-gxs45 [992.410119ms]
Jan 31 00:20:21.660: INFO: Got endpoints: latency-svc-frjmz [874.768718ms]
Jan 31 00:20:21.662: INFO: Got endpoints: latency-svc-vw2bb [900.082588ms]
Jan 31 00:20:21.691: INFO: Created: latency-svc-hhlg9
Jan 31 00:20:21.699: INFO: Got endpoints: latency-svc-hhlg9 [82.242449ms]
Jan 31 00:20:22.242: INFO: Created: latency-svc-kd7t4
Jan 31 00:20:22.245: INFO: Created: latency-svc-hcw7v
Jan 31 00:20:22.257: INFO: Created: latency-svc-7kwd7
Jan 31 00:20:22.258: INFO: Created: latency-svc-rskb8
Jan 31 00:20:22.259: INFO: Created: latency-svc-srr49
Jan 31 00:20:22.259: INFO: Created: latency-svc-wn2h6
Jan 31 00:20:22.264: INFO: Created: latency-svc-j7nqb
Jan 31 00:20:22.265: INFO: Created: latency-svc-jqfch
Jan 31 00:20:22.266: INFO: Created: latency-svc-b7jsk
Jan 31 00:20:22.267: INFO: Created: latency-svc-gb9b4
Jan 31 00:20:22.267: INFO: Got endpoints: latency-svc-kd7t4 [606.699218ms]
Jan 31 00:20:22.267: INFO: Created: latency-svc-v7c7x
Jan 31 00:20:22.268: INFO: Created: latency-svc-xp5sg
Jan 31 00:20:22.268: INFO: Created: latency-svc-x5nqj
Jan 31 00:20:22.268: INFO: Got endpoints: latency-svc-hcw7v [611.971865ms]
Jan 31 00:20:22.269: INFO: Created: latency-svc-5ktr9
Jan 31 00:20:22.270: INFO: Created: latency-svc-v8rtk
Jan 31 00:20:22.270: INFO: Got endpoints: latency-svc-j7nqb [646.15427ms]
Jan 31 00:20:22.270: INFO: Got endpoints: latency-svc-7kwd7 [571.188995ms]
Jan 31 00:20:22.271: INFO: Got endpoints: latency-svc-rskb8 [608.581985ms]
Jan 31 00:20:22.284: INFO: Got endpoints: latency-svc-xp5sg [633.453258ms]
Jan 31 00:20:22.285: INFO: Got endpoints: latency-svc-x5nqj [667.016843ms]
Jan 31 00:20:22.286: INFO: Got endpoints: latency-svc-v7c7x [633.017323ms]
Jan 31 00:20:22.287: INFO: Got endpoints: latency-svc-gb9b4 [636.146213ms]
Jan 31 00:20:22.287: INFO: Got endpoints: latency-svc-5ktr9 [668.843928ms]
Jan 31 00:20:22.312: INFO: Got endpoints: latency-svc-v8rtk [675.88578ms]
Jan 31 00:20:22.318: INFO: Got endpoints: latency-svc-wn2h6 [680.548425ms]
Jan 31 00:20:22.322: INFO: Got endpoints: latency-svc-b7jsk [698.138637ms]
Jan 31 00:20:22.330: INFO: Got endpoints: latency-svc-srr49 [701.062943ms]
Jan 31 00:20:22.331: INFO: Got endpoints: latency-svc-jqfch [687.352133ms]
Jan 31 00:20:22.344: INFO: Created: latency-svc-2bkwk
Jan 31 00:20:22.344: INFO: Got endpoints: latency-svc-2bkwk [77.565423ms]
Jan 31 00:20:22.363: INFO: Created: latency-svc-jj8cs
Jan 31 00:20:22.377: INFO: Got endpoints: latency-svc-jj8cs [108.216394ms]
Jan 31 00:20:22.399: INFO: Created: latency-svc-8d5kq
Jan 31 00:20:22.410: INFO: Got endpoints: latency-svc-8d5kq [140.273235ms]
Jan 31 00:20:22.479: INFO: Created: latency-svc-62d6m
Jan 31 00:20:22.482: INFO: Got endpoints: latency-svc-62d6m [211.264407ms]
Jan 31 00:20:22.488: INFO: Created: latency-svc-fhg8q
Jan 31 00:20:22.503: INFO: Got endpoints: latency-svc-fhg8q [231.959847ms]
Jan 31 00:20:22.521: INFO: Created: latency-svc-qp5l8
Jan 31 00:20:22.536: INFO: Got endpoints: latency-svc-qp5l8 [248.505591ms]
Jan 31 00:20:22.550: INFO: Created: latency-svc-zdd5p
Jan 31 00:20:22.568: INFO: Got endpoints: latency-svc-zdd5p [282.016289ms]
Jan 31 00:20:22.584: INFO: Created: latency-svc-8s22k
Jan 31 00:20:22.598: INFO: Got endpoints: latency-svc-8s22k [313.661124ms]
Jan 31 00:20:22.614: INFO: Created: latency-svc-8mtw4
Jan 31 00:20:22.630: INFO: Got endpoints: latency-svc-8mtw4 [343.731724ms]
Jan 31 00:20:22.642: INFO: Created: latency-svc-pvrsc
Jan 31 00:20:22.653: INFO: Got endpoints: latency-svc-pvrsc [365.576257ms]
Jan 31 00:20:22.674: INFO: Created: latency-svc-hq72v
Jan 31 00:20:22.696: INFO: Got endpoints: latency-svc-hq72v [383.694296ms]
Jan 31 00:20:22.773: INFO: Created: latency-svc-972qz
Jan 31 00:20:22.774: INFO: Created: latency-svc-286p4
Jan 31 00:20:22.781: INFO: Created: latency-svc-g77jz
Jan 31 00:20:22.787: INFO: Got endpoints: latency-svc-972qz [465.346394ms]
Jan 31 00:20:22.787: INFO: Got endpoints: latency-svc-286p4 [469.516878ms]
Jan 31 00:20:22.798: INFO: Got endpoints: latency-svc-g77jz [468.655496ms]
Jan 31 00:20:22.822: INFO: Created: latency-svc-n2qt8
Jan 31 00:20:22.846: INFO: Got endpoints: latency-svc-n2qt8 [514.133846ms]
Jan 31 00:20:22.860: INFO: Created: latency-svc-xlvjp
Jan 31 00:20:22.875: INFO: Got endpoints: latency-svc-xlvjp [528.807796ms]
Jan 31 00:20:22.903: INFO: Created: latency-svc-cwcvl
Jan 31 00:20:22.923: INFO: Got endpoints: latency-svc-cwcvl [545.46313ms]
Jan 31 00:20:22.939: INFO: Created: latency-svc-sswxp
Jan 31 00:20:22.953: INFO: Got endpoints: latency-svc-sswxp [543.077094ms]
Jan 31 00:20:22.998: INFO: Created: latency-svc-5djbq
Jan 31 00:20:22.998: INFO: Got endpoints: latency-svc-5djbq [515.766977ms]
Jan 31 00:20:23.033: INFO: Created: latency-svc-d578k
Jan 31 00:20:23.054: INFO: Got endpoints: latency-svc-d578k [550.954365ms]
Jan 31 00:20:23.062: INFO: Created: latency-svc-gb4h4
Jan 31 00:20:23.099: INFO: Got endpoints: latency-svc-gb4h4 [563.341351ms]
Jan 31 00:20:23.148: INFO: Created: latency-svc-q5vn5
Jan 31 00:20:23.148: INFO: Got endpoints: latency-svc-q5vn5 [579.888421ms]
Jan 31 00:20:23.156: INFO: Created: latency-svc-p8zq2
Jan 31 00:20:23.185: INFO: Got endpoints: latency-svc-p8zq2 [586.566299ms]
Jan 31 00:20:23.226: INFO: Created: latency-svc-n8qzj
Jan 31 00:20:23.238: INFO: Created: latency-svc-b665r
Jan 31 00:20:23.241: INFO: Got endpoints: latency-svc-n8qzj [610.76863ms]
Jan 31 00:20:23.251: INFO: Got endpoints: latency-svc-b665r [597.327885ms]
Jan 31 00:20:23.285: INFO: Created: latency-svc-78wzz
Jan 31 00:20:23.301: INFO: Got endpoints: latency-svc-78wzz [604.237848ms]
Jan 31 00:20:23.325: INFO: Created: latency-svc-l9l5q
Jan 31 00:20:23.340: INFO: Got endpoints: latency-svc-l9l5q [548.472721ms]
Jan 31 00:20:23.345: INFO: Created: latency-svc-bd6pc
Jan 31 00:20:23.353: INFO: Got endpoints: latency-svc-bd6pc [561.148679ms]
Jan 31 00:20:23.391: INFO: Created: latency-svc-lhqz4
Jan 31 00:20:23.410: INFO: Got endpoints: latency-svc-lhqz4 [611.956748ms]
Jan 31 00:20:23.481: INFO: Created: latency-svc-f6ptx
Jan 31 00:20:23.482: INFO: Got endpoints: latency-svc-f6ptx [636.478467ms]
Jan 31 00:20:23.492: INFO: Created: latency-svc-nn8dp
Jan 31 00:20:23.506: INFO: Got endpoints: latency-svc-nn8dp [630.986827ms]
Jan 31 00:20:23.517: INFO: Created: latency-svc-mpc87
Jan 31 00:20:23.543: INFO: Got endpoints: latency-svc-mpc87 [620.459265ms]
Jan 31 00:20:23.549: INFO: Created: latency-svc-26pz9
Jan 31 00:20:23.559: INFO: Got endpoints: latency-svc-26pz9 [605.419135ms]
Jan 31 00:20:23.573: INFO: Created: latency-svc-blsz8
Jan 31 00:20:23.586: INFO: Got endpoints: latency-svc-blsz8 [587.946123ms]
Jan 31 00:20:23.602: INFO: Created: latency-svc-6z2zk
Jan 31 00:20:23.616: INFO: Got endpoints: latency-svc-6z2zk [562.011423ms]
Jan 31 00:20:23.653: INFO: Created: latency-svc-7djrj
Jan 31 00:20:23.665: INFO: Got endpoints: latency-svc-7djrj [565.840078ms]
Jan 31 00:20:23.693: INFO: Created: latency-svc-d5g54
Jan 31 00:20:23.693: INFO: Got endpoints: latency-svc-d5g54 [545.643253ms]
Jan 31 00:20:23.703: INFO: Created: latency-svc-vtgnk
Jan 31 00:20:23.713: INFO: Got endpoints: latency-svc-vtgnk [528.476584ms]
Jan 31 00:20:23.740: INFO: Created: latency-svc-4rzc4
Jan 31 00:20:23.754: INFO: Got endpoints: latency-svc-4rzc4 [513.159901ms]
Jan 31 00:20:23.773: INFO: Created: latency-svc-56fjm
Jan 31 00:20:23.792: INFO: Got endpoints: latency-svc-56fjm [540.5771ms]
Jan 31 00:20:23.804: INFO: Created: latency-svc-zhkwr
Jan 31 00:20:23.816: INFO: Got endpoints: latency-svc-zhkwr [514.834385ms]
Jan 31 00:20:23.867: INFO: Created: latency-svc-w47nj
Jan 31 00:20:23.870: INFO: Created: latency-svc-lvbsx
Jan 31 00:20:23.890: INFO: Got endpoints: latency-svc-lvbsx [536.660422ms]
Jan 31 00:20:23.890: INFO: Got endpoints: latency-svc-w47nj [549.015786ms]
Jan 31 00:20:23.911: INFO: Created: latency-svc-m9dx5
Jan 31 00:20:23.925: INFO: Got endpoints: latency-svc-m9dx5 [514.306282ms]
Jan 31 00:20:23.925: INFO: Latencies: [68.688361ms 77.565423ms 82.242449ms 92.102491ms 93.038302ms 108.216394ms 127.012013ms 133.562881ms 136.566938ms 137.100169ms 140.273235ms 157.947293ms 179.031062ms 191.586215ms 192.432155ms 211.264407ms 224.629009ms 231.720729ms 231.959847ms 246.901438ms 248.505591ms 277.727112ms 282.016289ms 282.840613ms 286.761951ms 301.43912ms 313.661124ms 322.694731ms 323.678452ms 343.731724ms 358.04801ms 365.576257ms 375.943697ms 378.216465ms 383.694296ms 399.303244ms 419.738957ms 437.524501ms 454.341579ms 465.346394ms 468.655496ms 469.516878ms 482.337643ms 491.524762ms 504.938994ms 508.731038ms 509.986293ms 513.159901ms 513.755176ms 514.133846ms 514.306282ms 514.834385ms 515.766977ms 528.476584ms 528.807796ms 530.41422ms 534.619904ms 536.660422ms 540.5771ms 542.129697ms 543.077094ms 545.46313ms 545.643253ms 547.896174ms 548.472721ms 549.015786ms 550.954365ms 552.793196ms 553.716503ms 556.609024ms 561.148679ms 561.347681ms 562.011423ms 563.341351ms 565.014689ms 565.840078ms 571.188995ms 571.976543ms 572.395488ms 573.304778ms 578.19825ms 579.077627ms 579.110239ms 579.888421ms 581.021715ms 585.929419ms 586.566299ms 587.946123ms 590.016798ms 590.46786ms 597.327885ms 600.18401ms 602.081446ms 604.237848ms 604.465898ms 605.419135ms 606.699218ms 607.216869ms 607.902997ms 607.918566ms 607.969611ms 608.440581ms 608.581985ms 609.380758ms 610.76863ms 610.778822ms 611.956748ms 611.971865ms 612.07637ms 613.091906ms 620.459265ms 625.870519ms 626.954785ms 627.585761ms 628.862342ms 629.126054ms 629.157423ms 630.410755ms 630.986827ms 633.017323ms 633.453258ms 634.088311ms 636.146213ms 636.478467ms 641.248886ms 641.941125ms 642.053644ms 646.15427ms 648.437976ms 653.822158ms 654.596719ms 655.248743ms 655.521122ms 657.834448ms 659.656111ms 660.270917ms 660.400739ms 660.932923ms 664.431143ms 667.016843ms 668.124144ms 668.843928ms 668.879128ms 673.331313ms 673.533931ms 674.63397ms 675.88578ms 676.903803ms 680.548425ms 681.464083ms 682.858411ms 685.311232ms 687.111606ms 687.352133ms 687.433162ms 689.661978ms 690.060085ms 695.242794ms 698.138637ms 698.876424ms 701.062943ms 706.565857ms 707.818662ms 710.400821ms 719.627297ms 725.98253ms 728.290232ms 730.728743ms 736.38073ms 739.125524ms 760.579027ms 772.655049ms 793.91699ms 862.165056ms 874.768718ms 889.506196ms 894.035765ms 896.961237ms 900.082588ms 907.456826ms 913.183478ms 926.898597ms 942.097597ms 981.61499ms 992.410119ms 996.40759ms 1.009837273s 1.019998118s 1.034738108s 1.072693167s 1.072868017s 1.077331804s 1.08125858s 1.117507651s 1.118794506s 1.12349472s 1.142685905s 1.145617053s 1.147995859s 1.175196555s]
Jan 31 00:20:23.925: INFO: 50 %ile: 607.969611ms
Jan 31 00:20:23.925: INFO: 90 %ile: 913.183478ms
Jan 31 00:20:23.925: INFO: 99 %ile: 1.147995859s
Jan 31 00:20:23.925: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 31 00:20:23.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-8946" for this suite. 01/31/23 00:20:23.952
------------------------------
• [SLOW TEST] [11.573 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:20:12.405
    Jan 31 00:20:12.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svc-latency 01/31/23 00:20:12.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:12.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:12.477
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 31 00:20:12.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8946 01/31/23 00:20:12.492
    I0131 00:20:12.520442      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8946, replica count: 1
    I0131 00:20:13.572249      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0131 00:20:14.573161      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0131 00:20:15.573819      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 31 00:20:15.729: INFO: Created: latency-svc-vkwm6
    Jan 31 00:20:15.739: INFO: Got endpoints: latency-svc-vkwm6 [63.6411ms]
    Jan 31 00:20:15.794: INFO: Created: latency-svc-ttdv2
    Jan 31 00:20:15.808: INFO: Got endpoints: latency-svc-ttdv2 [68.688361ms]
    Jan 31 00:20:15.822: INFO: Created: latency-svc-wtlwd
    Jan 31 00:20:15.832: INFO: Got endpoints: latency-svc-wtlwd [93.038302ms]
    Jan 31 00:20:15.856: INFO: Created: latency-svc-dfh9m
    Jan 31 00:20:15.873: INFO: Got endpoints: latency-svc-dfh9m [133.562881ms]
    Jan 31 00:20:15.888: INFO: Created: latency-svc-w46t6
    Jan 31 00:20:15.898: INFO: Got endpoints: latency-svc-w46t6 [157.947293ms]
    Jan 31 00:20:15.920: INFO: Created: latency-svc-8p7px
    Jan 31 00:20:15.932: INFO: Got endpoints: latency-svc-8p7px [192.432155ms]
    Jan 31 00:20:15.952: INFO: Created: latency-svc-c7fxf
    Jan 31 00:20:15.965: INFO: Got endpoints: latency-svc-c7fxf [224.629009ms]
    Jan 31 00:20:16.030: INFO: Created: latency-svc-tlj5s
    Jan 31 00:20:16.042: INFO: Got endpoints: latency-svc-tlj5s [301.43912ms]
    Jan 31 00:20:16.085: INFO: Created: latency-svc-bc9ln
    Jan 31 00:20:16.098: INFO: Got endpoints: latency-svc-bc9ln [358.04801ms]
    Jan 31 00:20:16.101: INFO: Created: latency-svc-l472t
    Jan 31 00:20:16.118: INFO: Got endpoints: latency-svc-l472t [378.216465ms]
    Jan 31 00:20:16.128: INFO: Created: latency-svc-kwpxs
    Jan 31 00:20:16.140: INFO: Got endpoints: latency-svc-kwpxs [399.303244ms]
    Jan 31 00:20:16.156: INFO: Created: latency-svc-njfv2
    Jan 31 00:20:16.195: INFO: Got endpoints: latency-svc-njfv2 [454.341579ms]
    Jan 31 00:20:16.205: INFO: Created: latency-svc-4jv96
    Jan 31 00:20:16.222: INFO: Got endpoints: latency-svc-4jv96 [482.337643ms]
    Jan 31 00:20:16.264: INFO: Created: latency-svc-99kgg
    Jan 31 00:20:16.288: INFO: Got endpoints: latency-svc-99kgg [547.896174ms]
    Jan 31 00:20:16.304: INFO: Created: latency-svc-b8zfs
    Jan 31 00:20:16.318: INFO: Got endpoints: latency-svc-b8zfs [578.19825ms]
    Jan 31 00:20:16.356: INFO: Created: latency-svc-rml8d
    Jan 31 00:20:16.370: INFO: Created: latency-svc-4mwlr
    Jan 31 00:20:16.370: INFO: Got endpoints: latency-svc-rml8d [630.410755ms]
    Jan 31 00:20:16.387: INFO: Got endpoints: latency-svc-4mwlr [579.110239ms]
    Jan 31 00:20:16.419: INFO: Created: latency-svc-lzfhv
    Jan 31 00:20:16.440: INFO: Got endpoints: latency-svc-lzfhv [607.969611ms]
    Jan 31 00:20:16.449: INFO: Created: latency-svc-6w2rm
    Jan 31 00:20:16.463: INFO: Got endpoints: latency-svc-6w2rm [590.016798ms]
    Jan 31 00:20:16.485: INFO: Created: latency-svc-qcq5z
    Jan 31 00:20:16.506: INFO: Got endpoints: latency-svc-qcq5z [607.902997ms]
    Jan 31 00:20:16.518: INFO: Created: latency-svc-7fttf
    Jan 31 00:20:16.560: INFO: Got endpoints: latency-svc-7fttf [626.954785ms]
    Jan 31 00:20:16.575: INFO: Created: latency-svc-ljq26
    Jan 31 00:20:16.578: INFO: Created: latency-svc-rwztx
    Jan 31 00:20:16.591: INFO: Got endpoints: latency-svc-ljq26 [625.870519ms]
    Jan 31 00:20:16.598: INFO: Got endpoints: latency-svc-rwztx [556.609024ms]
    Jan 31 00:20:16.639: INFO: Created: latency-svc-vh7dp
    Jan 31 00:20:16.651: INFO: Got endpoints: latency-svc-vh7dp [552.793196ms]
    Jan 31 00:20:16.721: INFO: Created: latency-svc-bjlnq
    Jan 31 00:20:16.746: INFO: Got endpoints: latency-svc-bjlnq [627.585761ms]
    Jan 31 00:20:16.769: INFO: Created: latency-svc-6x7bn
    Jan 31 00:20:16.781: INFO: Got endpoints: latency-svc-6x7bn [641.248886ms]
    Jan 31 00:20:16.812: INFO: Created: latency-svc-7zmcn
    Jan 31 00:20:16.824: INFO: Got endpoints: latency-svc-7zmcn [629.157423ms]
    Jan 31 00:20:16.842: INFO: Created: latency-svc-fp464
    Jan 31 00:20:16.856: INFO: Got endpoints: latency-svc-fp464 [634.088311ms]
    Jan 31 00:20:16.877: INFO: Created: latency-svc-4nbvx
    Jan 31 00:20:16.892: INFO: Got endpoints: latency-svc-4nbvx [604.465898ms]
    Jan 31 00:20:17.401: INFO: Created: latency-svc-5k2gv
    Jan 31 00:20:17.401: INFO: Created: latency-svc-2mkxs
    Jan 31 00:20:17.405: INFO: Created: latency-svc-2kb7f
    Jan 31 00:20:17.405: INFO: Created: latency-svc-4hknr
    Jan 31 00:20:17.414: INFO: Created: latency-svc-kdrxc
    Jan 31 00:20:17.415: INFO: Created: latency-svc-5n2r2
    Jan 31 00:20:17.416: INFO: Created: latency-svc-v75l7
    Jan 31 00:20:17.416: INFO: Created: latency-svc-4mvjl
    Jan 31 00:20:17.416: INFO: Created: latency-svc-k862m
    Jan 31 00:20:17.417: INFO: Created: latency-svc-vbffs
    Jan 31 00:20:17.417: INFO: Created: latency-svc-fwqbp
    Jan 31 00:20:17.417: INFO: Created: latency-svc-qgq8x
    Jan 31 00:20:17.417: INFO: Created: latency-svc-hk27g
    Jan 31 00:20:17.417: INFO: Created: latency-svc-sv4h5
    Jan 31 00:20:17.417: INFO: Created: latency-svc-ccktc
    Jan 31 00:20:17.451: INFO: Got endpoints: latency-svc-2kb7f [1.08125858s]
    Jan 31 00:20:17.460: INFO: Got endpoints: latency-svc-fwqbp [996.40759ms]
    Jan 31 00:20:17.460: INFO: Got endpoints: latency-svc-5k2gv [1.072868017s]
    Jan 31 00:20:17.461: INFO: Got endpoints: latency-svc-4hknr [1.019998118s]
    Jan 31 00:20:17.461: INFO: Got endpoints: latency-svc-2mkxs [1.142685905s]
    Jan 31 00:20:17.473: INFO: Got endpoints: latency-svc-kdrxc [913.183478ms]
    Jan 31 00:20:17.487: INFO: Got endpoints: latency-svc-ccktc [981.61499ms]
    Jan 31 00:20:17.488: INFO: Got endpoints: latency-svc-4mvjl [889.506196ms]
    Jan 31 00:20:17.498: INFO: Got endpoints: latency-svc-hk27g [907.456826ms]
    Jan 31 00:20:17.498: INFO: Got endpoints: latency-svc-5n2r2 [641.941125ms]
    Jan 31 00:20:17.502: INFO: Got endpoints: latency-svc-qgq8x [609.380758ms]
    Jan 31 00:20:17.506: INFO: Got endpoints: latency-svc-sv4h5 [760.579027ms]
    Jan 31 00:20:17.507: INFO: Got endpoints: latency-svc-v75l7 [682.858411ms]
    Jan 31 00:20:17.514: INFO: Got endpoints: latency-svc-k862m [862.165056ms]
    Jan 31 00:20:17.517: INFO: Got endpoints: latency-svc-vbffs [736.38073ms]
    Jan 31 00:20:17.524: INFO: Created: latency-svc-44n8r
    Jan 31 00:20:17.544: INFO: Got endpoints: latency-svc-44n8r [92.102491ms]
    Jan 31 00:20:17.583: INFO: Created: latency-svc-2dz2j
    Jan 31 00:20:17.597: INFO: Got endpoints: latency-svc-2dz2j [136.566938ms]
    Jan 31 00:20:17.642: INFO: Created: latency-svc-jcctc
    Jan 31 00:20:17.652: INFO: Got endpoints: latency-svc-jcctc [191.586215ms]
    Jan 31 00:20:17.675: INFO: Created: latency-svc-tlvgk
    Jan 31 00:20:17.708: INFO: Got endpoints: latency-svc-tlvgk [246.901438ms]
    Jan 31 00:20:17.748: INFO: Created: latency-svc-fhpn6
    Jan 31 00:20:17.748: INFO: Got endpoints: latency-svc-fhpn6 [286.761951ms]
    Jan 31 00:20:17.792: INFO: Created: latency-svc-tr6kt
    Jan 31 00:20:17.797: INFO: Got endpoints: latency-svc-tr6kt [323.678452ms]
    Jan 31 00:20:17.869: INFO: Created: latency-svc-4s9q8
    Jan 31 00:20:17.907: INFO: Got endpoints: latency-svc-4s9q8 [419.738957ms]
    Jan 31 00:20:17.980: INFO: Created: latency-svc-vpfzf
    Jan 31 00:20:17.980: INFO: Got endpoints: latency-svc-vpfzf [491.524762ms]
    Jan 31 00:20:17.990: INFO: Created: latency-svc-clv8z
    Jan 31 00:20:17.991: INFO: Created: latency-svc-fxfcf
    Jan 31 00:20:18.004: INFO: Got endpoints: latency-svc-clv8z [504.938994ms]
    Jan 31 00:20:18.008: INFO: Got endpoints: latency-svc-fxfcf [508.731038ms]
    Jan 31 00:20:18.020: INFO: Created: latency-svc-65zqg
    Jan 31 00:20:18.033: INFO: Got endpoints: latency-svc-65zqg [530.41422ms]
    Jan 31 00:20:18.053: INFO: Created: latency-svc-ljnzx
    Jan 31 00:20:18.068: INFO: Got endpoints: latency-svc-ljnzx [561.347681ms]
    Jan 31 00:20:18.569: INFO: Created: latency-svc-2pmbw
    Jan 31 00:20:18.570: INFO: Created: latency-svc-wbtrb
    Jan 31 00:20:18.571: INFO: Created: latency-svc-kvml2
    Jan 31 00:20:18.571: INFO: Created: latency-svc-fg5pm
    Jan 31 00:20:18.572: INFO: Created: latency-svc-nlcrt
    Jan 31 00:20:18.572: INFO: Created: latency-svc-lst86
    Jan 31 00:20:18.572: INFO: Created: latency-svc-4t27g
    Jan 31 00:20:18.572: INFO: Created: latency-svc-jb74t
    Jan 31 00:20:18.572: INFO: Created: latency-svc-227zg
    Jan 31 00:20:18.572: INFO: Created: latency-svc-qfcjm
    Jan 31 00:20:18.573: INFO: Created: latency-svc-9fmx7
    Jan 31 00:20:18.573: INFO: Created: latency-svc-fljs6
    Jan 31 00:20:18.573: INFO: Created: latency-svc-44tt2
    Jan 31 00:20:18.573: INFO: Created: latency-svc-6tb4t
    Jan 31 00:20:18.573: INFO: Created: latency-svc-ql5gv
    Jan 31 00:20:18.635: INFO: Got endpoints: latency-svc-2pmbw [926.898597ms]
    Jan 31 00:20:18.638: INFO: Got endpoints: latency-svc-wbtrb [1.12349472s]
    Jan 31 00:20:18.638: INFO: Got endpoints: latency-svc-lst86 [730.728743ms]
    Jan 31 00:20:18.640: INFO: Got endpoints: latency-svc-ql5gv [571.976543ms]
    Jan 31 00:20:18.642: INFO: Got endpoints: latency-svc-44tt2 [894.035765ms]
    Jan 31 00:20:18.655: INFO: Got endpoints: latency-svc-fg5pm [1.147995859s]
    Jan 31 00:20:18.661: INFO: Got endpoints: latency-svc-nlcrt [681.464083ms]
    Jan 31 00:20:18.662: INFO: Got endpoints: latency-svc-9fmx7 [653.822158ms]
    Jan 31 00:20:18.662: INFO: Got endpoints: latency-svc-4t27g [1.009837273s]
    Jan 31 00:20:18.663: INFO: Got endpoints: latency-svc-227zg [1.118794506s]
    Jan 31 00:20:18.669: INFO: Got endpoints: latency-svc-fljs6 [1.072693167s]
    Jan 31 00:20:18.692: INFO: Got endpoints: latency-svc-6tb4t [687.433162ms]
    Jan 31 00:20:18.693: INFO: Got endpoints: latency-svc-qfcjm [1.175196555s]
    Jan 31 00:20:18.693: INFO: Got endpoints: latency-svc-jb74t [660.400739ms]
    Jan 31 00:20:18.694: INFO: Got endpoints: latency-svc-kvml2 [896.961237ms]
    Jan 31 00:20:18.723: INFO: Created: latency-svc-gskb5
    Jan 31 00:20:18.742: INFO: Created: latency-svc-7bgnr
    Jan 31 00:20:18.762: INFO: Got endpoints: latency-svc-gskb5 [127.012013ms]
    Jan 31 00:20:18.776: INFO: Got endpoints: latency-svc-7bgnr [137.100169ms]
    Jan 31 00:20:18.804: INFO: Created: latency-svc-m5hkf
    Jan 31 00:20:18.818: INFO: Got endpoints: latency-svc-m5hkf [179.031062ms]
    Jan 31 00:20:18.850: INFO: Created: latency-svc-ttp88
    Jan 31 00:20:18.872: INFO: Got endpoints: latency-svc-ttp88 [231.720729ms]
    Jan 31 00:20:18.905: INFO: Created: latency-svc-qzcht
    Jan 31 00:20:18.920: INFO: Got endpoints: latency-svc-qzcht [277.727112ms]
    Jan 31 00:20:18.938: INFO: Created: latency-svc-h677m
    Jan 31 00:20:18.938: INFO: Got endpoints: latency-svc-h677m [282.840613ms]
    Jan 31 00:20:18.970: INFO: Created: latency-svc-tlxv7
    Jan 31 00:20:18.985: INFO: Got endpoints: latency-svc-tlxv7 [322.694731ms]
    Jan 31 00:20:19.027: INFO: Created: latency-svc-47tjg
    Jan 31 00:20:19.039: INFO: Got endpoints: latency-svc-47tjg [375.943697ms]
    Jan 31 00:20:19.062: INFO: Created: latency-svc-fqlzp
    Jan 31 00:20:19.100: INFO: Got endpoints: latency-svc-fqlzp [437.524501ms]
    Jan 31 00:20:19.154: INFO: Created: latency-svc-lss2k
    Jan 31 00:20:19.173: INFO: Got endpoints: latency-svc-lss2k [509.986293ms]
    Jan 31 00:20:19.192: INFO: Created: latency-svc-gmrzr
    Jan 31 00:20:19.204: INFO: Got endpoints: latency-svc-gmrzr [534.619904ms]
    Jan 31 00:20:19.222: INFO: Created: latency-svc-t4nsc
    Jan 31 00:20:19.235: INFO: Got endpoints: latency-svc-t4nsc [542.129697ms]
    Jan 31 00:20:19.262: INFO: Created: latency-svc-gttgs
    Jan 31 00:20:19.273: INFO: Got endpoints: latency-svc-gttgs [579.077627ms]
    Jan 31 00:20:19.338: INFO: Created: latency-svc-cx449
    Jan 31 00:20:19.350: INFO: Got endpoints: latency-svc-cx449 [657.834448ms]
    Jan 31 00:20:19.366: INFO: Created: latency-svc-n7dvg
    Jan 31 00:20:19.381: INFO: Got endpoints: latency-svc-n7dvg [687.111606ms]
    Jan 31 00:20:19.397: INFO: Created: latency-svc-xcrtv
    Jan 31 00:20:19.410: INFO: Got endpoints: latency-svc-xcrtv [648.437976ms]
    Jan 31 00:20:19.425: INFO: Created: latency-svc-btzmj
    Jan 31 00:20:19.436: INFO: Got endpoints: latency-svc-btzmj [659.656111ms]
    Jan 31 00:20:19.500: INFO: Created: latency-svc-kg5df
    Jan 31 00:20:19.510: INFO: Created: latency-svc-tjl78
    Jan 31 00:20:19.513: INFO: Got endpoints: latency-svc-kg5df [695.242794ms]
    Jan 31 00:20:19.527: INFO: Got endpoints: latency-svc-tjl78 [654.596719ms]
    Jan 31 00:20:19.585: INFO: Created: latency-svc-2r2wr
    Jan 31 00:20:19.593: INFO: Got endpoints: latency-svc-2r2wr [673.331313ms]
    Jan 31 00:20:19.612: INFO: Created: latency-svc-9xw6f
    Jan 31 00:20:19.628: INFO: Got endpoints: latency-svc-9xw6f [690.060085ms]
    Jan 31 00:20:19.645: INFO: Created: latency-svc-8mhgr
    Jan 31 00:20:19.654: INFO: Got endpoints: latency-svc-8mhgr [668.879128ms]
    Jan 31 00:20:19.695: INFO: Created: latency-svc-r92dq
    Jan 31 00:20:19.713: INFO: Got endpoints: latency-svc-r92dq [673.533931ms]
    Jan 31 00:20:19.715: INFO: Created: latency-svc-g7r64
    Jan 31 00:20:19.729: INFO: Got endpoints: latency-svc-g7r64 [629.126054ms]
    Jan 31 00:20:19.737: INFO: Created: latency-svc-xnj6t
    Jan 31 00:20:19.802: INFO: Got endpoints: latency-svc-xnj6t [628.862342ms]
    Jan 31 00:20:19.856: INFO: Created: latency-svc-blmrh
    Jan 31 00:20:19.865: INFO: Got endpoints: latency-svc-blmrh [660.932923ms]
    Jan 31 00:20:19.924: INFO: Created: latency-svc-2d6v9
    Jan 31 00:20:19.925: INFO: Created: latency-svc-28l27
    Jan 31 00:20:19.957: INFO: Got endpoints: latency-svc-2d6v9 [719.627297ms]
    Jan 31 00:20:19.959: INFO: Got endpoints: latency-svc-28l27 [685.311232ms]
    Jan 31 00:20:20.014: INFO: Created: latency-svc-dc5mv
    Jan 31 00:20:20.025: INFO: Got endpoints: latency-svc-dc5mv [674.63397ms]
    Jan 31 00:20:20.091: INFO: Created: latency-svc-2nbs4
    Jan 31 00:20:20.091: INFO: Got endpoints: latency-svc-2nbs4 [710.400821ms]
    Jan 31 00:20:20.125: INFO: Created: latency-svc-rh425
    Jan 31 00:20:20.136: INFO: Got endpoints: latency-svc-rh425 [725.98253ms]
    Jan 31 00:20:20.165: INFO: Created: latency-svc-zd9kt
    Jan 31 00:20:20.175: INFO: Got endpoints: latency-svc-zd9kt [739.125524ms]
    Jan 31 00:20:20.192: INFO: Created: latency-svc-m76wm
    Jan 31 00:20:20.220: INFO: Got endpoints: latency-svc-m76wm [706.565857ms]
    Jan 31 00:20:20.226: INFO: Created: latency-svc-k4vrt
    Jan 31 00:20:20.235: INFO: Got endpoints: latency-svc-k4vrt [707.818662ms]
    Jan 31 00:20:20.253: INFO: Created: latency-svc-5zgth
    Jan 31 00:20:20.262: INFO: Got endpoints: latency-svc-5zgth [668.124144ms]
    Jan 31 00:20:20.292: INFO: Created: latency-svc-sbj5r
    Jan 31 00:20:20.305: INFO: Got endpoints: latency-svc-sbj5r [676.903803ms]
    Jan 31 00:20:20.352: INFO: Created: latency-svc-9pzr6
    Jan 31 00:20:20.352: INFO: Got endpoints: latency-svc-9pzr6 [698.876424ms]
    Jan 31 00:20:20.357: INFO: Created: latency-svc-4jstk
    Jan 31 00:20:20.368: INFO: Got endpoints: latency-svc-4jstk [655.248743ms]
    Jan 31 00:20:20.383: INFO: Created: latency-svc-jp572
    Jan 31 00:20:20.394: INFO: Got endpoints: latency-svc-jp572 [664.431143ms]
    Jan 31 00:20:20.411: INFO: Created: latency-svc-7njzg
    Jan 31 00:20:20.444: INFO: Got endpoints: latency-svc-7njzg [642.053644ms]
    Jan 31 00:20:20.455: INFO: Created: latency-svc-4f7kp
    Jan 31 00:20:20.473: INFO: Got endpoints: latency-svc-4f7kp [607.216869ms]
    Jan 31 00:20:20.499: INFO: Created: latency-svc-ws2d9
    Jan 31 00:20:20.511: INFO: Got endpoints: latency-svc-ws2d9 [553.716503ms]
    Jan 31 00:20:20.559: INFO: Created: latency-svc-sq474
    Jan 31 00:20:20.572: INFO: Got endpoints: latency-svc-sq474 [613.091906ms]
    Jan 31 00:20:20.601: INFO: Created: latency-svc-597xm
    Jan 31 00:20:20.615: INFO: Got endpoints: latency-svc-597xm [590.46786ms]
    Jan 31 00:20:20.658: INFO: Created: latency-svc-jh54z
    Jan 31 00:20:20.664: INFO: Got endpoints: latency-svc-jh54z [572.395488ms]
    Jan 31 00:20:20.696: INFO: Created: latency-svc-2m4c6
    Jan 31 00:20:20.710: INFO: Got endpoints: latency-svc-2m4c6 [573.304778ms]
    Jan 31 00:20:20.743: INFO: Created: latency-svc-4nq2z
    Jan 31 00:20:20.761: INFO: Got endpoints: latency-svc-4nq2z [585.929419ms]
    Jan 31 00:20:20.771: INFO: Created: latency-svc-tmwc5
    Jan 31 00:20:20.785: INFO: Got endpoints: latency-svc-tmwc5 [565.014689ms]
    Jan 31 00:20:20.843: INFO: Created: latency-svc-shql4
    Jan 31 00:20:20.843: INFO: Got endpoints: latency-svc-shql4 [607.918566ms]
    Jan 31 00:20:20.853: INFO: Created: latency-svc-vw5jz
    Jan 31 00:20:20.870: INFO: Got endpoints: latency-svc-vw5jz [608.440581ms]
    Jan 31 00:20:20.884: INFO: Created: latency-svc-hqttj
    Jan 31 00:20:20.908: INFO: Got endpoints: latency-svc-hqttj [602.081446ms]
    Jan 31 00:20:20.922: INFO: Created: latency-svc-2ms46
    Jan 31 00:20:20.934: INFO: Got endpoints: latency-svc-2ms46 [581.021715ms]
    Jan 31 00:20:20.954: INFO: Created: latency-svc-7pr86
    Jan 31 00:20:20.968: INFO: Got endpoints: latency-svc-7pr86 [600.18401ms]
    Jan 31 00:20:20.994: INFO: Created: latency-svc-4tght
    Jan 31 00:20:21.005: INFO: Got endpoints: latency-svc-4tght [610.778822ms]
    Jan 31 00:20:21.083: INFO: Created: latency-svc-ggh8v
    Jan 31 00:20:21.104: INFO: Got endpoints: latency-svc-ggh8v [660.270917ms]
    Jan 31 00:20:21.596: INFO: Created: latency-svc-cq8dk
    Jan 31 00:20:21.597: INFO: Created: latency-svc-srtgs
    Jan 31 00:20:21.608: INFO: Created: latency-svc-lldx6
    Jan 31 00:20:21.610: INFO: Created: latency-svc-dt2hf
    Jan 31 00:20:21.611: INFO: Created: latency-svc-6nrft
    Jan 31 00:20:21.611: INFO: Created: latency-svc-8nd2v
    Jan 31 00:20:21.611: INFO: Created: latency-svc-k6t7j
    Jan 31 00:20:21.613: INFO: Created: latency-svc-79csf
    Jan 31 00:20:21.614: INFO: Created: latency-svc-snp8k
    Jan 31 00:20:21.615: INFO: Created: latency-svc-gxs45
    Jan 31 00:20:21.615: INFO: Created: latency-svc-l69br
    Jan 31 00:20:21.616: INFO: Created: latency-svc-rz57t
    Jan 31 00:20:21.616: INFO: Created: latency-svc-tqmww
    Jan 31 00:20:21.616: INFO: Created: latency-svc-vw2bb
    Jan 31 00:20:21.617: INFO: Created: latency-svc-frjmz
    Jan 31 00:20:21.617: INFO: Got endpoints: latency-svc-cq8dk [612.07637ms]
    Jan 31 00:20:21.618: INFO: Got endpoints: latency-svc-lldx6 [513.755176ms]
    Jan 31 00:20:21.618: INFO: Got endpoints: latency-svc-srtgs [1.145617053s]
    Jan 31 00:20:21.624: INFO: Got endpoints: latency-svc-dt2hf [689.661978ms]
    Jan 31 00:20:21.624: INFO: Got endpoints: latency-svc-6nrft [655.521122ms]
    Jan 31 00:20:21.629: INFO: Got endpoints: latency-svc-tqmww [1.117507651s]
    Jan 31 00:20:21.636: INFO: Got endpoints: latency-svc-snp8k [728.290232ms]
    Jan 31 00:20:21.637: INFO: Got endpoints: latency-svc-8nd2v [793.91699ms]
    Jan 31 00:20:21.643: INFO: Got endpoints: latency-svc-rz57t [772.655049ms]
    Jan 31 00:20:21.650: INFO: Got endpoints: latency-svc-l69br [1.077331804s]
    Jan 31 00:20:21.650: INFO: Got endpoints: latency-svc-79csf [1.034738108s]
    Jan 31 00:20:21.652: INFO: Got endpoints: latency-svc-k6t7j [942.097597ms]
    Jan 31 00:20:21.656: INFO: Got endpoints: latency-svc-gxs45 [992.410119ms]
    Jan 31 00:20:21.660: INFO: Got endpoints: latency-svc-frjmz [874.768718ms]
    Jan 31 00:20:21.662: INFO: Got endpoints: latency-svc-vw2bb [900.082588ms]
    Jan 31 00:20:21.691: INFO: Created: latency-svc-hhlg9
    Jan 31 00:20:21.699: INFO: Got endpoints: latency-svc-hhlg9 [82.242449ms]
    Jan 31 00:20:22.242: INFO: Created: latency-svc-kd7t4
    Jan 31 00:20:22.245: INFO: Created: latency-svc-hcw7v
    Jan 31 00:20:22.257: INFO: Created: latency-svc-7kwd7
    Jan 31 00:20:22.258: INFO: Created: latency-svc-rskb8
    Jan 31 00:20:22.259: INFO: Created: latency-svc-srr49
    Jan 31 00:20:22.259: INFO: Created: latency-svc-wn2h6
    Jan 31 00:20:22.264: INFO: Created: latency-svc-j7nqb
    Jan 31 00:20:22.265: INFO: Created: latency-svc-jqfch
    Jan 31 00:20:22.266: INFO: Created: latency-svc-b7jsk
    Jan 31 00:20:22.267: INFO: Created: latency-svc-gb9b4
    Jan 31 00:20:22.267: INFO: Got endpoints: latency-svc-kd7t4 [606.699218ms]
    Jan 31 00:20:22.267: INFO: Created: latency-svc-v7c7x
    Jan 31 00:20:22.268: INFO: Created: latency-svc-xp5sg
    Jan 31 00:20:22.268: INFO: Created: latency-svc-x5nqj
    Jan 31 00:20:22.268: INFO: Got endpoints: latency-svc-hcw7v [611.971865ms]
    Jan 31 00:20:22.269: INFO: Created: latency-svc-5ktr9
    Jan 31 00:20:22.270: INFO: Created: latency-svc-v8rtk
    Jan 31 00:20:22.270: INFO: Got endpoints: latency-svc-j7nqb [646.15427ms]
    Jan 31 00:20:22.270: INFO: Got endpoints: latency-svc-7kwd7 [571.188995ms]
    Jan 31 00:20:22.271: INFO: Got endpoints: latency-svc-rskb8 [608.581985ms]
    Jan 31 00:20:22.284: INFO: Got endpoints: latency-svc-xp5sg [633.453258ms]
    Jan 31 00:20:22.285: INFO: Got endpoints: latency-svc-x5nqj [667.016843ms]
    Jan 31 00:20:22.286: INFO: Got endpoints: latency-svc-v7c7x [633.017323ms]
    Jan 31 00:20:22.287: INFO: Got endpoints: latency-svc-gb9b4 [636.146213ms]
    Jan 31 00:20:22.287: INFO: Got endpoints: latency-svc-5ktr9 [668.843928ms]
    Jan 31 00:20:22.312: INFO: Got endpoints: latency-svc-v8rtk [675.88578ms]
    Jan 31 00:20:22.318: INFO: Got endpoints: latency-svc-wn2h6 [680.548425ms]
    Jan 31 00:20:22.322: INFO: Got endpoints: latency-svc-b7jsk [698.138637ms]
    Jan 31 00:20:22.330: INFO: Got endpoints: latency-svc-srr49 [701.062943ms]
    Jan 31 00:20:22.331: INFO: Got endpoints: latency-svc-jqfch [687.352133ms]
    Jan 31 00:20:22.344: INFO: Created: latency-svc-2bkwk
    Jan 31 00:20:22.344: INFO: Got endpoints: latency-svc-2bkwk [77.565423ms]
    Jan 31 00:20:22.363: INFO: Created: latency-svc-jj8cs
    Jan 31 00:20:22.377: INFO: Got endpoints: latency-svc-jj8cs [108.216394ms]
    Jan 31 00:20:22.399: INFO: Created: latency-svc-8d5kq
    Jan 31 00:20:22.410: INFO: Got endpoints: latency-svc-8d5kq [140.273235ms]
    Jan 31 00:20:22.479: INFO: Created: latency-svc-62d6m
    Jan 31 00:20:22.482: INFO: Got endpoints: latency-svc-62d6m [211.264407ms]
    Jan 31 00:20:22.488: INFO: Created: latency-svc-fhg8q
    Jan 31 00:20:22.503: INFO: Got endpoints: latency-svc-fhg8q [231.959847ms]
    Jan 31 00:20:22.521: INFO: Created: latency-svc-qp5l8
    Jan 31 00:20:22.536: INFO: Got endpoints: latency-svc-qp5l8 [248.505591ms]
    Jan 31 00:20:22.550: INFO: Created: latency-svc-zdd5p
    Jan 31 00:20:22.568: INFO: Got endpoints: latency-svc-zdd5p [282.016289ms]
    Jan 31 00:20:22.584: INFO: Created: latency-svc-8s22k
    Jan 31 00:20:22.598: INFO: Got endpoints: latency-svc-8s22k [313.661124ms]
    Jan 31 00:20:22.614: INFO: Created: latency-svc-8mtw4
    Jan 31 00:20:22.630: INFO: Got endpoints: latency-svc-8mtw4 [343.731724ms]
    Jan 31 00:20:22.642: INFO: Created: latency-svc-pvrsc
    Jan 31 00:20:22.653: INFO: Got endpoints: latency-svc-pvrsc [365.576257ms]
    Jan 31 00:20:22.674: INFO: Created: latency-svc-hq72v
    Jan 31 00:20:22.696: INFO: Got endpoints: latency-svc-hq72v [383.694296ms]
    Jan 31 00:20:22.773: INFO: Created: latency-svc-972qz
    Jan 31 00:20:22.774: INFO: Created: latency-svc-286p4
    Jan 31 00:20:22.781: INFO: Created: latency-svc-g77jz
    Jan 31 00:20:22.787: INFO: Got endpoints: latency-svc-972qz [465.346394ms]
    Jan 31 00:20:22.787: INFO: Got endpoints: latency-svc-286p4 [469.516878ms]
    Jan 31 00:20:22.798: INFO: Got endpoints: latency-svc-g77jz [468.655496ms]
    Jan 31 00:20:22.822: INFO: Created: latency-svc-n2qt8
    Jan 31 00:20:22.846: INFO: Got endpoints: latency-svc-n2qt8 [514.133846ms]
    Jan 31 00:20:22.860: INFO: Created: latency-svc-xlvjp
    Jan 31 00:20:22.875: INFO: Got endpoints: latency-svc-xlvjp [528.807796ms]
    Jan 31 00:20:22.903: INFO: Created: latency-svc-cwcvl
    Jan 31 00:20:22.923: INFO: Got endpoints: latency-svc-cwcvl [545.46313ms]
    Jan 31 00:20:22.939: INFO: Created: latency-svc-sswxp
    Jan 31 00:20:22.953: INFO: Got endpoints: latency-svc-sswxp [543.077094ms]
    Jan 31 00:20:22.998: INFO: Created: latency-svc-5djbq
    Jan 31 00:20:22.998: INFO: Got endpoints: latency-svc-5djbq [515.766977ms]
    Jan 31 00:20:23.033: INFO: Created: latency-svc-d578k
    Jan 31 00:20:23.054: INFO: Got endpoints: latency-svc-d578k [550.954365ms]
    Jan 31 00:20:23.062: INFO: Created: latency-svc-gb4h4
    Jan 31 00:20:23.099: INFO: Got endpoints: latency-svc-gb4h4 [563.341351ms]
    Jan 31 00:20:23.148: INFO: Created: latency-svc-q5vn5
    Jan 31 00:20:23.148: INFO: Got endpoints: latency-svc-q5vn5 [579.888421ms]
    Jan 31 00:20:23.156: INFO: Created: latency-svc-p8zq2
    Jan 31 00:20:23.185: INFO: Got endpoints: latency-svc-p8zq2 [586.566299ms]
    Jan 31 00:20:23.226: INFO: Created: latency-svc-n8qzj
    Jan 31 00:20:23.238: INFO: Created: latency-svc-b665r
    Jan 31 00:20:23.241: INFO: Got endpoints: latency-svc-n8qzj [610.76863ms]
    Jan 31 00:20:23.251: INFO: Got endpoints: latency-svc-b665r [597.327885ms]
    Jan 31 00:20:23.285: INFO: Created: latency-svc-78wzz
    Jan 31 00:20:23.301: INFO: Got endpoints: latency-svc-78wzz [604.237848ms]
    Jan 31 00:20:23.325: INFO: Created: latency-svc-l9l5q
    Jan 31 00:20:23.340: INFO: Got endpoints: latency-svc-l9l5q [548.472721ms]
    Jan 31 00:20:23.345: INFO: Created: latency-svc-bd6pc
    Jan 31 00:20:23.353: INFO: Got endpoints: latency-svc-bd6pc [561.148679ms]
    Jan 31 00:20:23.391: INFO: Created: latency-svc-lhqz4
    Jan 31 00:20:23.410: INFO: Got endpoints: latency-svc-lhqz4 [611.956748ms]
    Jan 31 00:20:23.481: INFO: Created: latency-svc-f6ptx
    Jan 31 00:20:23.482: INFO: Got endpoints: latency-svc-f6ptx [636.478467ms]
    Jan 31 00:20:23.492: INFO: Created: latency-svc-nn8dp
    Jan 31 00:20:23.506: INFO: Got endpoints: latency-svc-nn8dp [630.986827ms]
    Jan 31 00:20:23.517: INFO: Created: latency-svc-mpc87
    Jan 31 00:20:23.543: INFO: Got endpoints: latency-svc-mpc87 [620.459265ms]
    Jan 31 00:20:23.549: INFO: Created: latency-svc-26pz9
    Jan 31 00:20:23.559: INFO: Got endpoints: latency-svc-26pz9 [605.419135ms]
    Jan 31 00:20:23.573: INFO: Created: latency-svc-blsz8
    Jan 31 00:20:23.586: INFO: Got endpoints: latency-svc-blsz8 [587.946123ms]
    Jan 31 00:20:23.602: INFO: Created: latency-svc-6z2zk
    Jan 31 00:20:23.616: INFO: Got endpoints: latency-svc-6z2zk [562.011423ms]
    Jan 31 00:20:23.653: INFO: Created: latency-svc-7djrj
    Jan 31 00:20:23.665: INFO: Got endpoints: latency-svc-7djrj [565.840078ms]
    Jan 31 00:20:23.693: INFO: Created: latency-svc-d5g54
    Jan 31 00:20:23.693: INFO: Got endpoints: latency-svc-d5g54 [545.643253ms]
    Jan 31 00:20:23.703: INFO: Created: latency-svc-vtgnk
    Jan 31 00:20:23.713: INFO: Got endpoints: latency-svc-vtgnk [528.476584ms]
    Jan 31 00:20:23.740: INFO: Created: latency-svc-4rzc4
    Jan 31 00:20:23.754: INFO: Got endpoints: latency-svc-4rzc4 [513.159901ms]
    Jan 31 00:20:23.773: INFO: Created: latency-svc-56fjm
    Jan 31 00:20:23.792: INFO: Got endpoints: latency-svc-56fjm [540.5771ms]
    Jan 31 00:20:23.804: INFO: Created: latency-svc-zhkwr
    Jan 31 00:20:23.816: INFO: Got endpoints: latency-svc-zhkwr [514.834385ms]
    Jan 31 00:20:23.867: INFO: Created: latency-svc-w47nj
    Jan 31 00:20:23.870: INFO: Created: latency-svc-lvbsx
    Jan 31 00:20:23.890: INFO: Got endpoints: latency-svc-lvbsx [536.660422ms]
    Jan 31 00:20:23.890: INFO: Got endpoints: latency-svc-w47nj [549.015786ms]
    Jan 31 00:20:23.911: INFO: Created: latency-svc-m9dx5
    Jan 31 00:20:23.925: INFO: Got endpoints: latency-svc-m9dx5 [514.306282ms]
    Jan 31 00:20:23.925: INFO: Latencies: [68.688361ms 77.565423ms 82.242449ms 92.102491ms 93.038302ms 108.216394ms 127.012013ms 133.562881ms 136.566938ms 137.100169ms 140.273235ms 157.947293ms 179.031062ms 191.586215ms 192.432155ms 211.264407ms 224.629009ms 231.720729ms 231.959847ms 246.901438ms 248.505591ms 277.727112ms 282.016289ms 282.840613ms 286.761951ms 301.43912ms 313.661124ms 322.694731ms 323.678452ms 343.731724ms 358.04801ms 365.576257ms 375.943697ms 378.216465ms 383.694296ms 399.303244ms 419.738957ms 437.524501ms 454.341579ms 465.346394ms 468.655496ms 469.516878ms 482.337643ms 491.524762ms 504.938994ms 508.731038ms 509.986293ms 513.159901ms 513.755176ms 514.133846ms 514.306282ms 514.834385ms 515.766977ms 528.476584ms 528.807796ms 530.41422ms 534.619904ms 536.660422ms 540.5771ms 542.129697ms 543.077094ms 545.46313ms 545.643253ms 547.896174ms 548.472721ms 549.015786ms 550.954365ms 552.793196ms 553.716503ms 556.609024ms 561.148679ms 561.347681ms 562.011423ms 563.341351ms 565.014689ms 565.840078ms 571.188995ms 571.976543ms 572.395488ms 573.304778ms 578.19825ms 579.077627ms 579.110239ms 579.888421ms 581.021715ms 585.929419ms 586.566299ms 587.946123ms 590.016798ms 590.46786ms 597.327885ms 600.18401ms 602.081446ms 604.237848ms 604.465898ms 605.419135ms 606.699218ms 607.216869ms 607.902997ms 607.918566ms 607.969611ms 608.440581ms 608.581985ms 609.380758ms 610.76863ms 610.778822ms 611.956748ms 611.971865ms 612.07637ms 613.091906ms 620.459265ms 625.870519ms 626.954785ms 627.585761ms 628.862342ms 629.126054ms 629.157423ms 630.410755ms 630.986827ms 633.017323ms 633.453258ms 634.088311ms 636.146213ms 636.478467ms 641.248886ms 641.941125ms 642.053644ms 646.15427ms 648.437976ms 653.822158ms 654.596719ms 655.248743ms 655.521122ms 657.834448ms 659.656111ms 660.270917ms 660.400739ms 660.932923ms 664.431143ms 667.016843ms 668.124144ms 668.843928ms 668.879128ms 673.331313ms 673.533931ms 674.63397ms 675.88578ms 676.903803ms 680.548425ms 681.464083ms 682.858411ms 685.311232ms 687.111606ms 687.352133ms 687.433162ms 689.661978ms 690.060085ms 695.242794ms 698.138637ms 698.876424ms 701.062943ms 706.565857ms 707.818662ms 710.400821ms 719.627297ms 725.98253ms 728.290232ms 730.728743ms 736.38073ms 739.125524ms 760.579027ms 772.655049ms 793.91699ms 862.165056ms 874.768718ms 889.506196ms 894.035765ms 896.961237ms 900.082588ms 907.456826ms 913.183478ms 926.898597ms 942.097597ms 981.61499ms 992.410119ms 996.40759ms 1.009837273s 1.019998118s 1.034738108s 1.072693167s 1.072868017s 1.077331804s 1.08125858s 1.117507651s 1.118794506s 1.12349472s 1.142685905s 1.145617053s 1.147995859s 1.175196555s]
    Jan 31 00:20:23.925: INFO: 50 %ile: 607.969611ms
    Jan 31 00:20:23.925: INFO: 90 %ile: 913.183478ms
    Jan 31 00:20:23.925: INFO: 99 %ile: 1.147995859s
    Jan 31 00:20:23.925: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:20:23.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-8946" for this suite. 01/31/23 00:20:23.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:20:23.992
Jan 31 00:20:23.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename events 01/31/23 00:20:23.993
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:24.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:24.07
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/31/23 00:20:24.082
STEP: listing events in all namespaces 01/31/23 00:20:24.112
STEP: listing events in test namespace 01/31/23 00:20:24.129
STEP: listing events with field selection filtering on source 01/31/23 00:20:24.145
STEP: listing events with field selection filtering on reportingController 01/31/23 00:20:24.164
STEP: getting the test event 01/31/23 00:20:24.176
STEP: patching the test event 01/31/23 00:20:24.187
STEP: getting the test event 01/31/23 00:20:24.222
STEP: updating the test event 01/31/23 00:20:24.236
STEP: getting the test event 01/31/23 00:20:24.285
STEP: deleting the test event 01/31/23 00:20:24.296
STEP: listing events in all namespaces 01/31/23 00:20:24.326
STEP: listing events in test namespace 01/31/23 00:20:24.34
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 31 00:20:24.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4815" for this suite. 01/31/23 00:20:24.373
------------------------------
• [0.407 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:20:23.992
    Jan 31 00:20:23.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename events 01/31/23 00:20:23.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:24.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:24.07
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/31/23 00:20:24.082
    STEP: listing events in all namespaces 01/31/23 00:20:24.112
    STEP: listing events in test namespace 01/31/23 00:20:24.129
    STEP: listing events with field selection filtering on source 01/31/23 00:20:24.145
    STEP: listing events with field selection filtering on reportingController 01/31/23 00:20:24.164
    STEP: getting the test event 01/31/23 00:20:24.176
    STEP: patching the test event 01/31/23 00:20:24.187
    STEP: getting the test event 01/31/23 00:20:24.222
    STEP: updating the test event 01/31/23 00:20:24.236
    STEP: getting the test event 01/31/23 00:20:24.285
    STEP: deleting the test event 01/31/23 00:20:24.296
    STEP: listing events in all namespaces 01/31/23 00:20:24.326
    STEP: listing events in test namespace 01/31/23 00:20:24.34
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:20:24.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4815" for this suite. 01/31/23 00:20:24.373
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:20:24.4
Jan 31 00:20:24.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename containers 01/31/23 00:20:24.403
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:24.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:24.478
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/31/23 00:20:24.491
Jan 31 00:20:24.533: INFO: Waiting up to 5m0s for pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61" in namespace "containers-4249" to be "Succeeded or Failed"
Jan 31 00:20:24.546: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 13.361992ms
Jan 31 00:20:26.561: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02788693s
Jan 31 00:20:28.563: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02972154s
Jan 31 00:20:30.564: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031190683s
STEP: Saw pod success 01/31/23 00:20:30.565
Jan 31 00:20:30.565: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61" satisfied condition "Succeeded or Failed"
Jan 31 00:20:30.592: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 container agnhost-container: <nil>
STEP: delete the pod 01/31/23 00:20:30.63
Jan 31 00:20:30.673: INFO: Waiting for pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 to disappear
Jan 31 00:20:30.686: INFO: Pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 31 00:20:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4249" for this suite. 01/31/23 00:20:30.707
------------------------------
• [SLOW TEST] [6.336 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:20:24.4
    Jan 31 00:20:24.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename containers 01/31/23 00:20:24.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:24.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:24.478
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/31/23 00:20:24.491
    Jan 31 00:20:24.533: INFO: Waiting up to 5m0s for pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61" in namespace "containers-4249" to be "Succeeded or Failed"
    Jan 31 00:20:24.546: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 13.361992ms
    Jan 31 00:20:26.561: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02788693s
    Jan 31 00:20:28.563: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02972154s
    Jan 31 00:20:30.564: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031190683s
    STEP: Saw pod success 01/31/23 00:20:30.565
    Jan 31 00:20:30.565: INFO: Pod "client-containers-0c5c2d96-c340-4471-8123-117b4c394a61" satisfied condition "Succeeded or Failed"
    Jan 31 00:20:30.592: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 container agnhost-container: <nil>
    STEP: delete the pod 01/31/23 00:20:30.63
    Jan 31 00:20:30.673: INFO: Waiting for pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 to disappear
    Jan 31 00:20:30.686: INFO: Pod client-containers-0c5c2d96-c340-4471-8123-117b4c394a61 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:20:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4249" for this suite. 01/31/23 00:20:30.707
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:20:30.739
Jan 31 00:20:30.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename statefulset 01/31/23 00:20:30.742
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:30.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:30.819
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1603 01/31/23 00:20:30.834
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/31/23 00:20:30.849
Jan 31 00:20:30.887: INFO: Found 0 stateful pods, waiting for 3
Jan 31 00:20:40.906: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 00:20:40.906: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 00:20:40.906: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 31 00:20:40.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 00:20:41.397: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 00:20:41.397: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 00:20:41.397: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/31/23 00:20:51.469
Jan 31 00:20:51.517: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/31/23 00:20:51.517
STEP: Updating Pods in reverse ordinal order 01/31/23 00:21:01.576
Jan 31 00:21:01.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 00:21:02.052: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 00:21:02.052: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 00:21:02.052: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 00:21:12.143: INFO: Waiting for StatefulSet statefulset-1603/ss2 to complete update
STEP: Rolling back to a previous revision 01/31/23 00:21:22.177
Jan 31 00:21:22.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 31 00:21:22.618: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 31 00:21:22.618: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 31 00:21:22.618: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 31 00:21:32.730: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/31/23 00:21:42.795
Jan 31 00:21:42.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 31 00:21:43.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 31 00:21:43.208: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 31 00:21:43.208: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 31 00:21:53.294: INFO: Waiting for StatefulSet statefulset-1603/ss2 to complete update
Jan 31 00:21:53.294: INFO: Waiting for Pod statefulset-1603/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 31 00:22:03.331: INFO: Deleting all statefulset in ns statefulset-1603
Jan 31 00:22:03.347: INFO: Scaling statefulset ss2 to 0
Jan 31 00:22:13.440: INFO: Waiting for statefulset status.replicas updated to 0
Jan 31 00:22:13.456: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:22:13.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1603" for this suite. 01/31/23 00:22:13.548
------------------------------
• [SLOW TEST] [102.840 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:20:30.739
    Jan 31 00:20:30.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename statefulset 01/31/23 00:20:30.742
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:20:30.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:20:30.819
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1603 01/31/23 00:20:30.834
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/31/23 00:20:30.849
    Jan 31 00:20:30.887: INFO: Found 0 stateful pods, waiting for 3
    Jan 31 00:20:40.906: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 31 00:20:40.906: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 31 00:20:40.906: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 31 00:20:40.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 31 00:20:41.397: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 31 00:20:41.397: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 31 00:20:41.397: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/31/23 00:20:51.469
    Jan 31 00:20:51.517: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/31/23 00:20:51.517
    STEP: Updating Pods in reverse ordinal order 01/31/23 00:21:01.576
    Jan 31 00:21:01.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 31 00:21:02.052: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 31 00:21:02.052: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 31 00:21:02.052: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 31 00:21:12.143: INFO: Waiting for StatefulSet statefulset-1603/ss2 to complete update
    STEP: Rolling back to a previous revision 01/31/23 00:21:22.177
    Jan 31 00:21:22.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 31 00:21:22.618: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 31 00:21:22.618: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 31 00:21:22.618: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 31 00:21:32.730: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/31/23 00:21:42.795
    Jan 31 00:21:42.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=statefulset-1603 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 31 00:21:43.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 31 00:21:43.208: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 31 00:21:43.208: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 31 00:21:53.294: INFO: Waiting for StatefulSet statefulset-1603/ss2 to complete update
    Jan 31 00:21:53.294: INFO: Waiting for Pod statefulset-1603/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 31 00:22:03.331: INFO: Deleting all statefulset in ns statefulset-1603
    Jan 31 00:22:03.347: INFO: Scaling statefulset ss2 to 0
    Jan 31 00:22:13.440: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 31 00:22:13.456: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:22:13.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1603" for this suite. 01/31/23 00:22:13.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:22:13.583
Jan 31 00:22:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/31/23 00:22:13.585
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:13.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:13.689
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73 01/31/23 00:22:13.712
Jan 31 00:22:13.750: INFO: Pod name my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Found 0 pods out of 1
Jan 31 00:22:18.767: INFO: Pod name my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Found 1 pods out of 1
Jan 31 00:22:18.768: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73" are running
Jan 31 00:22:18.768: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" in namespace "replication-controller-1617" to be "running"
Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn": Phase="Running", Reason="", readiness=true. Elapsed: 19.033959ms
Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" satisfied condition "running"
Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:13 +0000 UTC Reason: Message:}])
Jan 31 00:22:18.788: INFO: Trying to dial the pod
Jan 31 00:22:23.893: INFO: Controller my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Got expected result from replica 1 [my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn]: "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 31 00:22:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1617" for this suite. 01/31/23 00:22:23.944
------------------------------
• [SLOW TEST] [10.386 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:22:13.583
    Jan 31 00:22:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/31/23 00:22:13.585
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:13.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:13.689
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73 01/31/23 00:22:13.712
    Jan 31 00:22:13.750: INFO: Pod name my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Found 0 pods out of 1
    Jan 31 00:22:18.767: INFO: Pod name my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Found 1 pods out of 1
    Jan 31 00:22:18.768: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73" are running
    Jan 31 00:22:18.768: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" in namespace "replication-controller-1617" to be "running"
    Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn": Phase="Running", Reason="", readiness=true. Elapsed: 19.033959ms
    Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" satisfied condition "running"
    Jan 31 00:22:18.787: INFO: Pod "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-31 00:22:13 +0000 UTC Reason: Message:}])
    Jan 31 00:22:18.788: INFO: Trying to dial the pod
    Jan 31 00:22:23.893: INFO: Controller my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73: Got expected result from replica 1 [my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn]: "my-hostname-basic-ed285d08-8c8a-47f2-84e3-a6e9861d8b73-rzskn", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:22:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1617" for this suite. 01/31/23 00:22:23.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:22:23.974
Jan 31 00:22:23.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename resourcequota 01/31/23 00:22:23.977
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:24.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:24.049
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/31/23 00:22:24.061
STEP: Ensuring ResourceQuota status is calculated 01/31/23 00:22:24.08
STEP: Creating a ResourceQuota with not terminating scope 01/31/23 00:22:26.097
STEP: Ensuring ResourceQuota status is calculated 01/31/23 00:22:26.116
STEP: Creating a long running pod 01/31/23 00:22:28.133
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/31/23 00:22:28.179
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/31/23 00:22:30.192
STEP: Deleting the pod 01/31/23 00:22:32.209
STEP: Ensuring resource quota status released the pod usage 01/31/23 00:22:32.277
STEP: Creating a terminating pod 01/31/23 00:22:34.297
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/31/23 00:22:34.383
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/31/23 00:22:36.404
STEP: Deleting the pod 01/31/23 00:22:38.426
STEP: Ensuring resource quota status released the pod usage 01/31/23 00:22:38.484
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 31 00:22:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1499" for this suite. 01/31/23 00:22:40.533
------------------------------
• [SLOW TEST] [16.586 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:22:23.974
    Jan 31 00:22:23.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename resourcequota 01/31/23 00:22:23.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:24.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:24.049
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/31/23 00:22:24.061
    STEP: Ensuring ResourceQuota status is calculated 01/31/23 00:22:24.08
    STEP: Creating a ResourceQuota with not terminating scope 01/31/23 00:22:26.097
    STEP: Ensuring ResourceQuota status is calculated 01/31/23 00:22:26.116
    STEP: Creating a long running pod 01/31/23 00:22:28.133
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/31/23 00:22:28.179
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/31/23 00:22:30.192
    STEP: Deleting the pod 01/31/23 00:22:32.209
    STEP: Ensuring resource quota status released the pod usage 01/31/23 00:22:32.277
    STEP: Creating a terminating pod 01/31/23 00:22:34.297
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/31/23 00:22:34.383
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/31/23 00:22:36.404
    STEP: Deleting the pod 01/31/23 00:22:38.426
    STEP: Ensuring resource quota status released the pod usage 01/31/23 00:22:38.484
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:22:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1499" for this suite. 01/31/23 00:22:40.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:22:40.563
Jan 31 00:22:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:22:40.567
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:40.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:40.653
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 31 00:22:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/31/23 00:22:43.701
Jan 31 00:22:43.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 create -f -'
Jan 31 00:22:44.630: INFO: stderr: ""
Jan 31 00:22:44.630: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 31 00:22:44.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 delete e2e-test-crd-publish-openapi-5847-crds test-cr'
Jan 31 00:22:44.852: INFO: stderr: ""
Jan 31 00:22:44.852: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 31 00:22:44.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 apply -f -'
Jan 31 00:22:45.227: INFO: stderr: ""
Jan 31 00:22:45.227: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 31 00:22:45.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 delete e2e-test-crd-publish-openapi-5847-crds test-cr'
Jan 31 00:22:45.464: INFO: stderr: ""
Jan 31 00:22:45.464: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/31/23 00:22:45.464
Jan 31 00:22:45.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 explain e2e-test-crd-publish-openapi-5847-crds'
Jan 31 00:22:46.287: INFO: stderr: ""
Jan 31 00:22:46.287: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5847-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:22:48.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4513" for this suite. 01/31/23 00:22:48.646
------------------------------
• [SLOW TEST] [8.139 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:22:40.563
    Jan 31 00:22:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:22:40.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:40.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:40.653
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 31 00:22:40.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/31/23 00:22:43.701
    Jan 31 00:22:43.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 create -f -'
    Jan 31 00:22:44.630: INFO: stderr: ""
    Jan 31 00:22:44.630: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 31 00:22:44.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 delete e2e-test-crd-publish-openapi-5847-crds test-cr'
    Jan 31 00:22:44.852: INFO: stderr: ""
    Jan 31 00:22:44.852: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 31 00:22:44.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 apply -f -'
    Jan 31 00:22:45.227: INFO: stderr: ""
    Jan 31 00:22:45.227: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 31 00:22:45.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 --namespace=crd-publish-openapi-4513 delete e2e-test-crd-publish-openapi-5847-crds test-cr'
    Jan 31 00:22:45.464: INFO: stderr: ""
    Jan 31 00:22:45.464: INFO: stdout: "e2e-test-crd-publish-openapi-5847-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/31/23 00:22:45.464
    Jan 31 00:22:45.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-4513 explain e2e-test-crd-publish-openapi-5847-crds'
    Jan 31 00:22:46.287: INFO: stderr: ""
    Jan 31 00:22:46.287: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5847-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:22:48.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4513" for this suite. 01/31/23 00:22:48.646
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:22:48.709
Jan 31 00:22:48.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:22:48.712
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:48.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:48.784
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 31 00:22:48.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/31/23 00:22:51.304
Jan 31 00:22:51.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 create -f -'
Jan 31 00:22:52.246: INFO: stderr: ""
Jan 31 00:22:52.246: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 31 00:22:52.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 delete e2e-test-crd-publish-openapi-9854-crds test-cr'
Jan 31 00:22:52.424: INFO: stderr: ""
Jan 31 00:22:52.424: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 31 00:22:52.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 apply -f -'
Jan 31 00:22:52.805: INFO: stderr: ""
Jan 31 00:22:52.805: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 31 00:22:52.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 delete e2e-test-crd-publish-openapi-9854-crds test-cr'
Jan 31 00:22:53.015: INFO: stderr: ""
Jan 31 00:22:53.015: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/31/23 00:22:53.015
Jan 31 00:22:53.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 explain e2e-test-crd-publish-openapi-9854-crds'
Jan 31 00:22:53.714: INFO: stderr: ""
Jan 31 00:22:53.714: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9854-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:22:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7227" for this suite. 01/31/23 00:22:56.077
------------------------------
• [SLOW TEST] [7.399 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:22:48.709
    Jan 31 00:22:48.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:22:48.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:48.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:48.784
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 31 00:22:48.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/31/23 00:22:51.304
    Jan 31 00:22:51.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 create -f -'
    Jan 31 00:22:52.246: INFO: stderr: ""
    Jan 31 00:22:52.246: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 31 00:22:52.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 delete e2e-test-crd-publish-openapi-9854-crds test-cr'
    Jan 31 00:22:52.424: INFO: stderr: ""
    Jan 31 00:22:52.424: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 31 00:22:52.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 apply -f -'
    Jan 31 00:22:52.805: INFO: stderr: ""
    Jan 31 00:22:52.805: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 31 00:22:52.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 --namespace=crd-publish-openapi-7227 delete e2e-test-crd-publish-openapi-9854-crds test-cr'
    Jan 31 00:22:53.015: INFO: stderr: ""
    Jan 31 00:22:53.015: INFO: stdout: "e2e-test-crd-publish-openapi-9854-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/31/23 00:22:53.015
    Jan 31 00:22:53.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=crd-publish-openapi-7227 explain e2e-test-crd-publish-openapi-9854-crds'
    Jan 31 00:22:53.714: INFO: stderr: ""
    Jan 31 00:22:53.714: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9854-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:22:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7227" for this suite. 01/31/23 00:22:56.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:22:56.109
Jan 31 00:22:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename security-context-test 01/31/23 00:22:56.112
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:56.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:56.187
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 31 00:22:56.238: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c" in namespace "security-context-test-7610" to be "Succeeded or Failed"
Jan 31 00:22:56.255: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.982324ms
Jan 31 00:22:58.275: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Running", Reason="", readiness=true. Elapsed: 2.037009808s
Jan 31 00:23:00.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Running", Reason="", readiness=false. Elapsed: 4.034918375s
Jan 31 00:23:02.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035268044s
Jan 31 00:23:02.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:02.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7610" for this suite. 01/31/23 00:23:02.299
------------------------------
• [SLOW TEST] [6.221 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:22:56.109
    Jan 31 00:22:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename security-context-test 01/31/23 00:22:56.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:22:56.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:22:56.187
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 31 00:22:56.238: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c" in namespace "security-context-test-7610" to be "Succeeded or Failed"
    Jan 31 00:22:56.255: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.982324ms
    Jan 31 00:22:58.275: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Running", Reason="", readiness=true. Elapsed: 2.037009808s
    Jan 31 00:23:00.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Running", Reason="", readiness=false. Elapsed: 4.034918375s
    Jan 31 00:23:02.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035268044s
    Jan 31 00:23:02.273: INFO: Pod "busybox-readonly-false-2dfce021-e67d-4845-b6ee-6ac74e56784c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:02.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7610" for this suite. 01/31/23 00:23:02.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:02.334
Jan 31 00:23:02.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/31/23 00:23:02.336
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:02.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:02.426
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9155.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/31/23 00:23:02.447
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9155.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/31/23 00:23:02.448
STEP: creating a pod to probe /etc/hosts 01/31/23 00:23:02.448
STEP: submitting the pod to kubernetes 01/31/23 00:23:02.448
Jan 31 00:23:02.497: INFO: Waiting up to 15m0s for pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180" in namespace "dns-9155" to be "running"
Jan 31 00:23:02.526: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Pending", Reason="", readiness=false. Elapsed: 29.205736ms
Jan 31 00:23:04.546: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04944858s
Jan 31 00:23:06.548: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Running", Reason="", readiness=true. Elapsed: 4.050713042s
Jan 31 00:23:06.548: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180" satisfied condition "running"
STEP: retrieving the pod 01/31/23 00:23:06.548
STEP: looking for the results for each expected name from probers 01/31/23 00:23:06.567
Jan 31 00:23:06.705: INFO: DNS probes using dns-9155/dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180 succeeded

STEP: deleting the pod 01/31/23 00:23:06.706
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9155" for this suite. 01/31/23 00:23:06.787
------------------------------
• [4.484 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:02.334
    Jan 31 00:23:02.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/31/23 00:23:02.336
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:02.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:02.426
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9155.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/31/23 00:23:02.447
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9155.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/31/23 00:23:02.448
    STEP: creating a pod to probe /etc/hosts 01/31/23 00:23:02.448
    STEP: submitting the pod to kubernetes 01/31/23 00:23:02.448
    Jan 31 00:23:02.497: INFO: Waiting up to 15m0s for pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180" in namespace "dns-9155" to be "running"
    Jan 31 00:23:02.526: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Pending", Reason="", readiness=false. Elapsed: 29.205736ms
    Jan 31 00:23:04.546: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04944858s
    Jan 31 00:23:06.548: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180": Phase="Running", Reason="", readiness=true. Elapsed: 4.050713042s
    Jan 31 00:23:06.548: INFO: Pod "dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180" satisfied condition "running"
    STEP: retrieving the pod 01/31/23 00:23:06.548
    STEP: looking for the results for each expected name from probers 01/31/23 00:23:06.567
    Jan 31 00:23:06.705: INFO: DNS probes using dns-9155/dns-test-e8304e05-bae1-45c8-bb06-e291e7c1b180 succeeded

    STEP: deleting the pod 01/31/23 00:23:06.706
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9155" for this suite. 01/31/23 00:23:06.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:06.828
Jan 31 00:23:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename events 01/31/23 00:23:06.83
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:06.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:06.902
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/31/23 00:23:06.919
STEP: listing all events in all namespaces 01/31/23 00:23:06.957
STEP: patching the test event 01/31/23 00:23:07.013
STEP: fetching the test event 01/31/23 00:23:07.051
STEP: updating the test event 01/31/23 00:23:07.074
STEP: getting the test event 01/31/23 00:23:07.133
STEP: deleting the test event 01/31/23 00:23:07.153
STEP: listing all events in all namespaces 01/31/23 00:23:07.195
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:07.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1683" for this suite. 01/31/23 00:23:07.247
------------------------------
• [0.451 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:06.828
    Jan 31 00:23:06.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename events 01/31/23 00:23:06.83
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:06.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:06.902
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/31/23 00:23:06.919
    STEP: listing all events in all namespaces 01/31/23 00:23:06.957
    STEP: patching the test event 01/31/23 00:23:07.013
    STEP: fetching the test event 01/31/23 00:23:07.051
    STEP: updating the test event 01/31/23 00:23:07.074
    STEP: getting the test event 01/31/23 00:23:07.133
    STEP: deleting the test event 01/31/23 00:23:07.153
    STEP: listing all events in all namespaces 01/31/23 00:23:07.195
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:07.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1683" for this suite. 01/31/23 00:23:07.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:07.285
Jan 31 00:23:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename custom-resource-definition 01/31/23 00:23:07.287
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:07.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:07.385
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 31 00:23:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:08.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2704" for this suite. 01/31/23 00:23:08.513
------------------------------
• [1.257 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:07.285
    Jan 31 00:23:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename custom-resource-definition 01/31/23 00:23:07.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:07.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:07.385
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 31 00:23:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:08.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2704" for this suite. 01/31/23 00:23:08.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:08.552
Jan 31 00:23:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename kubelet-test 01/31/23 00:23:08.554
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:08.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:08.661
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/31/23 00:23:08.738
Jan 31 00:23:08.738: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277" in namespace "kubelet-test-7253" to be "completed"
Jan 31 00:23:08.762: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 24.144402ms
Jan 31 00:23:10.782: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043486401s
Jan 31 00:23:12.780: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041682043s
Jan 31 00:23:14.783: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044425698s
Jan 31 00:23:14.783: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7253" for this suite. 01/31/23 00:23:14.909
------------------------------
• [SLOW TEST] [6.386 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:08.552
    Jan 31 00:23:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename kubelet-test 01/31/23 00:23:08.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:08.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:08.661
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/31/23 00:23:08.738
    Jan 31 00:23:08.738: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277" in namespace "kubelet-test-7253" to be "completed"
    Jan 31 00:23:08.762: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 24.144402ms
    Jan 31 00:23:10.782: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043486401s
    Jan 31 00:23:12.780: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041682043s
    Jan 31 00:23:14.783: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044425698s
    Jan 31 00:23:14.783: INFO: Pod "agnhost-host-aliases7f148e0f-1f60-412b-a218-5f08f7a8c277" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:14.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7253" for this suite. 01/31/23 00:23:14.909
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:14.939
Jan 31 00:23:14.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/31/23 00:23:14.942
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:14.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:15.014
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/31/23 00:23:15.032
Jan 31 00:23:15.074: INFO: Waiting up to 5m0s for pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6" in namespace "downward-api-215" to be "Succeeded or Failed"
Jan 31 00:23:15.095: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.449157ms
Jan 31 00:23:17.120: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046195502s
Jan 31 00:23:19.114: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040478023s
Jan 31 00:23:21.114: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040690724s
STEP: Saw pod success 01/31/23 00:23:21.115
Jan 31 00:23:21.115: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6" satisfied condition "Succeeded or Failed"
Jan 31 00:23:21.134: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 container dapi-container: <nil>
STEP: delete the pod 01/31/23 00:23:21.179
Jan 31 00:23:21.251: INFO: Waiting for pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 to disappear
Jan 31 00:23:21.267: INFO: Pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:21.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-215" for this suite. 01/31/23 00:23:21.288
------------------------------
• [SLOW TEST] [6.377 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:14.939
    Jan 31 00:23:14.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/31/23 00:23:14.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:14.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:15.014
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/31/23 00:23:15.032
    Jan 31 00:23:15.074: INFO: Waiting up to 5m0s for pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6" in namespace "downward-api-215" to be "Succeeded or Failed"
    Jan 31 00:23:15.095: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.449157ms
    Jan 31 00:23:17.120: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046195502s
    Jan 31 00:23:19.114: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040478023s
    Jan 31 00:23:21.114: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040690724s
    STEP: Saw pod success 01/31/23 00:23:21.115
    Jan 31 00:23:21.115: INFO: Pod "downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6" satisfied condition "Succeeded or Failed"
    Jan 31 00:23:21.134: INFO: Trying to get logs from node 10.15.28.227 pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 container dapi-container: <nil>
    STEP: delete the pod 01/31/23 00:23:21.179
    Jan 31 00:23:21.251: INFO: Waiting for pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 to disappear
    Jan 31 00:23:21.267: INFO: Pod downward-api-6f8f0b5b-cf19-44ec-8885-5261d0eff4a6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:21.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-215" for this suite. 01/31/23 00:23:21.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:21.317
Jan 31 00:23:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:23:21.321
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:21.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:21.394
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:23:21.463
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:23:21.778
STEP: Deploying the webhook pod 01/31/23 00:23:21.816
STEP: Wait for the deployment to be ready 01/31/23 00:23:21.858
Jan 31 00:23:21.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:23:23.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:23:25.971
STEP: Verifying the service has paired with the endpoint 01/31/23 00:23:26.026
Jan 31 00:23:27.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 31 00:23:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/31/23 00:23:27.58
STEP: Creating a custom resource that should be denied by the webhook 01/31/23 00:23:27.71
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/31/23 00:23:29.847
STEP: Updating the custom resource with disallowed data should be denied 01/31/23 00:23:29.877
STEP: Deleting the custom resource should be denied 01/31/23 00:23:29.924
STEP: Remove the offending key and value from the custom resource data 01/31/23 00:23:29.957
STEP: Deleting the updated custom resource should be successful 01/31/23 00:23:30.005
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:30.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8238" for this suite. 01/31/23 00:23:30.867
STEP: Destroying namespace "webhook-8238-markers" for this suite. 01/31/23 00:23:30.894
------------------------------
• [SLOW TEST] [9.604 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:21.317
    Jan 31 00:23:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:23:21.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:21.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:21.394
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:23:21.463
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:23:21.778
    STEP: Deploying the webhook pod 01/31/23 00:23:21.816
    STEP: Wait for the deployment to be ready 01/31/23 00:23:21.858
    Jan 31 00:23:21.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:23:23.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:23:25.971
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:23:26.026
    Jan 31 00:23:27.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 31 00:23:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/31/23 00:23:27.58
    STEP: Creating a custom resource that should be denied by the webhook 01/31/23 00:23:27.71
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/31/23 00:23:29.847
    STEP: Updating the custom resource with disallowed data should be denied 01/31/23 00:23:29.877
    STEP: Deleting the custom resource should be denied 01/31/23 00:23:29.924
    STEP: Remove the offending key and value from the custom resource data 01/31/23 00:23:29.957
    STEP: Deleting the updated custom resource should be successful 01/31/23 00:23:30.005
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:30.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8238" for this suite. 01/31/23 00:23:30.867
    STEP: Destroying namespace "webhook-8238-markers" for this suite. 01/31/23 00:23:30.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:30.924
Jan 31 00:23:30.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename custom-resource-definition 01/31/23 00:23:30.927
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:30.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:30.992
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 31 00:23:31.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5902" for this suite. 01/31/23 00:23:34.672
------------------------------
• [3.778 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:30.924
    Jan 31 00:23:30.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename custom-resource-definition 01/31/23 00:23:30.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:30.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:30.992
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 31 00:23:31.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5902" for this suite. 01/31/23 00:23:34.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:34.703
Jan 31 00:23:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/31/23 00:23:34.706
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:34.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:34.779
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-c86688f9-9275-4499-93df-c4bb08c2fca5 01/31/23 00:23:34.798
STEP: Creating a pod to test consume secrets 01/31/23 00:23:34.819
Jan 31 00:23:34.853: INFO: Waiting up to 5m0s for pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1" in namespace "secrets-3518" to be "Succeeded or Failed"
Jan 31 00:23:34.873: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.906578ms
Jan 31 00:23:36.918: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064487927s
Jan 31 00:23:38.894: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04081458s
Jan 31 00:23:40.893: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039083396s
STEP: Saw pod success 01/31/23 00:23:40.893
Jan 31 00:23:40.893: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1" satisfied condition "Succeeded or Failed"
Jan 31 00:23:40.910: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 container secret-volume-test: <nil>
STEP: delete the pod 01/31/23 00:23:40.948
Jan 31 00:23:40.991: INFO: Waiting for pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 to disappear
Jan 31 00:23:41.007: INFO: Pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:41.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3518" for this suite. 01/31/23 00:23:41.032
------------------------------
• [SLOW TEST] [6.355 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:34.703
    Jan 31 00:23:34.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/31/23 00:23:34.706
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:34.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:34.779
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-c86688f9-9275-4499-93df-c4bb08c2fca5 01/31/23 00:23:34.798
    STEP: Creating a pod to test consume secrets 01/31/23 00:23:34.819
    Jan 31 00:23:34.853: INFO: Waiting up to 5m0s for pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1" in namespace "secrets-3518" to be "Succeeded or Failed"
    Jan 31 00:23:34.873: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.906578ms
    Jan 31 00:23:36.918: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064487927s
    Jan 31 00:23:38.894: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04081458s
    Jan 31 00:23:40.893: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039083396s
    STEP: Saw pod success 01/31/23 00:23:40.893
    Jan 31 00:23:40.893: INFO: Pod "pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1" satisfied condition "Succeeded or Failed"
    Jan 31 00:23:40.910: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 container secret-volume-test: <nil>
    STEP: delete the pod 01/31/23 00:23:40.948
    Jan 31 00:23:40.991: INFO: Waiting for pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 to disappear
    Jan 31 00:23:41.007: INFO: Pod pod-secrets-08af865c-d931-4654-a02b-a061319ba9e1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:41.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3518" for this suite. 01/31/23 00:23:41.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:41.064
Jan 31 00:23:41.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename containers 01/31/23 00:23:41.066
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:41.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:41.157
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/31/23 00:23:41.175
Jan 31 00:23:41.212: INFO: Waiting up to 5m0s for pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e" in namespace "containers-1337" to be "Succeeded or Failed"
Jan 31 00:23:41.231: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.716414ms
Jan 31 00:23:43.249: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03614481s
Jan 31 00:23:45.252: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038841842s
Jan 31 00:23:47.250: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03685901s
STEP: Saw pod success 01/31/23 00:23:47.25
Jan 31 00:23:47.251: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e" satisfied condition "Succeeded or Failed"
Jan 31 00:23:47.269: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e container agnhost-container: <nil>
STEP: delete the pod 01/31/23 00:23:47.312
Jan 31 00:23:47.356: INFO: Waiting for pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e to disappear
Jan 31 00:23:47.376: INFO: Pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1337" for this suite. 01/31/23 00:23:47.401
------------------------------
• [SLOW TEST] [6.365 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:41.064
    Jan 31 00:23:41.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename containers 01/31/23 00:23:41.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:41.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:41.157
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/31/23 00:23:41.175
    Jan 31 00:23:41.212: INFO: Waiting up to 5m0s for pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e" in namespace "containers-1337" to be "Succeeded or Failed"
    Jan 31 00:23:41.231: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.716414ms
    Jan 31 00:23:43.249: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03614481s
    Jan 31 00:23:45.252: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038841842s
    Jan 31 00:23:47.250: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03685901s
    STEP: Saw pod success 01/31/23 00:23:47.25
    Jan 31 00:23:47.251: INFO: Pod "client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e" satisfied condition "Succeeded or Failed"
    Jan 31 00:23:47.269: INFO: Trying to get logs from node 10.15.28.227 pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e container agnhost-container: <nil>
    STEP: delete the pod 01/31/23 00:23:47.312
    Jan 31 00:23:47.356: INFO: Waiting for pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e to disappear
    Jan 31 00:23:47.376: INFO: Pod client-containers-bfd4fdd2-aa1e-4a68-bf77-2c31dbbe759e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1337" for this suite. 01/31/23 00:23:47.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:47.439
Jan 31 00:23:47.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename ingressclass 01/31/23 00:23:47.442
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:47.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:47.523
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/31/23 00:23:47.537
STEP: getting /apis/networking.k8s.io 01/31/23 00:23:47.553
STEP: getting /apis/networking.k8s.iov1 01/31/23 00:23:47.561
STEP: creating 01/31/23 00:23:47.569
STEP: getting 01/31/23 00:23:47.631
STEP: listing 01/31/23 00:23:47.648
STEP: watching 01/31/23 00:23:47.666
Jan 31 00:23:47.666: INFO: starting watch
STEP: patching 01/31/23 00:23:47.674
STEP: updating 01/31/23 00:23:47.696
Jan 31 00:23:47.717: INFO: waiting for watch events with expected annotations
Jan 31 00:23:47.717: INFO: saw patched and updated annotations
STEP: deleting 01/31/23 00:23:47.717
STEP: deleting a collection 01/31/23 00:23:47.799
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:47.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-6450" for this suite. 01/31/23 00:23:47.941
------------------------------
• [0.552 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:47.439
    Jan 31 00:23:47.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename ingressclass 01/31/23 00:23:47.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:47.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:47.523
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/31/23 00:23:47.537
    STEP: getting /apis/networking.k8s.io 01/31/23 00:23:47.553
    STEP: getting /apis/networking.k8s.iov1 01/31/23 00:23:47.561
    STEP: creating 01/31/23 00:23:47.569
    STEP: getting 01/31/23 00:23:47.631
    STEP: listing 01/31/23 00:23:47.648
    STEP: watching 01/31/23 00:23:47.666
    Jan 31 00:23:47.666: INFO: starting watch
    STEP: patching 01/31/23 00:23:47.674
    STEP: updating 01/31/23 00:23:47.696
    Jan 31 00:23:47.717: INFO: waiting for watch events with expected annotations
    Jan 31 00:23:47.717: INFO: saw patched and updated annotations
    STEP: deleting 01/31/23 00:23:47.717
    STEP: deleting a collection 01/31/23 00:23:47.799
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:47.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-6450" for this suite. 01/31/23 00:23:47.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:47.997
Jan 31 00:23:47.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename secrets 01/31/23 00:23:47.999
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:48.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:48.088
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-72d2e381-8ebf-4a64-845e-4b8328bea261 01/31/23 00:23:48.104
STEP: Creating a pod to test consume secrets 01/31/23 00:23:48.122
Jan 31 00:23:48.168: INFO: Waiting up to 5m0s for pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408" in namespace "secrets-7613" to be "Succeeded or Failed"
Jan 31 00:23:48.184: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 15.430537ms
Jan 31 00:23:50.203: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034912639s
Jan 31 00:23:52.201: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032827768s
Jan 31 00:23:54.202: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034130614s
STEP: Saw pod success 01/31/23 00:23:54.203
Jan 31 00:23:54.203: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408" satisfied condition "Succeeded or Failed"
Jan 31 00:23:54.220: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 container secret-env-test: <nil>
STEP: delete the pod 01/31/23 00:23:54.284
Jan 31 00:23:54.338: INFO: Waiting for pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 to disappear
Jan 31 00:23:54.355: INFO: Pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:54.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7613" for this suite. 01/31/23 00:23:54.377
------------------------------
• [SLOW TEST] [6.407 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:47.997
    Jan 31 00:23:47.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename secrets 01/31/23 00:23:47.999
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:48.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:48.088
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-72d2e381-8ebf-4a64-845e-4b8328bea261 01/31/23 00:23:48.104
    STEP: Creating a pod to test consume secrets 01/31/23 00:23:48.122
    Jan 31 00:23:48.168: INFO: Waiting up to 5m0s for pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408" in namespace "secrets-7613" to be "Succeeded or Failed"
    Jan 31 00:23:48.184: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 15.430537ms
    Jan 31 00:23:50.203: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034912639s
    Jan 31 00:23:52.201: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032827768s
    Jan 31 00:23:54.202: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034130614s
    STEP: Saw pod success 01/31/23 00:23:54.203
    Jan 31 00:23:54.203: INFO: Pod "pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408" satisfied condition "Succeeded or Failed"
    Jan 31 00:23:54.220: INFO: Trying to get logs from node 10.15.28.227 pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 container secret-env-test: <nil>
    STEP: delete the pod 01/31/23 00:23:54.284
    Jan 31 00:23:54.338: INFO: Waiting for pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 to disappear
    Jan 31 00:23:54.355: INFO: Pod pod-secrets-25e97ef2-25e4-41bd-a469-5c3050bd6408 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:54.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7613" for this suite. 01/31/23 00:23:54.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:54.42
Jan 31 00:23:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename svcaccounts 01/31/23 00:23:54.422
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:54.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:54.506
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-vzcmm"  01/31/23 00:23:54.522
Jan 31 00:23:54.548: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-vzcmm"  01/31/23 00:23:54.548
Jan 31 00:23:54.596: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 31 00:23:54.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2093" for this suite. 01/31/23 00:23:54.617
------------------------------
• [0.229 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:54.42
    Jan 31 00:23:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename svcaccounts 01/31/23 00:23:54.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:54.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:54.506
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-vzcmm"  01/31/23 00:23:54.522
    Jan 31 00:23:54.548: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-vzcmm"  01/31/23 00:23:54.548
    Jan 31 00:23:54.596: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:23:54.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2093" for this suite. 01/31/23 00:23:54.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:23:54.656
Jan 31 00:23:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:23:54.658
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:54.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:54.722
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:23:54.793
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:23:55.415
STEP: Deploying the webhook pod 01/31/23 00:23:55.447
STEP: Wait for the deployment to be ready 01/31/23 00:23:55.491
Jan 31 00:23:55.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:23:57.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:23:59.623
STEP: Verifying the service has paired with the endpoint 01/31/23 00:23:59.683
Jan 31 00:24:00.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 31 00:24:00.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3295-crds.webhook.example.com via the AdmissionRegistration API 01/31/23 00:24:01.241
STEP: Creating a custom resource that should be mutated by the webhook 01/31/23 00:24:01.391
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:24:04.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6368" for this suite. 01/31/23 00:24:04.397
STEP: Destroying namespace "webhook-6368-markers" for this suite. 01/31/23 00:24:04.47
------------------------------
• [SLOW TEST] [9.840 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:23:54.656
    Jan 31 00:23:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:23:54.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:23:54.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:23:54.722
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:23:54.793
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:23:55.415
    STEP: Deploying the webhook pod 01/31/23 00:23:55.447
    STEP: Wait for the deployment to be ready 01/31/23 00:23:55.491
    Jan 31 00:23:55.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:23:57.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 23, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:23:59.623
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:23:59.683
    Jan 31 00:24:00.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 31 00:24:00.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3295-crds.webhook.example.com via the AdmissionRegistration API 01/31/23 00:24:01.241
    STEP: Creating a custom resource that should be mutated by the webhook 01/31/23 00:24:01.391
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:24:04.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6368" for this suite. 01/31/23 00:24:04.397
    STEP: Destroying namespace "webhook-6368-markers" for this suite. 01/31/23 00:24:04.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:24:04.506
Jan 31 00:24:04.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename discovery 01/31/23 00:24:04.509
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:04.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:04.584
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/31/23 00:24:04.608
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 31 00:24:05.290: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 31 00:24:05.299: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 31 00:24:05.299: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 31 00:24:05.299: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 31 00:24:05.299: INFO: Checking APIGroup: apps
Jan 31 00:24:05.305: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 31 00:24:05.305: INFO: Versions found [{apps/v1 v1}]
Jan 31 00:24:05.305: INFO: apps/v1 matches apps/v1
Jan 31 00:24:05.305: INFO: Checking APIGroup: events.k8s.io
Jan 31 00:24:05.312: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 31 00:24:05.312: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 31 00:24:05.312: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 31 00:24:05.312: INFO: Checking APIGroup: authentication.k8s.io
Jan 31 00:24:05.319: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 31 00:24:05.319: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 31 00:24:05.319: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 31 00:24:05.319: INFO: Checking APIGroup: authorization.k8s.io
Jan 31 00:24:05.326: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 31 00:24:05.326: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 31 00:24:05.326: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 31 00:24:05.326: INFO: Checking APIGroup: autoscaling
Jan 31 00:24:05.332: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 31 00:24:05.332: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 31 00:24:05.332: INFO: autoscaling/v2 matches autoscaling/v2
Jan 31 00:24:05.332: INFO: Checking APIGroup: batch
Jan 31 00:24:05.340: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 31 00:24:05.340: INFO: Versions found [{batch/v1 v1}]
Jan 31 00:24:05.340: INFO: batch/v1 matches batch/v1
Jan 31 00:24:05.340: INFO: Checking APIGroup: certificates.k8s.io
Jan 31 00:24:05.347: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 31 00:24:05.347: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 31 00:24:05.347: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 31 00:24:05.347: INFO: Checking APIGroup: networking.k8s.io
Jan 31 00:24:05.355: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 31 00:24:05.355: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 31 00:24:05.355: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 31 00:24:05.355: INFO: Checking APIGroup: policy
Jan 31 00:24:05.362: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 31 00:24:05.362: INFO: Versions found [{policy/v1 v1}]
Jan 31 00:24:05.362: INFO: policy/v1 matches policy/v1
Jan 31 00:24:05.362: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 31 00:24:05.369: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 31 00:24:05.369: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 31 00:24:05.369: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 31 00:24:05.369: INFO: Checking APIGroup: storage.k8s.io
Jan 31 00:24:05.375: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 31 00:24:05.375: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 31 00:24:05.375: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 31 00:24:05.376: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 31 00:24:05.383: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 31 00:24:05.383: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 31 00:24:05.383: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 31 00:24:05.383: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 31 00:24:05.389: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 31 00:24:05.389: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 31 00:24:05.389: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 31 00:24:05.389: INFO: Checking APIGroup: scheduling.k8s.io
Jan 31 00:24:05.396: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 31 00:24:05.396: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 31 00:24:05.397: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 31 00:24:05.397: INFO: Checking APIGroup: coordination.k8s.io
Jan 31 00:24:05.403: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 31 00:24:05.403: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 31 00:24:05.404: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 31 00:24:05.404: INFO: Checking APIGroup: node.k8s.io
Jan 31 00:24:05.409: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 31 00:24:05.409: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 31 00:24:05.409: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 31 00:24:05.409: INFO: Checking APIGroup: discovery.k8s.io
Jan 31 00:24:05.416: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 31 00:24:05.416: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 31 00:24:05.416: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 31 00:24:05.416: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 31 00:24:05.424: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 31 00:24:05.424: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 31 00:24:05.424: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 31 00:24:05.424: INFO: Checking APIGroup: crd.projectcalico.org
Jan 31 00:24:05.431: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 31 00:24:05.431: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 31 00:24:05.431: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 31 00:24:05.431: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 31 00:24:05.439: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 31 00:24:05.439: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 31 00:24:05.439: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 31 00:24:05.439: INFO: Checking APIGroup: ibm.com
Jan 31 00:24:05.445: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jan 31 00:24:05.445: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jan 31 00:24:05.445: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jan 31 00:24:05.445: INFO: Checking APIGroup: metrics.k8s.io
Jan 31 00:24:05.451: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 31 00:24:05.451: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 31 00:24:05.451: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 31 00:24:05.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3960" for this suite. 01/31/23 00:24:05.476
------------------------------
• [1.004 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:24:04.506
    Jan 31 00:24:04.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename discovery 01/31/23 00:24:04.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:04.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:04.584
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/31/23 00:24:04.608
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 31 00:24:05.290: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 31 00:24:05.299: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 31 00:24:05.299: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 31 00:24:05.299: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 31 00:24:05.299: INFO: Checking APIGroup: apps
    Jan 31 00:24:05.305: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 31 00:24:05.305: INFO: Versions found [{apps/v1 v1}]
    Jan 31 00:24:05.305: INFO: apps/v1 matches apps/v1
    Jan 31 00:24:05.305: INFO: Checking APIGroup: events.k8s.io
    Jan 31 00:24:05.312: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 31 00:24:05.312: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 31 00:24:05.312: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 31 00:24:05.312: INFO: Checking APIGroup: authentication.k8s.io
    Jan 31 00:24:05.319: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 31 00:24:05.319: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 31 00:24:05.319: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 31 00:24:05.319: INFO: Checking APIGroup: authorization.k8s.io
    Jan 31 00:24:05.326: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 31 00:24:05.326: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 31 00:24:05.326: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 31 00:24:05.326: INFO: Checking APIGroup: autoscaling
    Jan 31 00:24:05.332: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 31 00:24:05.332: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 31 00:24:05.332: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 31 00:24:05.332: INFO: Checking APIGroup: batch
    Jan 31 00:24:05.340: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 31 00:24:05.340: INFO: Versions found [{batch/v1 v1}]
    Jan 31 00:24:05.340: INFO: batch/v1 matches batch/v1
    Jan 31 00:24:05.340: INFO: Checking APIGroup: certificates.k8s.io
    Jan 31 00:24:05.347: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 31 00:24:05.347: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 31 00:24:05.347: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 31 00:24:05.347: INFO: Checking APIGroup: networking.k8s.io
    Jan 31 00:24:05.355: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 31 00:24:05.355: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 31 00:24:05.355: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 31 00:24:05.355: INFO: Checking APIGroup: policy
    Jan 31 00:24:05.362: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 31 00:24:05.362: INFO: Versions found [{policy/v1 v1}]
    Jan 31 00:24:05.362: INFO: policy/v1 matches policy/v1
    Jan 31 00:24:05.362: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 31 00:24:05.369: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 31 00:24:05.369: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 31 00:24:05.369: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 31 00:24:05.369: INFO: Checking APIGroup: storage.k8s.io
    Jan 31 00:24:05.375: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 31 00:24:05.375: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 31 00:24:05.375: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 31 00:24:05.376: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 31 00:24:05.383: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 31 00:24:05.383: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 31 00:24:05.383: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 31 00:24:05.383: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 31 00:24:05.389: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 31 00:24:05.389: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 31 00:24:05.389: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 31 00:24:05.389: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 31 00:24:05.396: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 31 00:24:05.396: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 31 00:24:05.397: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 31 00:24:05.397: INFO: Checking APIGroup: coordination.k8s.io
    Jan 31 00:24:05.403: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 31 00:24:05.403: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 31 00:24:05.404: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 31 00:24:05.404: INFO: Checking APIGroup: node.k8s.io
    Jan 31 00:24:05.409: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 31 00:24:05.409: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 31 00:24:05.409: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 31 00:24:05.409: INFO: Checking APIGroup: discovery.k8s.io
    Jan 31 00:24:05.416: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 31 00:24:05.416: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 31 00:24:05.416: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 31 00:24:05.416: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 31 00:24:05.424: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 31 00:24:05.424: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 31 00:24:05.424: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 31 00:24:05.424: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 31 00:24:05.431: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 31 00:24:05.431: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 31 00:24:05.431: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 31 00:24:05.431: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 31 00:24:05.439: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 31 00:24:05.439: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jan 31 00:24:05.439: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 31 00:24:05.439: INFO: Checking APIGroup: ibm.com
    Jan 31 00:24:05.445: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Jan 31 00:24:05.445: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Jan 31 00:24:05.445: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Jan 31 00:24:05.445: INFO: Checking APIGroup: metrics.k8s.io
    Jan 31 00:24:05.451: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 31 00:24:05.451: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 31 00:24:05.451: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:24:05.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3960" for this suite. 01/31/23 00:24:05.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:24:05.512
Jan 31 00:24:05.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/31/23 00:24:05.515
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:05.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:05.598
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/31/23 00:24:05.641
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4921;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4921;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +notcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_tcp@PTR;sleep 1; done
 01/31/23 00:24:05.713
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4921;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4921;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +notcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_tcp@PTR;sleep 1; done
 01/31/23 00:24:05.713
STEP: creating a pod to probe DNS 01/31/23 00:24:05.714
STEP: submitting the pod to kubernetes 01/31/23 00:24:05.715
Jan 31 00:24:05.797: INFO: Waiting up to 15m0s for pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902" in namespace "dns-4921" to be "running"
Jan 31 00:24:05.816: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Pending", Reason="", readiness=false. Elapsed: 17.94078ms
Jan 31 00:24:07.847: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049552885s
Jan 31 00:24:09.855: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Running", Reason="", readiness=true. Elapsed: 4.056905014s
Jan 31 00:24:09.855: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902" satisfied condition "running"
STEP: retrieving the pod 01/31/23 00:24:09.856
STEP: looking for the results for each expected name from probers 01/31/23 00:24:09.874
Jan 31 00:24:09.972: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:09.993: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.034: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.255: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.418: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.448: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.503: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.546: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.573: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:10.799: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc]

Jan 31 00:24:15.825: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.848: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.931: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.955: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:15.978: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.149: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.190: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.212: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.258: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.281: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:16.421: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

Jan 31 00:24:20.825: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:20.849: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:20.887: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:20.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:20.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:20.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.215: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.247: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.346: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.369: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:21.535: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

Jan 31 00:24:25.837: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:25.860: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:25.884: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:25.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:25.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:25.982: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.178: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.206: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.235: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.258: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.281: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:26.592: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

Jan 31 00:24:30.854: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:30.887: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:30.910: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.066: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.232: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.253: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.301: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.322: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:31.540: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

Jan 31 00:24:35.827: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:35.849: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:35.902: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:35.925: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:35.950: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.169: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.193: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.218: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.242: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.294: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
Jan 31 00:24:36.491: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

Jan 31 00:24:41.610: INFO: DNS probes using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 succeeded

STEP: deleting the pod 01/31/23 00:24:41.611
STEP: deleting the test service 01/31/23 00:24:41.678
STEP: deleting the test headless service 01/31/23 00:24:41.758
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 31 00:24:41.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4921" for this suite. 01/31/23 00:24:41.828
------------------------------
• [SLOW TEST] [36.380 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:24:05.512
    Jan 31 00:24:05.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/31/23 00:24:05.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:05.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:05.598
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/31/23 00:24:05.641
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4921;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4921;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +notcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_tcp@PTR;sleep 1; done
     01/31/23 00:24:05.713
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4921;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4921;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4921.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4921.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4921.svc;check="$$(dig +notcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.127.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.127.89_tcp@PTR;sleep 1; done
     01/31/23 00:24:05.713
    STEP: creating a pod to probe DNS 01/31/23 00:24:05.714
    STEP: submitting the pod to kubernetes 01/31/23 00:24:05.715
    Jan 31 00:24:05.797: INFO: Waiting up to 15m0s for pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902" in namespace "dns-4921" to be "running"
    Jan 31 00:24:05.816: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Pending", Reason="", readiness=false. Elapsed: 17.94078ms
    Jan 31 00:24:07.847: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049552885s
    Jan 31 00:24:09.855: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902": Phase="Running", Reason="", readiness=true. Elapsed: 4.056905014s
    Jan 31 00:24:09.855: INFO: Pod "dns-test-4cc24367-601d-4afd-8355-6da2ff773902" satisfied condition "running"
    STEP: retrieving the pod 01/31/23 00:24:09.856
    STEP: looking for the results for each expected name from probers 01/31/23 00:24:09.874
    Jan 31 00:24:09.972: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:09.993: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.034: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.255: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.418: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.448: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.503: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.546: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.573: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:10.799: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc jessie_tcp@_http._tcp.dns-test-service.dns-4921.svc]

    Jan 31 00:24:15.825: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.848: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.872: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.931: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.955: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:15.978: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.149: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.190: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.212: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.258: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.281: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:16.421: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc wheezy_udp@_http._tcp.dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

    Jan 31 00:24:20.825: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:20.849: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:20.887: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:20.913: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:20.937: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:20.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.215: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.247: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.270: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.293: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.346: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.369: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:21.535: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

    Jan 31 00:24:25.837: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:25.860: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:25.884: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:25.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:25.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:25.982: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.178: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.206: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.235: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.258: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.281: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.331: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:26.592: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

    Jan 31 00:24:30.854: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:30.887: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:30.910: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.044: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.066: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.232: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.253: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.276: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.301: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.322: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:31.540: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

    Jan 31 00:24:35.827: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:35.849: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:35.902: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:35.925: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:35.950: INFO: Unable to read wheezy_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.169: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.193: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.218: INFO: Unable to read jessie_udp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.242: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921 from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.294: INFO: Unable to read jessie_udp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-4921.svc from pod dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902: the server could not find the requested resource (get pods dns-test-4cc24367-601d-4afd-8355-6da2ff773902)
    Jan 31 00:24:36.491: INFO: Lookups using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4921 wheezy_tcp@dns-test-service.dns-4921 wheezy_udp@dns-test-service.dns-4921.svc wheezy_tcp@dns-test-service.dns-4921.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4921 jessie_tcp@dns-test-service.dns-4921 jessie_udp@dns-test-service.dns-4921.svc jessie_tcp@dns-test-service.dns-4921.svc]

    Jan 31 00:24:41.610: INFO: DNS probes using dns-4921/dns-test-4cc24367-601d-4afd-8355-6da2ff773902 succeeded

    STEP: deleting the pod 01/31/23 00:24:41.611
    STEP: deleting the test service 01/31/23 00:24:41.678
    STEP: deleting the test headless service 01/31/23 00:24:41.758
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:24:41.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4921" for this suite. 01/31/23 00:24:41.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:24:41.893
Jan 31 00:24:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:24:41.895
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:41.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:41.961
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/31/23 00:24:41.975
Jan 31 00:24:42.012: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa" in namespace "emptydir-8475" to be "running"
Jan 31 00:24:42.039: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Pending", Reason="", readiness=false. Elapsed: 27.747757ms
Jan 31 00:24:44.057: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04533764s
Jan 31 00:24:46.063: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Running", Reason="", readiness=false. Elapsed: 4.050882118s
Jan 31 00:24:46.063: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/31/23 00:24:46.063
Jan 31 00:24:46.063: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8475 PodName:pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:24:46.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:24:46.066: INFO: ExecWithOptions: Clientset creation
Jan 31 00:24:46.066: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-8475/pods/pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 31 00:24:46.303: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:24:46.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8475" for this suite. 01/31/23 00:24:46.325
------------------------------
• [4.487 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:24:41.893
    Jan 31 00:24:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:24:41.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:41.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:41.961
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/31/23 00:24:41.975
    Jan 31 00:24:42.012: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa" in namespace "emptydir-8475" to be "running"
    Jan 31 00:24:42.039: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Pending", Reason="", readiness=false. Elapsed: 27.747757ms
    Jan 31 00:24:44.057: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04533764s
    Jan 31 00:24:46.063: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa": Phase="Running", Reason="", readiness=false. Elapsed: 4.050882118s
    Jan 31 00:24:46.063: INFO: Pod "pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/31/23 00:24:46.063
    Jan 31 00:24:46.063: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8475 PodName:pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:24:46.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:24:46.066: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:24:46.066: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-8475/pods/pod-sharedvolume-ce3c61a5-8aad-4849-b406-f13a0fd6ecfa/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 31 00:24:46.303: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:24:46.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8475" for this suite. 01/31/23 00:24:46.325
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:24:46.381
Jan 31 00:24:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename deployment 01/31/23 00:24:46.383
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:46.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:46.522
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 31 00:24:46.538: INFO: Creating deployment "webserver-deployment"
Jan 31 00:24:46.565: INFO: Waiting for observed generation 1
Jan 31 00:24:48.604: INFO: Waiting for all required pods to come up
Jan 31 00:24:48.641: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/31/23 00:24:48.641
Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-txf8w" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2g5v4" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4bnlk" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4c2pm" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9f5cw" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-djx9f" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jcj88" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m48s6" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sxflc" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jlk4n" in namespace "deployment-9097" to be "running"
Jan 31 00:24:48.685: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm": Phase="Pending", Reason="", readiness=false. Elapsed: 43.149566ms
Jan 31 00:24:48.686: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w": Phase="Pending", Reason="", readiness=false. Elapsed: 44.570101ms
Jan 31 00:24:48.705: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88": Phase="Pending", Reason="", readiness=false. Elapsed: 62.016426ms
Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc": Phase="Pending", Reason="", readiness=false. Elapsed: 62.465992ms
Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n": Phase="Pending", Reason="", readiness=false. Elapsed: 62.621968ms
Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw": Phase="Pending", Reason="", readiness=false. Elapsed: 63.949978ms
Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f": Phase="Pending", Reason="", readiness=false. Elapsed: 63.886623ms
Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4": Phase="Running", Reason="", readiness=true. Elapsed: 64.679874ms
Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk": Phase="Pending", Reason="", readiness=false. Elapsed: 64.45477ms
Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4" satisfied condition "running"
Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6": Phase="Pending", Reason="", readiness=false. Elapsed: 64.342835ms
Jan 31 00:24:50.713: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.071544976s
Jan 31 00:24:50.713: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w" satisfied condition "running"
Jan 31 00:24:50.718: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm": Phase="Running", Reason="", readiness=true. Elapsed: 2.075798876s
Jan 31 00:24:50.718: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm" satisfied condition "running"
Jan 31 00:24:50.728: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw": Phase="Running", Reason="", readiness=true. Elapsed: 2.085161944s
Jan 31 00:24:50.728: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw" satisfied condition "running"
Jan 31 00:24:50.730: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.087613989s
Jan 31 00:24:50.730: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f" satisfied condition "running"
Jan 31 00:24:50.731: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88": Phase="Running", Reason="", readiness=true. Elapsed: 2.087852832s
Jan 31 00:24:50.731: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88" satisfied condition "running"
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk": Phase="Running", Reason="", readiness=true. Elapsed: 2.089588957s
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk" satisfied condition "running"
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.088531246s
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n" satisfied condition "running"
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6": Phase="Running", Reason="", readiness=true. Elapsed: 2.088815043s
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6" satisfied condition "running"
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc": Phase="Running", Reason="", readiness=true. Elapsed: 2.088666945s
Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc" satisfied condition "running"
Jan 31 00:24:50.733: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 31 00:24:50.797: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 31 00:24:50.871: INFO: Updating deployment webserver-deployment
Jan 31 00:24:50.872: INFO: Waiting for observed generation 2
Jan 31 00:24:52.911: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 31 00:24:52.956: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 31 00:24:52.990: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 31 00:24:53.062: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 31 00:24:53.062: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 31 00:24:53.083: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 31 00:24:53.147: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 31 00:24:53.147: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 31 00:24:53.199: INFO: Updating deployment webserver-deployment
Jan 31 00:24:53.199: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 31 00:24:53.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 31 00:24:53.304: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 31 00:24:53.374: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9097  f019e1ba-310a-453c-b467-83b20c65bca9 53413 3 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370a238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-31 00:24:51 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-31 00:24:53 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 31 00:24:53.464: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9097  06c909ba-ac23-451b-b898-d19cc4b67166 53396 3 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f019e1ba-310a-453c-b467-83b20c65bca9 0xc003d16357 0xc003d16358}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f019e1ba-310a-453c-b467-83b20c65bca9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d163f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 31 00:24:53.464: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 31 00:24:53.464: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9097  66bb1ec6-3c66-4caf-b43a-f7e56ade6103 53441 3 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f019e1ba-310a-453c-b467-83b20c65bca9 0xc003d16257 0xc003d16258}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f019e1ba-310a-453c-b467-83b20c65bca9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d162f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 31 00:24:53.576: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2g5v4 webserver-deployment-7f5969cbc7- deployment-9097  77473674-90c4-4a6a-a5b1-954bae7c8c95 53242 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d5f34f66c2369c635f125e7e7c7c755ef12e7097867ce59c0109ff3f52fdce44 cni.projectcalico.org/podIP:172.30.199.57/32 cni.projectcalico.org/podIPs:172.30.199.57/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450827 0xc003450828}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzlp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzlp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.57,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e8c1c3057cef1019bab2498247555b891af6f64f6f4eee987e712adff6ad0308,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.576: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4bnlk webserver-deployment-7f5969cbc7- deployment-9097  2d5bfe84-02f2-4435-93c9-d53ef780d91f 53298 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5857803658558568e8b1ba87fcd826d80b7a6d8a1fa11345ada8710c944ddb69 cni.projectcalico.org/podIP:172.30.248.21/32 cni.projectcalico.org/podIPs:172.30.248.21/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450a57 0xc003450a58}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d47h5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d47h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.21,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e6220f085159350131fda1f644706ec07eb9006b1b784515c73fd00e7629145e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4c2pm webserver-deployment-7f5969cbc7- deployment-9097  23b914a0-0928-474d-87f9-aa1f40c34572 53290 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f47deee6f9448f9e16b6347511f32dcaf20fea9b7e9b4bad31bcfe65ed7dc7e8 cni.projectcalico.org/podIP:172.30.199.45/32 cni.projectcalico.org/podIPs:172.30.199.45/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450c87 0xc003450c88}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzbgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzbgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.45,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e53eaeffde301371dbed02f809e3407f67902656b462597872c8391984898b94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-5dj6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5dj6v webserver-deployment-7f5969cbc7- deployment-9097  10258463-2cba-49e4-bcf2-e8d45e5d9049 53427 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450e97 0xc003450e98}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhgzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhgzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9f5cw webserver-deployment-7f5969cbc7- deployment-9097  df3d83b8-5293-454a-a811-d5ca31f364e7 53286 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2be1ea2b5ca874b80161834a2269b548ecf1c70749b241b5b97a154618b88f3a cni.projectcalico.org/podIP:172.30.237.168/32 cni.projectcalico.org/podIPs:172.30.237.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451000 0xc003451001}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hkrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hkrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.168,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://34105e4023e18a8769cd87b800507a7ab5fadf6bcd24fb296df690f39c709dde,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-9n2bt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9n2bt webserver-deployment-7f5969cbc7- deployment-9097  a5528406-d9bc-4000-bc98-547205447e69 53463 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451207 0xc003451208}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ppv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ppv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-b8qbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b8qbn webserver-deployment-7f5969cbc7- deployment-9097  1264346a-ca0e-4fc8-b66e-07b513ec619c 53447 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034513e7 0xc0034513e8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb5ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb5ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-bbqwr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbqwr webserver-deployment-7f5969cbc7- deployment-9097  bcca691c-c8f1-4293-85ca-4fed2317f2eb 53451 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451550 0xc003451551}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxb67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxb67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-djx9f webserver-deployment-7f5969cbc7- deployment-9097  717d7e4f-5353-4889-a060-4aa6747bb353 53256 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:170e0bb0d282bae5a428450de9e2f94364a9d6b6527edd34d2446ec2f4cab857 cni.projectcalico.org/podIP:172.30.248.57/32 cni.projectcalico.org/podIPs:172.30.248.57/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034516d0 0xc0034516d1}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gs4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gs4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.57,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9efba44c089ce00d2386c04d31be46abbfb9e998a098b30ce9104740abaecf17,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-hk5nq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk5nq webserver-deployment-7f5969cbc7- deployment-9097  1d16122b-ff15-4797-b5a8-db564f3fe538 53421 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034518d7 0xc0034518d8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqq9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqq9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jlk4n webserver-deployment-7f5969cbc7- deployment-9097  06a05ccd-1417-43db-8684-40600eaddc67 53281 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d92cc909f29489beb7e0693d990ffb1a55d03d05e6d9c838800fe8e1157d28e cni.projectcalico.org/podIP:172.30.237.171/32 cni.projectcalico.org/podIPs:172.30.237.171/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451ac7 0xc003451ac8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vd966,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vd966,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.171,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5c426a712fae916e206cb4371bf12ad1fd3caf3a9a9f50612ecaf2dc90b47bad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-k5n7s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k5n7s webserver-deployment-7f5969cbc7- deployment-9097  7f7b666c-93d4-41e9-978c-a81b688ff9e4 53459 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451d07 0xc003451d08}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42kv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42kv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-mcfp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mcfp6 webserver-deployment-7f5969cbc7- deployment-9097  475cce13-9faa-450c-9ec9-b545a5a71f0c 53439 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451ed7 0xc003451ed8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kkzbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kkzbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-nr6w8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nr6w8 webserver-deployment-7f5969cbc7- deployment-9097  2bf69733-c77c-4a28-8c46-84294fa4cec1 53454 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a2d7 0xc005b4a2d8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbv74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbv74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-qgnrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qgnrt webserver-deployment-7f5969cbc7- deployment-9097  097fc8a7-6e17-4b82-95d0-cbd029b042c0 53426 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a440 0xc005b4a441}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj4dk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj4dk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sxflc webserver-deployment-7f5969cbc7- deployment-9097  7fc0b7e3-df9d-426a-8e73-67933c2a5072 53277 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:47295b82fc8fc28d4765a2a4b4413778657b6cc6d23ba0375ef638acb0f342e2 cni.projectcalico.org/podIP:172.30.237.170/32 cni.projectcalico.org/podIPs:172.30.237.170/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a5a0 0xc005b4a5a1}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtlfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtlfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.170,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ba72e3e9593f9ed04c7fe75381c3553eeff8c05b97f3e4dc68600bd595e184f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-tl952" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tl952 webserver-deployment-7f5969cbc7- deployment-9097  27cf4739-134f-42fa-b9f9-bb518d9e0e88 53448 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a7a7 0xc005b4a7a8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xng7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xng7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-txf8w webserver-deployment-7f5969cbc7- deployment-9097  4768496b-ab7d-47ef-9c2a-ae02a6bd25d1 53296 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:de8d6dadc6ad94e83332e9ea7c622fd40db744c2844a310ddeac3d13b4e9a5d6 cni.projectcalico.org/podIP:172.30.248.50/32 cni.projectcalico.org/podIPs:172.30.248.50/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a930 0xc005b4a931}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p69vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p69vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.50,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa00bc2930a61ae4e371ddbdd2040796427c3f0ccba0188bb2a4311f2bfddee9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-vhn2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vhn2v webserver-deployment-7f5969cbc7- deployment-9097  65ddd77d-952c-4507-86d2-a2860a92ed9f 53440 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4ab37 0xc005b4ab38}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnj82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnj82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-7f5969cbc7-vnfks" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vnfks webserver-deployment-7f5969cbc7- deployment-9097  3050be2a-7cec-4505-9f47-2e8c4da84955 53455 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4ad07 0xc005b4ad08}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2gvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2gvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-d9f79cb5-25hr4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-25hr4 webserver-deployment-d9f79cb5- deployment-9097  adc57db4-c14e-4c4f-9f9b-890f3c903d86 53423 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4ae5f 0xc005b4ae70}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvz88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvz88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-d9f79cb5-646qt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-646qt webserver-deployment-d9f79cb5- deployment-9097  91e87b94-b66e-4faa-a398-a97c91ff08c4 53457 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4afcf 0xc005b4afe0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gsrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gsrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-85dg8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-85dg8 webserver-deployment-d9f79cb5- deployment-9097  b84c9f5b-b0c7-4986-8883-1a6f9b1d91f0 53386 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:37232d8931e2847f071282bed396fabc9d4615511d3c07ca737d4f1486722d06 cni.projectcalico.org/podIP:172.30.237.166/32 cni.projectcalico.org/podIPs:172.30.237.166/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b13f 0xc005b4b150}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqh4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqh4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-8fc97" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8fc97 webserver-deployment-d9f79cb5- deployment-9097  8cc42cfe-1dfc-4d99-a9d2-80e3bfa61bd1 53422 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b3a7 0xc005b4b3a8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7xjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7xjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-bhlj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bhlj5 webserver-deployment-d9f79cb5- deployment-9097  5f3dfac9-ede8-4267-bfa1-b0102ee13266 53453 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b51f 0xc005b4b540}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w27r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w27r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-d6jgt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6jgt webserver-deployment-d9f79cb5- deployment-9097  eeb2c95d-f7f7-45be-ac39-94c5557e3424 53378 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6fa5cc5c80324b6389e0882421f1cb7c4fcdf236ec1bb6d542ab27c5d0d11dd4 cni.projectcalico.org/podIP:172.30.248.35/32 cni.projectcalico.org/podIPs:172.30.248.35/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b6af 0xc005b4b6f0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m2xx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m2xx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-jdjf5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jdjf5 webserver-deployment-d9f79cb5- deployment-9097  052d1bd5-61c1-4c16-95de-906df7da82a0 53456 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b927 0xc005b4b928}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgn2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgn2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-jw8wc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jw8wc webserver-deployment-d9f79cb5- deployment-9097  fde35281-28af-4169-8088-6f05c948ccea 53452 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4ba9f 0xc005b4bab0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7tk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7tk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-kp88h" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kp88h webserver-deployment-d9f79cb5- deployment-9097  ff2d5159-0730-4931-bd64-3efd207b169e 53458 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4bc2f 0xc005b4bc40}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2htk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2htk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-ntslr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ntslr webserver-deployment-d9f79cb5- deployment-9097  2b182b23-534c-43d6-a14c-cd3c9e5a0dac 53377 0 2023-01-31 00:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ddaa2d01c5185a8a269750e1842e2f8bd16b7b3aa9f76d32bd59839a67d756b7 cni.projectcalico.org/podIP:172.30.199.53/32 cni.projectcalico.org/podIPs:172.30.199.53/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4bd9f 0xc005b4bdd0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x5d7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x5d7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-tkq2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tkq2r webserver-deployment-d9f79cb5- deployment-9097  2c903d40-1d77-453c-8aec-773839495eab 53366 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4084f016a740775a5be4e6e10618ac9f3b3fafe2bee15b784356757b89854a7a cni.projectcalico.org/podIP:172.30.199.43/32 cni.projectcalico.org/podIPs:172.30.199.43/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458007 0xc004458008}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sghcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sghcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-v482g" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v482g webserver-deployment-d9f79cb5- deployment-9097  de8295bf-3f41-47b0-82f3-8ec037e23fe5 53438 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458217 0xc004458218}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvvpk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvvpk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:24:53.586: INFO: Pod "webserver-deployment-d9f79cb5-xbscm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xbscm webserver-deployment-d9f79cb5- deployment-9097  2e291c89-555a-4ce2-b070-2b25d4cd2278 53373 0 2023-01-31 00:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:191a5c2718cf1ee125b354f08a608e717cf768521e4bea2c448de50a3638dd7e cni.projectcalico.org/podIP:172.30.237.164/32 cni.projectcalico.org/podIPs:172.30.237.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458427 0xc004458428}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7m7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7m7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 31 00:24:53.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9097" for this suite. 01/31/23 00:24:53.612
------------------------------
• [SLOW TEST] [7.262 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:24:46.381
    Jan 31 00:24:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename deployment 01/31/23 00:24:46.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:46.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:46.522
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 31 00:24:46.538: INFO: Creating deployment "webserver-deployment"
    Jan 31 00:24:46.565: INFO: Waiting for observed generation 1
    Jan 31 00:24:48.604: INFO: Waiting for all required pods to come up
    Jan 31 00:24:48.641: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/31/23 00:24:48.641
    Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-txf8w" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-2g5v4" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4bnlk" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4c2pm" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.642: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9f5cw" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-djx9f" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jcj88" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m48s6" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sxflc" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.643: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jlk4n" in namespace "deployment-9097" to be "running"
    Jan 31 00:24:48.685: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm": Phase="Pending", Reason="", readiness=false. Elapsed: 43.149566ms
    Jan 31 00:24:48.686: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w": Phase="Pending", Reason="", readiness=false. Elapsed: 44.570101ms
    Jan 31 00:24:48.705: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88": Phase="Pending", Reason="", readiness=false. Elapsed: 62.016426ms
    Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc": Phase="Pending", Reason="", readiness=false. Elapsed: 62.465992ms
    Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n": Phase="Pending", Reason="", readiness=false. Elapsed: 62.621968ms
    Jan 31 00:24:48.706: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw": Phase="Pending", Reason="", readiness=false. Elapsed: 63.949978ms
    Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f": Phase="Pending", Reason="", readiness=false. Elapsed: 63.886623ms
    Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4": Phase="Running", Reason="", readiness=true. Elapsed: 64.679874ms
    Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk": Phase="Pending", Reason="", readiness=false. Elapsed: 64.45477ms
    Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4" satisfied condition "running"
    Jan 31 00:24:48.707: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6": Phase="Pending", Reason="", readiness=false. Elapsed: 64.342835ms
    Jan 31 00:24:50.713: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.071544976s
    Jan 31 00:24:50.713: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w" satisfied condition "running"
    Jan 31 00:24:50.718: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm": Phase="Running", Reason="", readiness=true. Elapsed: 2.075798876s
    Jan 31 00:24:50.718: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm" satisfied condition "running"
    Jan 31 00:24:50.728: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw": Phase="Running", Reason="", readiness=true. Elapsed: 2.085161944s
    Jan 31 00:24:50.728: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw" satisfied condition "running"
    Jan 31 00:24:50.730: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f": Phase="Running", Reason="", readiness=true. Elapsed: 2.087613989s
    Jan 31 00:24:50.730: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f" satisfied condition "running"
    Jan 31 00:24:50.731: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88": Phase="Running", Reason="", readiness=true. Elapsed: 2.087852832s
    Jan 31 00:24:50.731: INFO: Pod "webserver-deployment-7f5969cbc7-jcj88" satisfied condition "running"
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk": Phase="Running", Reason="", readiness=true. Elapsed: 2.089588957s
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk" satisfied condition "running"
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.088531246s
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n" satisfied condition "running"
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6": Phase="Running", Reason="", readiness=true. Elapsed: 2.088815043s
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-m48s6" satisfied condition "running"
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc": Phase="Running", Reason="", readiness=true. Elapsed: 2.088666945s
    Jan 31 00:24:50.732: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc" satisfied condition "running"
    Jan 31 00:24:50.733: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 31 00:24:50.797: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 31 00:24:50.871: INFO: Updating deployment webserver-deployment
    Jan 31 00:24:50.872: INFO: Waiting for observed generation 2
    Jan 31 00:24:52.911: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 31 00:24:52.956: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 31 00:24:52.990: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 31 00:24:53.062: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 31 00:24:53.062: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 31 00:24:53.083: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 31 00:24:53.147: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 31 00:24:53.147: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 31 00:24:53.199: INFO: Updating deployment webserver-deployment
    Jan 31 00:24:53.199: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 31 00:24:53.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 31 00:24:53.304: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 31 00:24:53.374: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9097  f019e1ba-310a-453c-b467-83b20c65bca9 53413 3 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00370a238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-31 00:24:51 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-31 00:24:53 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 31 00:24:53.464: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9097  06c909ba-ac23-451b-b898-d19cc4b67166 53396 3 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f019e1ba-310a-453c-b467-83b20c65bca9 0xc003d16357 0xc003d16358}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f019e1ba-310a-453c-b467-83b20c65bca9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d163f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 31 00:24:53.464: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 31 00:24:53.464: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9097  66bb1ec6-3c66-4caf-b43a-f7e56ade6103 53441 3 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f019e1ba-310a-453c-b467-83b20c65bca9 0xc003d16257 0xc003d16258}] [] [{kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f019e1ba-310a-453c-b467-83b20c65bca9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d162f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 31 00:24:53.576: INFO: Pod "webserver-deployment-7f5969cbc7-2g5v4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2g5v4 webserver-deployment-7f5969cbc7- deployment-9097  77473674-90c4-4a6a-a5b1-954bae7c8c95 53242 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d5f34f66c2369c635f125e7e7c7c755ef12e7097867ce59c0109ff3f52fdce44 cni.projectcalico.org/podIP:172.30.199.57/32 cni.projectcalico.org/podIPs:172.30.199.57/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450827 0xc003450828}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzlp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzlp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.57,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e8c1c3057cef1019bab2498247555b891af6f64f6f4eee987e712adff6ad0308,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.576: INFO: Pod "webserver-deployment-7f5969cbc7-4bnlk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4bnlk webserver-deployment-7f5969cbc7- deployment-9097  2d5bfe84-02f2-4435-93c9-d53ef780d91f 53298 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5857803658558568e8b1ba87fcd826d80b7a6d8a1fa11345ada8710c944ddb69 cni.projectcalico.org/podIP:172.30.248.21/32 cni.projectcalico.org/podIPs:172.30.248.21/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450a57 0xc003450a58}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d47h5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d47h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.21,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e6220f085159350131fda1f644706ec07eb9006b1b784515c73fd00e7629145e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-4c2pm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4c2pm webserver-deployment-7f5969cbc7- deployment-9097  23b914a0-0928-474d-87f9-aa1f40c34572 53290 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f47deee6f9448f9e16b6347511f32dcaf20fea9b7e9b4bad31bcfe65ed7dc7e8 cni.projectcalico.org/podIP:172.30.199.45/32 cni.projectcalico.org/podIPs:172.30.199.45/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450c87 0xc003450c88}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.199.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzbgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzbgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:172.30.199.45,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e53eaeffde301371dbed02f809e3407f67902656b462597872c8391984898b94,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.199.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-5dj6v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5dj6v webserver-deployment-7f5969cbc7- deployment-9097  10258463-2cba-49e4-bcf2-e8d45e5d9049 53427 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003450e97 0xc003450e98}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhgzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhgzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.577: INFO: Pod "webserver-deployment-7f5969cbc7-9f5cw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9f5cw webserver-deployment-7f5969cbc7- deployment-9097  df3d83b8-5293-454a-a811-d5ca31f364e7 53286 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2be1ea2b5ca874b80161834a2269b548ecf1c70749b241b5b97a154618b88f3a cni.projectcalico.org/podIP:172.30.237.168/32 cni.projectcalico.org/podIPs:172.30.237.168/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451000 0xc003451001}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hkrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hkrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.168,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://34105e4023e18a8769cd87b800507a7ab5fadf6bcd24fb296df690f39c709dde,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-9n2bt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9n2bt webserver-deployment-7f5969cbc7- deployment-9097  a5528406-d9bc-4000-bc98-547205447e69 53463 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451207 0xc003451208}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ppv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ppv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-b8qbn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b8qbn webserver-deployment-7f5969cbc7- deployment-9097  1264346a-ca0e-4fc8-b66e-07b513ec619c 53447 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034513e7 0xc0034513e8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb5ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb5ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.578: INFO: Pod "webserver-deployment-7f5969cbc7-bbqwr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bbqwr webserver-deployment-7f5969cbc7- deployment-9097  bcca691c-c8f1-4293-85ca-4fed2317f2eb 53451 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451550 0xc003451551}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxb67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxb67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-djx9f" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-djx9f webserver-deployment-7f5969cbc7- deployment-9097  717d7e4f-5353-4889-a060-4aa6747bb353 53256 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:170e0bb0d282bae5a428450de9e2f94364a9d6b6527edd34d2446ec2f4cab857 cni.projectcalico.org/podIP:172.30.248.57/32 cni.projectcalico.org/podIPs:172.30.248.57/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034516d0 0xc0034516d1}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gs4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gs4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.57,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9efba44c089ce00d2386c04d31be46abbfb9e998a098b30ce9104740abaecf17,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-hk5nq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk5nq webserver-deployment-7f5969cbc7- deployment-9097  1d16122b-ff15-4797-b5a8-db564f3fe538 53421 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc0034518d7 0xc0034518d8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqq9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqq9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.579: INFO: Pod "webserver-deployment-7f5969cbc7-jlk4n" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jlk4n webserver-deployment-7f5969cbc7- deployment-9097  06a05ccd-1417-43db-8684-40600eaddc67 53281 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d92cc909f29489beb7e0693d990ffb1a55d03d05e6d9c838800fe8e1157d28e cni.projectcalico.org/podIP:172.30.237.171/32 cni.projectcalico.org/podIPs:172.30.237.171/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451ac7 0xc003451ac8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vd966,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vd966,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.171,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5c426a712fae916e206cb4371bf12ad1fd3caf3a9a9f50612ecaf2dc90b47bad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-k5n7s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k5n7s webserver-deployment-7f5969cbc7- deployment-9097  7f7b666c-93d4-41e9-978c-a81b688ff9e4 53459 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451d07 0xc003451d08}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42kv8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42kv8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-mcfp6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mcfp6 webserver-deployment-7f5969cbc7- deployment-9097  475cce13-9faa-450c-9ec9-b545a5a71f0c 53439 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc003451ed7 0xc003451ed8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kkzbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kkzbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-nr6w8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nr6w8 webserver-deployment-7f5969cbc7- deployment-9097  2bf69733-c77c-4a28-8c46-84294fa4cec1 53454 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a2d7 0xc005b4a2d8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbv74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbv74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.580: INFO: Pod "webserver-deployment-7f5969cbc7-qgnrt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qgnrt webserver-deployment-7f5969cbc7- deployment-9097  097fc8a7-6e17-4b82-95d0-cbd029b042c0 53426 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a440 0xc005b4a441}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj4dk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj4dk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-sxflc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sxflc webserver-deployment-7f5969cbc7- deployment-9097  7fc0b7e3-df9d-426a-8e73-67933c2a5072 53277 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:47295b82fc8fc28d4765a2a4b4413778657b6cc6d23ba0375ef638acb0f342e2 cni.projectcalico.org/podIP:172.30.237.170/32 cni.projectcalico.org/podIPs:172.30.237.170/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a5a0 0xc005b4a5a1}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.237.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtlfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtlfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:172.30.237.170,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ba72e3e9593f9ed04c7fe75381c3553eeff8c05b97f3e4dc68600bd595e184f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.237.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-tl952" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tl952 webserver-deployment-7f5969cbc7- deployment-9097  27cf4739-134f-42fa-b9f9-bb518d9e0e88 53448 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a7a7 0xc005b4a7a8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xng7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xng7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-txf8w" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-txf8w webserver-deployment-7f5969cbc7- deployment-9097  4768496b-ab7d-47ef-9c2a-ae02a6bd25d1 53296 0 2023-01-31 00:24:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:de8d6dadc6ad94e83332e9ea7c622fd40db744c2844a310ddeac3d13b4e9a5d6 cni.projectcalico.org/podIP:172.30.248.50/32 cni.projectcalico.org/podIPs:172.30.248.50/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4a930 0xc005b4a931}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-31 00:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-31 00:24:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.248.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p69vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p69vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:172.30.248.50,StartTime:2023-01-31 00:24:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-31 00:24:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aa00bc2930a61ae4e371ddbdd2040796427c3f0ccba0188bb2a4311f2bfddee9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.248.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.581: INFO: Pod "webserver-deployment-7f5969cbc7-vhn2v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vhn2v webserver-deployment-7f5969cbc7- deployment-9097  65ddd77d-952c-4507-86d2-a2860a92ed9f 53440 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4ab37 0xc005b4ab38}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnj82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnj82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-7f5969cbc7-vnfks" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vnfks webserver-deployment-7f5969cbc7- deployment-9097  3050be2a-7cec-4505-9f47-2e8c4da84955 53455 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 66bb1ec6-3c66-4caf-b43a-f7e56ade6103 0xc005b4ad07 0xc005b4ad08}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bb1ec6-3c66-4caf-b43a-f7e56ade6103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2gvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2gvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-d9f79cb5-25hr4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-25hr4 webserver-deployment-d9f79cb5- deployment-9097  adc57db4-c14e-4c4f-9f9b-890f3c903d86 53423 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4ae5f 0xc005b4ae70}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvz88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvz88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.582: INFO: Pod "webserver-deployment-d9f79cb5-646qt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-646qt webserver-deployment-d9f79cb5- deployment-9097  91e87b94-b66e-4faa-a398-a97c91ff08c4 53457 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4afcf 0xc005b4afe0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gsrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gsrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-85dg8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-85dg8 webserver-deployment-d9f79cb5- deployment-9097  b84c9f5b-b0c7-4986-8883-1a6f9b1d91f0 53386 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:37232d8931e2847f071282bed396fabc9d4615511d3c07ca737d4f1486722d06 cni.projectcalico.org/podIP:172.30.237.166/32 cni.projectcalico.org/podIPs:172.30.237.166/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b13f 0xc005b4b150}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqh4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqh4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-8fc97" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8fc97 webserver-deployment-d9f79cb5- deployment-9097  8cc42cfe-1dfc-4d99-a9d2-80e3bfa61bd1 53422 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b3a7 0xc005b4b3a8}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7xjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7xjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.583: INFO: Pod "webserver-deployment-d9f79cb5-bhlj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bhlj5 webserver-deployment-d9f79cb5- deployment-9097  5f3dfac9-ede8-4267-bfa1-b0102ee13266 53453 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b51f 0xc005b4b540}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w27r8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w27r8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-d6jgt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d6jgt webserver-deployment-d9f79cb5- deployment-9097  eeb2c95d-f7f7-45be-ac39-94c5557e3424 53378 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6fa5cc5c80324b6389e0882421f1cb7c4fcdf236ec1bb6d542ab27c5d0d11dd4 cni.projectcalico.org/podIP:172.30.248.35/32 cni.projectcalico.org/podIPs:172.30.248.35/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b6af 0xc005b4b6f0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m2xx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m2xx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-jdjf5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jdjf5 webserver-deployment-d9f79cb5- deployment-9097  052d1bd5-61c1-4c16-95de-906df7da82a0 53456 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4b927 0xc005b4b928}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgn2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgn2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.584: INFO: Pod "webserver-deployment-d9f79cb5-jw8wc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jw8wc webserver-deployment-d9f79cb5- deployment-9097  fde35281-28af-4169-8088-6f05c948ccea 53452 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4ba9f 0xc005b4bab0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7tk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7tk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-kp88h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kp88h webserver-deployment-d9f79cb5- deployment-9097  ff2d5159-0730-4931-bd64-3efd207b169e 53458 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4bc2f 0xc005b4bc40}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2htk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2htk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-ntslr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ntslr webserver-deployment-d9f79cb5- deployment-9097  2b182b23-534c-43d6-a14c-cd3c9e5a0dac 53377 0 2023-01-31 00:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ddaa2d01c5185a8a269750e1842e2f8bd16b7b3aa9f76d32bd59839a67d756b7 cni.projectcalico.org/podIP:172.30.199.53/32 cni.projectcalico.org/podIPs:172.30.199.53/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc005b4bd9f 0xc005b4bdd0}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x5d7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x5d7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-tkq2r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tkq2r webserver-deployment-d9f79cb5- deployment-9097  2c903d40-1d77-453c-8aec-773839495eab 53366 0 2023-01-31 00:24:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:4084f016a740775a5be4e6e10618ac9f3b3fafe2bee15b784356757b89854a7a cni.projectcalico.org/podIP:172.30.199.43/32 cni.projectcalico.org/podIPs:172.30.199.43/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458007 0xc004458008}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sghcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sghcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.227,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.227,PodIP:,StartTime:2023-01-31 00:24:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.585: INFO: Pod "webserver-deployment-d9f79cb5-v482g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v482g webserver-deployment-d9f79cb5- deployment-9097  de8295bf-3f41-47b0-82f3-8ec037e23fe5 53438 0 2023-01-31 00:24:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458217 0xc004458218}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvvpk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvvpk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.237,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.237,PodIP:,StartTime:2023-01-31 00:24:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:24:53.586: INFO: Pod "webserver-deployment-d9f79cb5-xbscm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xbscm webserver-deployment-d9f79cb5- deployment-9097  2e291c89-555a-4ce2-b070-2b25d4cd2278 53373 0 2023-01-31 00:24:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:191a5c2718cf1ee125b354f08a608e717cf768521e4bea2c448de50a3638dd7e cni.projectcalico.org/podIP:172.30.237.164/32 cni.projectcalico.org/podIPs:172.30.237.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 06c909ba-ac23-451b-b898-d19cc4b67166 0xc004458427 0xc004458428}] [] [{kube-controller-manager Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06c909ba-ac23-451b-b898-d19cc4b67166\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-31 00:24:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-31 00:24:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7m7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7m7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.15.28.225,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-31 00:24:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.15.28.225,PodIP:,StartTime:2023-01-31 00:24:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:24:53.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9097" for this suite. 01/31/23 00:24:53.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:24:53.658
Jan 31 00:24:53.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:24:53.661
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:53.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:53.745
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-1d634e9c-ff63-479c-a04b-e332ada14adf 01/31/23 00:24:53.759
STEP: Creating a pod to test consume secrets 01/31/23 00:24:53.779
Jan 31 00:24:53.824: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953" in namespace "projected-3135" to be "Succeeded or Failed"
Jan 31 00:24:53.841: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Pending", Reason="", readiness=false. Elapsed: 16.438733ms
Jan 31 00:24:55.863: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038703839s
Jan 31 00:24:57.860: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Running", Reason="", readiness=true. Elapsed: 4.035864773s
Jan 31 00:24:59.899: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074848609s
STEP: Saw pod success 01/31/23 00:24:59.899
Jan 31 00:24:59.899: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953" satisfied condition "Succeeded or Failed"
Jan 31 00:24:59.954: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 container secret-volume-test: <nil>
STEP: delete the pod 01/31/23 00:25:00.021
Jan 31 00:25:00.182: INFO: Waiting for pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 to disappear
Jan 31 00:25:00.206: INFO: Pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 31 00:25:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3135" for this suite. 01/31/23 00:25:00.229
------------------------------
• [SLOW TEST] [6.649 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:24:53.658
    Jan 31 00:24:53.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:24:53.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:24:53.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:24:53.745
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-1d634e9c-ff63-479c-a04b-e332ada14adf 01/31/23 00:24:53.759
    STEP: Creating a pod to test consume secrets 01/31/23 00:24:53.779
    Jan 31 00:24:53.824: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953" in namespace "projected-3135" to be "Succeeded or Failed"
    Jan 31 00:24:53.841: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Pending", Reason="", readiness=false. Elapsed: 16.438733ms
    Jan 31 00:24:55.863: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038703839s
    Jan 31 00:24:57.860: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Running", Reason="", readiness=true. Elapsed: 4.035864773s
    Jan 31 00:24:59.899: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074848609s
    STEP: Saw pod success 01/31/23 00:24:59.899
    Jan 31 00:24:59.899: INFO: Pod "pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953" satisfied condition "Succeeded or Failed"
    Jan 31 00:24:59.954: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 container secret-volume-test: <nil>
    STEP: delete the pod 01/31/23 00:25:00.021
    Jan 31 00:25:00.182: INFO: Waiting for pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 to disappear
    Jan 31 00:25:00.206: INFO: Pod pod-projected-secrets-61d93ae2-f3c1-44e1-be27-87ba59039953 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:25:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3135" for this suite. 01/31/23 00:25:00.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:25:00.308
Jan 31 00:25:00.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:25:00.312
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:00.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:00.433
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/31/23 00:25:00.451
Jan 31 00:25:00.504: INFO: Waiting up to 5m0s for pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a" in namespace "emptydir-1212" to be "Succeeded or Failed"
Jan 31 00:25:00.534: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.30496ms
Jan 31 00:25:02.560: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056447829s
Jan 31 00:25:04.553: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Running", Reason="", readiness=false. Elapsed: 4.049118039s
Jan 31 00:25:06.580: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076291942s
STEP: Saw pod success 01/31/23 00:25:06.58
Jan 31 00:25:06.580: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a" satisfied condition "Succeeded or Failed"
Jan 31 00:25:06.600: INFO: Trying to get logs from node 10.15.28.227 pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a container test-container: <nil>
STEP: delete the pod 01/31/23 00:25:06.652
Jan 31 00:25:06.708: INFO: Waiting for pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a to disappear
Jan 31 00:25:06.749: INFO: Pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:25:06.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1212" for this suite. 01/31/23 00:25:06.773
------------------------------
• [SLOW TEST] [6.492 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:25:00.308
    Jan 31 00:25:00.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:25:00.312
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:00.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:00.433
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/31/23 00:25:00.451
    Jan 31 00:25:00.504: INFO: Waiting up to 5m0s for pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a" in namespace "emptydir-1212" to be "Succeeded or Failed"
    Jan 31 00:25:00.534: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.30496ms
    Jan 31 00:25:02.560: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056447829s
    Jan 31 00:25:04.553: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Running", Reason="", readiness=false. Elapsed: 4.049118039s
    Jan 31 00:25:06.580: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076291942s
    STEP: Saw pod success 01/31/23 00:25:06.58
    Jan 31 00:25:06.580: INFO: Pod "pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a" satisfied condition "Succeeded or Failed"
    Jan 31 00:25:06.600: INFO: Trying to get logs from node 10.15.28.227 pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a container test-container: <nil>
    STEP: delete the pod 01/31/23 00:25:06.652
    Jan 31 00:25:06.708: INFO: Waiting for pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a to disappear
    Jan 31 00:25:06.749: INFO: Pod pod-2d2fdf1d-f2f3-4ead-b5e0-0f121968bb9a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:25:06.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1212" for this suite. 01/31/23 00:25:06.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:25:06.809
Jan 31 00:25:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename dns 01/31/23 00:25:06.81
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:06.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:06.898
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/31/23 00:25:06.914
Jan 31 00:25:06.956: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9247  6ca7f94d-de3e-4e23-af00-8bcec4d8dbe0 53913 0 2023-01-31 00:25:06 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-31 00:25:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-248ck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-248ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 31 00:25:06.956: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9247" to be "running and ready"
Jan 31 00:25:07.031: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 74.901139ms
Jan 31 00:25:07.031: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 31 00:25:09.055: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.09908485s
Jan 31 00:25:09.055: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 31 00:25:09.055: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/31/23 00:25:09.055
Jan 31 00:25:09.056: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9247 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:25:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:25:09.057: INFO: ExecWithOptions: Clientset creation
Jan 31 00:25:09.057: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9247/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/31/23 00:25:09.385
Jan 31 00:25:09.386: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9247 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 31 00:25:09.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:25:09.388: INFO: ExecWithOptions: Clientset creation
Jan 31 00:25:09.388: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9247/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 31 00:25:09.675: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 31 00:25:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9247" for this suite. 01/31/23 00:25:09.764
------------------------------
• [2.983 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:25:06.809
    Jan 31 00:25:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename dns 01/31/23 00:25:06.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:06.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:06.898
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/31/23 00:25:06.914
    Jan 31 00:25:06.956: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9247  6ca7f94d-de3e-4e23-af00-8bcec4d8dbe0 53913 0 2023-01-31 00:25:06 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-31 00:25:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-248ck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-248ck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 31 00:25:06.956: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9247" to be "running and ready"
    Jan 31 00:25:07.031: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 74.901139ms
    Jan 31 00:25:07.031: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 31 00:25:09.055: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.09908485s
    Jan 31 00:25:09.055: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 31 00:25:09.055: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/31/23 00:25:09.055
    Jan 31 00:25:09.056: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9247 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:25:09.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:25:09.057: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:25:09.057: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9247/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/31/23 00:25:09.385
    Jan 31 00:25:09.386: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9247 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 31 00:25:09.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:25:09.388: INFO: ExecWithOptions: Clientset creation
    Jan 31 00:25:09.388: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9247/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 31 00:25:09.675: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:25:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9247" for this suite. 01/31/23 00:25:09.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:25:09.793
Jan 31 00:25:09.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename projected 01/31/23 00:25:09.796
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:09.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:09.901
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-b8a70fbf-91af-40b0-8a55-8ea51d90b7ba 01/31/23 00:25:09.925
STEP: Creating a pod to test consume secrets 01/31/23 00:25:09.954
Jan 31 00:25:09.992: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a" in namespace "projected-2283" to be "Succeeded or Failed"
Jan 31 00:25:10.036: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.981166ms
Jan 31 00:25:12.057: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Running", Reason="", readiness=true. Elapsed: 2.06481036s
Jan 31 00:25:14.053: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Running", Reason="", readiness=false. Elapsed: 4.06083234s
Jan 31 00:25:16.056: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064546133s
STEP: Saw pod success 01/31/23 00:25:16.057
Jan 31 00:25:16.058: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a" satisfied condition "Succeeded or Failed"
Jan 31 00:25:16.076: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a container projected-secret-volume-test: <nil>
STEP: delete the pod 01/31/23 00:25:16.13
Jan 31 00:25:16.219: INFO: Waiting for pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a to disappear
Jan 31 00:25:16.240: INFO: Pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 31 00:25:16.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2283" for this suite. 01/31/23 00:25:16.265
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:25:09.793
    Jan 31 00:25:09.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename projected 01/31/23 00:25:09.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:09.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:09.901
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-b8a70fbf-91af-40b0-8a55-8ea51d90b7ba 01/31/23 00:25:09.925
    STEP: Creating a pod to test consume secrets 01/31/23 00:25:09.954
    Jan 31 00:25:09.992: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a" in namespace "projected-2283" to be "Succeeded or Failed"
    Jan 31 00:25:10.036: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.981166ms
    Jan 31 00:25:12.057: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Running", Reason="", readiness=true. Elapsed: 2.06481036s
    Jan 31 00:25:14.053: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Running", Reason="", readiness=false. Elapsed: 4.06083234s
    Jan 31 00:25:16.056: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064546133s
    STEP: Saw pod success 01/31/23 00:25:16.057
    Jan 31 00:25:16.058: INFO: Pod "pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a" satisfied condition "Succeeded or Failed"
    Jan 31 00:25:16.076: INFO: Trying to get logs from node 10.15.28.227 pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/31/23 00:25:16.13
    Jan 31 00:25:16.219: INFO: Waiting for pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a to disappear
    Jan 31 00:25:16.240: INFO: Pod pod-projected-secrets-566fd857-d973-4d91-8cc9-23b86b41720a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:25:16.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2283" for this suite. 01/31/23 00:25:16.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:25:16.328
Jan 31 00:25:16.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename replication-controller 01/31/23 00:25:16.33
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:16.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:16.425
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 31 00:25:16.437: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/31/23 00:25:17.535
STEP: Checking rc "condition-test" has the desired failure condition set 01/31/23 00:25:17.562
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/31/23 00:25:18.665
Jan 31 00:25:18.709: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/31/23 00:25:18.709
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 31 00:25:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-307" for this suite. 01/31/23 00:25:18.758
------------------------------
• [2.507 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:25:16.328
    Jan 31 00:25:16.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename replication-controller 01/31/23 00:25:16.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:16.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:16.425
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 31 00:25:16.437: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/31/23 00:25:17.535
    STEP: Checking rc "condition-test" has the desired failure condition set 01/31/23 00:25:17.562
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/31/23 00:25:18.665
    Jan 31 00:25:18.709: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/31/23 00:25:18.709
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:25:18.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-307" for this suite. 01/31/23 00:25:18.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:25:18.845
Jan 31 00:25:18.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename var-expansion 01/31/23 00:25:18.846
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:18.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:18.949
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/31/23 00:25:18.97
Jan 31 00:25:19.012: INFO: Waiting up to 2m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285" to be "running"
Jan 31 00:25:19.046: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 33.91995ms
Jan 31 00:25:21.076: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064110146s
Jan 31 00:25:23.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053710427s
Jan 31 00:25:25.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056318879s
Jan 31 00:25:27.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052114839s
Jan 31 00:25:29.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052438379s
Jan 31 00:25:31.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053904731s
Jan 31 00:25:33.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 14.053937991s
Jan 31 00:25:35.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 16.053454317s
Jan 31 00:25:37.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 18.055146847s
Jan 31 00:25:39.077: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 20.064823733s
Jan 31 00:25:41.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 22.053026485s
Jan 31 00:25:43.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 24.05420954s
Jan 31 00:25:45.081: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 26.069067743s
Jan 31 00:25:47.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 28.052423692s
Jan 31 00:25:49.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 30.053552627s
Jan 31 00:25:51.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 32.051871754s
Jan 31 00:25:53.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 34.052855353s
Jan 31 00:25:55.096: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 36.084280484s
Jan 31 00:25:57.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 38.053706711s
Jan 31 00:25:59.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 40.051899107s
Jan 31 00:26:01.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 42.053590084s
Jan 31 00:26:03.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 44.053206142s
Jan 31 00:26:05.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 46.052273911s
Jan 31 00:26:07.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 48.053441901s
Jan 31 00:26:09.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 50.055002476s
Jan 31 00:26:11.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 52.054201336s
Jan 31 00:26:13.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 54.053886456s
Jan 31 00:26:15.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 56.052543364s
Jan 31 00:26:17.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 58.053154124s
Jan 31 00:26:19.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.054079743s
Jan 31 00:26:21.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.05331399s
Jan 31 00:26:23.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.054161818s
Jan 31 00:26:25.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.053731056s
Jan 31 00:26:27.070: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.058272489s
Jan 31 00:26:29.095: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.083214236s
Jan 31 00:26:31.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.055146068s
Jan 31 00:26:33.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.055897234s
Jan 31 00:26:35.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.055805799s
Jan 31 00:26:37.072: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.060042448s
Jan 31 00:26:39.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.053572492s
Jan 31 00:26:41.073: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.060657737s
Jan 31 00:26:43.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.054880832s
Jan 31 00:26:45.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.05197771s
Jan 31 00:26:47.069: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.057050507s
Jan 31 00:26:49.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.055106132s
Jan 31 00:26:51.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.05584406s
Jan 31 00:26:53.070: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.057788868s
Jan 31 00:26:55.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.053949849s
Jan 31 00:26:57.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.051664406s
Jan 31 00:26:59.069: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.057173352s
Jan 31 00:27:01.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.052215875s
Jan 31 00:27:03.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.056033879s
Jan 31 00:27:05.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.052735707s
Jan 31 00:27:07.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.054449179s
Jan 31 00:27:09.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.053735897s
Jan 31 00:27:11.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.054456752s
Jan 31 00:27:13.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.053880952s
Jan 31 00:27:15.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.053309786s
Jan 31 00:27:17.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.051666351s
Jan 31 00:27:19.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.053802642s
Jan 31 00:27:19.080: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.067914556s
STEP: updating the pod 01/31/23 00:27:19.08
Jan 31 00:27:19.628: INFO: Successfully updated pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456"
STEP: waiting for pod running 01/31/23 00:27:19.628
Jan 31 00:27:19.628: INFO: Waiting up to 2m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285" to be "running"
Jan 31 00:27:19.649: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 20.752969ms
Jan 31 00:27:21.672: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Running", Reason="", readiness=true. Elapsed: 2.043472095s
Jan 31 00:27:21.672: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" satisfied condition "running"
STEP: deleting the pod gracefully 01/31/23 00:27:21.672
Jan 31 00:27:21.672: INFO: Deleting pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285"
Jan 31 00:27:21.706: INFO: Wait up to 5m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 31 00:27:55.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-285" for this suite. 01/31/23 00:27:55.767
------------------------------
• [SLOW TEST] [156.953 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:25:18.845
    Jan 31 00:25:18.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename var-expansion 01/31/23 00:25:18.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:25:18.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:25:18.949
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/31/23 00:25:18.97
    Jan 31 00:25:19.012: INFO: Waiting up to 2m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285" to be "running"
    Jan 31 00:25:19.046: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 33.91995ms
    Jan 31 00:25:21.076: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064110146s
    Jan 31 00:25:23.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053710427s
    Jan 31 00:25:25.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056318879s
    Jan 31 00:25:27.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052114839s
    Jan 31 00:25:29.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052438379s
    Jan 31 00:25:31.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053904731s
    Jan 31 00:25:33.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 14.053937991s
    Jan 31 00:25:35.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 16.053454317s
    Jan 31 00:25:37.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 18.055146847s
    Jan 31 00:25:39.077: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 20.064823733s
    Jan 31 00:25:41.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 22.053026485s
    Jan 31 00:25:43.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 24.05420954s
    Jan 31 00:25:45.081: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 26.069067743s
    Jan 31 00:25:47.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 28.052423692s
    Jan 31 00:25:49.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 30.053552627s
    Jan 31 00:25:51.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 32.051871754s
    Jan 31 00:25:53.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 34.052855353s
    Jan 31 00:25:55.096: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 36.084280484s
    Jan 31 00:25:57.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 38.053706711s
    Jan 31 00:25:59.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 40.051899107s
    Jan 31 00:26:01.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 42.053590084s
    Jan 31 00:26:03.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 44.053206142s
    Jan 31 00:26:05.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 46.052273911s
    Jan 31 00:26:07.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 48.053441901s
    Jan 31 00:26:09.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 50.055002476s
    Jan 31 00:26:11.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 52.054201336s
    Jan 31 00:26:13.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 54.053886456s
    Jan 31 00:26:15.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 56.052543364s
    Jan 31 00:26:17.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 58.053154124s
    Jan 31 00:26:19.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.054079743s
    Jan 31 00:26:21.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.05331399s
    Jan 31 00:26:23.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.054161818s
    Jan 31 00:26:25.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.053731056s
    Jan 31 00:26:27.070: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.058272489s
    Jan 31 00:26:29.095: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.083214236s
    Jan 31 00:26:31.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.055146068s
    Jan 31 00:26:33.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.055897234s
    Jan 31 00:26:35.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.055805799s
    Jan 31 00:26:37.072: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.060042448s
    Jan 31 00:26:39.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.053572492s
    Jan 31 00:26:41.073: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.060657737s
    Jan 31 00:26:43.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.054880832s
    Jan 31 00:26:45.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.05197771s
    Jan 31 00:26:47.069: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.057050507s
    Jan 31 00:26:49.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.055106132s
    Jan 31 00:26:51.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.05584406s
    Jan 31 00:26:53.070: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.057788868s
    Jan 31 00:26:55.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.053949849s
    Jan 31 00:26:57.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.051664406s
    Jan 31 00:26:59.069: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.057173352s
    Jan 31 00:27:01.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.052215875s
    Jan 31 00:27:03.068: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.056033879s
    Jan 31 00:27:05.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.052735707s
    Jan 31 00:27:07.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.054449179s
    Jan 31 00:27:09.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.053735897s
    Jan 31 00:27:11.067: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.054456752s
    Jan 31 00:27:13.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.053880952s
    Jan 31 00:27:15.065: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.053309786s
    Jan 31 00:27:17.064: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.051666351s
    Jan 31 00:27:19.066: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.053802642s
    Jan 31 00:27:19.080: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.067914556s
    STEP: updating the pod 01/31/23 00:27:19.08
    Jan 31 00:27:19.628: INFO: Successfully updated pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456"
    STEP: waiting for pod running 01/31/23 00:27:19.628
    Jan 31 00:27:19.628: INFO: Waiting up to 2m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285" to be "running"
    Jan 31 00:27:19.649: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Pending", Reason="", readiness=false. Elapsed: 20.752969ms
    Jan 31 00:27:21.672: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456": Phase="Running", Reason="", readiness=true. Elapsed: 2.043472095s
    Jan 31 00:27:21.672: INFO: Pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" satisfied condition "running"
    STEP: deleting the pod gracefully 01/31/23 00:27:21.672
    Jan 31 00:27:21.672: INFO: Deleting pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" in namespace "var-expansion-285"
    Jan 31 00:27:21.706: INFO: Wait up to 5m0s for pod "var-expansion-ba54fc25-de62-483e-a4dd-5efc2a302456" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:27:55.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-285" for this suite. 01/31/23 00:27:55.767
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:27:55.805
Jan 31 00:27:55.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename downward-api 01/31/23 00:27:55.808
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:27:55.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:27:55.914
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/31/23 00:27:55.929
Jan 31 00:27:55.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac" in namespace "downward-api-3279" to be "Succeeded or Failed"
Jan 31 00:27:55.990: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Pending", Reason="", readiness=false. Elapsed: 22.053033ms
Jan 31 00:27:58.007: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Running", Reason="", readiness=true. Elapsed: 2.039037592s
Jan 31 00:28:00.025: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Running", Reason="", readiness=false. Elapsed: 4.056884973s
Jan 31 00:28:02.007: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039033779s
STEP: Saw pod success 01/31/23 00:28:02.007
Jan 31 00:28:02.008: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac" satisfied condition "Succeeded or Failed"
Jan 31 00:28:02.026: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac container client-container: <nil>
STEP: delete the pod 01/31/23 00:28:02.146
Jan 31 00:28:02.233: INFO: Waiting for pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac to disappear
Jan 31 00:28:02.254: INFO: Pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 31 00:28:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3279" for this suite. 01/31/23 00:28:02.277
------------------------------
• [SLOW TEST] [6.540 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:27:55.805
    Jan 31 00:27:55.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename downward-api 01/31/23 00:27:55.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:27:55.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:27:55.914
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/31/23 00:27:55.929
    Jan 31 00:27:55.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac" in namespace "downward-api-3279" to be "Succeeded or Failed"
    Jan 31 00:27:55.990: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Pending", Reason="", readiness=false. Elapsed: 22.053033ms
    Jan 31 00:27:58.007: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Running", Reason="", readiness=true. Elapsed: 2.039037592s
    Jan 31 00:28:00.025: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Running", Reason="", readiness=false. Elapsed: 4.056884973s
    Jan 31 00:28:02.007: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039033779s
    STEP: Saw pod success 01/31/23 00:28:02.007
    Jan 31 00:28:02.008: INFO: Pod "downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac" satisfied condition "Succeeded or Failed"
    Jan 31 00:28:02.026: INFO: Trying to get logs from node 10.15.28.227 pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac container client-container: <nil>
    STEP: delete the pod 01/31/23 00:28:02.146
    Jan 31 00:28:02.233: INFO: Waiting for pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac to disappear
    Jan 31 00:28:02.254: INFO: Pod downwardapi-volume-fd5816da-ba65-4a19-a376-1a4b63899aac no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:28:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3279" for this suite. 01/31/23 00:28:02.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:28:02.347
Jan 31 00:28:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename pods 01/31/23 00:28:02.35
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:02.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:02.45
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/31/23 00:28:02.467
STEP: submitting the pod to kubernetes 01/31/23 00:28:02.468
STEP: verifying QOS class is set on the pod 01/31/23 00:28:02.514
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 31 00:28:02.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7438" for this suite. 01/31/23 00:28:02.57
------------------------------
• [0.274 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:28:02.347
    Jan 31 00:28:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename pods 01/31/23 00:28:02.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:02.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:02.45
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/31/23 00:28:02.467
    STEP: submitting the pod to kubernetes 01/31/23 00:28:02.468
    STEP: verifying QOS class is set on the pod 01/31/23 00:28:02.514
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:28:02.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7438" for this suite. 01/31/23 00:28:02.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:28:02.629
Jan 31 00:28:02.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:28:02.631
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:02.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:02.718
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/31/23 00:28:02.734
Jan 31 00:28:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
Jan 31 00:28:05.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:28:14.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8075" for this suite. 01/31/23 00:28:14.617
------------------------------
• [SLOW TEST] [12.036 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:28:02.629
    Jan 31 00:28:02.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename crd-publish-openapi 01/31/23 00:28:02.631
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:02.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:02.718
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/31/23 00:28:02.734
    Jan 31 00:28:02.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    Jan 31 00:28:05.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:28:14.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8075" for this suite. 01/31/23 00:28:14.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:28:14.678
Jan 31 00:28:14.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename emptydir 01/31/23 00:28:14.68
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:14.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:14.768
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/31/23 00:28:14.785
Jan 31 00:28:14.820: INFO: Waiting up to 5m0s for pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641" in namespace "emptydir-1477" to be "Succeeded or Failed"
Jan 31 00:28:14.840: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Pending", Reason="", readiness=false. Elapsed: 20.374421ms
Jan 31 00:28:16.860: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040859403s
Jan 31 00:28:18.870: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050669403s
STEP: Saw pod success 01/31/23 00:28:18.87
Jan 31 00:28:18.872: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641" satisfied condition "Succeeded or Failed"
Jan 31 00:28:18.890: INFO: Trying to get logs from node 10.15.28.227 pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 container test-container: <nil>
STEP: delete the pod 01/31/23 00:28:19.031
Jan 31 00:28:19.107: INFO: Waiting for pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 to disappear
Jan 31 00:28:19.126: INFO: Pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 31 00:28:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1477" for this suite. 01/31/23 00:28:19.15
------------------------------
• [4.508 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:28:14.678
    Jan 31 00:28:14.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename emptydir 01/31/23 00:28:14.68
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:14.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:14.768
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/31/23 00:28:14.785
    Jan 31 00:28:14.820: INFO: Waiting up to 5m0s for pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641" in namespace "emptydir-1477" to be "Succeeded or Failed"
    Jan 31 00:28:14.840: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Pending", Reason="", readiness=false. Elapsed: 20.374421ms
    Jan 31 00:28:16.860: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040859403s
    Jan 31 00:28:18.870: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050669403s
    STEP: Saw pod success 01/31/23 00:28:18.87
    Jan 31 00:28:18.872: INFO: Pod "pod-b9ad9bc4-189d-4b7e-a187-af78702dc641" satisfied condition "Succeeded or Failed"
    Jan 31 00:28:18.890: INFO: Trying to get logs from node 10.15.28.227 pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 container test-container: <nil>
    STEP: delete the pod 01/31/23 00:28:19.031
    Jan 31 00:28:19.107: INFO: Waiting for pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 to disappear
    Jan 31 00:28:19.126: INFO: Pod pod-b9ad9bc4-189d-4b7e-a187-af78702dc641 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:28:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1477" for this suite. 01/31/23 00:28:19.15
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:28:19.187
Jan 31 00:28:19.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename sched-pred 01/31/23 00:28:19.19
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:19.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:19.281
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 31 00:28:19.301: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 31 00:28:19.344: INFO: Waiting for terminating namespaces to be deleted...
Jan 31 00:28:19.368: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.225 before test
Jan 31 00:28:19.446: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.446: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 31 00:28:19.447: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.447: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 00:28:19.447: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.447: INFO: 	Container calico-typha ready: true, restart count 0
Jan 31 00:28:19.447: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.447: INFO: 	Container coredns ready: true, restart count 0
Jan 31 00:28:19.447: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.447: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 31 00:28:19.447: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.447: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 31 00:28:19.447: INFO: 	Container pause ready: true, restart count 0
Jan 31 00:28:19.447: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 31 00:28:19.448: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 31 00:28:19.448: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container config-watcher ready: true, restart count 0
Jan 31 00:28:19.448: INFO: 	Container metrics-server ready: true, restart count 0
Jan 31 00:28:19.448: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 31 00:28:19.448: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 31 00:28:19.448: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container e2e ready: true, restart count 0
Jan 31 00:28:19.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 00:28:19.448: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 00:28:19.448: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 00:28:19.448: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.227 before test
Jan 31 00:28:19.504: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 00:28:19.504: INFO: calico-typha-5fcb7c495f-hph5t from kube-system started at 2023-01-30 23:42:51 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container calico-typha ready: true, restart count 0
Jan 31 00:28:19.504: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 31 00:28:19.504: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 31 00:28:19.504: INFO: 	Container pause ready: true, restart count 0
Jan 31 00:28:19.504: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 31 00:28:19.504: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.504: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 31 00:28:19.504: INFO: pod-qos-class-ea05671f-6c79-4046-9c70-d76bf22882b5 from pods-7438 started at 2023-01-31 00:28:02 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.505: INFO: 	Container agnhost ready: false, restart count 0
Jan 31 00:28:19.505: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.505: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 31 00:28:19.505: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 00:28:19.505: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 31 00:28:19.505: INFO: 
Logging pods the apiserver thinks is on node 10.15.28.237 before test
Jan 31 00:28:19.573: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.573: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
Jan 31 00:28:19.573: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.574: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 31 00:28:19.574: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.574: INFO: 	Container calico-node ready: true, restart count 0
Jan 31 00:28:19.574: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.574: INFO: 	Container calico-typha ready: true, restart count 0
Jan 31 00:28:19.574: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.574: INFO: 	Container coredns ready: true, restart count 0
Jan 31 00:28:19.574: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.574: INFO: 	Container coredns ready: true, restart count 0
Jan 31 00:28:19.575: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container autoscaler ready: true, restart count 0
Jan 31 00:28:19.575: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 31 00:28:19.575: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 31 00:28:19.575: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 31 00:28:19.575: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 31 00:28:19.575: INFO: 	Container pause ready: true, restart count 0
Jan 31 00:28:19.575: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.575: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 31 00:28:19.576: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 31 00:28:19.576: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 31 00:28:19.576: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 31 00:28:19.576: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 31 00:28:19.576: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 31 00:28:19.576: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
Jan 31 00:28:19.576: INFO: 	Container config-watcher ready: true, restart count 0
Jan 31 00:28:19.576: INFO: 	Container metrics-server ready: true, restart count 0
Jan 31 00:28:19.576: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 31 00:28:19.577: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.577: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
Jan 31 00:28:19.577: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
Jan 31 00:28:19.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 31 00:28:19.577: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/31/23 00:28:19.578
Jan 31 00:28:19.612: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4538" to be "running"
Jan 31 00:28:19.637: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.361861ms
Jan 31 00:28:21.657: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044317308s
Jan 31 00:28:23.656: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.043378443s
Jan 31 00:28:23.656: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/31/23 00:28:23.676
STEP: Trying to apply a random label on the found node. 01/31/23 00:28:23.749
STEP: verifying the node has the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 95 01/31/23 00:28:23.788
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/31/23 00:28:23.816
Jan 31 00:28:23.856: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4538" to be "not pending"
Jan 31 00:28:23.879: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.714813ms
Jan 31 00:28:25.900: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043770203s
Jan 31 00:28:27.900: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.044257295s
Jan 31 00:28:27.900: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.15.28.227 on the node which pod4 resides and expect not scheduled 01/31/23 00:28:27.9
Jan 31 00:28:27.945: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4538" to be "not pending"
Jan 31 00:28:27.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.172029ms
Jan 31 00:28:29.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038028809s
Jan 31 00:28:31.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036335211s
Jan 31 00:28:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051867225s
Jan 31 00:28:35.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040361211s
Jan 31 00:28:37.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036345139s
Jan 31 00:28:39.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0413723s
Jan 31 00:28:41.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.038161343s
Jan 31 00:28:44.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.071211508s
Jan 31 00:28:45.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036860872s
Jan 31 00:28:47.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038799823s
Jan 31 00:28:50.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.064259426s
Jan 31 00:28:51.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039056921s
Jan 31 00:28:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.037336717s
Jan 31 00:28:55.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.036885122s
Jan 31 00:28:57.994: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.048510275s
Jan 31 00:28:59.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.038825183s
Jan 31 00:29:01.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.041140635s
Jan 31 00:29:03.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.038442771s
Jan 31 00:29:05.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.042318725s
Jan 31 00:29:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.037371677s
Jan 31 00:29:09.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.041357558s
Jan 31 00:29:11.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.039466649s
Jan 31 00:29:13.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.043581865s
Jan 31 00:29:15.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.035384738s
Jan 31 00:29:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.039019092s
Jan 31 00:29:19.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.039194257s
Jan 31 00:29:21.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.043008943s
Jan 31 00:29:24.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.063846074s
Jan 31 00:29:25.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.037473049s
Jan 31 00:29:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.038328896s
Jan 31 00:29:29.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.039147351s
Jan 31 00:29:31.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.038450421s
Jan 31 00:29:33.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042922642s
Jan 31 00:29:35.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.036061537s
Jan 31 00:29:37.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.041760777s
Jan 31 00:29:39.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.037604721s
Jan 31 00:29:42.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.061475592s
Jan 31 00:29:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.085510107s
Jan 31 00:29:45.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.04380604s
Jan 31 00:29:47.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039683357s
Jan 31 00:29:49.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.035755623s
Jan 31 00:29:51.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.036274339s
Jan 31 00:29:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038186099s
Jan 31 00:29:55.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.03835517s
Jan 31 00:29:58.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.069903331s
Jan 31 00:29:59.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.038913897s
Jan 31 00:30:01.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.040285857s
Jan 31 00:30:03.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.037626191s
Jan 31 00:30:05.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.035523331s
Jan 31 00:30:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.038310848s
Jan 31 00:30:09.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.03975793s
Jan 31 00:30:11.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.037374285s
Jan 31 00:30:14.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.070454641s
Jan 31 00:30:15.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.043638268s
Jan 31 00:30:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.039086109s
Jan 31 00:30:19.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.03749266s
Jan 31 00:30:21.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.036567901s
Jan 31 00:30:23.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.037547349s
Jan 31 00:30:25.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036185223s
Jan 31 00:30:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.039010828s
Jan 31 00:30:29.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.040060097s
Jan 31 00:30:31.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.037364439s
Jan 31 00:30:33.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.042851664s
Jan 31 00:30:35.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.036697913s
Jan 31 00:30:37.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.037901889s
Jan 31 00:30:39.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.03840566s
Jan 31 00:30:41.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.040212601s
Jan 31 00:30:43.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.042669174s
Jan 31 00:30:45.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.037799762s
Jan 31 00:30:47.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.037599011s
Jan 31 00:30:49.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.037932436s
Jan 31 00:30:51.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.040722533s
Jan 31 00:30:53.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.040636986s
Jan 31 00:30:55.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.039868727s
Jan 31 00:30:57.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.039323281s
Jan 31 00:30:59.991: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.046083246s
Jan 31 00:31:01.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.037993928s
Jan 31 00:31:03.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.041500464s
Jan 31 00:31:05.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.03685985s
Jan 31 00:31:07.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.036224468s
Jan 31 00:31:09.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.038151947s
Jan 31 00:31:11.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.037126313s
Jan 31 00:31:13.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.037014605s
Jan 31 00:31:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.038521609s
Jan 31 00:31:17.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.039955398s
Jan 31 00:31:19.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.039278631s
Jan 31 00:31:21.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.03838993s
Jan 31 00:31:23.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.038178756s
Jan 31 00:31:25.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.039357085s
Jan 31 00:31:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.038695771s
Jan 31 00:31:29.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.036451376s
Jan 31 00:31:31.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.038546397s
Jan 31 00:31:33.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.037850782s
Jan 31 00:31:35.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.039722229s
Jan 31 00:31:37.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.043562493s
Jan 31 00:31:39.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.038308417s
Jan 31 00:31:41.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.038044277s
Jan 31 00:31:43.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.050866279s
Jan 31 00:31:45.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.038187498s
Jan 31 00:31:47.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.038464185s
Jan 31 00:31:49.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.036735101s
Jan 31 00:31:51.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.04253008s
Jan 31 00:31:54.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.057640234s
Jan 31 00:31:55.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.036431237s
Jan 31 00:31:58.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.063304202s
Jan 31 00:31:59.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.035614736s
Jan 31 00:32:01.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.037210783s
Jan 31 00:32:03.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.036372683s
Jan 31 00:32:05.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.038020275s
Jan 31 00:32:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.038220974s
Jan 31 00:32:09.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.041080746s
Jan 31 00:32:11.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.037545188s
Jan 31 00:32:13.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.037484265s
Jan 31 00:32:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.039238277s
Jan 31 00:32:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.038415522s
Jan 31 00:32:19.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.037438849s
Jan 31 00:32:21.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.038151673s
Jan 31 00:32:23.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.039211075s
Jan 31 00:32:25.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.038571519s
Jan 31 00:32:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.038824577s
Jan 31 00:32:29.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.039661483s
Jan 31 00:32:31.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.036425595s
Jan 31 00:32:33.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.040199544s
Jan 31 00:32:35.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.038564169s
Jan 31 00:32:37.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.037560255s
Jan 31 00:32:39.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.03704127s
Jan 31 00:32:42.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.054773255s
Jan 31 00:32:43.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.03731252s
Jan 31 00:32:45.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.036822456s
Jan 31 00:32:47.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.039637059s
Jan 31 00:32:49.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.040540517s
Jan 31 00:32:51.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.040899473s
Jan 31 00:32:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.037924s
Jan 31 00:32:55.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.04065227s
Jan 31 00:32:57.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.036858744s
Jan 31 00:32:59.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.038172997s
Jan 31 00:33:01.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.037368352s
Jan 31 00:33:03.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.039556762s
Jan 31 00:33:05.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.039888955s
Jan 31 00:33:07.990: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.045098592s
Jan 31 00:33:10.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.062066291s
Jan 31 00:33:11.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.038433647s
Jan 31 00:33:13.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.038454036s
Jan 31 00:33:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.038377178s
Jan 31 00:33:17.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.041572295s
Jan 31 00:33:19.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.042383029s
Jan 31 00:33:21.990: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.045177733s
Jan 31 00:33:23.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.042181613s
Jan 31 00:33:25.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.037988145s
Jan 31 00:33:27.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.037372903s
Jan 31 00:33:28.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.055936963s
STEP: removing the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 off the node 10.15.28.227 01/31/23 00:33:28.001
STEP: verifying the node doesn't have the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 01/31/23 00:33:28.059
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:33:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4538" for this suite. 01/31/23 00:33:28.095
------------------------------
• [SLOW TEST] [308.942 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:28:19.187
    Jan 31 00:28:19.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename sched-pred 01/31/23 00:28:19.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:28:19.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:28:19.281
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 31 00:28:19.301: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 31 00:28:19.344: INFO: Waiting for terminating namespaces to be deleted...
    Jan 31 00:28:19.368: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.225 before test
    Jan 31 00:28:19.446: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-4pfxd from ibm-system started at 2023-01-30 21:03:47 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.446: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: calico-node-sgm4f from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.447: INFO: 	Container calico-node ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: calico-typha-5fcb7c495f-67gfv from kube-system started at 2023-01-30 20:33:17 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.447: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: coredns-56697bd765-q4pqr from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.447: INFO: 	Container coredns ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: ibm-keepalived-watcher-6j488 from kube-system started at 2023-01-30 20:32:33 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.447: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: ibm-master-proxy-static-10.15.28.225 from kube-system started at 2023-01-30 20:32:21 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.447: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: 	Container pause ready: true, restart count 0
    Jan 31 00:28:19.447: INFO: ibmcloud-block-storage-driver-rvhkt from kube-system started at 2023-01-30 20:32:41 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: konnectivity-agent-76wnq from kube-system started at 2023-01-30 20:48:14 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: metrics-server-5c45845f46-6mj46 from kube-system started at 2023-01-30 21:22:23 +0000 UTC (3 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-m4q8r from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: sonobuoy-e2e-job-1fdfddcee1544467 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container e2e ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-jj96q from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 31 00:28:19.448: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.227 before test
    Jan 31 00:28:19.504: INFO: calico-node-sqmvl from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container calico-node ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: calico-typha-5fcb7c495f-hph5t from kube-system started at 2023-01-30 23:42:51 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: ibm-keepalived-watcher-whvb4 from kube-system started at 2023-01-30 20:39:15 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: ibm-master-proxy-static-10.15.28.227 from kube-system started at 2023-01-30 20:38:59 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: 	Container pause ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: ibmcloud-block-storage-driver-6kbjz from kube-system started at 2023-01-30 20:39:19 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: konnectivity-agent-vvjd9 from kube-system started at 2023-01-30 20:48:18 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.504: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 31 00:28:19.504: INFO: pod-qos-class-ea05671f-6c79-4046-9c70-d76bf22882b5 from pods-7438 started at 2023-01-31 00:28:02 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.505: INFO: 	Container agnhost ready: false, restart count 0
    Jan 31 00:28:19.505: INFO: sonobuoy from sonobuoy started at 2023-01-30 22:43:31 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.505: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 31 00:28:19.505: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-qwp67 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 31 00:28:19.505: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 31 00:28:19.505: INFO: 
    Logging pods the apiserver thinks is on node 10.15.28.237 before test
    Jan 31 00:28:19.573: INFO: ibm-cloud-provider-ip-163-109-71-90-7c8d587f66-b8m7z from ibm-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.573: INFO: 	Container ibm-cloud-provider-ip-163-109-71-90 ready: true, restart count 0
    Jan 31 00:28:19.573: INFO: calico-kube-controllers-5ddbd89486-xb4dr from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.574: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 31 00:28:19.574: INFO: calico-node-qbh88 from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.574: INFO: 	Container calico-node ready: true, restart count 0
    Jan 31 00:28:19.574: INFO: calico-typha-5fcb7c495f-7clq5 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.574: INFO: 	Container calico-typha ready: true, restart count 0
    Jan 31 00:28:19.574: INFO: coredns-56697bd765-fzqcs from kube-system started at 2023-01-30 23:15:25 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.574: INFO: 	Container coredns ready: true, restart count 0
    Jan 31 00:28:19.574: INFO: coredns-56697bd765-rnndc from kube-system started at 2023-01-30 20:48:50 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.574: INFO: 	Container coredns ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: coredns-autoscaler-57c58584b6-sn4w2 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container autoscaler ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: dashboard-metrics-scraper-67f9957b6-q4kjj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: ibm-file-plugin-855c994c98-xsp27 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: ibm-keepalived-watcher-5787n from kube-system started at 2023-01-30 20:32:35 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: ibm-master-proxy-static-10.15.28.237 from kube-system started at 2023-01-30 20:32:23 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: 	Container pause ready: true, restart count 0
    Jan 31 00:28:19.575: INFO: ibm-storage-watcher-6b8f8bd5f7-jprpf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.575: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: ibmcloud-block-storage-driver-82h8n from kube-system started at 2023-01-30 20:32:44 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: ibmcloud-block-storage-plugin-8ddcf7ccb-5klq8 from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: ingress-cluster-healthcheck-bbddc799d-w79wj from kube-system started at 2023-01-30 23:15:24 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: konnectivity-agent-k7vgh from kube-system started at 2023-01-30 20:48:21 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: kubernetes-dashboard-58dffc9764-j7lxj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: metrics-server-5c45845f46-pmpxp from kube-system started at 2023-01-30 23:15:25 +0000 UTC (3 container statuses recorded)
    Jan 31 00:28:19.576: INFO: 	Container config-watcher ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 31 00:28:19.576: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: public-crcfc28jfz0g89phhsur6g-alb1-c68b8458d-wx6wh from kube-system started at 2023-01-30 20:59:39 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.577: INFO: 	Container nginx-ingress ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-bkzdf from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-g657k from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: snapshot-controller-6c8c86697-tlrsj from kube-system started at 2023-01-30 20:33:07 +0000 UTC (1 container statuses recorded)
    Jan 31 00:28:19.577: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: sonobuoy-systemd-logs-daemon-set-cf0cd52a9989410d-xqxz5 from sonobuoy started at 2023-01-30 22:43:37 +0000 UTC (2 container statuses recorded)
    Jan 31 00:28:19.577: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 31 00:28:19.577: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/31/23 00:28:19.578
    Jan 31 00:28:19.612: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4538" to be "running"
    Jan 31 00:28:19.637: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 24.361861ms
    Jan 31 00:28:21.657: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044317308s
    Jan 31 00:28:23.656: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.043378443s
    Jan 31 00:28:23.656: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/31/23 00:28:23.676
    STEP: Trying to apply a random label on the found node. 01/31/23 00:28:23.749
    STEP: verifying the node has the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 95 01/31/23 00:28:23.788
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/31/23 00:28:23.816
    Jan 31 00:28:23.856: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4538" to be "not pending"
    Jan 31 00:28:23.879: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.714813ms
    Jan 31 00:28:25.900: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043770203s
    Jan 31 00:28:27.900: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.044257295s
    Jan 31 00:28:27.900: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.15.28.227 on the node which pod4 resides and expect not scheduled 01/31/23 00:28:27.9
    Jan 31 00:28:27.945: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4538" to be "not pending"
    Jan 31 00:28:27.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.172029ms
    Jan 31 00:28:29.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038028809s
    Jan 31 00:28:31.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036335211s
    Jan 31 00:28:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051867225s
    Jan 31 00:28:35.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040361211s
    Jan 31 00:28:37.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.036345139s
    Jan 31 00:28:39.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0413723s
    Jan 31 00:28:41.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.038161343s
    Jan 31 00:28:44.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.071211508s
    Jan 31 00:28:45.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036860872s
    Jan 31 00:28:47.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038799823s
    Jan 31 00:28:50.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.064259426s
    Jan 31 00:28:51.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.039056921s
    Jan 31 00:28:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.037336717s
    Jan 31 00:28:55.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.036885122s
    Jan 31 00:28:57.994: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.048510275s
    Jan 31 00:28:59.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.038825183s
    Jan 31 00:29:01.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.041140635s
    Jan 31 00:29:03.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.038442771s
    Jan 31 00:29:05.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.042318725s
    Jan 31 00:29:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.037371677s
    Jan 31 00:29:09.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.041357558s
    Jan 31 00:29:11.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.039466649s
    Jan 31 00:29:13.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.043581865s
    Jan 31 00:29:15.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.035384738s
    Jan 31 00:29:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.039019092s
    Jan 31 00:29:19.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.039194257s
    Jan 31 00:29:21.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.043008943s
    Jan 31 00:29:24.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.063846074s
    Jan 31 00:29:25.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.037473049s
    Jan 31 00:29:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.038328896s
    Jan 31 00:29:29.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.039147351s
    Jan 31 00:29:31.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.038450421s
    Jan 31 00:29:33.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.042922642s
    Jan 31 00:29:35.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.036061537s
    Jan 31 00:29:37.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.041760777s
    Jan 31 00:29:39.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.037604721s
    Jan 31 00:29:42.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.061475592s
    Jan 31 00:29:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.085510107s
    Jan 31 00:29:45.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.04380604s
    Jan 31 00:29:47.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039683357s
    Jan 31 00:29:49.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.035755623s
    Jan 31 00:29:51.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.036274339s
    Jan 31 00:29:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.038186099s
    Jan 31 00:29:55.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.03835517s
    Jan 31 00:29:58.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.069903331s
    Jan 31 00:29:59.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.038913897s
    Jan 31 00:30:01.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.040285857s
    Jan 31 00:30:03.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.037626191s
    Jan 31 00:30:05.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.035523331s
    Jan 31 00:30:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.038310848s
    Jan 31 00:30:09.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.03975793s
    Jan 31 00:30:11.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.037374285s
    Jan 31 00:30:14.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.070454641s
    Jan 31 00:30:15.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.043638268s
    Jan 31 00:30:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.039086109s
    Jan 31 00:30:19.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.03749266s
    Jan 31 00:30:21.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.036567901s
    Jan 31 00:30:23.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.037547349s
    Jan 31 00:30:25.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.036185223s
    Jan 31 00:30:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.039010828s
    Jan 31 00:30:29.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.040060097s
    Jan 31 00:30:31.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.037364439s
    Jan 31 00:30:33.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.042851664s
    Jan 31 00:30:35.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.036697913s
    Jan 31 00:30:37.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.037901889s
    Jan 31 00:30:39.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.03840566s
    Jan 31 00:30:41.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.040212601s
    Jan 31 00:30:43.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.042669174s
    Jan 31 00:30:45.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.037799762s
    Jan 31 00:30:47.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.037599011s
    Jan 31 00:30:49.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.037932436s
    Jan 31 00:30:51.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.040722533s
    Jan 31 00:30:53.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.040636986s
    Jan 31 00:30:55.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.039868727s
    Jan 31 00:30:57.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.039323281s
    Jan 31 00:30:59.991: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.046083246s
    Jan 31 00:31:01.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.037993928s
    Jan 31 00:31:03.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.041500464s
    Jan 31 00:31:05.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.03685985s
    Jan 31 00:31:07.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.036224468s
    Jan 31 00:31:09.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.038151947s
    Jan 31 00:31:11.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.037126313s
    Jan 31 00:31:13.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.037014605s
    Jan 31 00:31:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.038521609s
    Jan 31 00:31:17.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.039955398s
    Jan 31 00:31:19.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.039278631s
    Jan 31 00:31:21.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.03838993s
    Jan 31 00:31:23.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.038178756s
    Jan 31 00:31:25.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.039357085s
    Jan 31 00:31:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.038695771s
    Jan 31 00:31:29.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.036451376s
    Jan 31 00:31:31.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.038546397s
    Jan 31 00:31:33.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.037850782s
    Jan 31 00:31:35.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.039722229s
    Jan 31 00:31:37.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.043562493s
    Jan 31 00:31:39.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.038308417s
    Jan 31 00:31:41.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.038044277s
    Jan 31 00:31:43.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.050866279s
    Jan 31 00:31:45.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.038187498s
    Jan 31 00:31:47.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.038464185s
    Jan 31 00:31:49.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.036735101s
    Jan 31 00:31:51.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.04253008s
    Jan 31 00:31:54.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.057640234s
    Jan 31 00:31:55.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.036431237s
    Jan 31 00:31:58.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.063304202s
    Jan 31 00:31:59.981: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.035614736s
    Jan 31 00:32:01.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.037210783s
    Jan 31 00:32:03.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.036372683s
    Jan 31 00:32:05.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.038020275s
    Jan 31 00:32:07.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.038220974s
    Jan 31 00:32:09.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.041080746s
    Jan 31 00:32:11.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.037545188s
    Jan 31 00:32:13.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.037484265s
    Jan 31 00:32:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.039238277s
    Jan 31 00:32:17.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.038415522s
    Jan 31 00:32:19.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.037438849s
    Jan 31 00:32:21.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.038151673s
    Jan 31 00:32:23.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.039211075s
    Jan 31 00:32:25.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.038571519s
    Jan 31 00:32:27.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.038824577s
    Jan 31 00:32:29.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.039661483s
    Jan 31 00:32:31.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.036425595s
    Jan 31 00:32:33.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.040199544s
    Jan 31 00:32:35.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.038564169s
    Jan 31 00:32:37.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.037560255s
    Jan 31 00:32:39.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.03704127s
    Jan 31 00:32:42.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.054773255s
    Jan 31 00:32:43.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.03731252s
    Jan 31 00:32:45.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.036822456s
    Jan 31 00:32:47.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.039637059s
    Jan 31 00:32:49.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.040540517s
    Jan 31 00:32:51.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.040899473s
    Jan 31 00:32:53.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.037924s
    Jan 31 00:32:55.986: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.04065227s
    Jan 31 00:32:57.982: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.036858744s
    Jan 31 00:32:59.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.038172997s
    Jan 31 00:33:01.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.037368352s
    Jan 31 00:33:03.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.039556762s
    Jan 31 00:33:05.985: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.039888955s
    Jan 31 00:33:07.990: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.045098592s
    Jan 31 00:33:10.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.062066291s
    Jan 31 00:33:11.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.038433647s
    Jan 31 00:33:13.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.038454036s
    Jan 31 00:33:15.984: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.038377178s
    Jan 31 00:33:17.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.041572295s
    Jan 31 00:33:19.988: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.042383029s
    Jan 31 00:33:21.990: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.045177733s
    Jan 31 00:33:23.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.042181613s
    Jan 31 00:33:25.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.037988145s
    Jan 31 00:33:27.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.037372903s
    Jan 31 00:33:28.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.055936963s
    STEP: removing the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 off the node 10.15.28.227 01/31/23 00:33:28.001
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-96da5dbf-a709-416a-8f6e-095f8de7f311 01/31/23 00:33:28.059
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:33:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4538" for this suite. 01/31/23 00:33:28.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:33:28.152
Jan 31 00:33:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename daemonsets 01/31/23 00:33:28.153
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:33:28.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:33:28.267
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 31 00:33:28.365: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/31/23 00:33:28.385
Jan 31 00:33:28.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:28.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/31/23 00:33:28.408
Jan 31 00:33:28.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:28.494: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:29.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:29.513: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:30.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:30.516: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:31.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 00:33:31.517: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/31/23 00:33:31.53
Jan 31 00:33:31.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 00:33:31.601: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 31 00:33:32.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:32.623: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/31/23 00:33:32.624
Jan 31 00:33:32.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:32.665: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:33.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:33.689: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:34.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:34.685: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:35.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:35.682: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
Jan 31 00:33:36.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 31 00:33:36.683: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/31/23 00:33:36.712
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6555, will wait for the garbage collector to delete the pods 01/31/23 00:33:36.713
Jan 31 00:33:36.800: INFO: Deleting DaemonSet.extensions daemon-set took: 20.657159ms
Jan 31 00:33:36.901: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788198ms
Jan 31 00:33:40.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 31 00:33:40.021: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 31 00:33:40.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54956"},"items":null}

Jan 31 00:33:40.053: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54956"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:33:40.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6555" for this suite. 01/31/23 00:33:40.175
------------------------------
• [SLOW TEST] [12.054 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:33:28.152
    Jan 31 00:33:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename daemonsets 01/31/23 00:33:28.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:33:28.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:33:28.267
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 31 00:33:28.365: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/31/23 00:33:28.385
    Jan 31 00:33:28.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:28.408: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/31/23 00:33:28.408
    Jan 31 00:33:28.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:28.494: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:29.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:29.513: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:30.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:30.516: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:31.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 31 00:33:31.517: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/31/23 00:33:31.53
    Jan 31 00:33:31.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 31 00:33:31.601: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 31 00:33:32.623: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:32.623: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/31/23 00:33:32.624
    Jan 31 00:33:32.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:32.665: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:33.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:33.689: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:34.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:34.685: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:35.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:35.682: INFO: Node 10.15.28.237 is running 0 daemon pod, expected 1
    Jan 31 00:33:36.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 31 00:33:36.683: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/31/23 00:33:36.712
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6555, will wait for the garbage collector to delete the pods 01/31/23 00:33:36.713
    Jan 31 00:33:36.800: INFO: Deleting DaemonSet.extensions daemon-set took: 20.657159ms
    Jan 31 00:33:36.901: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788198ms
    Jan 31 00:33:40.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 31 00:33:40.021: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 31 00:33:40.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54956"},"items":null}

    Jan 31 00:33:40.053: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54956"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:33:40.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6555" for this suite. 01/31/23 00:33:40.175
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/31/23 00:33:40.209
Jan 31 00:33:40.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
STEP: Building a namespace api object, basename webhook 01/31/23 00:33:40.213
STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:33:40.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:33:40.341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/31/23 00:33:40.42
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:33:40.977
STEP: Deploying the webhook pod 01/31/23 00:33:41.016
STEP: Wait for the deployment to be ready 01/31/23 00:33:41.061
Jan 31 00:33:41.111: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 31 00:33:43.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/31/23 00:33:45.189
STEP: Verifying the service has paired with the endpoint 01/31/23 00:33:45.253
Jan 31 00:33:46.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/31/23 00:33:46.267
STEP: create a pod 01/31/23 00:33:46.371
Jan 31 00:33:46.404: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2336" to be "running"
Jan 31 00:33:46.430: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.368016ms
Jan 31 00:33:48.450: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.046427587s
Jan 31 00:33:48.450: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/31/23 00:33:48.45
Jan 31 00:33:48.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=webhook-2336 attach --namespace=webhook-2336 to-be-attached-pod -i -c=container1'
Jan 31 00:33:48.674: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 31 00:33:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2336" for this suite. 01/31/23 00:33:48.92
STEP: Destroying namespace "webhook-2336-markers" for this suite. 01/31/23 00:33:48.952
------------------------------
• [SLOW TEST] [8.772 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/31/23 00:33:40.209
    Jan 31 00:33:40.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2325733257
    STEP: Building a namespace api object, basename webhook 01/31/23 00:33:40.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/31/23 00:33:40.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/31/23 00:33:40.341
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/31/23 00:33:40.42
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/31/23 00:33:40.977
    STEP: Deploying the webhook pod 01/31/23 00:33:41.016
    STEP: Wait for the deployment to be ready 01/31/23 00:33:41.061
    Jan 31 00:33:41.111: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 31 00:33:43.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 31, 0, 33, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/31/23 00:33:45.189
    STEP: Verifying the service has paired with the endpoint 01/31/23 00:33:45.253
    Jan 31 00:33:46.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/31/23 00:33:46.267
    STEP: create a pod 01/31/23 00:33:46.371
    Jan 31 00:33:46.404: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2336" to be "running"
    Jan 31 00:33:46.430: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.368016ms
    Jan 31 00:33:48.450: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.046427587s
    Jan 31 00:33:48.450: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/31/23 00:33:48.45
    Jan 31 00:33:48.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2325733257 --namespace=webhook-2336 attach --namespace=webhook-2336 to-be-attached-pod -i -c=container1'
    Jan 31 00:33:48.674: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 31 00:33:48.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2336" for this suite. 01/31/23 00:33:48.92
    STEP: Destroying namespace "webhook-2336-markers" for this suite. 01/31/23 00:33:48.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 31 00:33:48.986: INFO: Running AfterSuite actions on node 1
Jan 31 00:33:48.986: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 31 00:33:48.986: INFO: Running AfterSuite actions on node 1
    Jan 31 00:33:48.986: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.111 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6597.048 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h49m57.522851696s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

